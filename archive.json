{
  "magic": "E!vIA5L86J2I",
  "timestamp": "2024-08-29T01:06:22.279114+00:00",
  "repo": "ietf-wg-ppm/draft-ietf-ppm-dap",
  "labels": [
    {
      "name": "bug",
      "description": "Something isn't working",
      "color": "d73a4a"
    },
    {
      "name": "documentation",
      "description": "Improvements or additions to documentation",
      "color": "0075ca"
    },
    {
      "name": "duplicate",
      "description": "This issue or pull request already exists",
      "color": "ffffff"
    },
    {
      "name": "enhancement",
      "description": "New feature or request",
      "color": "a2eeef"
    },
    {
      "name": "good first issue",
      "description": "Good for newcomers",
      "color": "7057ff"
    },
    {
      "name": "help wanted",
      "description": "Extra attention is needed",
      "color": "008672"
    },
    {
      "name": "invalid",
      "description": "This doesn't seem right",
      "color": "e4e669"
    },
    {
      "name": "question",
      "description": "Further information is requested",
      "color": "d876e3"
    },
    {
      "name": "wontfix",
      "description": "This will not be worked on",
      "color": "ffffff"
    },
    {
      "name": "parking-lot",
      "description": "Parking lot for future discussions",
      "color": "cfd3d7"
    },
    {
      "name": "editorial",
      "description": "The issue raised is purely editorial (i.e., doesn't impact implementations).",
      "color": "C2E0C6"
    },
    {
      "name": "draft-01",
      "description": "",
      "color": "bfdadc"
    },
    {
      "name": "draft-02",
      "description": "",
      "color": "F5EF23"
    },
    {
      "name": "draft-03",
      "description": "",
      "color": "24F4EB"
    },
    {
      "name": "collecting a batch more than once",
      "description": "Issues that need to be resolved before deploying Poiplar1",
      "color": "FB74D7"
    },
    {
      "name": "differential privacy",
      "description": "",
      "color": "8709C0"
    },
    {
      "name": "draft-04",
      "description": "",
      "color": "fbca04"
    },
    {
      "name": "draft-05",
      "description": "",
      "color": "AC43FE"
    },
    {
      "name": "draft-06",
      "description": "",
      "color": "687BA4"
    },
    {
      "name": "draft-08",
      "description": "",
      "color": "F11B50"
    },
    {
      "name": "breaking",
      "description": "",
      "color": "43DB40"
    },
    {
      "name": "ietf118",
      "description": "",
      "color": "683A4A"
    },
    {
      "name": "operational considerations",
      "description": "",
      "color": "BBCB82"
    },
    {
      "name": "feature",
      "description": "",
      "color": "0D4864"
    },
    {
      "name": "ietf120",
      "description": "",
      "color": "C40CA0"
    },
    {
      "name": "wire breaking",
      "description": "",
      "color": "BF322F"
    }
  ],
  "issues": [
    {
      "number": 1,
      "id": "MDU6SXNzdWU4MTE2MjU1ODM=",
      "title": "Describe cryptographic dependencies",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/1",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood",
        "cjpatton"
      ],
      "labels": [],
      "body": "Key encapsulation mechanisms, PRGs, and a sprinkle finite fields. (We can of course elaborate on these in the actual PR.)",
      "createdAt": "2021-02-19T02:21:26Z",
      "updatedAt": "2021-02-27T03:20:25Z",
      "closedAt": "2021-02-27T03:20:25Z",
      "comments": []
    },
    {
      "number": 4,
      "id": "MDU6SXNzdWU4MTc4MDU2NjI=",
      "title": "Cross-aggregator data consistency ",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/4",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "duplicate"
      ],
      "body": "We assume that all aggregators process the share of a client's input, but what happens if one aggregator loses its share of the client input? How do we accommodate one aggregator losing its view of a share, either maliciously or accidentally? ",
      "createdAt": "2021-02-27T03:01:08Z",
      "updatedAt": "2021-12-30T17:43:04Z",
      "closedAt": "2021-12-30T17:43:04Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "[BCC+19](https://eprint.iacr.org/2019/188) consider two settings for ZKP systems on distributed data: one in which either the client or a subset of aggregators is malicious (this is the setting in which Prio was analyzed); and another in which the client may collude with a subset of aggregators. This issue corresponds to the latter setting.\r\n\r\nSecurity in this setting is addressed in Section 6.3. The basic idea seems to be to run the input-validation protocol with every subset of servers.",
          "createdAt": "2021-02-27T03:16:32Z",
          "updatedAt": "2021-02-27T03:16:32Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We addressed this problem in Prio v2 by having each aggregator evaluate proofs independently. So during intake, when an aggregator receives a data share, it extracts the proof share and transmits it to the other aggregator. At aggregation time, an aggregator will not include a data share in a sum unless it can assemble the triple of (data share, own proof share, peer proof share) and verify the proof. If it can't do that because the peer proof is unavailable, the aggregator drops the share from the sum, but still sums everything else.\r\n\r\nIn our design, the leader could be responsible for this. Clients would send encrypted data shares and proof shares to aggregators, and aggregators would then have to extract the proof share and send it to the leader. The leader could then work out the intersection of the sets of proof shares provided by all aggregators (which is the set of data that can be summed in the aggregation) and then only instruct aggregators to sum that set of shares.",
          "createdAt": "2021-03-18T23:41:59Z",
          "updatedAt": "2021-03-18T23:41:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> We addressed this problem in Prio v2 by having each aggregator evaluate proofs independently. So during intake, when an aggregator receives a data share, it extracts the proof share and transmits it to the other aggregator. At aggregation time, an aggregator will not include a data share in a sum unless it can assemble the triple of (data share, own proof share, peer proof share) and verify the proof. If it can't do that because the peer proof is unavailable, the aggregator drops the share from the sum, but still sums everything else.\r\n\r\nHmm, having all of the proof shares is a potential privacy issue, no? If I have all of the proof shares, then I can assemble the entire proof polynomial, which gives me the output of each intermediate G-gate evaluation and not just the final output. (Perhaps you mean \"verification share\"? See https://github.com/abetterinternet/prio-documents/pull/16#discussion_r600081599.)\r\n\r\n> In our design, the leader could be responsible for this. Clients would send encrypted data shares and proof shares to aggregators, and aggregators would then have to extract the proof share and send it to the leader. The leader could then work out the intersection of the sets of proof shares provided by all aggregators (which is the set of data that can be summed in the aggregation) and then only instruct aggregators to sum that set of shares.\r\n\r\nThis sounds like a good idea.\r\n",
          "createdAt": "2021-03-24T02:16:09Z",
          "updatedAt": "2021-03-24T02:16:09Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As of 2021/7/14, our answer is as follows: all aggregators need to be online for the duration of the protocol. In order to recover in case an aggregator drops, the collector could spin up multiple tasks, each with different sets of aggregators. We briefly considered formalizing this in the protocol, but decided this was too complex. (See #68.) Another protocol-level option is, potentially, threshold secret sharing (#22).",
          "createdAt": "2021-07-14T20:58:50Z",
          "updatedAt": "2021-07-14T20:58:50Z"
        }
      ]
    },
    {
      "number": 5,
      "id": "MDU6SXNzdWU4MTc4MDk4NjA=",
      "title": "Detail threat models",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/5",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "The title says it all.",
      "createdAt": "2021-02-27T03:22:04Z",
      "updatedAt": "2021-06-11T01:22:19Z",
      "closedAt": "2021-06-11T01:22:18Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "IMO this issue is closed by https://github.com/abetterinternet/prio-documents/pull/16. Eventually we'll want a more fleshed out \"Security Considerations\" section, but for now, I think we're all on the same page on this.",
          "createdAt": "2021-06-09T18:36:26Z",
          "updatedAt": "2021-06-09T18:36:26Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing per @cjpatton's comment above",
          "createdAt": "2021-06-11T01:22:18Z",
          "updatedAt": "2021-06-11T01:22:18Z"
        }
      ]
    },
    {
      "number": 7,
      "id": "MDU6SXNzdWU4MjAwNzg0MTE=",
      "title": "Compare to alternatives",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/7",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "We should probably describe applications wherein Prio is necessary (or at least desired) compared to other systems that might work, such as [ANONIZE](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.732.3675&rep=rep1&type=pdf). The original paper explains that the difference is in threat model: Prio assumes an adversary that can control the entire network, so outsourcing things to a proxy simply doesn't work. However, in practice, there may be circumstances wherein there *are* trusted proxies. In such cases, whether or not one actually wants to use them seems to depend on the application and data being collected. Specifically, if privacy depends on a collector not seeing individual client inputs, then Prio fits the bill, and if not, then perhaps simpler systems would work. It would probably be good to discuss this in more detail, if only to further motivate Prio.",
      "createdAt": "2021-03-02T14:37:05Z",
      "updatedAt": "2023-09-20T15:05:10Z",
      "closedAt": "2023-09-20T15:05:10Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Some alternatives worth considering:\r\n* Use an [OHTTP](https://datatracker.ietf.org/doc/draft-thomson-http-oblivious/) proxy to strip personally identifiable metadata, like client IP, from reports. The downside is that the collector still sees the set of inputs (and not just the output, as in Prio or, to a lesser degree, HH).\r\n* A pure differential privacy approach, like [RAPPOR](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf). The upside is that the communication model is much simpler, since RAPPOR doesn't require multiple aggregators. The downside is the privacy budget.",
          "createdAt": "2021-06-09T18:44:00Z",
          "updatedAt": "2021-06-09T18:44:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This still seems useful, but probably doesn't belong in the DAP spec. Closing for now... if someone wants to propose text, please feel free.",
          "createdAt": "2023-09-20T15:05:10Z",
          "updatedAt": "2023-09-20T15:05:10Z"
        }
      ]
    },
    {
      "number": 8,
      "id": "MDU6SXNzdWU4MjAwODY5ODY=",
      "title": "Consensus protocol",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/8",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We currently have an open issue for tracking the consensus protocol needed for this system to work. In working through this text, we should make note of what sort of synchronization we assume between different participants of the system.",
      "createdAt": "2021-03-02T14:46:50Z",
      "updatedAt": "2021-07-14T20:54:49Z",
      "closedAt": "2021-07-14T20:54:49Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "In the 3/17 design call we agreed that for purposes of the design doc and probably the initial deployments of Prio, we would assume that shared parameters (e.g., arithmetic circuits, primes, aggregation identifiers) would be exchanged between all parties (i.e., clients, aggregators, collector, leader) before the start of the protocol over some unspecified secure channel.\r\n\r\nHowever, eventually, it should be possible for a client that wishes to use Prio to programmatically:\r\n\r\n- discover participating servers (i.e., a directory of aggregators)\r\n- discover the capabilities of participating servers (i.e., what algorithms do they support for secret sharing or validity proofs or aggregations? What optional protocol features do they support?)\r\n- configure a Prio aggregation (i.e., select some number of aggregation servers, getting a unique handle, configuring field size, etc.)",
          "createdAt": "2021-03-18T23:50:31Z",
          "updatedAt": "2021-03-18T23:50:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "One thing that's still TBD is how to choose the joint randomness. It could be as simple as having the leader choose a seed for a PRG and distributing it to the aggregators over secure channels. It could be as complicated as querying [DRAND](https://drand.love/) at some point after the shares are received by the aggregators. ",
          "createdAt": "2021-03-24T02:10:47Z",
          "updatedAt": "2021-03-24T02:10:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "To close the loop on this issue: the PDA parameters are distributed to all entities out-of-band.",
          "createdAt": "2021-07-14T20:54:49Z",
          "updatedAt": "2021-07-14T20:54:49Z"
        }
      ]
    },
    {
      "number": 9,
      "id": "MDU6SXNzdWU4MjAyNzU1NDA=",
      "title": "Aggregator discovery",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/9",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Clients need to obtain aggregator public keys to encrypt shares. While we may not want to specify the exact mechanism by which deployments do this, we should minimally state the requirements for discovery, perhaps with a RECOMMENDED version.\r\n\r\ncc @ekr",
      "createdAt": "2021-03-02T18:29:30Z",
      "updatedAt": "2021-06-09T18:49:04Z",
      "closedAt": "2021-06-09T18:49:04Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Some quick thoughts: \r\n\r\n1. Thus far we have talked about clients discovering and trusting aggregators. There is a related problem of aggregators discovering each other (e.g. in order to exchange validations, or just for each aggregator to discover the leader), which can be probably be solved the same way.\r\n2. We didn't do an amazing job of client<->aggregator trust in Prio v2 (mostly because no matter how you sliced it, we had to trust that Apple and Google's client implementations weren't just sending data to Cupertino or Mountain View in the clear anyway), but we addressed aggregator<->aggregator and aggregator<->collector trust by bootstrapping from the web PKI: each participating server vended configuration parameters (including crypto keys) from a JSON file vended over TLS. Participating organizations agreed out of band on what trusted domains for config should be (i.e., `isrg-prio.org` for us, `en-analytics.cancer.gov` for the feds, `apple.com/exposure-notifications`, and so on). So one could imagine the leader in the Prio v3 scheme serving a directory of aggregators, which would be a list of URLs from which the config for each server could then be fetched and validated, independently of the client's trust relationship with the leader.",
          "createdAt": "2021-03-02T18:45:04Z",
          "updatedAt": "2021-03-02T18:45:04Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing this issue because it's now subsumed by #50.",
          "createdAt": "2021-06-09T18:49:04Z",
          "updatedAt": "2021-06-09T18:49:04Z"
        }
      ]
    },
    {
      "number": 10,
      "id": "MDU6SXNzdWU4MjEzOTQ1MTU=",
      "title": "Define \"Collector\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/10",
      "state": "CLOSED",
      "author": "aaomidi",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Currently we use it in the document but we don't define it here: https://github.com/abetterinternet/prio-documents/blob/83c4ad39a101c17dcfc2421353996297aa503506/design-document.md#terminology",
      "createdAt": "2021-03-03T18:58:46Z",
      "updatedAt": "2021-03-04T03:53:09Z",
      "closedAt": "2021-03-04T03:53:09Z",
      "comments": []
    },
    {
      "number": 14,
      "id": "MDU6SXNzdWU4MjQ5OTY2MjE=",
      "title": "Bias in output of PRG",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/14",
      "state": "CLOSED",
      "author": "tlepoint",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "> This can be instantiated using a standard stream cipher, e.g.., ChaCha20 as follows. Interpret k as the cipher key, and using a fixed nonce, generate l*n bytes of output, where l is the number of bytes needed to encode an element of K, then map each chunk of l bytes to an element of K by interpreting the chunk as an l-byte integer and reducing it modulo the prime modulus.\r\n> [[OPEN ISSUE: Mapping the output of PRG(.,.) to a vector over K induces a small amount of bias on the output. How much bias is induced depends on the how close the prime is to a power of 2. Should this be a criterion for selecting the prime?]]\r\n\r\nI strongly recommend **against** using a biased PRG (even though the bias is small). There are two easy way to solve this:\r\n- *Rejection sampling*: Accept the `l`-byte integer if it is strictly smaller than the prime. To increase the probability of accepting a number, you may want to zero the most significant bits (e.g., if the prime is in (2^125, 2^126), zero the two most significant bits of your 16-byte randomness, then do rejection sampling). This is the approach currently followed in [libprio-rs](https://github.com/abetterinternet/libprio-rs) and [libprio-cc](https://github.com/google/libprio-cc). The distribution is exactly the uniform distribution modulo the prime.\r\n- *Flooding/Extra Bits*: Reduce a `(l+l')`-byte integer modulo the prime, which gives a distribution close to the uniform distribution modulo the prime when l' is large enough. In practice however, we may want to set `l'=8`, which would be quite wasteful.\r\nBoth approaches are recommended for example when generating keys for EC cryptography, see Sections B.4.1 and B.4.2 of [NIST FIPS 186-4](https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf).",
      "createdAt": "2021-03-08T22:03:27Z",
      "updatedAt": "2021-06-17T19:55:10Z",
      "closedAt": "2021-06-17T19:55:10Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "My vote is for generating more bits than are necessary, where the amount of extra bits is a function of the system security parameter. The [hash-to-curve](https://cfrg.github.io/draft-irtf-cfrg-hash-to-curve/draft-irtf-cfrg-hash-to-curve.html#name-security-considerations) draft captures this quite nicely, I think. (Okay, I'm biased.)\r\n\r\n> In practice however, we may want to set l'=8, which would be quite wasteful.\r\n\r\nSomewhat of a bikeshed, but: is generating 8 extra bytes of randomness *really* a big deal? ",
          "createdAt": "2021-03-09T21:11:48Z",
          "updatedAt": "2021-03-09T21:11:48Z"
        },
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "> Somewhat of a bikeshed, but: is generating 8 extra bytes of randomness really a big deal?\r\n\r\nGood question. I actually don't think it's a bikeshed, because the main difference with hash-to-curve and EC key generation is that, in Prio, there are > # bins random values to generate, _per user_, and the extra bits needs to be generated for _every_ random value. \r\n\r\nFor example, if p is a 32-bit prime, `l=8` means that you are generating *12 bytes* to obtain _one_ 32-bit random integer via a modular reduction of a 96-bit integer. That's 3 times the amount of bytes used to represent the result and the code needs to perform modular arithmetic with numbers that do not fit in a `uint64` (but fits in a `uint128`). With rejection sampling instead, in *expectation* you generate less than 2*4 = 8 bytes, and you only need to perform comparisons over `uint32`, albeit the execution time may be varying.",
          "createdAt": "2021-03-09T22:47:29Z",
          "updatedAt": "2021-03-09T22:47:29Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> For example, if p is a 32-bit prime, l=8 means that you are generating 12 bytes to obtain one 32-bit random integer via a modular reduction of a 96-bit integer. That's 3 times the amount of bytes used to represent the result and the code needs to perform modular arithmetic with numbers that do not fit in a uint64 (but fits in a uint128). With rejection sampling instead, in expectation you generate less than 2*4 = 8 bytes, and you only need to perform comparisons over uint32, albeit the execution time may be varying.\r\n\r\nYeah, I get that it's excessively wasteful (relatively speaking). What it not clear to me is if it's actually wasteful in absolute terms. \r\n\r\nEither way, we can certainly be precise in the minimum number of extra bits needed. We should just do that!",
          "createdAt": "2021-03-09T22:50:30Z",
          "updatedAt": "2021-03-09T22:50:30Z"
        },
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "I wrote a small [benchmark in C++](https://gist.github.com/tlepoint/11d6fc3e8c763b080334009e98c14147) to compare the performance of the two PRGs approaches above for different vector sizes and the cost of an elliptic curve Diffie-Hellman. You can run it as follows\r\n```\r\nbazel run -c opt :prg_bench\r\n```\r\n\r\n***Disclaimer: It may not be the most optimized implementations***\r\n\r\nI think it is valuable to keep this issue open and experiment more comprehensively before choosing the definitive approach, because:\r\n1. Computing the PRG is likely to become the bottleneck compared to the ECDH, henceforth it will be important to optimize it;\r\n2. Using rejection sampling is likely giving a 2-5x speedup compared to drawing extra bits.\r\n\r\nOn my macbook air, I obtain the following timings\r\n```\r\nRun on (4 X 1700 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x2)\r\n  L1 Instruction 32 KiB (x2)\r\n  L2 Unified 256 KiB (x2)\r\n  L3 Unified 4096 KiB (x1)\r\nLoad Average: 6.86, 6.66, 5.96\r\n------------------------------------------------------------------------------------------\r\nBenchmark                                                Time             CPU   Iterations\r\n------------------------------------------------------------------------------------------\r\nBM_Extra32Bits_4293918721/8                            438 ns          432 ns      1604290\r\nBM_Extra32Bits_4293918721/64                           895 ns          879 ns       794083\r\nBM_Extra32Bits_4293918721/512                         3236 ns         3206 ns       217997\r\nBM_Extra32Bits_4293918721/4096                       24580 ns        24432 ns        28410\r\nBM_Extra32Bits_4293918721/32768                     201733 ns       199986 ns         3485\r\nBM_Extra96Bits_4293918721/8                            556 ns          554 ns      1245042\r\nBM_Extra96Bits_4293918721/64                           916 ns          901 ns       765404\r\nBM_Extra96Bits_4293918721/512                         4930 ns         4899 ns       137825\r\nBM_Extra96Bits_4293918721/4096                       41145 ns        40808 ns        17028\r\nBM_Extra96Bits_4293918721/32768                     341149 ns       338716 ns         2073\r\nBM_Extra64Bits_15564440312192434177/8                  611 ns          574 ns      1243273\r\nBM_Extra64Bits_15564440312192434177/64                 916 ns          908 ns       770985\r\nBM_Extra64Bits_15564440312192434177/512               5047 ns         4998 ns       136890\r\nBM_Extra64Bits_15564440312192434177/4096             42041 ns        41743 ns        16057\r\nBM_Extra64Bits_15564440312192434177/32768           356554 ns       349632 ns         2062\r\nBM_RejectionSampling_4293918721/8                      389 ns          387 ns      1772125\r\nBM_RejectionSampling_4293918721/64                     577 ns          574 ns      1179206\r\nBM_RejectionSampling_4293918721/512                   1417 ns         1400 ns       496095\r\nBM_RejectionSampling_4293918721/4096                  8928 ns         8879 ns        77181\r\nBM_RejectionSampling_4293918721/32768                75661 ns        75260 ns         8604\r\nBM_RejectionSampling_15564440312192434177/8            497 ns          491 ns      1390710\r\nBM_RejectionSampling_15564440312192434177/64           806 ns          800 ns       826046\r\nBM_RejectionSampling_15564440312192434177/512         2780 ns         2750 ns       255927\r\nBM_RejectionSampling_15564440312192434177/4096       21339 ns        21210 ns        32438\r\nBM_RejectionSampling_15564440312192434177/32768     174767 ns       173535 ns         4005\r\nBM_Prg/8                                               121 ns          120 ns      5771625\r\nBM_Prg/64                                              140 ns          128 ns      5836641\r\nBM_Prg/512                                             212 ns          211 ns      3297873\r\nBM_Prg/4096                                           1609 ns         1595 ns       434891\r\nBM_Prg/32768                                         12901 ns        12790 ns        54085\r\nBM_ECDH                                                298 ns          286 ns      2516772\r\n```\r\n\r\nIn particular, I looked at `ExtraXXBits` where `XX` is the number of extra bits, and looked at two moduli from the documentation, of respectively 32 and 64 bits. The value after the `/` is the size of the output vector. ",
          "createdAt": "2021-03-14T17:08:58Z",
          "updatedAt": "2021-03-14T17:08:58Z"
        },
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "Additionally, I would like to point out that the analysis in the hash-to-curve draft is for generating a _single_ random value. If the approach of extra bits is chosen, when generating `N` random values, the statistical distance also depends on `N`, and henceforth more than `k` bits should be used for a statistical distance of `2^(-k)` for the whole vector. (In which case, you may actually want to consider different methods to quantify the closeness of the distributions, such as [R\u00e9nyi divergences](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R%C3%A9nyi_divergence)).",
          "createdAt": "2021-03-14T17:13:53Z",
          "updatedAt": "2021-03-14T17:13:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It seems like rejection sampling is clearly the better method, based on the above benchmarks. It's also fairly straightforward to implement. I would favor updating the text to require this method.",
          "createdAt": "2021-03-18T21:06:08Z",
          "updatedAt": "2021-03-18T21:06:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm working on a PR to close this issue (we'll go with rejection sampling). @tlepoint, I can't confirm all of the behavior of libprio-rs and libprio-cc that you pointed to. Specifically, neither library appears to clear the two most significant bits, as you suggested they do. Here's the pertinent code in the latter: https://github.com/google/libprio-cc/blob/master/prio/prng/aes_128_ctr_seeded_prng.cc#L118-L121\r\n\r\nCan you confirm that the bits aren't being cleared?",
          "createdAt": "2021-06-09T18:20:51Z",
          "updatedAt": "2021-06-09T18:20:51Z"
        },
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "In libprio-rs (at least initially) and libprio-cc, the prime is hardcoded to a 32-bit prime, so you actually should *not* clear the most significant bits. More generally, if p is a k-bit prime in a word of l bits, you will want to clear the `l-k` most significant bits.\r\n- for k = 32, l=32, you want to clear 0 bits;\r\n- for k = 126, l=128, you want to clear 2 bits (example I gave in the first comment).",
          "createdAt": "2021-06-10T18:00:38Z",
          "updatedAt": "2021-06-10T18:00:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah, yes. That makes sense. Thanks!",
          "createdAt": "2021-06-10T22:16:46Z",
          "updatedAt": "2021-06-10T22:16:46Z"
        }
      ]
    },
    {
      "number": 15,
      "id": "MDU6SXNzdWU4MjUwMDY0MjA=",
      "title": "Enforce batch size (was \"Trust in the leader / Aggregation size enforcement at the aggregator\")",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/15",
      "state": "CLOSED",
      "author": "tlepoint",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "If I am not mistaken, the current design would actually allow a malicious leader to learn single client contributions, by only sending that specific data to the aggregator, reporting that the proof is valid, ask the aggregator to \"sum\" the single element and return the answer. While the current design is motivated by different types of constraints (soft deadlines / privacy threshold defined by a minimum number of input shares), it must be observed that such a responsibility should not only be that of the leader.\r\n\r\nIs there a plan to have aggregation size enforcement by the aggregators, and avoid aggregation of a small amount of (even a single) records?",
      "createdAt": "2021-03-08T22:16:48Z",
      "updatedAt": "2021-07-28T21:24:38Z",
      "closedAt": "2021-07-28T21:24:38Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We did discuss that problem, but it's good to have an open issue on it so we don't forget to address it; thank you for filing this! \r\n\r\nOne tricky dimension to this question is what the minimum threshold should be. I'm not sure if a single threshold value is appropriate for all use cases, and it might be that it's something the client should select while onboarding into the system.",
          "createdAt": "2021-03-08T22:37:16Z",
          "updatedAt": "2021-03-08T22:37:16Z"
        },
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "Having that be a choice of the client is interesting, but such value would need to be visible to the leader (since the leader needs to ask the aggregators to aggregate at least that number of shares together), but also need to be authenticated by the client for the aggregators (so that the leader cannot arbitrarily modify the value). This brings quite a few complications, especially with the optimization of using a KEM.\r\n\r\nAnother option (beyond enforcement by the aggregators, which I still find preferable) could be for the aggregators to keep a public and transparent log of how many shares a specific leader asked to aggregate. Such a public log may act as a deterrent for leader to ask for small aggregations, and they would need a strong justification on why a small aggregation was performed. On the downside, this would reveal publicly how much data is flowing throughout the system.",
          "createdAt": "2021-03-08T22:50:36Z",
          "updatedAt": "2021-03-08T22:51:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "My $0.02: aggregators should enforce the number of input shares aggregated before releasing their share of the output. One way to do this is, as you suggested, might be to replace the KEM with IND-CCA-secure encryption --- say, full HPKE, which takes associated data --- and authenticate the threshold by passing it in as associated data.",
          "createdAt": "2021-03-18T21:19:10Z",
          "updatedAt": "2021-03-18T21:19:10Z"
        }
      ]
    },
    {
      "number": 18,
      "id": "MDU6SXNzdWU4MjgxNDI3NTY=",
      "title": "Heavy Hitters is in scope",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/18",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "See [the paper](https://eprint.iacr.org/2021/017.pdf). ",
      "createdAt": "2021-03-10T18:12:53Z",
      "updatedAt": "2021-07-15T01:25:56Z",
      "closedAt": "2021-07-15T01:25:56Z",
      "comments": [
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "Please note that (regular, not necessarily incremental) DPFs can also be used for aggregation, and are described in a series of papers: \r\n- [Function Secret Sharing: Improvements and Extensions](https://eprint.iacr.org/2018/707)\r\n- [Function Secret Sharing](https://link.springer.com/chapter/10.1007/978-3-662-46803-6_12)\r\n- [Distributed Point Functions and Their Applications](https://link.springer.com/chapter/10.1007/978-3-642-55220-5_35)\r\n\r\nProof of concept implementations of DPFs can be found [here (golang)](https://github.com/dkales/dpf-go) or [here (C++)](https://github.com/dkales/dpf-cpp).",
          "createdAt": "2021-03-12T21:18:50Z",
          "updatedAt": "2021-03-12T21:19:00Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Strong +1 to this idea. I had a longer write-up motiving a use case and outlining a high level solution which leverages this technology for a web API use-case. Then I found this existing issue. Here's what I had:\r\n\r\n### Motivation: Aggregation under very large domains\r\nThe existing Prio protocol enables distributed computation of histograms which have buckets determined by a set of aggregation ids. This construction requires communication overhead per contribution per client that scales linearly with the size of the domain of aggregation (i.e. the size of the histogram vector).\r\n\r\nThis solution works for modest domains (e.g. ~1000s of entries), but starts to incur significant communication overhead as domains get larger (~1M entries or more).\r\n\r\nWith the [Privacy Sandbox](https://www.chromium.org/Home/chromium-privacy/privacy-sandbox) effort, we are looking at aggregation use-cases where domain sizes for aggregation are extremely large and sparse e.g. on the order of ~1B possible aggregation buckets, with ~1k - 1M non-zero buckets. This is analogous to the \u201cheavy hitters\u201d problem. Additionally, we are looking at protecting the output with central differential privacy noise.\r\n\r\nIn trying to solve for these use-cases, we are exploring a few optimizations:\r\n - Improving communication overhead via compression techniques that grow only logarithmically in the size of the output domain\r\n - Improving server-side performance by allowing for dynamic / hierarchical query models and partial evaluations of the output domain\r\n\r\nThese techniques are inspired by [Lightweight Techniques for Private Heavy Hitters](https://arxiv.org/abs/2012.14884).\r\n\r\nNote: we are also exploring MPC protocols which don\u2019t grow proportionally to the output domain at all, but they are at an early stage.\r\n\r\n### Function secret sharing and distributed point functions\r\nA new cryptographic primitive can help us achieve these goals.\r\n\r\nGiven a function f(x): X -> Y, a function secret sharing is a primitive defined by two algorithms\r\n - GenKey(f) outputs two keys K0 and K1\r\n - Eval(K_b, x) outputs value z_b for b=0, 1 and all x\u2208X such that z_0 and z_1 are two random shares of f(x) in Zp, i.e z_0 + z_1 = f(x) mod p.\r\n\r\nA point function fa,b(x) : X -> Y is a function such that f(a) = b and f(a) = 0 for all x \u2260 a.\r\n\r\nA distributed point function is a function secret sharing for a point function fa,b(x). There exist [DPF constructions](https://eprint.iacr.org/2018/707.pdf) such that the size of the DPF keys K0 and K1 is logarithmic in the function input domain |X| and output domain |Y|. \r\n\r\nBy themselves, DPFs can be drop-in replacements for vector secret shares (via calling Eval on the full domain), and there are secure ways of verifying input consistency, like in Prio, with interaction between the servers. This trades off a cpu increase for a large communication overhead decrease.\r\n\r\nAdditionally, Boneh et. al. have shown that these techniques can be extended to allow for evaluation on prefixes of the input point (\u201cincremental\u201d DPFs or \"IDPFs\"). In other words:\r\n - Eval(K_b, x_t) outputs value z_b for b=0, 1 and all x\u2208X such that z0 and z1 are two random shares of f(x) in Zp, i.e z_0 + z_1 = f(x) mod p, for x_t a t-bit prefix of x.\r\n\r\nPrefix evaluation critically allows for evaluating input in a hierarchical manner (i.e. counts at a fixed prefix).\r\n\r\n### Hierarchical / dynamic queries\r\nBy leveraging IDPFs, we can allow for dynamic queries on large, sparse domain data that traverses a hierarchy. For example, imagine a domain of size 2^25 where only a small fraction of the entries are expected to be non-zero. Both vector secret sharing and naive DPF expansion would struggle with this problem. \r\n\r\nWith IDPFs though, a client could:\r\n - Evaluate the data at a 12-bit prefix\r\n - Find that only 2^5 prefixes hold meaningful amounts of data\r\n - Partially evaluate the data again (unprefixed), but only for entries which match the 2^5 prefixes that have data. This will end up evaluating 2^(5 + 25 - 12) = 2^18 entries, which is more manageable (~250 kb expansion per record) than expanding the whole domain from the beginning\r\n - Timings from preliminary C++ benchmarks: \r\n   - 526ms for a full domain evaluation (size 2^25)\r\n   - 12ms for incremental / sparse evaluation.\r\n\r\nThis single pruning step is a simple example, but this technique can be leveraged in much more complex ways to solve real world private metrics problems. Of course, if we are applying DP noise in the computation we may need to split a privacy budget across each of these dynamic queries.\r\n\r\n### Existing implementations\r\n- Internally at Google we are working on a production C++ implementation of IDPFs (for fast performance), to be open sourced very soon. We are seeing ~10ms to fully expand a ~4 mb vector from a ~350 byte key, and ~1ms to expand a 500kb vector from a ~300 byte key. \r\n - There is a (non-production) [Rust implementation](https://github.com/henrycg/heavyhitters) of IDPFs by Henry Corrigan-Gibbs\r\n - @tlepoint referenced a golang and C++ implementation of DPFs above\r\n",
          "createdAt": "2021-03-17T16:58:34Z",
          "updatedAt": "2021-03-17T19:13:05Z"
        },
        {
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "body": "@henrycg we'd welcome your thoughts in this thread :-) ",
          "createdAt": "2021-03-17T17:40:25Z",
          "updatedAt": "2021-03-17T17:40:25Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for looping me in here!\r\n\r\nIt would definitely be nice if Prio v3 could support heavy-hitters queries. There are a number of applications that need this, so I like the idea of building it into the design. At the same time, the machinery needed to support these heavy-hitter applications involves a quite a few moving parts that are not needed for Prio v1/v2 functionality.\r\n\r\nIn particular, Prio v1/v2 uses:\r\n\r\n- Two or more servers (where privacy holds if at least one is honest)\r\n- Simple additive secret sharing with finite-field arithmetic\r\n- **ZK proofs on secret-shared data (called \"SNIPs\" in the Prio paper)**\r\n\r\nThe simplest version of the heavy-hitters scheme from the [BBCGI21](https://arxiv.org/abs/2012.14884) paper needs:\r\n\r\n- Two servers (where privacy holds if at least one is honest)\r\n- Simple additive secret sharing with finite-field arithmetic\r\n- **Distributed point functions**\r\n- **Sketch-verification schemes (Section 4 in the BBCGI21 paper)**\r\n\r\nDo we want to expand the scope of this document to include the more recent and more advanced techniques? Or, is it better to stick with the Prio v1/v2 technology, which is quite stable and which we understand pretty well at this point\u2014even if it does not support all of the functionality that we want?\r\n",
          "createdAt": "2021-03-18T21:36:58Z",
          "updatedAt": "2021-03-18T21:36:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think it's worth spinning up a new doc, though presumably there are pieces shared by both (i.e., ZK proofs on secret shared data). These could live in a third doc.",
          "createdAt": "2021-03-18T21:55:48Z",
          "updatedAt": "2021-03-18T21:55:48Z"
        },
        {
          "author": "schoppmp",
          "authorAssociation": "NONE",
          "body": "The C++ implementation of (I)DPFs mentioned by @csharrison above is now public:\r\nhttps://github.com/google/distributed_point_functions",
          "createdAt": "2021-03-19T14:45:59Z",
          "updatedAt": "2021-03-19T14:45:59Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "FYI I published an example query model for how the flexible, non-interactive heavy hitters could work via some configuration here:\r\nhttps://github.com/WICG/conversion-measurement-api/blob/main/AGGREGATE.md#example-query-model\r\n\r\nIt basically requires either the collector or leader to give some configuration outlining which bit-prefixes to expand.",
          "createdAt": "2021-05-07T21:08:07Z",
          "updatedAt": "2021-05-07T21:08:07Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we're well passed the relevant point on the decision tree here :) Any reason to keep this issue open? I suppose the links are useful, but it's easy enough to rummage around in the closed issues to find them.",
          "createdAt": "2021-07-14T20:53:35Z",
          "updatedAt": "2021-07-14T20:53:35Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's close it out.",
          "createdAt": "2021-07-15T01:25:56Z",
          "updatedAt": "2021-07-15T01:25:56Z"
        }
      ]
    },
    {
      "number": 19,
      "id": "MDU6SXNzdWU4MjgxNjk4ODM=",
      "title": "Accommodating randomized inputs (a la DP)",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/19",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "differential privacy"
      ],
      "body": "@tlepoint pointed out that PrioV2 requires all clients to submit randomized values for the purposes of ensuring that identical inputs don't yield the same output. Basically, with some very small probability, clients flip some bits in their data to maximize aggregate utility and privacy (in terms of differential privacy bounds). [Apple's paper](https://arxiv.org/abs/2012.12803) on this topic has details. \r\n\r\nWe should probably design the system such that either clients or aggregators can inject this sort of noise. ",
      "createdAt": "2021-03-10T18:33:51Z",
      "updatedAt": "2023-09-20T15:13:19Z",
      "closedAt": "2023-09-20T15:13:18Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This seems orthogonal to the core protocol logic, since its applicability depends on the data type. ",
          "createdAt": "2021-03-18T21:22:06Z",
          "updatedAt": "2021-03-18T21:22:06Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "There's an open question that fell out of the review of #91 that feels appropriate for this issue. Aggregators are encouraged to apply differential privacy noise to outputs. Collectors and aggregators may need to agree on DP parameters (e.g., the probability with which values are flipped) so that collectors can de-noise outputs. I think we might need to have a field in `PDAParams` for differential privacy parameters.",
          "createdAt": "2021-07-30T19:19:59Z",
          "updatedAt": "2021-07-30T19:19:59Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It's worth noting that in many situations (especially with server-added noise), the collectors don't need to do anything special to \"de-noise\" since it's often going to have a mean of 0. However, collectors may want to know things like the variance of the noise, or what distribution the noise was sampled from so they can understand the effective error.",
          "createdAt": "2021-07-30T19:25:03Z",
          "updatedAt": "2021-07-30T19:25:03Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I was originally going to post this as a new issue but I think this one should suffice. Originally titled \"Optional DAP configuration that achieves differential privacy\". Apologies in advance for the wall of text, I hope it is useful to the discussion.\r\n\r\n-----\r\n\r\nDifferential privacy (DP) is a useful property of an aggregate system. Among other things, it is a mitigation against Sybil attacks that aim to violate user privacy. This is because differential privacy holds under worst-case assumptions, including when the adversary knows the contents of the entire database except for the targeted user. In other words, malicious clients submitting data to the system does not adversely affect the privacy (in this worst-case sense) of any honest user.\r\n\r\nWe should consider whether we want to specify an optional configuration in DAP which deployments can use to achieve DP output. @cjpatton mentions [here](https://github.com/ietf-wg-ppm/ppm-specification/pull/235#discussion_r865016936) it would be useful, so here are my thoughts:\r\n\r\nDP typically involves noise addition, we ought to consider two flavors, since I think both are relevant and useful to this work:\r\n\r\n## 1. Clients add noise to their input\r\n\r\nWhen clients add noise to their input, it is possible to show the system is differentially private (in the \u201clocal\u201d model) without any downstream changes in DAP or VDAF. This is a trivial result though, and does not take advantage of anything relevant to DAP.\r\n\r\nThe security model of DAP is that input is sent to the system through input shares, which eliminates any knowledge in the system of which client sent which report. I believe that, by using a similar line of argument as the [ENPA white paper](https://covid19-static.cdn-apple.com/applications/covid19/current/static/contact-tracing/pdf/ENPA_White_Paper.pdf) (section 4.5), we can show that using local noise in DAP should always achieve differential privacy in the shuffle model, without any change to the protocol (or restriction on VDAF). This yields [much stronger](https://arxiv.org/abs/2012.12803) privacy bounds than purely local DP. It will depend on the number of users contributing in a given batch, so increasing the minimum batch size should increase privacy.\r\n\r\nSide note: with some specific VDAFs, and some specific types of local noise, you could show stronger privacy bounds than shuffle DP, even if noise is only added on the client. This is similar to the case of each aggregator adding independent noise, where you consider the client to be a 1-record aggregator.\r\n\r\nSide note: If clients add noise to their input, it puts at risk the \u201cverifiable\u201d aspect of VDAF, since the input is falsey from the very start. However, we can still add noise such that the output is well-bounded, and downstream VDAFs can expect data within the expected bounds.\r\n\r\n## 2. Noise is added during aggregation\r\n\r\nThere are a couple of ways I could see specifying this. This is an example of central DP, or more specifically, distributed DP since it is done via distributed noise addition.\r\n\r\n### 2A. VDAFs add noise\r\n\r\nThe idea here is that a particular VDAF could be implemented such that noise is added to each `agg_share` independently, resulting in a noisy `agg_result` that could be shown to be differentially private. Noise should be added in such a way that agg_result has summed noise distributed according to a particular distribution that we know achieves DP (e.g. discrete laplace, etc).\r\n\r\nFor example, n aggregators adding noise distributed according to difference-of-Polya distributions would, summed together, yield noise distributed according to two-sided geometric distribution ([link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5598559/), theorem 5.1) which we can show central DP bounds for. It is worth mentioning that in these schemes, you can often show (reduced) DP bounds even if only a subset of the aggregators are honestly adding noise, which is exactly the kind of property that you want out of DAP. In general, this is possible when the final distribution you want the output noise to follow is [infinitely divisible](https://en.wikipedia.org/wiki/Infinite_divisibility_(probability)), or has good infinitely divisible approximations.\r\n\r\nVDAFs should probably not make any assumptions about their input shares (i.e. how many are coming from which users, etc). However, if they know both:\r\n- The range that inputs to the VDAF must fall under, and\r\n- The amount of noise the VDAF adds\r\nThen we can specify in some cases the record-level differential privacy claim. That is, how much privacy loss we could expect (in the worst case) on any particular record in the input.\r\n\r\nThis means a DP-aware VDAF could parametrize itself with a new `PublicParam` that encodes a `(epsilon, delta)` tuple describing the desired record-level differential privacy bounds. Then the VDAF could configure itself to add noise scaled properly to achieve this bound. It\u2019s possible the VDAF would need new constants to advertise bounds on epsilon / delta if only some configurations are possible. VDAFs can also have public parameters specifying the distribution they are sampling from to add noise, so users of the VDAF can properly understand the expected variance from aggregating.\r\n\r\nWith VDAFs supporting record-level DP, it should be easy to achieve user-level DP in DAP. The easiest way would be to include a new constraint that each user produces exactly the same number of inputs as any other user (say, N inputs) in a given batch (see #210 also). It is straightforward to show (via group privacy, see Lemma 2.2 [here](https://privacytools.seas.harvard.edu/files/privacytools/files/complexityprivacy_1.pdf)) that this would result in user-level privacy loss at most `(N*epsilon, N*exp(N*epsilon)*delta)`. It should be possible to make DP claims in other settings, but it will be a bit more challenging.\r\n\r\n### 2B. Add noise as a post-processing step after VDAF aggregation\r\n\r\nVDAFs are already complicated things, so it might be possible for noise to be added outside that system (e.g. by DAP). This is appealing, but introduces some new challenges.\r\n\r\nDAP will need to know the effective sensitivity of the VDAF, i.e. how much each record can maximally impact the agg result in order to calibrate the noise. Note that there are multiple ways of measuring sensitivity (L0, L1, L2, Linf, etc.) that could be relevant here, and it might be more optimal to add noise if you know the L2 sensitivity vs. the L1 sensitivity.\r\n\r\nWhen this information is known, and using the type information that DAP should already know about the VDAF, DAP could specify techniques for adding noise to each agg share before they get sent to the collector, based on the exposed sensitivity, a desired privacy level, and the maximum number of input records a user could contribute. We would want generic methods for supporting DP given an arbitrary function F, its sensitivity, type, and #`SHARES` that a DAP deployment could use.\r\n\r\nIn offline discussions with @marianapr, she preferred this approach, but in my opinion it feels pretty complicated to decouple noise addition generically from the aggregation function itself. I'd be interested in other opinions though.\r\n\r\nOf course, we can also discuss parametrizing DAP as well as @tgeoghegan mentions above.\r\n\r\ncc @badih who double checked some of my work here.",
          "createdAt": "2022-05-05T16:59:56Z",
          "updatedAt": "2022-05-05T16:59:56Z"
        },
        {
          "author": "degregat",
          "authorAssociation": "NONE",
          "body": "Some information regarding de-risking of approach **2B**: \r\n\r\nA prototype extension for libprio based on this approach can be found [here](https://github.com/degregat/libprio/blob/345ab6f9795dcd60fddc54a24b43eefe9e48a0bd/pclient_dp/main.c#L201), this one being meant to be used as a backend for federated learning. \r\n\r\nProperties for the distributed discrete gaussian can be found [in this paper by google research](https://arxiv.org/abs/2102.06387).\r\nLooking at the above, one could use noise scaling techniques similar to **2A** and would receive an analogous graceful privacy reduction per defecting aggregator (implemented [here](https://github.com/degregat/libprio/blob/345ab6f9795dcd60fddc54a24b43eefe9e48a0bd/prio/server.c#L362)). Though I'm wondering how to think about the case when a subset is defecting, since then the validity checking from the PCPs breaks down, and we get different problems as well. (This should be true for **2A** as well, no?)\r\n\r\nThere are now [prototype tools](https://diffmu.github.io/DiffPrivacyInference.jl/dev/tutorial/01_sensitivity_functions/#Sensitivity), to infer sensitivity as well as privacy guarantees of functions, so it sounds more feasible by the day to find out the sensitivity of a given VDAF, without it being too labor intensive.\r\n\r\nIn any case, it is advisable to use discrete distributions to prevent being vulnerable to [floating point attacks](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/10/lsbs.pdf).",
          "createdAt": "2022-05-31T20:05:46Z",
          "updatedAt": "2022-06-01T08:45:59Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Thanks @degregat ! I think the distrusted aggregators adding \"too much noise\" to compromise the result is a good thing to try to protect against. However, I am not sure how necessary it is given that malicious aggregators can [already compromise the result](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-6.1.2.2) in the DAP threat model. In any case it's not obvious to me whether this is easier to achieve in 2A or 2B.\r\n- In 2A we could add another round to the initialization step to pass around noise shares.\r\n- In 2B perhaps the leader crafts noise shares before interacting with the other aggregators. This would move the idea from \"post-processing\" to \"pre-processing\", to take advantage of the VDAFs verification.\r\n\r\n> There are now [prototype tools](https://diffmu.github.io/DiffPrivacyInference.jl/dev/tutorial/01_sensitivity_functions/#Sensitivity), to infer sensitivity as well as privacy guarantees of functions, so it sounds more feasible by the day to find out the sensitivity of a given VDAF, without it being too labor intensive.\r\n\r\nThat is very interesting! Thanks for sharing.\r\n\r\n> In any case, it is advisable to use discrete distributions to prevent being vulnerable to [floating point attacks](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/10/lsbs.pdf).\r\n\r\nI agree, I think it's generally easier in the MPC to have support only on integers, so I don't think that's a big constraint.\r\n",
          "createdAt": "2022-06-01T14:13:54Z",
          "updatedAt": "2022-06-01T14:13:54Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "My preference would be **2B**, if we can make it work. I think the threat of an Aggregator fiddling with the noise distribution only matters insofar as it impacts privacy. If only correctness is impacted, we already concede that Aggregators are assumed to execute the protocol correctly.",
          "createdAt": "2022-06-29T15:10:04Z",
          "updatedAt": "2022-06-29T15:10:04Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "We might have a need for 2B.  Local/shuffle DP is harder to reason about from a privacy/utility perspective.  I would also be OK with the aggregates being initialized with noise before accepting any uploads; the effect is the same.\r\n\r\nHas someone done anything to specify this?  Has any effort been put into defining something here?\r\n\r\nIt seems like it might be an extension to the task creation process.  Though it seems like that is maybe considered out of scope.",
          "createdAt": "2023-04-27T05:13:25Z",
          "updatedAt": "2023-04-27T05:13:25Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Local/shuffle DP is harder to reason about from a privacy/utility perspective. I would also be OK with the aggregates being initialized with noise before accepting any uploads; the effect is the same.\r\n\r\nCan you say more about why you think privacy/utility is harder to reason about in the local model compared to the central model?",
          "createdAt": "2023-04-27T21:41:29Z",
          "updatedAt": "2023-04-27T21:41:29Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Because I'm not smart enough, basically.",
          "createdAt": "2023-04-27T22:12:52Z",
          "updatedAt": "2023-04-27T22:12:52Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Oh, I didn't mean it like that (and generally dismiss your imposter syndrome!). I was just wondering if there was something y'all learned during IPA's development that might be relevant here. For example, perhaps due to the nature of local DP, choosing epsilon in a way where you don't have full visibility into the distribution of input reports means you make more conservative choices (and worsen utility)? \ud83e\udd37 ",
          "createdAt": "2023-05-02T10:30:49Z",
          "updatedAt": "2023-05-02T10:30:49Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Mostly, the balance between utility and privacy is poor with local DP.  Individuals don't get great assurances about how much their contributions are protected and the aggregates tend to be overly noisy.  That's even if you use all the tricks available, like shuffle DP.\r\n\r\nThe main concern I'd have from the privacy perspective is that you might do something like randomized response with an effective/local epsilon of 10 or even higher, which is basically meaningless for that contribution.  1/20 or lower chance of picking a false option isn't worth much if you happen to be singled out; worse if that can happen more than once (as is likely here, as in IPA).  But from a utility perspective, that's a whole lot of noise.  10000 people each with with a 1/20 chance of lying suddenly looks like $\\mathcal {N}(0, 475)$, which is quite a bit when you have conversion rates of 1% being considered very high, such that your signal is $\\sim 100$ relative to that noise.\r\n\r\nThis system might be different in that the aggregates might be a bit less diffuse.  Measuring higher probability events or contributions that are more consistent might lend itself to higher rates of noise in the input, but it is hard to compete with central DP whatever way you cut it.",
          "createdAt": "2023-05-02T12:33:41Z",
          "updatedAt": "2023-05-02T12:33:41Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Shuffle DP is a very generic framework and statements like \"utility and privacy is poor\" are not accurate for all deployments and algorithms, especially tuned mechanisms that aren't just generically LDP mechanisms \"amplified\" by shuffling. Recent research into some shuffle DP frameworks can show very competitive performance with central DP, especially in the \"multi-message\" shuffle framework.\r\n\r\nThe main benefit of a shuffle notion here is that it's simpler for the system to handle, as the noise mechanism comes \"for free\" outside of the aggregator and won't require much infrastructure to support. For that reason it seems like we should just support it IMO. We already have deployed systems (e.g. ENPA) that use this kind of privacy and it would be beneficial if they could be specified here.",
          "createdAt": "2023-05-02T21:12:24Z",
          "updatedAt": "2023-05-02T21:12:24Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "We have some text in https://www.ietf.org/archive/id/draft-ietf-ppm-dap-07.html#section-7.5. We're also working on a dedicated draft for DP in DAP, which we hope will be ready for WG adoption call at 118: https://github.com/wangshan/draft-wang-ppm-differential-privacy\r\n\r\nAs a result of this work, we may end up needing changes to the DAP spec, but for now we think it's fine to shift this conversation to the drraft.",
          "createdAt": "2023-09-20T15:13:18Z",
          "updatedAt": "2023-09-20T15:13:18Z"
        }
      ]
    },
    {
      "number": 20,
      "id": "MDU6SXNzdWU4MjgxNzk0MDU=",
      "title": "Data stuffing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/20",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "duplicate",
        "differential privacy"
      ],
      "body": "To what extent should the system detect or mitigate fake clients from stuffing data into the system? We could so far as to say that all data must come from an authenticated client. (Maybe there's something with anonymous credentials we can do here?) We might also recommend simple mitigations, such as checking for repeated entries from the \"same client\" (same IP, same application-layer identifier, etc) during a batch. ",
      "createdAt": "2021-03-10T18:41:28Z",
      "updatedAt": "2023-09-20T15:16:29Z",
      "closedAt": "2023-09-20T15:16:28Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "In today's design call we discussed mitigating this issue using an \"ingestion server\" that sits in front of the leader. In practice, the leader and ingestor might be the same entity.",
          "createdAt": "2021-04-14T18:12:11Z",
          "updatedAt": "2021-04-14T18:12:11Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@csharrison, this issue tracks the \"double counting\" problem we discussed today.",
          "createdAt": "2021-04-14T18:19:41Z",
          "updatedAt": "2021-04-14T18:19:41Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "For our use-case, we'd likely want an application-layer identifier to impose limits on clients or even batches of clients (to make this kind of input tracking scale a bit better).\r\n\r\nWe are also interested in allowing clients to input the same piece of data multiple times, up to some fixed number. This is to support the batch querying model where a server has a collection of records and would like to learn e.g. both daily aggregates and weekly aggregates, which would require every piece of data getting processed twice.",
          "createdAt": "2021-04-28T17:08:03Z",
          "updatedAt": "2021-04-28T17:08:03Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "This is addressed in the newly rewritten security considerations, under [section 7.2, Sybil attacks](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-sybil-attacks).",
          "createdAt": "2023-09-20T15:16:29Z",
          "updatedAt": "2023-09-20T15:16:29Z"
        }
      ]
    },
    {
      "number": 21,
      "id": "MDU6SXNzdWU4MzQwMzU1MzY=",
      "title": "Client configuration information",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/21",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We should probably assume that configuration information, including the desired aggregation function, encoding details, choice of aggregators, etc., are all known to the client before it creates any data for submission. Online parameters such as batch identifiers should also be assumed known for a given client when producing data for the system. In practice, clients might be given batch identifiers to use for a given aggregation from some trusted party, or they might query it from the leader, or something else. The details are probably out of scope. \r\n\r\nEssentially, clients can submit data given configuration parameters and a batch identifier. And that's probably(?) the minimum dependencies here.",
      "createdAt": "2021-03-17T17:48:44Z",
      "updatedAt": "2021-07-14T21:00:29Z",
      "closedAt": "2021-07-14T21:00:29Z",
      "comments": []
    },
    {
      "number": 22,
      "id": "MDU6SXNzdWU4NDEzMTI3NjY=",
      "title": "Explore Shamir secret sharing (was \"Multiple protocol runs and/or threshold secret sharing\")",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/22",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "Prio v2 employs a linear secret sharing scheme in which all input shares are required in order to produce coherent outputs. This gives us robustness against malicious clients, but not malicious servers ([robustness per section 2 of the 2017 paper](https://crypto.stanford.edu/prio/paper.pdf)).\r\n\r\nWe could provide robustness against a minority of malicious servers by running the protocol over multiple subsets of aggregators. If the outputs across all aggregator subsets do not agree, then participants know that there is at least one malicious server.\r\n\r\nIf linear secret sharing is employed, then clients would have to generate input shares once for each subset. We could also use a threshold secret sharing scheme (e.g., [Shamir](https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing)), which would reduce work for clients.",
      "createdAt": "2021-03-25T21:13:29Z",
      "updatedAt": "2022-09-13T02:52:21Z",
      "closedAt": "2022-09-13T02:52:21Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This will be interesting :) ",
          "createdAt": "2021-03-25T23:42:08Z",
          "updatedAt": "2021-03-25T23:42:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "#43 partially addresses this issue by deciding that multiple helpers will be used for resilience. Whether we can use Shamir secret sharing to provide robustness against cheating servers is still an open question. Though IMO we shouldn't try to change the spec to allow for this.",
          "createdAt": "2021-06-09T17:48:25Z",
          "updatedAt": "2021-06-09T17:50:01Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This issue predates the VDAF abstraction and no longer applies (directly) to DAP. However, it's an interesting consideration for a future variant of Prio3 (or other VDAF).",
          "createdAt": "2022-09-13T02:52:21Z",
          "updatedAt": "2022-09-13T02:52:21Z"
        }
      ]
    },
    {
      "number": 24,
      "id": "MDU6SXNzdWU4NDIyNzk1MTA=",
      "title": "Try to write an overview so the cryptographic details aren't such a jump",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/24",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-03-26T20:35:15Z",
      "updatedAt": "2021-04-14T18:10:48Z",
      "closedAt": "2021-04-14T18:10:48Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "(Linking PR #26)",
          "createdAt": "2021-04-05T23:13:39Z",
          "updatedAt": "2021-04-05T23:13:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing this since the PR has landed.",
          "createdAt": "2021-04-14T18:10:48Z",
          "updatedAt": "2021-04-14T18:10:48Z"
        }
      ]
    },
    {
      "number": 27,
      "id": "MDU6SXNzdWU4NDczMzA5MzY=",
      "title": "Some notes on requirements",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/27",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "I tend to think of this from two perspectives (1) the client and\r\n(2) the collector. At a high level, we expect them to coordinate.\r\n\r\nThe basic unit here is the measurement, which is just a value\r\ncollected by the client. In a normal system, each value is\r\ncharacterized by:\r\n\r\n- A name\r\n- A type [scalar, integer, etc.]\r\n- Externally, some description of what it measures\r\n- Some description of what reasnable values are (so we can do outlier\r\n  rejection)\r\n\r\nIn Prio, you would also have:\r\n\r\n- The aggregation function you want to use with the measurement.\r\n- The precise ranges of a valid input (replacing the \"reasonable\")\r\n  filter above\r\n\r\nNote that this might mean you needed to duplicate measurements if you\r\nwanted to be able to do multiple aggregation functions.  This isn't\r\nnecessarily with traditional telemetry.\r\n\r\nThis information needs to be propagated both to the client and the\r\ncollector and they need to match. Practically, once a name is\r\nreserved, its properties shouldn't change because otherwise things get\r\nweird. Though sometimes we will slightly change how we collect some\r\nmeasurement a bit, but keep the types the same, etc.\r\n\r\n\r\nOperationally, clients will have some set of measurements that they\r\nmaintain and then will just periodically send them all to the server\r\nin a single package (Mozilla calls this a \"ping\"). In Mozilla\r\ntelemetry this is just a JSON blob, e.g.,\r\n\r\n         \"scalars\": {\r\n            \"contentblocking.trackers_blocked_count\": 258,\r\n            \"media.element_in_page_count\": 42,\r\n            \"browser.engagement.session_time_including_suspend\": 90858058,\r\n            \"browser.engagement.active_ticks\": 612,\r\n            \"browser.engagement.session_time_excluding_suspend\": 49487949\r\n          },\r\n\r\nWhen you want to add a new measurement, you just add a new entry to\r\nthe JSON blob. And in principle you could remove them, though that's\r\nless common. So, for instance, version 20 might have these scalars and\r\nversion 21 might have these plus another.\r\n\r\n\r\nOn the collector side, what you generally want to be able to do is to\r\nfilter the reports by demographic type information (e.g., browser\r\nversion, location, time) and then compute some aggregate over the\r\nfiltered result. For instance, suppose you wanted to know the fraction\r\nof IPv4 versus IPv6, you might just take every client that reported\r\nthat measurement, but you might also want to look at Beta versus\r\nRelease. One important implication is that when you are looking\r\nat measurement A, you don't care if the client reported measurement\r\nB. And in particular, if some clients report [A, B] and others report\r\n[A], you want to capture all of them (unless you don't!).\r\n\r\nObviously Prio limits what you can do here. In particular:\r\n\r\n- You can't just compute any statistic over a given measurement,\r\n  but only the ones compatible with the encoding.\r\n\r\n- It's not safe to let people just compute aggregates over\r\n  arbitrary-sized subsets of the data because then they can\r\n  pull out specific reports.\r\n\r\nHowever, we'd obviously like to support as flexible queries as\r\npossible. In particular, it's desirable to have modes in which\r\nyou can \"drill down\" into the data. For instance, you might\r\nnotice that the rate of some error had gone up recently and\r\nwant to try to isolate it to some subpopulation. This implies\r\nthat you don't just want pure batch processing.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "createdAt": "2021-03-31T21:01:52Z",
      "updatedAt": "2021-08-11T22:12:46Z",
      "closedAt": "2021-08-11T22:12:46Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for writing up this clear description of the use case, @ekr. Your understanding of the limits of Prio with respect to this particular usage appears to match my own. The good news is that there's lots of ways to provide flexibility of the system, though perhaps at the cost of time complexity, communication overhead, or both. By way of fleshing out the design space, let me sketch what I see as the two possible extremes.\r\n\r\n**Maximizing flexibility.** Let's start with how we would maximize flexibility of the system. Essentially what we need to do is move all metadata and versioning logic outside of Prio. Concretely, we secret share and validate each value in the JSON object Individually. So instead of the plaintext JSON blob above, each aggregator would see something like\r\n```\r\n     \"scalars\": {\r\n        \"contentblocking.trackers_blocked_count\": {\r\n           \"proof share\": <share of proof 1>, \r\n           \"input share\": <share of input 1>\r\n        },\r\n        \"media.element_in_page_count\": {\r\n           \"proof share\": <share of proof 2>, \r\n           \"input share\": <share of input 2>\r\n        },\r\n        \"browser.engagement.session_time_including_suspend\": {\r\n           \"proof share\": <share of proof 3>, \r\n           \"input share\": <share of input 3>\r\n        },\r\n        \"browser.engagement.active_ticks\": {\r\n           \"proof share\": <share of proof 4>, \r\n           \"input share\": <share of input 4>\r\n        },\r\n        \"browser.engagement.session_time_excluding_suspend\": {\r\n           \"proof share\": <share of proof 5>, \r\n           \"input share\": <share of input 5>\r\n        }\r\n      }\r\n```\r\nHere, the proof and input shares look like sequences of random field elements. The aggregators would collectively validate each input using the proof share corresponding to their input share. Since there are 5 inputs here, the aggregators would do 5 runs of the input validation protocol. Notice, however, that since the runs are independent, it's possible to batch the messages together so that the round complexity (i.e, round trips over the network) is the same as if there were just one input to validate.\r\n\r\nHaving validated all of the inputs, the aggregators can do whatever they want with them. For example, if they want to compute a statistic over the measurements by all clients of a particular agent type, then each aggregator would simply sum up the inputs shares sent by clients with that agent type. (One only needs to take care that this doesn't reduce the anonymity set further than the threshold specified by the collector: see https://github.com/abetterinternet/prio-documents/issues/15.) Versioning is also pretty straightforward, since each input is labeled separately.\r\n\r\nAs you correctly point out, the main limitation of Prio is that you need to know in advance what sorts of statistics you want to compute over measurements. This is because the client needs to know beforehand how to encode and generate a proof for each input. However, inputs can be encoded in a way that allows for multiple statistics to be computed about the same set of measurements. For example, if we're already computing the mean over a set of measurements, then it's fairly cheap to add the standard deviation. We could also throw in other statistics, like median, min, or max, but we need to be careful because some statistics have a higher communication cost than others.\r\n\r\n**Minimize overhead.** The main downside to the above approach is that it usually won't provide optimal time complexity or communication overhead. The reason, basically, is that by handling all of the inputs \"monolithically\", we can take advantage of redundancy in the structure of the input in order to construct a more efficient proof system. In particular, the JSON becomes simply\r\n```\r\n     \"scalars\": {\r\n           \"proof share\": <share of proof>, \r\n           \"input share\": <share of input>\r\n      }\r\n```\r\nand there are procedures for encoding and decoding this \"monolithic input\" as measurements `contentblocking.trackers_blocked_count`, `media.element_in_page_count`, and so on.\r\n\r\nHere, the length of the input share is about the same as the sum of the lengths of the input shares above; however, it is often possible to reduce the size of the proof share significantly compared to the proof shares above. In more detail, the main way we optimize the proof size is by being creative in how we apply Theorem 4.3 of [[BBG+19]](https://eprint.iacr.org/2019/188.pdf). The proof size is linear in the number of \"G-gate\" evaluations in the validity circuit that recognizes valid inputs. If there is lots of redundancy in the validity circuit, then we can make G \"big\", thereby reducing the number evaluations of G and shortening the proof.\r\n\r\nSo what's the downside of the \"monolithic input\" approach? In terms of flexibility, it's still possible to do things like aggregate measurements based on agent type, Beta/Release, etc. However, versioning becomes much more difficult: if we want to change the set of measurements that are encoded by the input, then we would need to change the way we construct the proof system. This isn't so bad if the client and aggregators are running the same code (there are lots of situations in which this is feasible), but if they are running different code, then coordinating this will be significant.\r\n\r\n**So, what do we do?** The best solution for a given deployment is going to lie somewhere in the middle of these two extremes. I think it should be a goal of ours to design the protocol in a way that allows deployments to make different trade-offs between flexibility and efficiency. Regardless of what trade-off a particular deployment makes, what is needed is a way of annotating inputs in a way that ensures clients and aggregators know how to generate and verify them. We also need a way of failing gracefully if an aggregator doesn't know how to verify an input.\r\n\r\nThat said, it's important to note that this problem isn't knew. It's somewhat akin to assigning code points to DH groups in TLS, in the sense that the code point prescribes the code that is going to be run by the parties.",
          "createdAt": "2021-04-02T22:52:53Z",
          "updatedAt": "2021-04-02T22:59:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@ekr, I wanted to follow up on the performance benefits of stuffing many measurements into the same proof. If we know in advance that clients will submit many measurements of the same type, then it's possible to construct a proof that is much smaller than the sum of the sizes of proofs for the individual measurements. The following PR for libprio-rs demonstrates this for vectors of booleans:\r\nhttps://github.com/abetterinternet/libprio-rs/pull/30\r\n\r\nThe table compares the proof size of \"Prio v2\", the current proof system for boolean vectors, to \"PolyCheckedVector\", the proof system that the PR implements. Prio v2 constructs an `O(n)` sized proof, where `n` is the number of booleans. In terms of communication overhead, this is no better than constructing a constant-sized proof for each bit individually. ~PolyCheckedVector constructs a proof of size `O(sqrt(n))`.~ UPDATE: There should be a way to get this down to `O(sqrt(n))`: see https://github.com/abetterinternet/libprio-rs/issues/35.\r\n\r\nThe same optimization is possible for a vector of integers, so long as each of the integers has the same range of valid values. In fact, this optimization can be applied regardless of the set of aggregates we want for each integer, so long as as the set is the same for each aggregator.\r\n\r\nThis optimization won't make sense everywhere, of course, but one instance in which I can imagine using it is when clients report several counts or timings that have the same expected range.\r\n",
          "createdAt": "2021-04-14T23:47:31Z",
          "updatedAt": "2021-04-22T02:31:39Z"
        }
      ]
    },
    {
      "number": 30,
      "id": "MDU6SXNzdWU4NjQxMTM3MTQ=",
      "title": "Message encoding",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/30",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We should specify structure and syntax for messages exchange in the protocol, e.g., data/proof shares. We'll pass these around with high-level HTTP APIs, maybe carried in JSON messages or whatever. ",
      "createdAt": "2021-04-21T17:29:47Z",
      "updatedAt": "2021-06-09T22:56:38Z",
      "closedAt": "2021-06-09T22:56:38Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to JSON blobs, as long as input shares can be encoded in binary. Seems to be the simplest to implement in JS.",
          "createdAt": "2021-04-21T18:15:09Z",
          "updatedAt": "2021-04-21T18:15:09Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I guess I don't much care if we need to encode input shares in base64 or whatever.",
          "createdAt": "2021-04-21T18:22:41Z",
          "updatedAt": "2021-04-21T18:22:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "With #43 we are now using a binary format, specifically TLS syntax. Does anyone have any objections to continuing to use it?",
          "createdAt": "2021-06-09T19:18:20Z",
          "updatedAt": "2021-06-09T19:18:20Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "None -- let's just stick with TLS. :-) ",
          "createdAt": "2021-06-09T22:56:38Z",
          "updatedAt": "2021-06-09T22:56:38Z"
        }
      ]
    },
    {
      "number": 32,
      "id": "MDU6SXNzdWU4NjY0MjU2MTg=",
      "title": "Update field section criteria for Prio",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/32",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "parking-lot"
      ],
      "body": "The current criteria:\r\n\r\n> 1. **Field size.** How big the field needs to be depends on the type of data\r\n>    being aggregated and how many users there are. The field size also impacts\r\n>    the security level: the longer the validity circuit, the larger the field\r\n>    needs to be in order to effectively detect malicious clients. Typically the \r\n>    soundness error (i.e., the probability of an invalid input being deemed valid\r\n>    by the aggregators) will be 2n/(p-n), where n is the size of the input and p\r\n>    is the prime modulus.\r\n> 2. **Fast polynomial operations.** In order to make Prio practical, it's\r\n>    important that implementations employ FFT to speed up polynomial operations.\r\n>    In particular, the prime modulus p should be chosen so that (p-1) = 2^b * s \r\n>    for large b and odd s. Then g^s is a principle, 2^b-th root of unity (i.e.,\r\n>    g^(s\\*2^b) = 1), where g is the generator of the multiplicative subgroup.\r\n>    This fact allows us to quickly evaluate and interpolate polynomials at 2^a-th\r\n>    roots of unity for 1 <= a <= b.\r\n> 3. **Highly composite subgroup.** Suppose that (p-1) = 2^b * s. It's best if s\r\n>    is highly composite because this minimizes the number of multiplications\r\n>    required to compute the inverse or apply Fermat's Little Theorem. (See\r\n>    [BBG+19, Section 5.2].)\r\n> 4. **Code optimization.** [[TODO: What properties of the field make\r\n>    it possible to write faster implementations?]]\r\n\r\nCriterion (3) doesn't seem that important to me any more. I've yet to think of a use case where we want to do inversion in a validity circuit, or for which we don't have an alternative to FLT. In any case, we already can get pretty good performance with a more standard, generic algorithm: https://github.com/abetterinternet/libprio-rs/blob/main/src/fp.rs#L156-L167\r\n\r\nIn addition to dropping (3), we should modify (2) by saying that `2^b` needs to be large enough to accommodate the largest proofs we would ever generate. `b=20` seems like a reasonable minimum.\r\n\r\nFinally, an important criterion to add would be to pick a field size that is as close as possible to a power of 2 without being larger. This helps to minimize the cost of rejection sampling when generating random field elements (see #14).",
      "createdAt": "2021-04-23T20:54:38Z",
      "updatedAt": "2021-10-08T18:00:20Z",
      "closedAt": "2021-10-08T18:00:20Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing this since it's no longer relevant to this document. (It will be relevant to https://github.com/cjpatton/vdaf, however.)",
          "createdAt": "2021-10-08T18:00:18Z",
          "updatedAt": "2021-10-08T18:00:18Z"
        }
      ]
    },
    {
      "number": 39,
      "id": "MDU6SXNzdWU4NzUwMjAwMDU=",
      "title": "Which LICENSE is right?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/39",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "LICENSE (CC) or LICENSE.md ?",
      "createdAt": "2021-05-04T01:01:40Z",
      "updatedAt": "2021-05-04T01:43:31Z",
      "closedAt": "2021-05-04T01:08:53Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "https://github.com/abetterinternet/prio-documents/blob/main/LICENSE",
          "createdAt": "2021-05-04T01:04:15Z",
          "updatedAt": "2021-05-04T01:04:15Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for flagging! I just pushed a fix.",
          "createdAt": "2021-05-04T01:06:22Z",
          "updatedAt": "2021-05-04T01:06:22Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we may actually want something more like CC(0). If we submit to IETF we don't want to carry this copyright notice around.",
          "createdAt": "2021-05-04T01:43:31Z",
          "updatedAt": "2021-05-04T01:43:31Z"
        }
      ]
    },
    {
      "number": 41,
      "id": "MDU6SXNzdWU4NzUwMjE1MTU=",
      "title": "Maybe reconsider the draft name",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/41",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The template things that this is for the \"core\" working group:\r\n\r\n> Discussion of this document takes place on the Constrained RESTful Environments Working Group mailing list (core@ietf.org), which is archived at https://mailarchive.ietf.org/arch/browse/core/.\r\n\r\nThat's probably not what you wanted.",
      "createdAt": "2021-05-04T01:06:11Z",
      "updatedAt": "2021-05-21T16:17:08Z",
      "closedAt": "2021-05-21T16:16:47Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Good catch, thank you @martinthomson!",
          "createdAt": "2021-05-21T16:17:08Z",
          "updatedAt": "2021-05-21T16:17:08Z"
        }
      ]
    },
    {
      "number": 44,
      "id": "MDU6SXNzdWU4NzkyNzA1ODk=",
      "title": "Possible performance impact of decoupled verify / collect stages with heavy hitters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/44",
      "state": "CLOSED",
      "author": "csharrison",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "(moving a comment made in https://github.com/abetterinternet/prio-documents/pull/43 to an issue)\r\nI have a high level concern with the architecture here, following up from our meeting on Wednesday related to heavy hitters.\r\n\r\n**TL;DR evaluating a hierarchy at only a subset of levels (which are specified at query time) is not optimal for performance if the verification step does not have access to those specified levels.**\r\n\r\nFor heavy hitters, we are considering a model where records form a binary tree, where aggregates can be reported at different levels of the tree. There are a few possible ways this could go:\r\n\r\n 1. For every record, verify and evaluate the entire binary tree (i.e. bit-by-bit evaluation)\r\n 2. For every record, verify and evaluate only a subset of the tree (at the limit, this can look more like Prio which only evaluates the last level of the tree)\r\n    2a. The subset of the tree to evaluate is configured within a particular record at record-creation time\r\n    2b. The subset of the tree to evaluate is configured dynamically at collection / aggregation time\r\n\r\nFor our use-case (discussed in #18 (comment)), we are interested in (2b), for a few reasons:\r\n\r\n(1) is potentially inefficient from both a performance and accuracy POV in that it requires more rounds, and additionally when used with differential privacy requires splitting a DP budget across all levels. Some prefixes might just not be interesting to the caller so we shouldn't waste privacy budget / compute on them.\r\n\r\n(2a) is problematic in two dimensions.\r\n-  Embedding configuration in records may compromise user privacy if the configurations can be set by an adversary colluding with one of the aggregators (e.g. you can leak log2(n choose k) bits of information if you allow any subset of k prefixes for an n-bit domain).\r\n- Levels must be pre-specified at record-creation time, which is non-ideal if new information comes up at collection time which informs how aggregation should work (e.g. realizing you have fewer records than expected so it is better to focus on querying only up to a given prefix).\r\n\r\nHowever, I think (2b) runs into a few issues with this architecture that separates verification from collection, in that without knowing the specific levels to evaluate, we run the risk of spending unnecessary compute verifying levels that will never end up being used. The protocol is more efficient (less computation, fewer rounds) if we can verify multiple levels at once (i.e. the levels the collector cares about).\r\n\r\nPossible solutions:\r\n\r\n1. Add a step where the collector and leader communicate prior to verification (but keep verification and collection separate)\r\n2. More tightly couple \"verify\" and \"collect\" stages, so that verification can be done based on communication with the collector.\r\n3. \r\nIn our setting, the \"leader\" and \"collector\" roles are somewhat merged which aligns with option (1), but it seems like a good idea to make the general architecture robust to this use-case.\r\n\r\ncc @schoppmp who is working on our C++ implementation of IDPFs.\r\n",
      "createdAt": "2021-05-07T16:01:38Z",
      "updatedAt": "2021-07-07T21:27:20Z",
      "closedAt": "2021-07-07T21:27:20Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for the feedback @csharrison! I've finally had a chance today to dig into the heavy hitters paper. I agree that the framework described in #43 isn't going to be feasible for heavy hitters. I think your option (2b) is the only way to go, as I explain below.\r\n\r\nFor context, #43 thinks of PA (\"Private Aggregation\") protocols as having three distinct \"phases\": \r\n* **Upload**: Each client uploads its input shares to the aggregators.\r\n* **Verify**: The aggregators verify each client's input.\r\n* **Collect**: Each aggregators add up their verified input shares and emits its output share to the collector.\r\n\r\nIn order to fit heavy hitters into his framework, we could take your option (1) or (2a). However, these options are only  feasible for very small inputs, say, `N=16`, where `N` is the length of each client input in bits. In order to make the protocol feasible for large inputs, heavy hitters involves multiple rounds of the Verify/Collect phases, where the output of the previous Collect phase is used to decide what to do in the next Verify phase. So that we're all on the same page about why interaction is necessary, I've included a brief overview of the protocol below [1]. (Please correct me if I've misunderstood anything!)\r\n\r\nAccounting for this interaction in the protocol spec should be fairly straightforward. The delta, I think, is that there needs to be a way to re-run the Verify/Collect phases on the same inputs, but using different parameters each time, i.e., with different sets of prefixes specified by the collector.\r\n\r\n___\r\n[1] Here's a bird's eye view of the protocol, as I understand it. (I'm ignoring DP for now.) First, (**Upload**) each client generates IDPF shares of its `N`-bit input and sends a share to each aggregator. After all clients have uploaded their IDPF key shares, the collector and aggregators verify and collect the heavy hitters as follows. Let `S(1) = {0,1}`. For each `L=1, ..., N`:\r\n1. (**Verify**) For each client upload:\r\n  a. For each prefix `p` in `S(L)`, each aggregator evaluates its IDPF share on input of `p`.\r\n  b. The aggregators verify (in zero-knowledge) that their IDPF outputs are well-formed, i.e., that only one of the outputs is `1` and the rest are `0`.\r\n2. (**Collect**) \r\n  a. For each verified IDPF output, each aggregator adds up its shares and sends the resulting \"histogram share\" to the collector.\r\n  b. The collector adds the histogram shares together to get the histogram. This encodes the frequency of each prefix `p` in `S(L)` among the client inputs.\r\n  c. The collector assembles `S(L+1)`, the next set of candidate prefixes. To do so, it collects the set `M` of prefixes that appear at least `t` times in the histogram. It lets `S(L+1)` be the set of strings obtained by appending `1` or `0` to each element of `M`.\r\n  d. Finally, the collector  disseminates `S(L+1)` among the aggregators.\r\n\r\nWhat makes this feasible is that the size of each `S(L)` is at most the number of clients divided `t`. \r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2021-05-09T00:47:57Z",
          "updatedAt": "2021-05-09T00:47:57Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton this sounds about right to me. Just a small nit but I wanted to point out that your (Collect) stage, multiple interactions with the collector are not strictly necessary. What you described is a zero-knowledge protocol but alternatively the aggregators could interact to learn the aggregates at each level and proceed down without interaction with the collector.\r\n\r\nWe discussed this variant a few weeks ago, it trades off aggregator knowledge with protocol simplicity (e.g. collectors don't need to follow an interactive protocol).",
          "createdAt": "2021-05-10T22:08:34Z",
          "updatedAt": "2021-05-10T22:08:34Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @cjpatton this sounds about right to me. Just a small nit but I wanted to point out that your (Collect) stage, multiple interactions with the collector are not strictly necessary. What you described is a zero-knowledge protocol but alternatively the aggregators could interact to learn the aggregates at each level and proceed down without interaction with the collector.\r\n\r\nLet me see if I understand you correctly. Concretely, the helper would send the leader its share of the aggregate at the end of each round. Next, the leader would use the aggregate shares to compute the next set of prefixes, then  send this to the helper for the next round.\r\n\r\nIf my understanding is right, then it seems to me that this is the same protocol as [1], but with leader == collector. What do you think?\r\n\r\n> We discussed this variant a few weeks ago, it trades off aggregator knowledge with protocol simplicity (e.g. collectors don't need to follow an interactive protocol).\r\n\r\nMaybe I'm missing something. I'll do my best to read the design doc you shared by the end of the week.\r\n\r\n",
          "createdAt": "2021-05-11T15:07:06Z",
          "updatedAt": "2021-05-11T15:07:06Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> If my understanding is right, then it seems to me that this is the same protocol as [1], but with leader == collector. What do you think?\r\n\r\nYes that's right. I was outlining a \"simpler\" communication model where helpers communicated directly with each other but  in this architecture it would go through the leader.",
          "createdAt": "2021-05-12T16:47:31Z",
          "updatedAt": "2021-05-12T16:47:31Z"
        }
      ]
    },
    {
      "number": 45,
      "id": "MDU6SXNzdWU4ODQ5NjUwNjM=",
      "title": "Mode to make proof optional ",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/45",
      "state": "CLOSED",
      "author": "siyengar",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "Some applications might not need as much protection against malicious users and might be okay with a malicious user changing the values by a larger amount.  They might be using Prio purely for privacy and might have other ways to guard against malicious inputs, for example by verify the user account or by attesting the application. \r\nIt might be useful for the Validity proof to be optional depending on the application. This would reduce overheads perhaps of  communication between servers to validate the proofs and also in communication of the proofs from the client -> server.  \r\n\r\nThese applications could still benefit from:\r\n1. the multi-server deployment model\r\n2. AFE encoding specifications",
      "createdAt": "2021-05-10T19:44:28Z",
      "updatedAt": "2022-03-15T18:51:10Z",
      "closedAt": "2022-03-15T18:51:10Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This seems reasonable to me. Right now we have several pending design decisions floating around, so I'd like to suggest we wait on text until more of the protocol is settled.",
          "createdAt": "2021-05-11T15:09:49Z",
          "updatedAt": "2021-05-11T15:09:49Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "2021/6/16 design call: Something to note here: if we have have a bit in the PA parameters that disables verification, we'll need to make sure this bit is explicitly authenticated.",
          "createdAt": "2021-06-16T17:20:32Z",
          "updatedAt": "2021-06-16T17:41:33Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton can we close this out and track under the VDAF draft? In particular, Prio without verification can just be a different VDAF from the perspective of PPM. I don't think anything else really needs to change.",
          "createdAt": "2022-03-15T18:31:47Z",
          "updatedAt": "2022-03-15T18:31:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Sounds good to me. Here is the issue for reference: https://github.com/cjpatton/vdaf/issues/20",
          "createdAt": "2022-03-15T18:51:10Z",
          "updatedAt": "2022-03-15T18:51:10Z"
        }
      ]
    },
    {
      "number": 48,
      "id": "MDU6SXNzdWU5MTA2OTc4NDI=",
      "title": "Consider making Upload Start an idempotent GET",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/48",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Per #43, the first step in the upload protocol is a POST to the leader server which returns information about helpers. In some yet to be specified cases, the response could include a challenge to be incorporated into reports. However I suspect that in most cases, the response will be static and so an idempotent, cacheable GET would be a better fit, especially since it would save clients a roundtrip to the helper on the hot path of submitting reports, and would make it possible for helpers to implement upload_start by putting some static content in a CDN.\r\n\r\nHTTP GET requests don't have bodies, so we would have to figure out how to encode all the data in the current `PAUploadStartReq` into the `upload_start` URI, which could be done as either path fragments in the URI (i.e., `[leader]/upload_start/[task.version]/[task.id]`) or as query parameters (i.e., `[leader]/upload_start?version=[task.version];id=[task.id]`).\r\n\r\nWe would of course need to make sure that [the semantics of HTTP GET](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET) don't preclude proof systems like the one alleged to exist in section 5.2 of \"BBCp19\".",
      "createdAt": "2021-06-03T17:14:10Z",
      "updatedAt": "2021-07-14T20:35:34Z",
      "closedAt": "2021-07-14T20:35:34Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Also, because the current protocol uses a POST for upload_start, the implication is that for every report they submit, clients need to hit `upload_start` and then `upload_finish`. If we make `upload_start` idempotent and cacheable, then that request can be skipped in the majority of cases. I suppose the significance of this depends on how we expect clients to use PDA: are they going to be continuously streaming hundreds or thousands of reports into the leader as events of interest occur (in which case I think eliminating `upload_start` requests is interesting) or are they expected to accumulate several reports locally before submitting them all in one or a few requests?",
          "createdAt": "2021-06-03T17:29:23Z",
          "updatedAt": "2021-06-03T17:29:23Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree this would be a good change, if we can swing it. It's notable that Prio v2 and HH don't require a random challenge from the leader. But as discussed, this is useful for other proof systems one might want to implement for Prio. For example, see the data type implemented here: https://github.com/abetterinternet/libprio-rs/pull/48. See below for an explanation.\r\n\r\nThere might be other ways to implement the challenge.\r\n* One possibility is to derive the challenge from the TLS key schedule (or from whatever state the client and leader share for the secure channel). @ekr and @tgeoghegan  have both pointed out that adding this dependency on the underlying transport is problematic.\r\n* Another possibility might be to use some external source of randomness that the leader trusts. [DRAND](https://drand.love/) comes to mind, though it is somewhat problematic because clients would effectively use the same randomness for some period of time. The security model of BBCp19 would need to be extended to account for this. (I believe this could be done.)\r\n* Some other solution I'm missing?\r\n\r\n____\r\nThis data type is used to compute the mean and variance of each integer in a sequence of integers. To do so, the client encodes each integer `x` of its input vector as a pair `(x_vec, xx)`, where `x_vec` is the representation of `x` as a bit vector and `xx` is equal to `x^2`. To prove validity of its input, the proof needs to do two things: verify that (a) `x_vec` is indeed a bit vector, and (b) the integer encoded by `x_vec` is the square root of `xx`.\r\n\r\nIn order to check these things efficiently, the proof system uses a random challenge generated by the leader. At a very high level, the client wants to prove that the following boolean conjunction holds: `x_vec[0]` is 1 or 0 *AND* `x_vec[1]` is 1 or 0 *AND* ... *AND* `x_vec` encodes the square root of `xx`. In Prio, we need to represent this conjunction as an arithmetic circuit, which the random challenge helps us do.\r\n\r\nIn a bit more detail now: Let's rewrite this expression as w[0] *AND* w[1] *AND* ... *AND* w[s]. As an arithmetic circuit, this expression is actually `v[0] + v[1] + ... + v[s]`, where `v[i] == 0` if and only if w[i] is true. If the input is valid, then the expression should evaluate to `0`. But because we're talking about field elements, a malicious client may try to play games like setting `v[i] = 2` and `v[j] = -2` so that the sum is still `0`, but the statement is false. To detect this, we actually compute the following expression: `v[0] * r + v[1] * r^2 + ... v[s] * r^(s+1)`, where `r` is a randomly chosen field element. If the statement is true, then the expression evaluates to `0`. If the statement is false, then with high probability, the expression will evaluate to something other than `0`. (This idea comes from Theorem 5.3)",
          "createdAt": "2021-06-09T17:38:15Z",
          "updatedAt": "2021-06-09T17:38:15Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Thank you for the detailed explanalation @cjpatton! Having thought about it more, I suspect an HTTP GET wouldn't be the right way to support proof systems that need a challenge. Assuming that a unique challenge is needed for every report[^1], then intuitively the leader would have to maintain a mapping of reports to issued challenges so that the leader and helper(s) can later use the challenge value when evaluating the client's proof.\r\n\r\nBut now that the leader has to maintain state per-report, I think the upload protocol is missing a notion of a report identifier. The `PAUploadStartReq` contains a `PATask`, but IIUC the `PATask` will be the same across many reports[^2] (e.g., if a browser is sending daily reports on how often users click a button, they use the same `PATask` every time they submit). So `PAUploadStartResp` needs to include a report ID, which must be echoed back to the leader in `PAUploadFinishReq` so that the leader can look up the challenge for the report. \r\n\r\n[^1] Is this true? Or would it suffice for each `PATask` to use the same challenge across multiple reports?\r\n[^2] I notice that the design doc lacks a strong definition of a report and how it relates to tasks, inputs, measurements: #53\r\n because GET should be \"safe\" in the sense of not changing any server state, and it seems like the server would have to maintain a mapping of reports to issued challenges so that the leader and helper can later use the challenge value when evaluating the client's proof.",
          "createdAt": "2021-06-09T19:16:23Z",
          "updatedAt": "2021-06-09T19:16:23Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Continuing on the assumption that `upload_start` causes a state change on the leader, POST is then the appropriate HTTP method to use. [Mozilla sez POST is cacheable](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST), though it's a little unusual. There may be some big drawback to cacheable POST responses, but I think that might let us support proof systems that require live challenges while also eliminating the majority of `upload_start` requests when the proof system is such that the `PAUploadStartResp` will change very rarely.",
          "createdAt": "2021-06-09T19:20:01Z",
          "updatedAt": "2021-06-09T19:20:01Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> But now that the leader has to maintain state per-report, I think the upload protocol is missing a notion of a report identifier. The `PAUploadStartReq` contains a `PATask`, but IIUC the `PATask` will be the same across many reports[^2] (e.g., if a browser is sending daily reports on how often users click a button, they use the same `PATask` every time they submit). So `PAUploadStartResp` needs to include a report ID, which must be echoed back to the leader in `PAUploadFinishReq` so that the leader can look up the challenge for the report.\r\n\r\nThe requirement for the leader to keep state across upload start and upload finish may be avoidable. Suppose the leader has a long-lived PRF key K. It would could choose a random nonce N, compute R = PRF(K, N) as the challenge, and hand (N, R) to the client in its upload start response. The client would then use R to generate the proof and send N to the leader in its upload finish request. The leader could then re-derive R.\r\n\r\n@chris-wood and I kicked around the idea of having the client generate a report ID. I think this may end up being useful, but we weren't sure if it was better to have the leader choose it or the client.\r\n\r\n> [^1] Is this true? Or would it suffice for each `PATask` to use the same challenge across multiple reports?\r\n\r\nTo conform to the security model of BBCp19, each report would need its own challenge. It may be possible to relax this, but this would be risky.",
          "createdAt": "2021-06-09T19:59:59Z",
          "updatedAt": "2021-06-09T19:59:59Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> The requirement for the leader to keep state across upload start and upload finish may be avoidable. Suppose the leader has a long-lived PRF key K. It would could choose a random nonce N, compute R = PRF(K, N) as the challenge, and hand (N, R) to the client in its upload start response. The client would then use R to generate the proof and send N to the leader in its upload finish request. The leader could then re-derive R.\r\n\r\nI like this a lot! I'm a big fan of deterministic derivation schemes like this -- besides reintroducing the possibility of an idempotent GET, eliminating storage requirements for challenges makes it much easier to scale up leaders. If the report IDs are big enough (UUIDs?) we could even use them as `N`.\r\n\r\n> @chris-wood and I kicked around the idea of having the client generate a report ID. I think this may end up being useful, but we weren't sure if it was better to have the leader choose it or the client.\r\n\r\nI would say the leader, because if you let the client do it, then the leader would have to check for things like collisions with existing report IDs or other malicious ID choices. If the leader chooses it, then all it has to do is generate a few bytes of randomness.\r\n\r\n> > [^1] Is this true? Or would it suffice for each `PATask` to use the same challenge across multiple reports?\r\n> \r\n> To conform to the security model of BBCp19, each report would need its own challenge. It may be possible to relax this, but this would be risky.\r\n\r\nI agree.",
          "createdAt": "2021-06-09T22:36:37Z",
          "updatedAt": "2021-06-09T22:36:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@henrycg, we'd appreciate your feedback on this issue because it involves security considerations for ZKP systems on secret-shared data [1]. I'll quickly sum up the problem so that you don't need to worry about the conversation above.\r\n\r\nThe problem at hand regards systems like [1, Theorem 5.3] that require interaction between the prover and verifier. Specifically, we're considering a special case of Theorem 5.3 in which the verifier sends the prover a random \"challenge\" `R` that is used to generate and verify a (non-interactive) FLPCP (call it `P`):\r\n```\r\n                    Prover                               Verifier\r\n                        | (upload start)                  |\r\n                        |-------------------------------> |  Generate R\r\n                        |                      R          |\r\n                        | <------------------------------ |\r\n                        | (upload finish)      P          |\r\n             Generate P |-------------------------------> |  Verify P\r\n             (using R)  |                                 |  (using R)\r\n                        V                                 V\r\n```\r\nWe refer to the prover's first and second messages as \"upload start\" and \"upload finish\" respectively. Each message corresponds to an HTTP request made by a web client to a web server. Ideally, the verifier would be able to handle these requests *statelessly*, i.e., without having to store `R`.\r\n\r\nConsider the following change. Suppose the verifier has a long-term PRF key `K`. In response to an upload start request, it chooses a unique \"upload id\" `N` --- maybe this is a 16-byte integer chosen at random --- and computes `R = PRF(K, N)` and sends `(R, N)` to the prover. In the upload finish request, the prover sends `N` so that the verifier can re-derive `R` instead of needing to remember it:\r\n```\r\n                    Prover                               Verifier^K\r\n                        | (upload start)                  |\r\n                        |-------------------------------> |  Generate N\r\n                        |                     (R, N)      |  Compute R = PRF(K, N)\r\n                        | <------------------------------ |\r\n                        | (upload finish)     (P, N)      |\r\n             Generate P |-------------------------------> |  Verify P\r\n             (using R)  |                                 |  (using R = PRF(K, N))\r\n                        V                                 V\r\n```\r\nThe (potential) problem with this variant is that a malicious client is able to replay a challenge as many times as it wants. This behavior isn't accounted for in Definition 3.9, so when you try to \"compile\" this system into a ZKP on secret-shared data as described in Section 6.2 and consider its security with respect to Definition 6.4 (Setting I), you can no longer appeal to Theorem 6.6 to reason about its security.\r\n\r\nOur question is therefore: **What impact do you expect this change to have on the soundness of the system?** Can you think of an attack that exploits the fact that challenges can be replayed? My expectation is that the problem is merely theoretical, i.e., we ought to be able to close the gap with an appropriate refinement of Definition of 3.9 and reproving Theorem 5.3.\r\n\r\n[1] https://eprint.iacr.org/2019/188\r\n",
          "createdAt": "2021-06-18T00:11:18Z",
          "updatedAt": "2021-06-18T17:59:55Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for looping me in. In the discussion below, I am assuming that the client sends its data vector (called \"x\" in the paper [1]) at the same time as it sends its proof to the verifier. Even if the client sends its data vector in the first message, it seems that the client can reuse an (R,N) pair from upload request 1  to later submit upload request 2, in which case the client sees (R, N) before it chooses its data vector and proof. So I think we can assume that the client's data vector and proof can depend on the verifier's random value R. (I'll call this value \"r\" to match the paper.)\r\n\r\nFor soundness: the client/prover must commit to its data vector _before_ it learns the verifier's random value r. If the prover can choose its data value and proof in a way that depends on the verifier's randomness r, the soundness guarantee no longer holds. There is also an efficient attack. So I suspect that the stateless-verifier optimization is unsound.\r\n\r\nThe attack works as follows: given r, the verifier finds a non-satisfying vector x such that \\sum_i r_i C(A_i(x)) = 0, again using the notation from Theorem 5.3 in the paper [1]. For the circuits C used in most applications, it will be easy to find such a vector x. Then, the prover constructs a proof that x that  \\sum_i r_i C(A_i(x)) = 0. The proof is valid, so the verifier will accept always.\r\n\r\n\r\nTo remove interaction, the Fiat-Shamir-like technique in Section 6.2.3 is probably the best way to go. We only sketched the idea in the paper (without formal analysis or proof) but if it's going to be important for your application, I'm happy to sanity-check the protocol you come up with.\r\n\r\nAnother idea\u2014the details of which I have not checked carefully\u2014would be to modify your protocol as follows:\r\n\r\n1. The prover sends its data vector x to verifiers, split into k shares x_1, ..., x_k\u2014one for each of the k verifiers.\r\n2. Each verifier holds a PRF key k_i computes r_i = PRF(k_i, x_i) and they return r = r_1 + ... + r_k to the prover. The verifiers store no state.\r\n3. The prover computes the proof \\pi and sends (x_1, \\pi_1), ..., (x_k, \\pi_k) to the verifiers\u2014again, one for each of the k verifiers. Each verifier computes r_i = PRF(k_i, x_i), they jointly compute r = r_1 + ... + r_k, and check the proof using randomness r.\r\n\r\nThe Fiat-Shamir-like approach seems slightly cleaner to me, since it only requires a single prover-to-verifier message.\r\n\r\nEither way, please let me know if I misunderstood your question or if anything else here is unclear.\r\n\r\n[1] https://eprint.iacr.org/2019/188",
          "createdAt": "2021-06-18T20:28:39Z",
          "updatedAt": "2021-06-18T20:28:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for the nice explanation, Henry. It's perfectly clear and answers my question. It also points to something that I, for one, missed the importance of: the need for the prover to \"commit\" to the input shares before generating the r-dependent proof. I appreciate you pointing this out.\r\n\r\nThe only requirement for making upload start not idempotent is so that the randomness r can be sent to the client. It would be nice to do away with this requirement. I think the way forward is using the Fiat-Shamir technique. @henrycg, I'll get cracking on this and let you know when I have something for you to look at.",
          "createdAt": "2021-06-18T21:15:19Z",
          "updatedAt": "2021-06-18T21:15:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@henrycg, I believe the change is something like this. Let `H` be a hash function with range `\\bits^h`. Suppose there are `k` verifiers. The prover proceeds as follows:\r\n- Splits its inputs `x` into shares `x_1, ..., x_k`.\r\n- For each `i \\in \\{1, ..., k\\}`, do:\r\n  - Choose blinding factor `V_i` from `\\bits^h` at random.\r\n  - Let `R_i \\gets H(i, x_i, V_i)`.\r\n- Set `R \\gets R_1 \\xor ... \\xor R_k`.\r\n- Derive field element `r` from `R` (e.g., seed a PRNG with `R` and map the output to a field element by rejection sampling on chunks of the output).\r\n- Generate the proof `\\pi` using `x` and `r`.\r\n- Split `\\pi` into shares `\\pi_1, ..., \\pi_k`.\r\n- For each `i \\in \\[1, ..., k\\}` send `(x_i, \\pi_i, V_i)` to the `i`-th verifier.",
          "createdAt": "2021-06-21T16:58:13Z",
          "updatedAt": "2021-06-21T16:58:13Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, this looks right to me!",
          "createdAt": "2021-06-21T20:23:30Z",
          "updatedAt": "2021-06-21T20:23:30Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As of https://github.com/abetterinternet/prio-documents/pull/79, the upload start request no longer exists. When we're ready to specify Prio, we'll want to include the Fiat-Shamir approach for protocols that use joint randomness. I've noted this and am closing the issue.",
          "createdAt": "2021-07-14T20:35:34Z",
          "updatedAt": "2021-07-14T20:35:34Z"
        }
      ]
    },
    {
      "number": 49,
      "id": "MDU6SXNzdWU5MTA3MDE5MzQ=",
      "title": "Rotation of HPKE keys for encrypting helper shares",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/49",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "#43 includes a detailed specification of how helpers advertise HPKE configs for clients to use when encrypting helper shares. We should consider how key rotation works.",
      "createdAt": "2021-06-03T17:18:30Z",
      "updatedAt": "2021-07-14T19:28:01Z",
      "closedAt": "2021-07-14T19:28:01Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Might this just be deployment specific, or should all helpers do the same thing?\r\n\r\nWhat we (i.e., Cloudflare) do for [the ECH extension for TLS](https://datatracker.ietf.org/doc/html/draft-ietf-tls-esni-10) is rotate the HPKE config every hour. To add reliability in case a client has an old key, we keep around a rotating set of keys from the last hour or so. The config id is used to determine which key was used to encrypt the payload.",
          "createdAt": "2021-06-09T17:44:09Z",
          "updatedAt": "2021-06-09T17:53:00Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "This seems deployment specific.\n\nOn Wed, Jun 9, 2021 at 10:44 AM Christopher Patton ***@***.***>\nwrote:\n\n> Might this just be deployment specific, or should all helpers do the same\n> thing?\n>\n> What we do for the ECH extension for TLS\n> <https://datatracker.ietf.org/doc/html/draft-ietf-tls-esni-10> is rotate\n> the HPKE config every hour. To add reliability in case a client has an old\n> key, we keep around a rotating set of keys from the last hour or so. The\n> config id is used to determine which key was used to encrypt the payload.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/issues/49#issuecomment-857903832>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIKL7RRTTFTPY62E4EDTR6R7RANCNFSM46BGCH5Q>\n> .\n>\n",
          "createdAt": "2021-06-09T17:49:06Z",
          "updatedAt": "2021-06-09T17:49:06Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree that the rotation _mechanism_ is deployment specific, but how the protocol reacts when a key rotation event occurs, possibly causing a mismatch between client and helper keys, should be in scope. (I assumed this issue was meant to address the latter.)",
          "createdAt": "2021-06-10T12:21:18Z",
          "updatedAt": "2021-06-10T12:21:18Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "2021/06/16 design call: The servers need to agree, roughly, on how long reports are valid. (Once a helper rotates its key, reports encrypted under the previous key won't be decryptable.) Should the HPKE config lifetime be a paramater (e.g., of PAParam)? Does the report need to carry this parameter as well?",
          "createdAt": "2021-06-16T17:40:50Z",
          "updatedAt": "2021-06-16T17:46:26Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Do they need to agree, or should the data just naturally become invalid once rotation occurs? ",
          "createdAt": "2021-06-16T18:08:30Z",
          "updatedAt": "2021-06-16T18:08:30Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "It would be nice if submissions were rejected if they used invalid keys\n\nOn Wed, Jun 16, 2021 at 11:08 AM Christopher Wood ***@***.***>\nwrote:\n\n> Do they need to agree, or should the data just naturally become invalid\n> once rotation occurs?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/issues/49#issuecomment-862599002>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIKJTDGWHD6D6Y7VSBLTTDSC5ANCNFSM46BGCH5Q>\n> .\n>\n",
          "createdAt": "2021-06-16T18:20:29Z",
          "updatedAt": "2021-06-16T18:20:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Clients reports encrypted with a public key that's too old will indeed be rejected by helper. The validity period we're talking about is of interest for the collector in a protocol like Hits where the collector makes multiple, interactive requests to the aggregators: collectors need to have a sense of how long after the verify phase they can expect to be able to make collect requests before helpers are unable to decrypt the reports. i.e., collectors may not be able to make collect queries over reports that are more than 30 days old. In some cases, a validity period of hours could suffice if the successive queries are being made by an automated system, but maybe the queries are being formulated by a team of human analysts who need a day or so to consider what the n+1th query they want to send is based on the results of the nth query.",
          "createdAt": "2021-06-16T18:36:58Z",
          "updatedAt": "2021-06-16T18:36:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we might be converging on something like this: There's no explicit notion of report lifetime that's enforced by the protocol. Perhaps what we need instead is a way for the helper to tell the leader which reports are invalid and should be pruned. (Note that this is not unrelated to #57.)",
          "createdAt": "2021-06-16T19:37:33Z",
          "updatedAt": "2021-06-16T19:37:33Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Any solution to #57 will also solve this problem. I'm going to make a call and say this issue is a duplicate and can be closed.",
          "createdAt": "2021-07-14T19:28:01Z",
          "updatedAt": "2021-07-14T19:28:01Z"
        }
      ]
    },
    {
      "number": 50,
      "id": "MDU6SXNzdWU5MTA3MTM0OTI=",
      "title": "Specify how clients choose helpers",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/50",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Currently, clients obtain a list of supported helpers from the leader. They are then expected to select some subset of the supported helpers to run the protocol with. Clients must be able to establish trust with helpers independently from the leader to prevent leaders from impersonating helpers and obtaining all the shares of an input.\r\n\r\nWhile clients will establish an authenticated channel with helpers (typically over TLS), clients still need some way to know what helpers they trust (in the TLS case, what SANs they trust to run a helper).\r\n\r\nThe simplest way forward would be to require clients to maintain their own list of trusted helpers and intersect that with the list of supported helpers provided by the leader.",
      "createdAt": "2021-06-03T17:35:44Z",
      "updatedAt": "2021-07-14T20:30:48Z",
      "closedAt": "2021-07-14T20:30:48Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "An even simpler solution (though it may not be desirable from a security standpoint) is to lean on X.509 and trust the client's set of root certificates. That is, if the client can verify the helper's SAN using one of its root certs, then it trusts whatever public key it gets from the helper over the secure channel.",
          "createdAt": "2021-06-09T17:41:07Z",
          "updatedAt": "2021-06-09T17:41:07Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is now obsolete because we have agreed that PAParams will only have a single helper in them.",
          "createdAt": "2021-07-14T20:30:48Z",
          "updatedAt": "2021-07-14T20:30:48Z"
        }
      ]
    },
    {
      "number": 51,
      "id": "MDU6SXNzdWU5MTY0MDQ4MTA=",
      "title": "Requirements for Collect (or, Revisiting the Pipeline Model)",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/51",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [],
      "body": "At the moment, the spec thinks of a PA task as a one-way data-processing pipeline with three phases: upload, in which clients send inputs to the leader; verify, in which the leader and helper verify each input; and collect, in which the leader and helper aggregate inputs and push the output to the collector. The upload and verify phases are currently specified; our next task is to specify the collect phase.\r\n\r\nThis pipeline model is a perfectly fine way of thinking about Prio. (Although there are ways in which one might use Prio metrics that don't quite fit into this model; see #27.) However, HH doesn't fit into this model at all (see #44). It's apparently necessary to re-think the communication model for PA protocols. @chris-wood and I were kicking this around and wanted to share some thoughts.\r\n\r\nFirst off, let's try to enumerate the relevant requirements for the PA framework.\r\n\r\n1. The helper is stateless.\r\n2. Aggregators (i.e., the leader and helper) enforce the batch size (#15): As long as one of the aggregators is honest, the collector should be unable to see outputs aggregated over a small amount of inputs.\r\n3. Aggregators should never learn more information about outputs than the collector. Moreover, it should be possible to design a PA protocol in which neither the aggregator sees the output in the clear. (This is possible for Prio. In HH, the aggregators inevitably learn the set of candidate prefixes at each round, which leaks some information about the output.)\r\n4. Implementing HH requires O(n) space for the leader, where n is the number of inputs. However, it should be possible for the leader to implement Prio with O(1) space.\r\n5. The aggregators verify and aggregate inputs over multiple rounds. In each round, the collector specifies the parameters that will be used. (In HH, the parameters are the set of candidate prefixes.)\r\n6. [Needs discussion] Not every PA protocol will verify inputs (#45).\r\n\r\nWe can't satisfy all of these requirements in the upload->verify->collect model. Instead, we might think of a PA task as two concurrent processes running asynchronously:\r\n* **Upload process:** Clients *push* inputs to the leader via the upload start and upload finish requests.\r\n* **Collect process:** Collector *pulls* outputs from the leader via a yet-to-be-specified collect request. The request carries the parameters the aggregators will use for verification and aggregation.\r\n\r\nIn this communication model, the collector may attempt to pull the output for a PA task whenever it likes. A pull will only be successful if the aggregators have verified and aggregated enough inputs.\r\n\r\nIf/when inputs are verified is up to the PA protocol, but must occur before the leader can respond to a collect request. (In Prio, reports can verified as soon as they're uploaded; in HH, verification can only occur once all the reports have been uploaded and the collector has issued a collect request.)\r\n\r\nThe helper will keep a long-term symmetric key for encrypting the state that's managed by the collector. This ensures privacy while allowing the helper to be stateless.\r\n\r\nAt the end of aggregation, each of the aggregators holds a share of the output. Before responding to a collect request, the leader requests the helper's output share. To ensure that the leader doesn't see the output in the clear, the helper encrypts its share under the collector's HPKE public key.\r\n\r\nOpen questions\r\n1. Should collect requests be idempotent?\r\n2. A collect request may take a long time. (For HH, the latency may be several minutes!) Is HTTP the right application?\r\n3. How does the helper learn the collector's HPKE public key? The collector needs to prove ownership of the corresponding secret key in some way.",
      "createdAt": "2021-06-09T16:40:23Z",
      "updatedAt": "2021-08-11T14:01:00Z",
      "closedAt": "2021-08-11T14:01:00Z",
      "comments": [
        {
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "body": "Perhaps a slight adjustment to upload-verify-collect would be to split `collect` into `aggregate` and `publish`/`reconstruct` like the v1/v2 protocols. It would prevent overloading the term `collect` e.g.:\r\n\r\nPrio: upload -> verify -> aggregate -> publish\r\nHH: upload -> (verify -> aggregate -> verify -> ... -> aggregate) -> publish\r\n\r\nI've been thinking about the protocol through the lens of map-reduce pipeline with network storage (e.g. Spark and S3), where computation is represented as a DAG. It looks the like HH computation would fit neatly into a batched pipeline, given a fixed number of verify-aggregate rounds. \r\n\r\n> Should collect requests be idempotent?\r\n\r\nWhat is the benefit of idempotency in this context? This is so the results of earlier rounds can be used as inputs into later rounds to improve performance?\r\n\r\n> A collect request may take a long time. (For HH, the latency may be several minutes!) Is HTTP the right application?\r\n\r\nOne way to deal with the long latency time is to use a task queue e.g. rabbitmq to wait for responses. The collector would then poll the task queue for completion before moving onto the next step (pushing the results to the leader for another round?). \r\n\r\n> How does the helper learn the collector's HPKE public key? The collector needs to prove ownership of the corresponding secret key in some way.\r\n\r\nIs this not solved as part of the upload phase? Or does the upload phase only authenticate the leader with the helpers?",
          "createdAt": "2021-06-09T18:41:50Z",
          "updatedAt": "2021-06-09T18:41:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Perhaps a slight adjustment to upload-verify-collect would be to split `collect` into `aggregate` and `publish`/`reconstruct` like the v1/v2 protocols. It would prevent overloading the term `collect` e.g.:\r\n> \r\n> Prio: upload -> verify -> aggregate -> publish\r\n> HH: upload -> (verify -> aggregate -> verify -> ... -> aggregate) -> publish\r\n> \r\n> I've been thinking about the protocol through the lens of map-reduce pipeline with network storage (e.g. Spark and S3), where computation is represented as a DAG. It looks the like HH computation would fit neatly into a batched pipeline, given a fixed number of verify-aggregate rounds.\r\n\r\nThis seems reasonable to me. The one downside I see is that the number of verify-aggregate rounds may not be fixed. For example, the collector might want to start with a set of candidate prefixes. By doing so, it can reduce the number of rounds significantly in applications where the search space can be constrained in some way. (I'd love to hear @csharrison's thoughts on this.)\r\n\r\nCan you be more specific on how this fits into a map-reduce pipeline? My guess is that you'd have a map-reduce for each round. It seems to me like this is still possible.\r\n\r\n> > Should collect requests be idempotent?\r\n> \r\n> What is the benefit of idempotency in this context? This is so the results of earlier rounds can be used as inputs into later rounds to improve performance?\r\n\r\nThis question is about usability, not performance. Imagine the collector is some person on their laptop, interacting with the system. Should the protocol ensure that they can re-run a query multiple times and get back the same result? Or is this up to the implementation?\r\n\r\n> > How does the helper learn the collector's HPKE public key? The collector needs to prove ownership of the corresponding secret key in some way.\r\n> \r\n> Is this not solved as part of the upload phase? Or does the upload phase only authenticate the leader with the helpers?\r\n\r\nSorry, this is a bit confusing. The upload process involves the helper's public key (used by the clients); the collector phase involves collector's public key (used by the helper).\r\n\r\n",
          "createdAt": "2021-06-09T19:08:48Z",
          "updatedAt": "2021-06-09T19:08:48Z"
        },
        {
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "body": "> Can you be more specific on how this fits into a map-reduce pipeline? My guess is that you'd have a map-reduce for each round. It seems to me like this is still possible.\r\n\r\nEach verification/aggregation step would just map over the data, joining appropriately with the outputs of the previous step. The reduce step is done at the very end (with the collect phase). Iterative algorithms work fine with map-reduce (e.g. power iteration with thresholds for eigenvector computation). It's trickier when the workflow involves multiple parties because you have to persist intermediate results to a shared location, but it's certainly doable.\r\n\r\nCoordinating work is the challenge, a task queue or something similar in the collector/leader would abstract some of those details. \r\n\r\n> Should the protocol ensure that they can re-run a query multiple times and get back the same result? Or is this up to the implementation?\r\n\r\nIt would be difficult to guarantee idempotency if differential privacy were applied server-side, something which has been mentioned before.",
          "createdAt": "2021-06-09T20:00:51Z",
          "updatedAt": "2021-06-09T20:00:51Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I've been thinking about the protocol through the lens of map-reduce pipeline with network storage (e.g. Spark and S3), where computation is represented as a DAG. It looks the like HH computation would fit neatly into a batched pipeline, given a fixed number of verify-aggregate rounds.\r\n\r\n@acmiyaguchi I think the difference being proposed in this issue is, rather than have the servers drive aggregation (by running some map and reduce functions), it has the collector drive aggregation. This seems to have the benefit of turning aggregators into very dumb entities that hold reports and compute functions over them. (One could still build the map-reduce pipeline that sends aggregate data to some sink this way by having the collector drive aggregation and write it to a sink.)",
          "createdAt": "2021-06-09T21:10:26Z",
          "updatedAt": "2021-06-09T21:10:26Z"
        },
        {
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "body": "At a high level, there's still a comparable amount of coordination and delegation. Scheduling is nontrivial (\"A collect request may take a long time\") and will probably involve some task management in the collector.\r\n\r\nFor reference, here's the cluster management model of Spark. I don't think it's too far fetched to imagine the collector as a driver program and stateless cloud functions as executors. \r\n\r\n![image](https://user-images.githubusercontent.com/3304040/121431113-d5a4f100-c92d-11eb-92df-5a35b15bb88f.png)\r\nhttps://spark.apache.org/docs/latest/cluster-overview.html\r\n\r\nThis is (mostly?) orthogonal to the data flow e.g. upload -> verify -> aggregate -> publish.\r\n",
          "createdAt": "2021-06-09T21:50:24Z",
          "updatedAt": "2021-06-09T21:50:24Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> At a high level, there's still a comparable amount of coordination and delegation. Scheduling is nontrivial (\"A collect request may take a long time\") and will probably involve some task management in the collector.\r\n\r\nYep, they seem to match pretty nicely!\r\n\r\n> This is (mostly?) orthogonal to the data flow e.g. upload -> verify -> aggregate -> publish.\r\n\r\nI need to think about this more, but in my mental model, the protocol looks very different if, say, the collector drives things instead of the being a dumb consumer of data output from the aggregators, even though the flow of data is mostly the same. ",
          "createdAt": "2021-06-09T21:58:44Z",
          "updatedAt": "2021-06-09T21:58:44Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "A few thoughts here.\r\n\r\nAs a preliminary, I want to talk about the commercial relationships\r\nrather than the network relationships.  First, let's call the\r\n\"Customer\" the entity which is paying for the service and presumably\r\n(1) is deploying the clients and (2) is interested in the data. So, in\r\nthe case where we are doing Fx telemetry, Mozilla would be the\r\nCustomer. The Customer contracts with 1 or more Operators who\r\ncollectively run the service.\r\n\r\nIn the simplest conceptual topology, there are two Operators, call\r\nthem A and B, each of whom runs an Aggregator. One could also have a\r\nsituation in which the Customer contracts with one Operator and then\r\nruns the second Aggregator themselves, but that seems like a\r\ncase that is likely to be less widely used. In the former scenario,\r\n\r\n\r\nWith that as background:\r\n\r\n1. I think this is going to work best if the PA system presents as a\r\n   single service to the common Customer, at least technically.  By\r\n   this I mean that once it's set up, the Customer/Collecter should\r\n   only have to talk to one system (presumably operated by the same\r\n   entity that operates the Leader). The Leader would in turn talk to\r\n   the other aggregators. This does not preclude concealing the\r\n   results from the Leader, but merely requires that the aggregated\r\n   shares be encrypted so that the Leader cannot see them.\r\n\r\n2. We have been a bit vague about what the Leader/Collector interface\r\n   is, but I think it's most reasonable to think of it as a standard\r\n   Web service API. This of course means that if you just want\r\n   a dashboard a la Amplitude, someone will have to implement that,\r\n   but the most flexible way to do so is as a system that talks\r\n   to the Leader over that API. It's been noted that there are\r\n   times when things are long-running, but there are well-established\r\n   mechanisms for Web services APIs to handle this kind of thing,\r\n   such as by the Collector having a notification endpoint or\r\n   polling.\r\n\r\n3. There are multiple kinds of state to be stored. My expectation\r\n   is that the Leader operates as effectively a query oracle,\r\n   but that the Collector be responsible for the direction\r\n   and order of queries.\r\n\r\n   - The Leader will be responsible for storing the reports and\r\n     whatever memoized information is necessary between queries\r\n     (e.g., suppose for query A you need to verify X, Y, Z\r\n     and then for B you need to verify X, Y, W, the Leader\r\n     could memoize X and Y.). Some of this can be soft state\r\n     and the Leader can recompute as needed.\r\n\r\n   - The Collector is responsible for storing its own context\r\n     and for knowing what's already been asked and what is\r\n     to be asked next.\r\n\r\n   - The Helpers are stateless.\r\n   \r\n   Note that this doesn't address the question of any limits on\r\n   what queries can be made (e.g., for DP). Those have to be\r\n   enforced at the Leader.\r\n\r\n",
          "createdAt": "2021-06-11T13:39:09Z",
          "updatedAt": "2021-06-11T13:39:09Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Two quick comments:\r\n> This seems reasonable to me. The one downside I see is that the number of verify-aggregate rounds may not be fixed. For example, the collector might want to start with a set of candidate prefixes. By doing so, it can reduce the number of rounds significantly in applications where the search space can be constrained in some way. (I'd love to hear @csharrison's thoughts on this.)\r\n\r\nYes I think we want the computational model to be robust enough to handle dynamic # of rounds, although maybe it is sufficient to just have the \"stopping point\" of the protocol be dynamic to fail early when there is no point further expanding the domain.\r\n\r\nTo ekr's point on helpers being stateless and\r\n> Note that this doesn't address the question of any limits on\r\nwhat queries can be made (e.g., for DP). Those have to be\r\nenforced at the Leader\r\n\r\nI wanted to point out that this choice affects the security guarantees of DP since we wouldn't be ensuring DP for repeated queries / data-stuffing in the 2-pc model but by a single actor (the leader). Within the MPC system we'd only be guaranteeing per-query DP.\r\n\r\ncc @hostirosti who is starting to look at these problems on our side as well.",
          "createdAt": "2021-06-11T15:24:22Z",
          "updatedAt": "2021-06-11T15:24:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Uh, not sure how that got closed! Re-opening.",
          "createdAt": "2021-06-17T19:56:29Z",
          "updatedAt": "2021-06-17T19:56:29Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi folks, PR #59 tries to specify data collection in a way that satisfies the requirements enumerated here. Also, I believe this closes #44.",
          "createdAt": "2021-06-21T16:38:51Z",
          "updatedAt": "2021-06-21T16:38:51Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As discussed on the 2021/7/14 call, all that's left for this issue is to enumerate the design constraints discussed here in the spec. (@chris-wood will file a PR.)",
          "createdAt": "2021-07-14T20:25:02Z",
          "updatedAt": "2021-07-14T20:25:02Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "#93  enumerates the design constraints considered here. It's time to close out this sucker.",
          "createdAt": "2021-08-11T14:01:00Z",
          "updatedAt": "2021-08-11T14:01:00Z"
        }
      ]
    },
    {
      "number": 53,
      "id": "MDU6SXNzdWU5MTY1NzcxODI=",
      "title": "Terminology section should define \"report\" ",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/53",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The design document discusses \"reports\" which are a group of measurements being submitted to a DPA instantiation. There should be an entry in the \"Terminology\" section that explicitly defines a report and how it relates to \"measurements\" and \"inputs\".",
      "createdAt": "2021-06-09T19:07:09Z",
      "updatedAt": "2021-06-09T19:59:34Z",
      "closedAt": "2021-06-09T19:59:34Z",
      "comments": []
    },
    {
      "number": 57,
      "id": "MDU6SXNzdWU5MjI5Mjc2NTA=",
      "title": "Propagation of errors from helper/leader to collector",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/57",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "Client reports could be rejected by the leader and helper for any number of reasons (decryption failure, proof didn't check out, malformed data, etc.). Since client participation in the protocol ends after the upload phase, there is no way for the client to find out about it (even if we move as much validation as possible to the upload phase, I believe there will always be failures that can't be detected until verify). The protocol should provide a way for leaders and helpers to inform collectors of errors so that misbehaving clients can be fixed.",
      "createdAt": "2021-06-16T18:02:36Z",
      "updatedAt": "2023-05-26T19:47:51Z",
      "closedAt": "2023-05-26T19:47:51Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Some prior art here from Prio v2: https://github.com/abetterinternet/prio-server/issues/6. We never implemented this, except that a list of UUIDs of packets (packets are analogous to a report in Priov3) rejected due to bad proofs is included with the sum parts transmitted to the portal server.",
          "createdAt": "2021-06-16T18:03:44Z",
          "updatedAt": "2021-06-16T18:03:44Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "A specific error we need to account for here is decryption failure due to a stale HPKE config. (This was discussed originally in #49.)",
          "createdAt": "2021-07-14T20:31:23Z",
          "updatedAt": "2021-07-14T20:31:23Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Side-note: As long as the aggregators can decrypt their shares they can run validation even if a report would not be included in a batch because it has expired. I doubt that it would be worth the effort but since Max was asking about detecting when validation is too restrictive on the mailing list it might make sense in some cases.",
          "createdAt": "2022-04-12T10:43:00Z",
          "updatedAt": "2022-04-12T10:43:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As of DAP-01 we have enumerated in the spec the reasons for which a report can be rejected, so it's may be worth dusting off this conversation.\r\n\r\n@tgeoghegan what would be useful here? Do we need to change the wire protocol? What about a JSON blob in the HTTP response header for the CollectResp that includes some telemetry, e.g., the number of times each rejection type occurred for the batch?",
          "createdAt": "2022-09-13T02:49:58Z",
          "updatedAt": "2022-09-13T02:49:58Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think if we do anything here, it would be to have the leader nest the helper's RFC 7807 problem document in the problem document that the leader presents to the collector in response to a request to a collect URI. So I think maybe we'd have to define a problem type like \"helperRefusedRequest\".\r\n\r\nHowever, I also think we should avoid prescribing anything unless necessary. I think the only case where we might need to relay an error like this is if a collector polls a collect URI, which causes a leader to make an `AggregateShareReq` to the helper, and then the helper fails to service that request. So under what circumstances would that happen, and what would we want the collector to do about it?\r\n\r\nIf the `AggregateShareReq` fails because of something like the helper having crashed or being unavailable, then there's nothing for the collector to do but try again later, and the leader providing an HTTP 500 suffices for that, meaning we don't need to spend more protocol text spelling out a more specific error.\r\n\r\nThe other failure scenario that comes to mind is if a helper rejects an `AggregateShareReq` because of something like insufficient reports, or some other kind of privacy violation. But if that's the case, then wouldn't the leader have rejected the `CollectReq` before the helper got a chance to reject the `AggregateShareReq`? This scenario points to some kind of disagreement between the leader and helper on the state of the world, probably due to a bug in one or the other. But again, what's the collector meant to do here but try again later?\r\n\r\nLong story short, I think we should say nothing until we come across a scenario where propagating an error actually informs the behavior of a protocol actor. The nice thing about RFC 7807 is that it [allows extensions to errors](https://www.rfc-editor.org/rfc/rfc7807#section-3.2), which means that implementations are free to construct complex error objects that provide whatever context they want. With that in mind, I'd support closing this issue.",
          "createdAt": "2022-09-14T23:23:35Z",
          "updatedAt": "2022-09-14T23:23:35Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This was mentioned in some other issue: Assuming that the Collector probably also provides the Client, it would be good to tell the Collector things like \"the client is using the wrong HPKE config\" or \"I am receiving a lot of requests for your tasks which do not aggregate into any reports because of their metadata\". I probably wouldn't prescribe this kind of UX issue on the protocol level, though.",
          "createdAt": "2022-09-15T07:57:58Z",
          "updatedAt": "2022-09-15T07:57:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@simon-friedberger something we might do in our own deployments is have the Aggregators expose something like Prometheus metrics to the Collector. Would that be sufficient?\r\n\r\nDoes anyone still think we need language in the DAP spec to cover this issue?",
          "createdAt": "2023-03-07T01:14:42Z",
          "updatedAt": "2023-03-07T01:14:42Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "The only other option would be to have the client report errors via non-DAP telemetry but this doesn't work if the leader doesn't immediately process a report (even decrypting is optional) or if the helper encounters an error.\r\n\r\nI agree that a separate API of sorts is probably the right choice but we should at least think hard about specifying it. It would be problematic if the Client+Collector participants have to e.g. talk to different APIs for Leader & Helper to get this information. Or there might be errors for which the Leader & Helper need to talk to each other to get good debug output.",
          "createdAt": "2023-03-07T10:41:27Z",
          "updatedAt": "2023-03-07T10:41:27Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This issue has been around for a long time and I don't think anyone has made a compelling case for including aggregator to collector error reporting mechanisms in-band in DAP. I'm closing this issue as we don't plan to take action on it in DAP. Naturally this doesn't rule out implementations of DAP from setting up their own bespoke error reporting mechanisms.",
          "createdAt": "2023-05-26T19:47:51Z",
          "updatedAt": "2023-05-26T19:47:51Z"
        }
      ]
    },
    {
      "number": 58,
      "id": "MDU6SXNzdWU5MjI5MzQ2NzQ=",
      "title": "Consider using OHTTP ",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/58",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "Prio v2 incorporates ingestion servers which sit between clients and aggregators. The ingestion servers:\r\n\r\n- verify attestations from a trusted computing base in the client (i.e., secure enclave or TrustZone), making it harder for malicious clients to submit invalid inputs;\r\n- remove any potentially identifying metadata from client reports before forwarding them to aggregators.\r\n\r\n[Oblivious HTTP](https://datatracker.ietf.org/doc/draft-thomson-http-oblivious/) \"allows a client to make multiple requests of a server without the server being able to link those requests to the client or to identify the requests as having come from the same client.\" We should see if we can integrate OHTTP into the DPA protocol as an optional extension that achieves the goals of the Prio v2 ingestion servers.",
      "createdAt": "2021-06-16T18:09:29Z",
      "updatedAt": "2021-07-28T21:24:38Z",
      "closedAt": "2021-07-28T21:24:38Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this issue is orthogonal to the PA protocol spec, but is certainly a useful enhancement for deployments.",
          "createdAt": "2021-06-25T21:44:14Z",
          "updatedAt": "2021-06-25T21:44:14Z"
        }
      ]
    },
    {
      "number": 61,
      "id": "MDU6SXNzdWU5Mjc3MDgxNTE=",
      "title": "Align spec with ACME (was \"Version all API endpoints\")",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/61",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [],
      "body": "The protocol doc specifies HTTP endpoints like `[leader]/upload_start` or `[helper]/key_config`. We should make sure to insert a protocol version in the endpoints so that we can iterate on this in the future. i.e., `[leader]/v1/upload_start`.",
      "createdAt": "2021-06-22T23:27:45Z",
      "updatedAt": "2023-05-26T19:49:54Z",
      "closedAt": "2023-05-26T19:49:54Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I was thinking the same thing :)\r\n\r\nIn fact, we might always include the version and task id in the request URI. For example:\r\n\r\nexample.com/[version]/[task_id]/upload_start",
          "createdAt": "2021-06-22T23:32:46Z",
          "updatedAt": "2021-06-22T23:32:46Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't believe that this is considered current best practice. See, rather,\nthe structure used by ACME:\n\nhttps://tools.ietf.org/rfcmarkup?doc=8555#section-7.1\n\nOn Tue, Jun 22, 2021 at 4:32 PM Christopher Patton ***@***.***>\nwrote:\n\n> I was thinking the same thing :)\n>\n> In fact, we might always include the version and task id in the request\n> URI. For example:\n>\n> example.com/[version]/[task_id]/upload_start\n> <http://example.com/%5Bversion%5D/%5Btask_id%5D/upload_start>\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/issues/61#issuecomment-866404758>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIOWFWVODVT55LTWJALTUEMSRANCNFSM47EUXJWQ>\n> .\n>\n",
          "createdAt": "2021-06-22T23:52:15Z",
          "updatedAt": "2021-06-22T23:52:15Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I don't know if this is recorded anywhere, but that is a bad way to version APIs.  Mostly because it is unnecessary.  Just change the `[leader]` part to something different.  [Don't standardize URIs](https://datatracker.ietf.org/doc/html/rfc8820).  Our services team has a simple policy: if you are going to make an incompatible change,  spin up an entirely new endpoint on a new domain name.  Compatible changes (new request or response formats for example) can use content negotiation.  And generally we use extensible formats (like JSON) that allow you to add protocol elements in a consistent fashion that result in predictable outcomes.",
          "createdAt": "2021-06-23T00:08:27Z",
          "updatedAt": "2021-06-23T00:09:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The reason to do this is not just about versioning, since we also want to specify a \"task id\" for the request.\r\n",
          "createdAt": "2021-06-23T21:59:39Z",
          "updatedAt": "2021-06-23T21:59:39Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "On Wed, Jun 23, 2021 at 2:59 PM Christopher Patton ***@***.***>\nwrote:\n\n> The reason to do this is not just about versioning, since we also want to\n> specify a \"task id\" for the request.\n>\n\nI'm not following how that changes the situation. Can you expand?\n",
          "createdAt": "2021-06-23T22:06:38Z",
          "updatedAt": "2021-06-23T22:06:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm responding to @martinthomson's comment that the URI should not encode the version. I'm fine with that, I'm just not sure how to structure the request in a way that conveys the task id as well as the version. (I haven't yet looked at your link to the ACME spec, but I will)\r\n",
          "createdAt": "2021-06-23T22:25:09Z",
          "updatedAt": "2021-06-23T22:25:09Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "If the goal is to ensure that a client is able to communicate specific information to a server, in HTTP that's something we use headers for sometimes, but mostly it is what the content of a request is for.  I imagine here that we're talking about posting a bunch of stuff to a server, so there will be a payload into which you can put lots of goodies.\r\n\r\nThe message content might include a version, but versioning for content can and should use media types instead.  That too doesn't directly need an explicit version (not `application/my-protocol-v1` or far worse `application/my-protocol;v=1`), you just use a media type to ensure that if the client wants to send something else, the server won't get confused.\r\n\r\nIn practice, if you need to make a breaking change, it's better to do that at the level of URI (as stated earlier).  You might also make new content types at the same time, which is good practice, but the primary thing that stops things from going badly is the entirely new endpoint that you talk to.",
          "createdAt": "2021-06-24T02:13:31Z",
          "updatedAt": "2021-06-24T02:13:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Another thing to consider here (came up in the 2021/7/14 call): Responding to requests may take some time:\r\n1. Collect requests are blocked until the batch is saturated\r\n2. In Hits, each collect request is blocked until the previous requests is finished.\r\n\r\n@ekr mentioned that ACME has a way of dealing with this.",
          "createdAt": "2021-07-14T19:21:18Z",
          "updatedAt": "2021-07-14T19:21:18Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "ACME effectively uses what you might call a \"transaction resource\" for this purpose.  When a request is made, the response does not answer that request, but instead provides an link to an alternative resource from which the answer will eventually be available.  Clients then poll that resource to learn when their certificate is ready.  For the stated reasons, this is probably necessary here also.",
          "createdAt": "2021-07-15T00:20:27Z",
          "updatedAt": "2021-07-15T00:20:27Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I am closing this issue because I don't think there's any action to take here in DAP: we're not going to do in-band protocol versioning for the reasons Martin laid out above, and I don't think there's anything else from ACME we want to borrow.",
          "createdAt": "2023-05-26T19:49:54Z",
          "updatedAt": "2023-05-26T19:49:54Z"
        }
      ]
    },
    {
      "number": 62,
      "id": "MDU6SXNzdWU5Mjg2MDI2NzU=",
      "title": "Replay attacks by the leader and helper statefulness",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/62",
      "state": "CLOSED",
      "author": "csharrison",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "**TL;DR both aggregators need to maintain state to enforce privacy guarantees. Seems like there is an explicit state/communication trade-off to allow this**\r\n\r\nFiling this issue based on the design meeting today.\r\n\r\nIn the updated design in https://github.com/abetterinternet/prio-documents/pull/59 the leader is in a somewhat privileged position in that it receives reports/batches directly and is the primary entity that keeps track of state.\r\n\r\nFor differential privacy guarantees, we want to bound the number of times any record / batch can contribute to an output to the collector. This requires some statefulness that the leader could help facilitate. However, if the leader is colluding with the collector the following attack is possible:\r\n\r\n1. Collector issues a query to the leader: \"aggregate the last day of data\"\r\n2. The protocol proceeds as normal and the collector receives the aggregate data + noise\r\n3. Collector issues a similar query to the (corrupt) leader, asking for that same data again\r\n4. Leader ignores its state that would otherwise say not to aggregate this data again and communicates to the helper the same data as in (2)\r\n5. The collector receives the same data with newly sampled noise, and can leverage averaging attacks to diminish privacy\r\n\r\nThis is all possible because keeping track of \"budgets\" is the sole responsibility of the leader in step (4). To mitigate this we would need both helper and leader to maintain this information independently, so that neither aggregator has an advantage or can reduce user privacy on their own.\r\n\r\nTo do this, the helper will be required to keep track of (some) state. Additionally, we can rely on the leader to keep some state on behalf of the helper in some way that maintains the integrity of the data and prevents mutation. I believe there is an explicit tradeoff here in terms of (a) statefulness of the helper and (b) communication cost with the leader to maintain the auxiliary state.\r\n\r\nOn one extreme is the maximum statefulness in which all state is maintained by the helper (i.e. the helper and leader keep similar privacy budget databases). On the other extreme is maximum statelessness which would entail something like the helper keeping track of a single counter which \"versions\" the helper's database stored on the leader. The entire database would be communicated to the helper during the protocol, and a new version would be pushed to the leader for storage.\r\n\r\nHow big would this database be? This is a bit of an open question but it will scale proportionally to the number of _partitions_ of the data we would support querying independently (without impacting budget of other partitions). In the design outlined in https://github.com/WICG/conversion-measurement-api/blob/main/AGGREGATE.md we would partition based on:\r\n - Some time window (e.g. hour)\r\n - The collector's identity\r\n - Some application specific key which is specified by the client\r\n\r\nI could imagine this being large in practice though I don't have exact numbers. One natural compromise design between these two extreme options would be to partition the auxiliary databases stored on the leader per one of these keys (e.g. collector identity). This would entail the helper keeping c counters, one for each collector. Then the aux state transferred during each protocol is just the state for the relevant collector. You could presumably do this for any number of parameters that are available in the clear to both aggregators (e.g. information added to or already in `PAParam`).",
      "createdAt": "2021-06-23T20:13:32Z",
      "updatedAt": "2021-06-30T21:24:01Z",
      "closedAt": "2021-06-30T21:24:00Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "After the design call I made two changes to #59 that might help:\r\n\r\n1. The client now includes with its report a timestamp for report creation time. The associated data for encryption of the helper's helper's share will include this timestamp. That means the helper can authenticate the creation time for reports generated by honest clients.\r\n2. The PA parameters now include a \"batch window\", which specifies the minimum time difference between the oldest and newest report in a batch. Moreover, collect requests specify the batch via a contiguous sequence of batch windows.\r\n\r\nI wonder if this is enough plumbing to allow the helper to prevent replays without needing more overhead than a counter. Suppose for simplicity that a helper is configured for only one PA task and only handles requests from a single leader. Its encrypted state (held by the leader) could keep track of the number of times a collect request has been made for the reports in a given window. It knows the set of reports that fall in that window, so it also knows how many times each report has been used.\r\n\r\n\r\n",
          "createdAt": "2021-06-23T22:23:03Z",
          "updatedAt": "2021-06-23T22:23:03Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Its encrypted state (held by the leader) could keep track of the number of times a collect request has been made for the reports in a given window. It knows the set of reports that fall in that window, so it also knows how many times each report has been used.\r\n\r\nCan you elaborate on exactly what the encrypted state is going to look like? It seems like your suggestion is the \"stateless extreme\" option I outlined in the original post. My concern is that the state will need to be a map of {batch_identifier --> counter} for all possible batch identifiers. A batch identifier could contain a time window for sure, but there may be other parameters that make this map large (collector identity, etc.).\r\n\r\nFor example, we probably want to support multiple collectors issuing queries (with different data) for batches in the same time window.",
          "createdAt": "2021-06-23T22:38:50Z",
          "updatedAt": "2021-06-23T22:38:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, I'm advocating for the stateless extreme. My assessement is that the communication overhead will be negligible compared to the cost of re-transmitting (intermediate stages of) the IDPF shares in some aggregate requests:\r\n* IIUC the cost of the counters is `O(num_batches)` bits, where where `num_batches` is the number of batches that are valid for a collect request at any given time.  If multiple collectors are allowed to make requests for any window, then this increases to `O(num_batches * num_collectors)`. Concretely, if the batch window is 1 hour, and the leader agrees to hang on to reports for at least 24 hours, then `num_batches == 24`. (Not sure what a realistic value of `num_collectors` would be.)\r\n* Each aggregate query includes (some intermediate stage of) the encrypted IDPF shares of a subset of reports in a batch. The size of each IDPF share is `O(n)` bits, where `n` is the number of bits of each input. That's `O(n * batch_size)` bits, summing over all the aggregate requests, where `batch_size` is the minimum number of reports per batch. Concretely, `batch_size` might be on the order of a million.\r\n\r\nPerhaps I'm missing something?",
          "createdAt": "2021-06-23T23:19:22Z",
          "updatedAt": "2021-06-30T15:43:47Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Sorry for the delay on this. Yeah I think this hinges on num_collectors and num_batches. For ad-tech consumers I think this could be in the thousands+. I think num_batches may need to be larger than 24 too, if we wanted to support something like larger like weekly / monthly aggregations.\r\n\r\nBut I agree this may not be a problem in practice depending on these params and I see the benefit in avoiding premature optimization.",
          "createdAt": "2021-06-30T16:54:49Z",
          "updatedAt": "2021-06-30T16:54:49Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As discussed on today's design call, we'll resolve this issue by allowing trade-off to be made by the helper implementation. In particular, we will neither specify how the helper's state is encrypted nor \"require\" the helper to be stateless. (I pushed a change to #59 that reflects this resolution.)",
          "createdAt": "2021-06-30T20:53:53Z",
          "updatedAt": "2021-06-30T20:53:53Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> As discussed on today's design call, we'll resolve this issue by allowing trade-off to be made by the helper implementation. In particular, we will neither specify how the helper's state is encrypted nor \"require\" the helper to be stateless. (I pushed a change to #59 that reflects this resolution.)\r\n\r\nThis looks good. This model supports a stateful helper and a \"mostly stateless\" helper that gets all its state from the cookie (It doesn't support sharding the cookie, but I think that's fine for the time being.)\r\n\r\nClosing this out!",
          "createdAt": "2021-06-30T21:24:00Z",
          "updatedAt": "2021-06-30T21:24:00Z"
        }
      ]
    },
    {
      "number": 64,
      "id": "MDU6SXNzdWU5MzQxNzI3OTc=",
      "title": "Supporting batch uploads from the client (and routing reports through the collector)",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/64",
      "state": "CLOSED",
      "author": "csharrison",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "In today's design call we discussed the collector receiving encrypted reports from clients and forwarding them to the leader. This aligns with the design we have in the WICG with some of the reasoning documented [here](https://github.com/WICG/conversion-measurement-api/blob/main/SERVICE.md#sending-data-directly-to-the-helper-servers).\r\n\r\nI also brought this up for discussion on our regular calls in the WICG ([minutes](https://github.com/WICG/conversion-measurement-api/blob/main/meetings/2021-06-28-minutes.md)). Where there was some agreement that this was a good idea.\r\n\r\n\r\n### Pros / Cons of routing reports through the collector\r\nThese are probably non-exhaustive.\r\nPros:\r\n\r\n- Doesn't require aggregation servers to be highly online / available\r\n- Supports graceful failure (\"If something goes wrong, we could re-query\")\r\n- Gives some indication that \"the API is working\" on the server without needing to wait until query time, or via some other side-channel.\r\n- Distributes state out of the aggregation servers (% protection from replay attacks). Arguably this aligns more in our API with who \"owns\" the data at some fundamental level.\r\n- Adds query flexibility \"for free\" without explicitly adding support in the aggregation servers by allowing querying subsets of reports (for instance)\r\n- Allows support for some level of report authentication by the collector, who (in our model) is the entity that is best in the position to validate reports. This could be done completely outside the protocol.\r\n\r\nCons:\r\n\r\n- Adds query flexibility, which could be detrimental to privacy\r\n- Leaks some metadata about each request that may otherwise be only visible to the leader (e.g. ip address), unless using some anonymizing proxy\r\n- Introduces a new vector for replay attacks\r\n\r\n### Protocol solution\r\nIt seems there is a fairly simple solution to this problem, and that is to simply:\r\n\r\n- Instantiate the protocol where the collector is also the client, where the interactions between the \"real clients\" and the \"client/collector\" is unspecified by the protocol.\r\n- Allow the \"client\" to optionally batch upload reports in the protocol rather than sending them one by one.\r\n\r\nIn the existing protocol there is no client authentication so it is technically possible to have a collector that just collects encrypted reports from clients and forwards them on to the leader. Of course the actual clients would need to be set up to do this but it is permitted by the protocol. By allowing batch uploads we just optimize this already-permitted configuration.\r\n\r\nAlternatively, if we deem collector-clients to be bad for the protocol, we ought to have a mechanism which actually forbids them (e.g. by authenticating clients). However, I think that it is pretty reasonable to have this allowed by the protocol and leave it up to specific instantiations how the \"client\" is configured/trusted.",
      "createdAt": "2021-06-30T22:25:06Z",
      "updatedAt": "2023-09-20T15:19:59Z",
      "closedAt": "2023-09-20T15:19:58Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": ">Gives some indication that \"the API is working\" on the server without needing to wait until query time, or via some other side-channel.\r\n\r\nCan you elaborate on this? Is the idea here that when a client (that is, a real end user client, not a batching one) gets a 200 OK after posting a report to a batching client, the client can be assured that its report has been durably persisted somewhere? I think a leader server could provide a similar guarantee at the end of the upload phase so i'm trying to understand what extra assurances the batching client provides.\r\n\r\n>Adds query flexibility \"for free\" without explicitly adding support in the aggregation servers by allowing querying subsets of reports (for instance)\r\n\r\nIIUC the query flexibility is because the batching client can submit the same reports multiple times, in different-sized batches. If we decide this query flexibility is bad, we could mitigate this by having the original client include a report timestamp in the encrypted input, where it can't be tampered with by the batching client. Aggregators would then maintain query/privacy budgets per aggregation window and would be able to refuse queries on reports that fall in an aggregation window whose budget is already spent.",
          "createdAt": "2021-07-02T21:34:19Z",
          "updatedAt": "2021-07-02T21:34:19Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Can you elaborate on this? Is the idea here that when a client (that is, a real end user client, not a batching one) gets a 200 OK after posting a report to a batching client, the client can be assured that its report has been durably persisted somewhere? I think a leader server could provide a similar guarantee at the end of the upload phase so i'm trying to understand what extra assurances the batching client provides.\r\n\r\nI think the use-case is more that the collector is assured the system is working without requiring an interaction with the helpers. It is possible this case could be met by introducing some \"do I have some reports\" functionality though.\r\n\r\n> IIUC the query flexibility is because the batching client can submit the same reports multiple times, in different-sized batches. If we decide this query flexibility is bad, we could mitigate this by having the original client include a report timestamp in the encrypted input, where it can't be tampered with by the batching client. Aggregators would then maintain query/privacy budgets per aggregation window and would be able to refuse queries on reports that fall in an aggregation window whose budget is already spent.\r\n\r\nI think this is one part of it. There are a few ways this batching introduces flexibility even if reports can only be queried once. Mainly this is via separating / combining reports across multiple in-the-clear dimensions (in our design we give some info in the clear like the advertiser site a user converted on). A collector could combine multiple small advertisers reports together if they are too small to receive aggregate data. This is recoverable with a robust query model in the helpers though it adds complexity.\r\n\r\nAnother example along these lines is time-based querying. One collector might want data on hour boundaries, another might want on 4 hour boundaries etc.",
          "createdAt": "2021-07-07T16:51:10Z",
          "updatedAt": "2021-07-07T19:35:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed the PR, but here's where we left the discussion: https://github.com/abetterinternet/prio-documents/pull/78#issuecomment-880096898 ",
          "createdAt": "2021-07-14T18:00:43Z",
          "updatedAt": "2021-07-14T18:00:43Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Seems like the protocol already has everything needed to address this issue. In a combined Collector-Leader deployment, the details of the upload protocol in the spec can probably just be disregarded. What matters for interop in that case is the aggregation and collection flows running between Leader and Helper.\r\n\r\nClosing as \"won't fix\". @csharrison please feel free to re-open if there's more to discuss.",
          "createdAt": "2023-09-20T15:19:58Z",
          "updatedAt": "2023-09-20T15:19:58Z"
        }
      ]
    },
    {
      "number": 65,
      "id": "MDU6SXNzdWU5MzUyODgyNTc=",
      "title": "Deployment architecture and PAParam",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/65",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "In the current design, the leader provides the configuration information to the client in response to ```upload_start```.  This is conceptually problematic for several reasons:\r\n\r\n1. The leader cannot be trusted to tell the client about the helpers because it might supply its own helpers.\r\n2. The client learns all kinds of information it doesn't need (e.g., batch size).\r\n3. It is an error for the client and the PA system to disagree about a number of parameters, so why are we giving this an opportunity for it to happen? For instance, suppose the client thinks it's doing Prio but the leader says it's doing heavy hitters? Why is that even possible.\r\n\r\nForgetting about the protocol for a second, IMO the right way for this to work is:\r\n\r\n1. The *collector* decides on all the parameters, potentially in cooperation with the rest of the PA system (e.g., the leader might have a minimum batch size, not be able to do some protocol, etc.)\r\n1. The *collector* then configures those parameters into the leader, helpers, etc. as well as the client\r\n1. The *PA system* (probably the leader?) provides a task ID associated with those parameters.\r\n1. The *client* connects to the leader and tells it it wants to use a specific task ID.\r\n1. The leader then responds with an OK or error and any information that might be needed to actually deliver that data, which might well be empty (or could be \"here is the node to upload to\").\r\n\r\nThe key point here is that the client's trust relationship is with the collector, not the PA system, and so the PA system shouldn't be telling it much of anything.\r\n\r\nTwo asides:\r\n1. You might, I suppose, want the collector to be able to reconfigure the client through the leader, but then this structure would need to be signed by the collector, which sounds like more trouble than it's worth.\r\n1. You might also want some way to verify that there hadn't been some kind of configuration mismatch (e.g., the collector changed the size of the prime but didn't change the task ID) but the way you do that is by having the client supply a hash of the config and the leader return an error on mismatch.\r\n\r\n",
      "createdAt": "2021-07-01T23:31:14Z",
      "updatedAt": "2021-08-11T22:13:07Z",
      "closedAt": "2021-08-11T22:13:07Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "To expand on these points a bit, the parameters need to be determined by the entity operating the collector and the propagated to:\r\n\r\n1. The aggregators\r\n1. The collector\r\n1. The client\r\n\r\nOnly the first of these necessarily requires any kind of cooperation within the scope of this protocol: the collector and clients are effectively operated by the same entity and can be configured in some out of band mechanism. This leaves us with the aggregators. If we want to define a complete system, we might need some way for them to be in-band configured, though you could also imagine a simpler system in which that was done by some unspecified mechanism, which you'll probably need anyway for account creation etc. For instance, you could just have a Web page. If you do that, we don't need PAParam at all. OTOH, if we don't do that, then we need to define a helper configuration protocol.\r\n",
          "createdAt": "2021-07-01T23:41:55Z",
          "updatedAt": "2021-07-01T23:41:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for the thoughtful feedback, @ekr! I largely agree with how you envision the protocol working. The specific issues with the current spec notwithstanding, I think a big chunk of this question ties into nailing down how parameters are distributed, which we haven't done. It seems to me like the collector is going to have to sign PAParam at some point, at least so that the helper can ensure the leader isn't giving it the wrong parameters; but we probably don't want to require that the client be able to verify that signature.\r\n\r\n> The key point here is that the client's trust relationship is with the collector, not the PA system, and so the PA system shouldn't be telling it much of anything.\r\n\r\nThis is an interesting point. I have been thinking of the collector/client relationship differently, but perhaps there's an inconsistency in my mental model of the problem. I would think that, ideally, the client would only need to trust that the aggregators don't collude. In particular, it wouldn't even need to trust the collector, at least not directly, to even choose the parameters honestly. Our goal would be that, as long as least one of the aggregator is honest, some standard of privacy can be enforced (batch size, DP budget, etc.). Perhaps this trust model is too pessimistic for security to be achievable? In any case, our protocol certainly doesn't achieve it right now.",
          "createdAt": "2021-07-02T01:34:48Z",
          "updatedAt": "2021-07-02T01:43:35Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> This is an interesting point. I have been thinking of the collector/client relationship differently, but perhaps there's an inconsistency in my mental model of the problem. I would think that, ideally, the client would only need to trust that the aggregators don't collude. In particular, it wouldn't even need to trust the collector, at least not directly. Our goal would be that, as long as least one of the aggregator is honest, some standard of privacy can be enforced (batch size, DP budget, etc.). Perhaps this trust model is too pessimistic for security to be achievable? In any case, our protocol certainly doesn't achieve it right now.\r\n\r\nWell, I think the relationship of the client and the collector is different than the relationship with the rest of the system.\r\n\r\nIn the simplest case, the client was actually written by the collector (that's what you would see in product telemetry, for instance). So, then how does the *user* get confidence that the client is using reasonable parameters, having a set of helpers that the user can trust, etc.? At some level, they can inspect the source code, but more likely the expectation is that the vendor makes assertions about their practices and then the code is open to inspection to verify that those assertions are correct. But note that set of mechanisms needs to not just cover PA but also ensure that that software is otherwise behaving as advertised, so this is a more general problem that we need not solve here (this is where open source, reproducible builds, and binary transparency come in).\r\n\r\nNow there is a more generic case in which the client is written separately but for some reason wants to report to the collector, in which case I would assume that the client vendor somehow imports the parameters (again, in a way we don't need to standardize) from the operator of the collector, but I would assume that here too we would expect the client vendor to assure themselves that those parameters were reasonable.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2021-07-02T01:45:01Z",
          "updatedAt": "2021-07-02T01:45:01Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It sounds like you don't see much value in the client discovering parameters online? This might be useful, say, if the client has a binary that it can use to run many different PA tasks, with different parameters.",
          "createdAt": "2021-07-02T01:57:33Z",
          "updatedAt": "2021-07-02T01:57:33Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> It sounds like you don't see much value in the client discovering parameters online? This might be useful, say, if the client has a binary that it can use to run many different PA tasks, with different parameters.\r\n\r\nWell, it's not clear to me how this case would arise. I mean, the client still has to *collect* the data before it submits it in a report, so presumably that part is code, right?\r\n\r\n",
          "createdAt": "2021-07-02T02:02:13Z",
          "updatedAt": "2021-07-02T02:02:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "True, though it might be useful to tune the parameters used to collect the same data. I don't have a specific use case in mind, so this optimization could be premature.",
          "createdAt": "2021-07-02T02:06:34Z",
          "updatedAt": "2021-07-02T02:06:34Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Well but in this case, why can't the client just download it from the vendor somehow? I mean, you've probably already got some channel for updates. Why does this have to be something that this protocol solves?",
          "createdAt": "2021-07-02T02:08:34Z",
          "updatedAt": "2021-07-02T02:08:34Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hey @ekr, have we addressed everything in this issue?",
          "createdAt": "2021-08-04T17:46:17Z",
          "updatedAt": "2021-08-04T17:46:17Z"
        }
      ]
    },
    {
      "number": 66,
      "id": "MDU6SXNzdWU5MzUyODg4NTI=",
      "title": "PAParam needs a length field",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/66",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "PAParam now has:\r\n\r\n```\r\nstruct {\r\n  PATask task;\r\n  uint64 batch_size;\r\n  PAProto proto;\r\n  select (PAClientParam.proto) {\r\n    case prio: PrioParam;\r\n    case hits: HitsParam;\r\n  }\r\n} PAParam;\r\n```\r\nIf you introduce a new protocol, this structure will not be parseable by anyone who doesn't know about it. The fix is to put a length field here, typically before the ```select```",
      "createdAt": "2021-07-01T23:32:46Z",
      "updatedAt": "2021-07-08T12:15:42Z",
      "closedAt": "2021-07-08T12:15:42Z",
      "comments": []
    },
    {
      "number": 67,
      "id": "MDU6SXNzdWU5MzUyODg5ODg=",
      "title": "Why does PATask include version?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/67",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This seems unnecessary.",
      "createdAt": "2021-07-01T23:33:09Z",
      "updatedAt": "2021-07-13T18:54:04Z",
      "closedAt": "2021-07-13T18:54:04Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The PATask is always the first bit of a protocol message, and the version is the first bit of the PATask. I figured it would be helpful to version each message because messages are carried by HTTP requests. There might be a simpler/better way to do this.",
          "createdAt": "2021-07-02T00:14:16Z",
          "updatedAt": "2021-07-02T00:14:16Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think we should assume that's the case. If we want to have the version not determined at the URL level (a mistake, IMO) then we should have some sort of message wrapper that contains that, rather than overloading PATask.",
          "createdAt": "2021-07-02T00:21:40Z",
          "updatedAt": "2021-07-02T00:21:40Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm inclined to agree with @ekr: we're discussing versioning the whole API in #61, so PATask doesn't need to also provide version information. So, setting aside the question of message format versions, is there any value in maintaining a version on PATasks at the protocol level? I say no, because we want to be able to treat different PATasks completely independently and don't otherwise have any notion of a relationship between any two PATasks in the protocol.",
          "createdAt": "2021-07-02T21:46:23Z",
          "updatedAt": "2021-07-02T21:46:23Z"
        }
      ]
    },
    {
      "number": 68,
      "id": "MDU6SXNzdWU5MzUyOTE0Mjc=",
      "title": "Multiple helpers (was \"Should multipler helpers be baked into the core protocol?\")",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/68",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "The reason for having multiple helpers is to (partially) recover the protocol output in case a helper goes offline. (See #4.) We currently bake this into the protocol by associating a set of helper URLs with the PPM parameters. This complicates the protocol in two ways:\r\n1. When uploading reports, the client has to pick the set of helpers it will upload reports for (see #50).\r\n2. When collecting outputs (see #59), the collector has to specify which helper the leader will engage with.\r\n\r\nWhen reviewing #59, @chris-wood pointed out that we can simplify the protocol by having one helper per task. If deployments want to add redundancy, they can do so by spinning up more tasks with identical parameter except with a different helper for each task.",
      "createdAt": "2021-07-01T23:39:36Z",
      "updatedAt": "2023-05-26T19:57:35Z",
      "closedAt": "2023-05-26T19:57:34Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think that high availability is a good reason to allow for multiple helpers. If you want HA, stand up multiple instances in different data centers, etc.\r\n\r\nThe reason to allow for multiple helpers is to provide a higher level of assurance, namely to require >2 entities to collude in order to violate user privacy. It might be that we could defer that, though it's not clear to me why a fixed set of 2 helpers is more complicated than a fixed set of 1 helper.\r\n\r\nWRT #50 I don't think that the leader should e telling the client about helpers at all. See #65 \r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2021-07-01T23:47:28Z",
          "updatedAt": "2021-07-01T23:47:28Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I don't think that high availability is a good reason to allow for multiple helpers. If you want HA, stand up multiple instances in different data centers, etc.\r\n\r\nWhen we discussed this last we agreed that multiple helpers would be used for resiliency and not for weakening the trust model. Making this change at this point is fine, but I just want to note that it's a course correction and want to make sure we're all on the same page about.\r\n\r\n> The reason to allow for multiple helpers is to provide a higher level of assurance, namely to require >2 entities to collude in order to violate user privacy. It might be that we could defer that, though it's not clear to me why a fixed set of 2 helpers is more complicated than a fixed set of 1 helper.\r\n\r\nThere are two ways in which this complicates things:\r\n\r\n1. Not every PA protocol is well-defined for more than one helper. In particular, heavy hitters, as specified in [1], is defined for exactly two servers (one leader and one helper). The protocol could probably be extended, but the cost might be high in terms of round complexity.\r\n2. Input validation gets more complicated. For Prio, the leader will have to make a request to each helper, combine the results and make the validity decision, then make a second request to each helper in order to disseminate the decision. Right now Prio can be implemented with just one request to the helper.\r\n\r\nWith these wrinkles in mind, I'd be in favor of allowing multiple helpers to be used this way.\r\n\r\n[1] https://eprint.iacr.org/2021/017",
          "createdAt": "2021-07-02T00:12:23Z",
          "updatedAt": "2021-07-02T00:18:51Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> > I don't think that high availability is a good reason to allow for multiple helpers. If you want HA, stand up multiple instances in different data centers, etc.\r\n> \r\n> When we discussed this last we agreed that multiple helpers would be used for resiliency and not for weakening the trust model. Making this change at this point is fine, but I just want to note that it's a course correction and want to make sure we're all on the same page about.\r\n\r\nWell, at this non-WG stage of the process \"we\" and \"agreed\" are necessarily kind of fuzzy. so I think there's really a presumption that there's a high bar to make changes. In any case, I don't recall being part of that discussion, so what was the rationale for this? Remember, we're not talking about multiple machines here but rather multiple operators. Is there some reason that ISRG is the leader and Cloudflare is the helper that the system needs to survive a Cloudflare outage? It seems like we've got bigger problems in that case. And note that the system as specified won't survive an outage at the leader.\r\n\r\n> \r\n> > The reason to allow for multiple helpers is to provide a higher level of assurance, namely to require >2 entities to collude in order to violate user privacy. It might be that we could defer that, though it's not clear to me why a fixed set of 2 helpers is more complicated than a fixed set of 1 helper.\r\n> \r\n> There are two ways in which this complicates things:\r\n> \r\n>     1. Not every PA protocol is well-defined for more than one helper. In particular, heavy hitters, as specified in [1], is defined for exactly two servers (one leader and one helper).\r\n> \r\n>     2. Input validation gets more complicated. For Prio, the leader will have to make a request to each helper, combine the results and make the validity decision, then make a second request to each helper in order to disseminate the decision. Right now Prio can be implemented with just one request to the helper.\r\n> \r\n> \r\n> With these wrinkles in mind, I'd be in favor of allowing multiple helpers to be used this way.\r\n\r\nWell, I'm certainly open to having only one helper, if that makes things much easier; I'm just saying that that's the only reason  I am aware of for having > 1\r\n> \r\n> [1] https://eprint.iacr.org/2021/017\r\n\r\n",
          "createdAt": "2021-07-02T00:19:17Z",
          "updatedAt": "2021-07-02T00:19:17Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Well, at this non-WG stage of the process \"we\" and \"agreed\" are necessarily kind of fuzzy. so I think there's really a presumption that there's a high bar to make changes.\r\n\r\nBy \"we\" I mean the folks who were on the call when we discussed it and who have chimed in on issues and PRs. Yes, this is necessarily fuzzy and subject to change.\r\n\r\n> In any case, I don't recall being part of that discussion, so what was the rationale for this? Remember, we're not talking about multiple machines here but rather multiple operators. Is there some reason that ISRG is the leader and Cloudflare is the helper that the system needs to survive a Cloudflare outage? It seems like we've got bigger problems in that case. And note that the system as specified won't survive an outage at the leader.\r\n\r\nNo, we were thinking more of a scenario where the leader is a well-provisioned service provider, like ISRG or Cloudflare, and the helper is a dinky third-party that fails to handle a sudden load spike.\r\n\r\n\r\n",
          "createdAt": "2021-07-02T00:28:35Z",
          "updatedAt": "2021-07-02T00:28:35Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> No, we were thinking more of a scenario where the leader is a well-provisioned service provider, like ISRG or Cloudflare, and the helper is a dinky third-party that fails to handle a sudden load spike.\r\n\r\nI think this is a case of \"don't do that then\", especially given how straightforward it now is to set up HA systems on cloud services.",
          "createdAt": "2021-07-02T00:29:47Z",
          "updatedAt": "2021-07-02T00:30:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The only way I would push back is that requiring that everyone be on a cloud service means there's less diversity in where secret shares are going. If they're going to service providers on the same infrastructure (GCP, say), I could argue that this isn't adding much to privacy.\r\n\r\nPut another way: I think it's good if the bar for running a helper is low.",
          "createdAt": "2021-07-02T00:31:48Z",
          "updatedAt": "2021-07-02T00:32:40Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Sure, but I think one deals with that by having the Collector retry.",
          "createdAt": "2021-07-02T00:33:06Z",
          "updatedAt": "2021-07-02T00:33:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Well, you're not going to be able to solve every problem this way. It's easy to imagine a helper getting into a state in which the output can't be recovered for a batch. (As a simple example, suppose the helper forgets its key.)\r\n\r\nBut getting back to the original issue: if you want to add redundancy, you can do so without the details of how you do it bleeding into the protocol. I'm fine with that. But we do need to make a decision on the scope of a task.",
          "createdAt": "2021-07-02T00:35:25Z",
          "updatedAt": "2021-07-02T00:37:51Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> Well, you're not going to be able to solve every problem this way. It's easy to imagine a helper getting into a state in which the output can't be recovered for a batch. (As a simple example, suppose the helper forgets its key.)\r\n\r\nWell, now we're not talking about load spikes but about something different.\r\n\r\n\r\n> But getting back to the original issue: if you want to add redundancy, you can do so without the details of how you do it bleeding into the protocol. I'm fine with that. But we do need to make a decision on the scope of a task.\r\n\r\nI don't know what you mean here. Can you elaborate?\r\n\r\n",
          "createdAt": "2021-07-02T00:38:38Z",
          "updatedAt": "2021-07-02T00:38:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> > Well, you're not going to be able to solve every problem this way. It's easy to imagine a helper getting into a state in which the output can't be recovered for a batch. (As a simple example, suppose the helper forgets its key.)\r\n> \r\n> Well, now we're not talking about load spikes but about something different.\r\n\r\nThat's just an example. My larger point is that we should lower the bar to entry wherever possible. And to re-iterate, this was the original intention of multiple helpers. (Personally, I'm open to changing this requirement.)\r\n\r\n> > But getting back to the original issue: if you want to add redundancy, you can do so without the details of how you do it bleeding into the protocol. I'm fine with that. But we do need to make a decision on the scope of a task.\r\n> \r\n> I don't know what you mean here. Can you elaborate?\r\n\r\nIf we change the spec so that there's just one helper per PAParam, then adding another helper (for redundancy) would require distributing another PAParam. That's fine, except that right now we're assuming a 1:1 correspondence between PATask and PAParam.\r\n",
          "createdAt": "2021-07-02T00:50:12Z",
          "updatedAt": "2021-07-02T00:50:12Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> That's just an example. My larger point is that we should lower the bar to entry wherever possible.\r\n\r\nI would say \"where practical\". \r\n\r\n> And to re-iterate, this was the original intention of multiple helpers. (Personally, I'm open to changing this requirement.)\r\n\r\nOK. Well, perhaps we should press pause on this discussion and see what others say.\r\n\r\n\r\n\r\n> If we change the spec so that there's just one helper per PAParam, then adding another helper (for redundancy) would require distributing another PAParam. That's fine, except that right now we're assuming a 1:1 correspondence between PATask and PAParam.\r\n\r\nWell, this is more of a topic for #65. But I don't see a problem with having multiple task IDs for the same logical operation with different helpers. That seems a lot simpler in general and in specific pushes complexity out from the main system into the collector/clients, which, as noted in #65, have an arbitrarily rich channel that we don't need to standardize.\r\n\r\n",
          "createdAt": "2021-07-02T00:53:43Z",
          "updatedAt": "2021-07-02T00:53:43Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Just want to cross-reference this comment here: https://github.com/abetterinternet/prio-documents/pull/70#pullrequestreview-703109761\r\n",
          "createdAt": "2021-07-09T16:05:12Z",
          "updatedAt": "2021-07-09T16:05:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "What's left for this issue is to decide if/how to allow for multiple helpers for privacy.",
          "createdAt": "2021-07-12T15:34:52Z",
          "updatedAt": "2021-07-12T15:34:52Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We discussed the question of multiple helpers at [IETF 116](https://datatracker.ietf.org/meeting/116/materials/agenda-116-ppm-03) and [on the mailing list](https://mailarchive.ietf.org/arch/msg/ppm/TnAYiud5OVlQNfUv71BM6Tl6tNs/). The consensus we arrived at is that we want to specialize DAP for exactly 2 aggregators for simplicity and to enable [performance gains](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393). I don't think there's anything else to do in this issue, so I'm closing it.",
          "createdAt": "2023-05-26T19:57:34Z",
          "updatedAt": "2023-05-26T19:57:34Z"
        }
      ]
    },
    {
      "number": 69,
      "id": "MDU6SXNzdWU5MzUyOTkxOTY=",
      "title": "Encrypt to the leader as well",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/69",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "ACME originally had this \"just trust TLS\" structure and then ended up with end-to-end signatures to support designs in which you might have some kind of TLS decryption device on-path (either a forward proxy on the client or a reverse proxy on the server). ISTM that similar concerns apply here, and we should just HPKE to the leader. Note that this would also simplify the protocol because you wouldn't need to have parallel structures of leader/helper and could just have:\r\n\r\n```\r\nPAEncryptedInputShare input_share<..2^24-1>;\r\n```\r\n\r\nWith the convention that leader was 0.",
      "createdAt": "2021-07-02T00:00:54Z",
      "updatedAt": "2021-07-12T15:04:33Z",
      "closedAt": "2021-07-12T15:04:33Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'd be fine with this change.",
          "createdAt": "2021-07-02T00:17:13Z",
          "updatedAt": "2021-07-02T00:17:13Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this is especially important in light of #64: if we're going to allow* batching clients to sit between real clients and leaders, then we should prevent those batching clients from being able to observe or tamper with the contents of individual reports, especially if we're going to put some information (timestamps or some unique identifier) into the report to enable aggregators to enforce query/privacy budgets.\r\n\r\n* or rather, \"if we're going to acknowledge that we can't prevent\"",
          "createdAt": "2021-07-02T21:38:29Z",
          "updatedAt": "2021-07-02T21:38:29Z"
        }
      ]
    },
    {
      "number": 72,
      "id": "MDU6SXNzdWU5MzkzMDg2NDI=",
      "title": "Derive the PA task from the parameters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/72",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "On the 2021/7/7 design call we decided that the ID should be derived deterministically from the serialized PAParam by, for example, hashing it. This solves a few problems simultaneously. This settles the question of whether there should be 1:1 correspondence between a task ID and the parameters used for that task.\r\n\r\n@tgeoghegan brought up an interesting question, which is how to make the task ID unique when two different data collection tasks use the same parameters. Thoughts?",
      "createdAt": "2021-07-07T22:24:36Z",
      "updatedAt": "2021-07-20T17:54:50Z",
      "closedAt": "2021-07-20T17:54:50Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The consensus here is to do SHA-256 over the PDAParams structure and use that as the 32 byte PDATaskID. However, hashing a protocol message requires us to decide on a canonical representation of a protocol message, which I think forces us to decide how to encode messages on the wire. I filed #85 for that question. I'll proceed with a PR for this issue on the assumption that we'll pick JSON.",
          "createdAt": "2021-07-14T23:19:49Z",
          "updatedAt": "2021-07-14T23:19:49Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As I mentioned there, the assumption would be that we're adopting TLS' binary encoding. If so, in TLS notation, you would write `SHA-256(PDAParam)` as the hash of `PDAParam`, which itself is a serialized data structure.",
          "createdAt": "2021-07-14T23:34:40Z",
          "updatedAt": "2021-07-14T23:34:40Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> @tgeoghegan brought up an interesting question, which is how to make the task ID unique when two different data collection tasks use the same parameters. Thoughts?\r\n\r\nAll we require is that each PAParam have some unique value, right? A random nonce would probably suffice.\r\n\r\nr.e. the encoding, yes, let's stick with TLS's wire format as @cjpatton suggests.",
          "createdAt": "2021-07-15T01:24:54Z",
          "updatedAt": "2021-07-15T01:24:54Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, my thinking was that we would just stick 16 bytes of random data in `PDAParams` (a UUID's worth). I agree that we should stick with TLS binary encoding.",
          "createdAt": "2021-07-15T01:43:35Z",
          "updatedAt": "2021-07-15T01:43:35Z"
        }
      ]
    },
    {
      "number": 75,
      "id": "MDU6SXNzdWU5NDEwNjgwMDY=",
      "title": "PAUploadFinishReq timestamp",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/75",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Currently, clients specify the report timestamp when uploading a report for a task. Presumably this timestamp is used to determine the window/batch in which a report is aggregated. Do we care if clients can spoof or change this timestamp to put reports into different batches? Since clients have no understanding of what batch their report goes into, and this is something that aggregators manage, perhaps we should drop this and let the aggregators figure out the report<>batch grouping?",
      "createdAt": "2021-07-09T21:11:59Z",
      "updatedAt": "2021-07-14T01:30:53Z",
      "closedAt": "2021-07-14T01:30:53Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Currently, clients specify the report timestamp when uploading a report for a task. Presumably this timestamp is used to determine the window/batch in which a report is aggregated.\r\n\r\nYes, that's the idea.\r\n\r\n> Do we care if clients can spoof or change this timestamp to put reports into different batches?\r\n\r\nWe have no way of ensuring that the client doesn't lie about the timestamp. This \"attack\" is akin to the client just lying about the input itself (see #20): It's something we can't do anything about, short of using a secure enclave on the client's machine. I think dealing with these problems is orthogonal to the spec here. On the other hand, it should our goal to allow an honest aggregator to ensure that each report appears in most one batch.\r\n\r\n> Since clients have no understanding of what batch their report goes into, and this is something that aggregators manage, perhaps we should drop this and let the aggregators figure out the report<>batch grouping?\r\n\r\nThe timestamp and batch window is intended to allow an honest aggregator to manage this report<->batch mapping *efficiently*, i.e., with minimal state between aggregate requests. Unless we can think of a better way, I think we should keep this in.",
          "createdAt": "2021-07-12T14:58:39Z",
          "updatedAt": "2021-07-12T14:58:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This issue is now subsumed by #82.",
          "createdAt": "2021-07-14T01:30:53Z",
          "updatedAt": "2021-07-14T01:30:53Z"
        }
      ]
    },
    {
      "number": 81,
      "id": "MDU6SXNzdWU5NDM3ODA0NDE=",
      "title": "Intra-batch replay detection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/81",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "For privacy reasons, leaders should not be allowed to replay a given report more than once within the context of a single batch. The current verify+collect model runs in a \"streaming mode,\" wherein the leader sends slices of batch shares to helpers for verification and aggregation. \r\n\r\nIf we changed this to require the leader to send _all_ batch shares in a single verify+collect flow, then replay prevention would be simpler: just scan the list and check for dupes. \r\n\r\nIf we stick with the current model, then the helper must necessarily store some state (on the leader, most likely) representing the shares processed. This state could then be consulted when asked to verify+collect any new share. (We can probably spell out a number of ways to do store this state, but ideally it should not be linear in the number of shares collected so far.) We'd likely also need a way for the leader to request that the helper produce the final aggregate output once the batch is \"done\" being verified and collected.",
      "createdAt": "2021-07-13T19:59:13Z",
      "updatedAt": "2021-07-29T20:55:06Z",
      "closedAt": "2021-07-29T20:55:06Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> If we changed this to require the leader to send _all_ batch shares in a single verify+collect flow, then replay prevention would be simpler: just scan the list and check for dupes.\r\n\r\nOne downside to this approach is that it requires the leader to queue up the entire batch of reports before sending the first aggregate request to the helper. This may be OK for some deployments, but it might not be workable for all. \r\n \r\n> If we stick with the current model, then the helper must necessarily store some state (on the leader, most likely) representing the shares processed. This state could then be consulted when asked to verify+collect any new share. (We can probably spell out a number of ways to do store this state, but ideally it should not be linear in the number of shares collected so far.)\r\n\r\nI think this is worth pursuing.\r\n\r\n> We'd likely also need a way for the leader to request that the helper produce the final aggregate output once the batch is \"done\" being verified and collected.\r\n\r\nThis is already in the protocol. (See \"output share request\".)\r\n\r\n",
          "createdAt": "2021-07-14T00:48:25Z",
          "updatedAt": "2021-07-14T00:48:25Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> (We can probably spell out a number of ways to do store this state, but ideally it should not be linear in the number of shares collected so far.)\r\n\r\nWe could require that the leader feed the helper reports with monotonically increasing timestamps, such that the helper can simply maintain a single timestamp and reject any report older than that. However I'm not sure if that's practical given that clients may submit reports out of order.",
          "createdAt": "2021-07-14T16:12:11Z",
          "updatedAt": "2021-07-14T16:12:11Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "One intermediate option would be to maintain a jitter buffer on the leader,\nso that they keep reports for a few minutes and then feed them out in\norder. You would still lose some reports, but not too many.\n\nOn Wed, Jul 14, 2021 at 9:12 AM Tim Geoghegan ***@***.***>\nwrote:\n\n> (We can probably spell out a number of ways to do store this state, but\n> ideally it should not be linear in the number of shares collected so far.)\n>\n> We could require that the leader feed the helper reports with\n> monotonically increasing timestamps, such that the helper can simply\n> maintain a single timestamp and reject any report older than that. However\n> I'm not sure if that's practical given that clients may submit reports out\n> of order.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/issues/81#issuecomment-880024893>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIL2KNYFFHI2IGAFBGLTXWZONANCNFSM5AKAXSFA>\n> .\n>\n",
          "createdAt": "2021-07-14T16:22:01Z",
          "updatedAt": "2021-07-14T16:22:01Z"
        }
      ]
    },
    {
      "number": 82,
      "id": "MDU6SXNzdWU5NDM4MTgxNDQ=",
      "title": "Allow batch \"replay\" as required by the PDA protocol (was \"Inter-batch replay prevention\")",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/82",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Similar to #81, it should not be possible for a share to be included in more than one batch. The timestamp currently gives us some way of ensuring that reports map to at most batch, but not a way of ensuring that reports aren't included two _overlapping_ batch windows. It's not immediately clear if include a single report in two overlapping batch windows is a problem, or if a problem only arises when these windows are disjoint. We should probably clarify what are the inter-batch requirements here and what sort of replay prevention is required.",
      "createdAt": "2021-07-13T20:52:28Z",
      "updatedAt": "2021-12-29T20:03:37Z",
      "closedAt": "2021-12-29T20:03:37Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The aggregators (either the leader or helper) can prevent the collector from getting outputs from overlapping batches by remembering batch intervals. For example, suppose the leader responded successfully to a collect request for time interval `[t0, t1)`. Then suppose that, some time later, the collector requests output shares for `[t0, t2)`, where `t0 < t1 < t2`. The potential privacy violation occurs because the sets of reports in each batch are non-disjoint. To prevent this, the leader might respond to the second request with an error, or it might just include the reports in `[t1, t2)`.",
          "createdAt": "2021-07-14T00:37:47Z",
          "updatedAt": "2021-07-14T00:37:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "If aggregators are meant to use the report timestamp to protect against replay, then I think it needs to appear in the AEAD-ed message constructed by the client (i.e. `PAEncryptedInputShare`). Otherwise a leader could insert arbitrary timestamps into `PAAggregateSubReq.time` and trick helpers into exceeding privacy budgets or double counting inputs.\r\n\r\nAdditionally, a batching client could insert arbitrary values into `PAUploadFinishReq.time`.",
          "createdAt": "2021-07-14T15:56:12Z",
          "updatedAt": "2021-07-14T15:57:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I created a PR that addresses this issue partially. What's left to do is allow multiple collect requests over the same batch, *as required by the PDA protocol* (e.g., Hits). I have an idea for how to do this, I'll follow up with a PR.",
          "createdAt": "2021-08-04T19:24:34Z",
          "updatedAt": "2021-08-04T19:24:34Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by https://github.com/abetterinternet/ppm-specification/pull/137.",
          "createdAt": "2021-12-29T20:03:37Z",
          "updatedAt": "2021-12-29T20:03:37Z"
        }
      ]
    },
    {
      "number": 85,
      "id": "MDU6SXNzdWU5NDQ4NjUwMTU=",
      "title": "Choose encoding format for protocol messages",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/85",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The protocol document uses TLS notation to describe messages exchanged between servers, e.g.:\r\n```\r\nstruct {\r\n  uint8 config_id;\r\n  opaque enc<1..2^16-1>;\r\n  opaque payload<1..2^16-1>;\r\n} PDAEncryptedInputShare;\r\n```\r\n\r\nHowever it's not clear how we will actually encode messages on the wire. JSON bodies seems most conventional for REST APIs, but at least @ekr assumed we would use TLS encoding. We should choose an encoding format and update the document appropriately.",
      "createdAt": "2021-07-14T23:17:54Z",
      "updatedAt": "2021-07-15T01:42:47Z",
      "closedAt": "2021-07-15T01:42:47Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The document should specify the encoding. We went with TLS binary encoding because it's what most folks were comfortable with at the time of writing. FWIW, I'm happy to change course if there's a good reason to :)",
          "createdAt": "2021-07-14T23:32:29Z",
          "updatedAt": "2021-07-14T23:32:29Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we should close this and stick with TLS.",
          "createdAt": "2021-07-15T01:25:15Z",
          "updatedAt": "2021-07-15T01:25:15Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Having thought about it over dinner: I raised this because I assumed that \"most people\" would prefer JSON bodies over a binary encoding, but personally I've never liked JSON (too much `\"`) so I'll let \"most people\" argue for it somewhere down the line instead of making a poor case for it myself.",
          "createdAt": "2021-07-15T01:42:47Z",
          "updatedAt": "2021-07-15T01:42:47Z"
        }
      ]
    },
    {
      "number": 86,
      "id": "MDU6SXNzdWU5NDQ4NjY5OTg=",
      "title": "Revisiting the security model",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/86",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This is a follow up to a discussion we had during the 2021/7/14 design call. We were concerned about a handful of attacks that may require some fundamental design changes to address properly. To focus this discussion, it was decided that we would revisit the security model consider how those attacks fit in. I've taken the liberty of recalling the model and filling in some gaps where necessary. Please, feel free to correct anything that is incorrect or unclear --- we can edit the description as we go. The goal is to get a common understanding of what attacks are in scope and which aren't.\r\n\r\ncc/ @ekr, @csharrison\r\n\r\n### Security model (for privacy)\r\n\r\n**Execution model.** Our execution model has two phases.\r\n\r\n1. In the _trusted setup phase_, public parameters, like the PDAParam and any assets needed to establish a server-authenticated secure channel (i.e., certificates and signing keys) are distributed. The adversary is given the public parameters as well.\r\n1. In the _attack phase_, the adversary runs the attack on the protocol. We assume the adversary has complete control of the network, meaning it relays all protocol messages and may send any message it wants to any party at any time. Further, it may corrupt any party it wants at any time, so long as this doesn't violate the non-collusion assumption of the security goal. In doing so, it learns that party's secret assets, i.e., any signing or encryption keys.\r\n\r\n[UPDATE 2021/7/26] An important difference between our execution model and that of Prio [BC17] and Hits [BBC+21] is that their network is synchronous (the adversary doesn't control transmission of messages from honest parties) and they assume each connection is ideally authenticated. We're assuming neither, which means our attacker is significantly stronger.\r\n\r\n**Privacy goal.** Currently the doc describes the following, informal security goal: As long as one of the aggregators is honest, the adversary learns nothing about the inputs of honest clients _except what it can infer from the output of the protocol_. This thinks of the attacker as colluding with the collector.\r\n\r\nThe details vary slightly, but the Prio [BC17] and Hits [BBC+21] papers formalize this security goal in roughly the following way (see [BC17, Appendix A]): The \"view\" of the adversary is defined to be the set of messages exchanged during the trusted setup and attack phases, as well as any assets belonging to corrupted parties. A PDA protocol is \"private\" if the view of every reasonably efficient (i.e., PPT) adversary can be efficiently simulated, _given the output of the aggregation function computed over the honest inputs_. More precisely, for all inputs `x_1, ..., x_N` and every PPT adversary that corrupts all but one aggregator and all but `N` clients, there exists a PPT simulator that, on input of `f(x_1, ..., x_N)`, outputs a string that is computationally indistinguishable from the adversary's view.\r\n\r\n### Current attacks\r\n\r\nThe attacks we discussed are:\r\n1. #82 A malicious leader can replay a report across two batches (this may be mitigated already).\r\n1. #81 A malicious leader can replay a report within a batch.\r\n1. #20 A network attacker can try a Sybil attack (\"stuff\" the batch with `n-1` bogus reports).\r\n\r\n(As a reminder, these kinds of attacks were anticipated in the original Prio paper. I would suggest folks go back and read [BC17, Section 7]. I found it to be a helpful refresher.)\r\n\r\nWhat's notable about the formalism above is that it concedes Sybil attacks: An attacker can learn a client's input in full, but this would not be deemed an attack by the model. (In other words, the expression \"except what it can infer from the output of the protocol\" is doing a lot of heavy lifting! See [BC17, Section 7].) On the other hand, a malicious aggregator attempting to replay an honestly generated report would be considered an attack in our model.\r\n\r\n[UPDATE 2021/7/26] I don't think the replay attacks are captured by existing formal definitions [BC17, BBC+21].\r\n\r\n---\r\n[BC17] Boneh and Corrigan-Gibbs. \"Prio: Private, Robust, and Scalable Computation of Aggregate Statistics.\" https://crypto.stanford.edu/prio/paper.pdf\r\n[BBC+21] Boneh et al. \"Lightweight Techniques for Private Heavy Hitters.\" https://eprint.iacr.org/2021/017",
      "createdAt": "2021-07-14T23:22:32Z",
      "updatedAt": "2021-12-30T17:18:59Z",
      "closedAt": "2021-12-30T17:18:59Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "For my part, I will concede that, if the attacker colludes with the collector, then there is no hope of defeating Sybil attacks without doing more than what we're doing. Further, I tend to agree with @csharrison and @hostirosti's assessment that client authentication is the best mitigation. However, I don't think we want to _require_ client authentication, do we? There are cheaper (if less complete) ways to mitigate these attacks. In any case, they're out-of-scope for the current model. (We might also consider changing the model.)",
          "createdAt": "2021-07-14T23:28:06Z",
          "updatedAt": "2021-07-14T23:35:54Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> However, I don't think we want to require client authentication, do we?\r\n\r\nYeah, requiring client authentication seems like a non-starter for a lot of use cases.",
          "createdAt": "2021-07-15T01:29:27Z",
          "updatedAt": "2021-07-15T01:29:27Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree that we should not require client authentication because it sets a high bar for what kind of client can participate in a private data aggregation system. Additionally, I think that we would have a hard time usefully specifying how client authentication should work because the means of injecting identities into clients and establishing trust between clients and aggregation servers will vary widely from deployment to deployment.\r\n\r\nHowever, it's not hard to imagine deployments where client auth is viable and valuable so we should make sure that it's possible for deployments to use an authenticating proxy that can defend against Sybil attacks for additional assurances beyond what this protocol offers. The idea is that the real clients (i.e., app installations or individual garage door openers) can authenticate to a batching client server using some bespoke authentication protocol, and then the batching client can relay a batch of reports to a PDA leader. Since we're already talking about allowing batching clients (#64), I think we're on our way.\r\n\r\nTo enable that, I think we need a way for a deployment of PDA to restrict who can submit reports. In a batching client setup, if an attacker can obtain the `PDAParam`s (which we don't consider confidential; some deployments may wish to make them widely available for transparency, and reverse engineers will be able to extract them from clients anyway), then they can submit reports directly to the leader and defeat the batching client's Sybil attack protection.\r\n\r\nSo maybe I've come back around to convincing myself that we do want client authentication, or at least enough of it to enable deployments to implement batching clients?\r\n\r\nI think we should also evaluate what the state of the art in the space of metrics collection is. What do existing, non-MPC telemetry systems do to protect against Sybil attacks?",
          "createdAt": "2021-07-21T15:55:19Z",
          "updatedAt": "2021-07-21T15:58:33Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I think we should also evaluate what the state of the art in the space of metrics collection is. What do existing, non-MPC telemetry systems do to protect against Sybil attacks?\r\n\r\nSuch defenses aren't necessary, since normally the collector sees all users' data in the clear. They only matter in the context of anonymous communication.\r\n",
          "createdAt": "2021-07-21T16:02:37Z",
          "updatedAt": "2021-07-21T16:02:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As of the 2021/07/21 design call, I think this is where we are: Sybil attacks are out-of-scope for the core protocol, but we ought to account for deployments that can afford some sort of client attestation mechanism to mitigate them.",
          "createdAt": "2021-07-21T21:34:23Z",
          "updatedAt": "2021-07-21T21:34:23Z"
        }
      ]
    },
    {
      "number": 89,
      "id": "MDU6SXNzdWU5NTAxMzYzMzQ=",
      "title": "Discuss client attestation in security considerations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/89",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-06"
      ],
      "body": "Many deployments will be in a position to have clients attest that (1) the report was generated by a trusted client or (2) the report was generated within a trusted execution environment. (1) allows you to mitigate Sybil attacks by only consuming reports generated by trusted clients (#86, #20). On top of that, with (2) you might choose to forego expensive MPC for validating inputs altogether (#45). We don't want to require these features for all deployments, but they're useful enough that we should make sure our protocol accommodates them.\r\n\r\nWith that in mind, it would be useful if folks that plan to do this would provide some details about how they envision composing (1) or (2) with PDA**. The question to answer is what changes, if any, are needed for the core spec.\r\n\r\ncc/ @tgeoghegan, @csharrison, and @hostirosti\r\n\r\n** 2022/1/18: Note that \"PDA\" was the working name for the PPM protocol at the time this issue was initially discussed.",
      "createdAt": "2021-07-21T21:49:00Z",
      "updatedAt": "2023-09-20T15:18:39Z",
      "closedAt": "2023-09-20T15:18:39Z",
      "comments": [
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "For our use-case we were thinking of something like [Privacy Pass](https://datatracker.ietf.org/wg/privacypass/documents/) for anonymous attestation. However, this was in the context of a collector / proxy sitting between the client and the leader who is responsible for issuing and redeeming tokens. This design would put all attestation outside of the PDA protocol, and allow the collector to determine which clients are trusted. However, it only benefits the collector which has a different set of requirements from the PDA system.\r\n\r\nCollectors want to make sure their data is accurate. The PDA system wants to ensure privacy. To that end I could imagine two layers of attestation:\r\n1. A layer of attestation that records are trusted by the collector\r\n2. A layer of attestation that records are trusted by PDA\r\n\r\nHowever, I don't have any concrete ideas on how to accomplish (2) for our use-case realistically.",
          "createdAt": "2021-07-23T02:05:25Z",
          "updatedAt": "2021-07-23T02:05:25Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> For our use-case we were thinking of something like [Privacy Pass](https://datatracker.ietf.org/wg/privacypass/documents/) for anonymous attestation. However, this was in the context of a collector / proxy sitting between the client and the leader who is responsible for issuing and redeeming tokens. This design would put all attestation outside of the PDA protocol, and allow the collector to determine which clients are trusted. \r\n\r\nThis batched upload idea is still on the table, I think: see https://github.com/abetterinternet/prio-documents/issues/64. In particular, I think we're all in agreement that some collector/proxy should be able to opaquely carry out some client attestation thing and upload only \"trusted\" reports to the leader. (@tgeoghegan calls this the \"ingestor\", I believe.) The problem, as you point out, is the trust relationship between the leader and uploader.\r\n\r\n> Collectors want to make sure their data is accurate. The PDA system wants to ensure privacy. To that end I could imagine two layers of attestation:\r\n> \r\n>     1. A layer of attestation that records are trusted by the collector\r\n> \r\n>     2. A layer of attestation that records are trusted by PDA\r\n> \r\n> \r\n> However, I don't have any concrete ideas on how to accomplish (2) for our use-case realistically.\r\n\r\nYeah, it's not clear to me either what the best thing would be here. As a strawman, consider having the uploader sign the set of reports and have the aggregators verify the signature. If the uploader *is* the collector, then this scheme implies a trust relationship between the collector and at least one aggregator. (In particular, the adversary could still do a Sybil attack.) For this to work, you'd want there to be some sort of organizational separation between the collector and uploader.",
          "createdAt": "2021-07-26T16:35:30Z",
          "updatedAt": "2021-07-26T17:01:58Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Taking a step back, why do we need any sort of attestation _in this protocol_? Both (1) and (2) seem very deployment specific to me. In particular, if Sybil attacks are out of scope for the core protocol, then surely they should be addressed _outside_ of the core protocol, right? One can add attestation via Privacy Pass or whatever in the protocol that wraps PDA. ",
          "createdAt": "2021-07-26T17:19:08Z",
          "updatedAt": "2021-07-26T17:19:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ideally yes, but the question we're asking here is whether one would need to change our spec in order to accommodate a mechanism for (1) or (2).  ",
          "createdAt": "2021-07-26T17:21:00Z",
          "updatedAt": "2021-07-26T17:21:00Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "This seems like a great case for an extensible protocol, or as @chris-wood  suggests, a wrapper protocol.\r\n\r\nI think it's clear that one can have a PDA MVP without this.\r\n\r\n",
          "createdAt": "2021-07-26T17:23:31Z",
          "updatedAt": "2021-07-26T17:23:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Looks like the consensus is to park this.",
          "createdAt": "2021-07-29T17:04:17Z",
          "updatedAt": "2021-07-29T17:04:17Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Way back in DAP-02 (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/305), we agreed to push request authentication out of the DAP protocol, on the premise that implementations and deployments are better positioned to make these choices than DAP is. I think the same is true of request attestation, especially if we think of attestation as being a feature of an authentication scheme.\r\n\r\nAt most, we should discuss in security considerations how client auth and attestation can help mitigate Sybil attacks. I'm repurposing this issue for that objective and we'll deal with it in the scope of #460.",
          "createdAt": "2023-05-26T21:02:58Z",
          "updatedAt": "2023-05-26T21:02:58Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "This is addressed in the recent security considerations rewrite, under [section 7.2.1, Client authentication](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-client-authentication).",
          "createdAt": "2023-09-20T15:18:39Z",
          "updatedAt": "2023-09-20T15:18:39Z"
        }
      ]
    },
    {
      "number": 92,
      "id": "MDU6SXNzdWU5NTYwNzIxNjU=",
      "title": "Term \"sub-request\" is confusing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/92",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Aggregate and output share requests are specified in a way that allows the leader to \"batch\" multiple reports in the same request. Right now we avoid using the word \"batch\" to describe this, since batch refers to the set of reports used to compute an output. To disambiguate this, we use the term \"sub-request\" to refer to the input share and PDA-specific protocol message for each report. This is fairly awkward language and may lead to confusion. ",
      "createdAt": "2021-07-29T17:49:28Z",
      "updatedAt": "2022-05-11T15:19:41Z",
      "closedAt": "2022-05-11T15:19:40Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #223.",
          "createdAt": "2022-05-11T15:19:40Z",
          "updatedAt": "2022-05-11T15:19:40Z"
        }
      ]
    },
    {
      "number": 95,
      "id": "MDU6SXNzdWU5NjA4MDIzMDg=",
      "title": "Have the leader encrypt its output share to the collector",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/95",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "Right now only the helper's output share for a batch of reports is encrypted to the collector. We previously agreed that all input shares should be encrypted to the corresponding aggregator (https://github.com/abetterinternet/prio-documents/issues/69). It makes sense to encrypt the leader's output share for the same reasons we discussed there.\r\n\r\nAlong the way, the message format should be made to accommodate multiple helpers, as we did for the input shares.",
      "createdAt": "2021-08-04T17:52:17Z",
      "updatedAt": "2021-08-23T20:36:59Z",
      "closedAt": "2021-08-23T20:34:45Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Fixed in #103. Making \"the message format [...] accommodate multiple helpers\" is specifically tracked in #117 and resolved by #135.",
          "createdAt": "2021-08-23T20:36:59Z",
          "updatedAt": "2021-08-23T20:36:59Z"
        }
      ]
    },
    {
      "number": 98,
      "id": "MDU6SXNzdWU5NjU0MjI5NDY=",
      "title": "The wrapper-core protocol interface",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/98",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "On the last design call (2021/08/04) we decided to split our work into two (sets of) documents: one for the \"core\" cryptographic protocols (Prio, Hits, ...) and another for the \"wrapper\" protocol (basically the protocol bits we've been working on so far). The next step is to specify the \"interface\" between these (sets of) documents. This is our goal for this issue.\r\n\r\nSo far we've been reasoning about this core-wrapper interface by analogy of HPKE and its applications (ECH, OHTTP, etc.), which may indeed be an apt analogy, but it falls short of a precise definition. To that end, I'd like to suggest that the interface needs to accomplish two things.\r\n\r\n- (1) Specify the syntax of every PDA protocol. The syntax should cover:\r\n        - (1a) generation of input shares from a measurement;\r\n        - (1b) the state machine of the leader and helper and how they respond to messages (in the core protocol);\r\n        - (1c) accumulation of input shares into an output share; and\r\n        - (1d) combination of output shares into the output.\r\n- (2) Specify the security requirements of every PDA protocol. This has three main components:\r\n        - (2a) The expected _communication model_, which determines how much control over the network the adversary has. (This may be different from the communication model of the wrapper protocol, as discussed below.)\r\n        - (2b) A baseline notion of privacy in the expected communication model that allows any number of clients and all but one aggregator to be corrupted.\r\n        - (2c) A baseline notion of robustness in the expected communication that allows any number of corrupt clients to be corrupted, but requires all aggregators to be honest.\r\n\r\nMuch of what we need is already worked out, but there are some gaps that require us to make some decisions. Two such gaps are discussed below.\r\n\r\n## (2a) Communication model\r\n\r\nAs observed in #86, we've been considering attacks outside the formal model of either Prio or Hits (specifically Sybil and replay attacks). The communication model for the core protocol may therefore differ from (i.e., make stronger assumptions about the adversary than) the communication model for the wrapper protocol. I would argue that this is perfectly reasonable. In fact, the main job of the wrapper protocol should be to \"emulate\" whatever communication model is expected by the core protocol.\r\n\r\nWe are already well on our way here. The following mechanisms are used to provide properties that are implicit in the communication models of the papers:\r\n- HPKE encryption creates a confidential channel between the client and each honest aggregator.\r\n- The report's timestamp and jitter fields define a total ordering on reports, which the aggregators can use to ensure each report is counted in at most one batch.\r\n\r\nSome bits are still missing, however:\r\n- Who manages the report queue? It seems simplest if the core protocol can assume in-order, non-repeated reports. The wrapper protocol can then do this enforcement. \r\n- How are messages authenticated? Currently, the adversary can impersonate the leader since leader->helper messages are not leader authenticated. This may lead to an attack on robustness. We should probably address this in the wrapper protocol such that the core protocol can assume these messages are authenticated.\r\n\r\n## (2b) Baseline privacy goal\r\n\r\nIn Prio, the adversary learns nothing beyond the aggregate of the honest clients' inputs. In contrast, in Hits, the adversary doesn't just learn the heavy hitters (HHs); it may also learn the frequency of non-HH inputs that share a common prefix of a non-HH input with some HH. This leakage is inevitable because the aggregators learn each intermediate set of candidate prefixes at the start of each collect request.\r\n\r\nWe've observed that we can mitigate this leakage somewhat [with differential privacy (DP)](https://github.com/WICG/conversion-measurement-api/blob/main/AGGREGATE.md#privacy-budgeting). Thus, one way to address this gap between Prio and Hits may be to require that each PDA protocol provide DP. This is probably too high a cost, however, given that not all PDA tasks need it.\r\n\r\nA simpler alternative might be to limit the syntax so that a single collect request is in scope. This seems natural, since a single round of Hits provides basically the same security properties as Prio. DP could still be added, but this would merely augment whatever the baseline privacy goal aims to provide.\r\n",
      "createdAt": "2021-08-10T21:38:03Z",
      "updatedAt": "2021-12-30T17:17:26Z",
      "closedAt": "2021-12-30T17:17:26Z",
      "comments": []
    },
    {
      "number": 102,
      "id": "MDU6SXNzdWU5Njc1ODkxNjA=",
      "title": "Collector configuration endpoint",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/102",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "PPM uses HPKE to achieve confidentiality and authenticity independently of the underlying transport. To that end, any server that receives messages in the protocol must provide an HPKE configuration to other entities so that messages can be encrypted.\r\n\r\nFor the leader and helper servers, this is achieved by the `leader_url` and `helper_url` fields (respectively) in `struct PPMParam`. Those servers' current HPKE configs can then be obtained from a well-defined HTTP endpoint relative to those URLs.\r\n\r\nFor the collector, we instead have `HpkeConfig collector_config` in `struct PPMParam`. The advantage of this approach is that it absolves the collector of needing to act as an HTTP server, since the collector otherwise is merely a client of the leader's `collect` endpoint. The downsides are that it's different from the other servers, which is a bit surprising and unappealing, and that it makes it impossible to rotate a collector's HPKE key without constructing and distributing a new `struct PPMParam` to all the participants, which includes the potentially numerous clients, in spite of the fact that none of them will ever need to encrypt messages to the collector.\r\n\r\nThis issue tracks improvements to collector configuration discovery. We should consider making collector HPKE configs discoverable the same way that leader and helper ones are, weighing that against the extra operational burden placed on collectors.",
      "createdAt": "2021-08-11T22:39:06Z",
      "updatedAt": "2023-09-21T15:27:02Z",
      "closedAt": "2023-09-21T15:27:02Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Per today's design meeting, putting in the parking lot for now, as this doesn't need to block prototyping or other forward progress on the protocol.",
          "createdAt": "2021-08-11T22:39:35Z",
          "updatedAt": "2021-08-11T22:39:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hey @tgeoghegan, this issue make some pretty crusty references, including to `PPMParam`!!! It would be useful to get an update on this issue. How are you thinking about it now?\r\n\r\nMy take is that this is something we still want to be deployment specific. I would suggest we close it.",
          "createdAt": "2022-09-13T02:37:23Z",
          "updatedAt": "2022-09-13T02:37:23Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, `PPMParam` is a non-sequitur now, a year later, but I think the core issue still exists, which is that we can't rotate collector HPKE configs without defining new tasks. I still think that's a problem we should consider.",
          "createdAt": "2022-09-14T23:07:50Z",
          "updatedAt": "2022-09-14T23:07:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As of #411, the VDAF verification key is immutable over the lifetime of a task. It seems like we could get away with pegging the HPKE collector config to the task as well.\r\n\r\nIn either case, do you think we can close this issue in favor of a solution to #237? We would need to update a reference to this issue in the text.\r\n\r\n",
          "createdAt": "2023-03-07T00:53:51Z",
          "updatedAt": "2023-03-07T00:54:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan: We need to make sure that the spec doesn't forbid rotating the collector HPKE config over a task's lifetime.",
          "createdAt": "2023-09-20T15:28:09Z",
          "updatedAt": "2023-09-20T15:28:09Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Apparently we reached the exact opposite conclusion back in #237, and at that time decided to add language that explicitly forbids rotating collector HPKE configurations ([ref](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.2-8)).\r\n\r\n>A task's parameters are immutable for the lifetime of that task. The only way to\r\nchange parameters or to rotate secret values like collector HPKE configuration\r\nand the VDAF verification key is to define a new task.\r\n\r\nI'm inclined to stick with the decision made in #237 and not do anything here. I'll close this issue in another 24 hours unless someone objects.",
          "createdAt": "2023-09-20T16:51:29Z",
          "updatedAt": "2023-09-20T16:51:29Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah! good catch. I'd say this is up to you guys: if the restriction is workable for Divvi Up, then let's close.",
          "createdAt": "2023-09-20T17:14:21Z",
          "updatedAt": "2023-09-20T17:14:21Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing, on the premise that in DAP, key rotation means task rotation.",
          "createdAt": "2023-09-21T15:27:02Z",
          "updatedAt": "2023-09-21T15:27:02Z"
        }
      ]
    },
    {
      "number": 104,
      "id": "MDU6SXNzdWU5Njk0ODYzMDg=",
      "title": "Does PPMParam need to be defined at all?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/104",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "The current specification describes a PPMParam struct and defines a TaskID as SHA-256(PPMParam). However, because the PPMParam is never rendered on the wire. Rather it is just serialized in order to be fed into SHA-256. The parameters are provided to the endpoints out of band.\r\n\r\nConsider a client implementor who behaves as follows:\r\n\r\n- Create a JSON structure J that defines the parameters.\r\n- Translate J into PPAParam P.\r\n- Send the client the pair [J, TaskID = SHA-256(P)]\r\n\r\nYou'll note that the client never sees the PPAParam at all. As far as I can tell, this would be a completely conformant implementation. Similar comments apply to the aggregators as we also do not define how *they* are configured. So, similarly they could just be sent JSON and have a TaskID -> JSON lookup table. This isn't just a possible implementation, but it's likely to be a common one, because people have whatever central configuration system they have and don't necessarily want to carry around some binary blob.\r\n\r\nIn fact, the only reason people need to see the PPAParam structure at all as opposed to the information in it is to hash it to create TaskID (yes, i appreciate that I suggested the hashing thing).\r\n\r\nSo what I suggest is we merely make a list of the things that the client and the aggregators need to know respectively and not define any structure that contains that list. Then we can just pick the TaskID randomly.\r\n\r\nWhen we originally discussed hashing, the idea was to have parameters be transitively incorporated into the protocol to avoid situations where each side is using different parameters. It's not clear to me that this is that important. A malicious client can just lie and use its own parameters, and if there is a bug, then it's just as likely it involves misinterpreting the parameters.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "createdAt": "2021-08-12T20:09:19Z",
      "updatedAt": "2021-09-15T20:20:51Z",
      "closedAt": "2021-09-15T20:20:51Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think our goal was to force clients to commit to a set of PPMParams, but you're right, nothing really ties the actual behavior of clients or aggregators to the PPMParams. We put a 16 byte nonce into PPMParam with the intention of guaranteeing that PPMTaskID would be unique, but I think we can just instead have PPMTaskID be that nonce. As for the sizes, we picked len(PPMTaskID) = 32 to match the length of a SHA-256, and len(PPMParam.nonce) == 16 to match a v4 UUID. If PPMTaskID is going to be randomly generated instead of derived, then 16 bytes should suffice.",
          "createdAt": "2021-08-12T21:28:34Z",
          "updatedAt": "2021-08-12T21:28:34Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> So what I suggest is we merely make a list of the things that the client and the aggregators need to know respectively and not define any structure that contains that list. Then we can just pick the TaskID randomly.\r\n> \r\n> When we originally discussed hashing, the idea was to have parameters be transitively incorporated into the protocol to avoid situations where each side is using different parameters. It's not clear to me that this is that important. A malicious client can just lie and use its own parameters, and if there is a bug, then it's just as likely it involves misinterpreting the parameters.\r\n\r\nThe problem we're trying to solve by binding the PPM params to encryption has less to do with malicious clients than honest ones. Namely, it's meant to ensure that honest aggregators reject reports that were generated with incorrect parameters.\r\n\r\nAs to whether this binding is strictly necessary, I agree that it's not clear that there's an attack it prevents right now. This binding is conservative in the sense that it aims to mitigate attacks that may come up in the future, as the set of parameters evolve.\r\n\r\nAnother benefit of this binding is that it ensures the client knows what privacy-sensitive parameters are enforced by honest aggreagators. Namely, the batch size, minimum batch window, and, eventually, parameters for DP.\r\n\r\nTo conclude, there may not be a need to define a serialization format for PPM params, and it would be fine to just randomly the task ID. However, there should be some sort of binding between the params and HPKE encryption.\r\n\r\n",
          "createdAt": "2021-08-16T16:33:09Z",
          "updatedAt": "2021-08-16T16:33:09Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "You seem to be assuming a very specific implementation, but nothing in the spec requires that, as shown by my example above.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2021-08-16T18:30:18Z",
          "updatedAt": "2021-08-16T18:30:18Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Based on our conversation in the call on 8/18, I think the consensus is\r\n\r\n- we should not define `struct Param` but instead informally list the parameters that all participants must agree on\r\n- `task_id` (which does appear in several on-the-wire messages) will become a randomly generated sequence of bytes instead of being a SHA-256 of the `Param`\r\n\r\nUnless someone objects (tagging @chris-wood to make sure he sees this issue as he hasn't participated yet) I'm going to make this doc change this week, after landing #135.",
          "createdAt": "2021-08-23T18:38:54Z",
          "updatedAt": "2021-08-23T18:38:54Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "#135 seems like it complicates this, since we'd have to spell out some way to sort aggregators. Do we want to land this change before that one?",
          "createdAt": "2021-08-23T19:15:01Z",
          "updatedAt": "2021-08-23T19:15:07Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I want to land #135 first, since it's just about done. I don't think it'll be too hard to change references to `Param.aggregators` into references to an ordered list of aggregators.",
          "createdAt": "2021-08-23T20:26:11Z",
          "updatedAt": "2021-08-23T20:26:11Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Sounds good!",
          "createdAt": "2021-08-23T20:32:00Z",
          "updatedAt": "2021-08-23T20:32:00Z"
        }
      ]
    },
    {
      "number": 105,
      "id": "MDU6SXNzdWU5NzA1NzU5MjE=",
      "title": "DefineMIME types for all of the things we are carrying over HTTP",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/105",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-13T17:04:22Z",
      "updatedAt": "2021-08-19T16:21:11Z",
      "closedAt": "2021-08-19T16:21:11Z",
      "comments": []
    },
    {
      "number": 106,
      "id": "MDU6SXNzdWU5NzA1ODA1MjM=",
      "title": "Caching for HpkeConfig",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/106",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [],
      "body": "It seems silly to have clients revalidating the HPKE key for each request, so we probably want some kind of caching. I'm not sure HTTP Caching is quite what we want, but something... \r\n\r\n@martinthomson @mnot recommendations?",
      "createdAt": "2021-08-13T17:11:13Z",
      "updatedAt": "2021-08-18T20:33:17Z",
      "closedAt": "2021-08-18T20:33:17Z",
      "comments": [
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'm going to need a little more context to provide good advice.\r\n\r\nIf we don't need key consistency (it doesn't appear so) and you only need correctness, then the client should just be able to talk directly to the collector and get keys periodically.  HTTP caching works fine for that purpose.\r\n\r\nThis would also need a rejection message so that the collector (or the leader on their behalf - the leader is more likely to learn about bad key identifiers before clients) can reject an out-of-date key in case the cache needs to be invalidated.",
          "createdAt": "2021-08-16T01:47:44Z",
          "updatedAt": "2021-08-16T01:47:44Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "We do not need key consistency.\r\n\r\nThe rejection message seems like a good idea.",
          "createdAt": "2021-08-16T02:06:52Z",
          "updatedAt": "2021-08-16T02:06:52Z"
        }
      ]
    },
    {
      "number": 107,
      "id": "MDU6SXNzdWU5NzA1ODgyNTI=",
      "title": "Redefine errors to line up with ACME",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/107",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "ACME has a pretty well defined error structure, including RFC 8707 problem documents. We should look at adopting that.\r\n\r\n\r\n",
      "createdAt": "2021-08-13T17:23:55Z",
      "updatedAt": "2021-08-18T00:36:32Z",
      "closedAt": "2021-08-18T00:36:32Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Fixed by #120 ",
          "createdAt": "2021-08-17T21:01:08Z",
          "updatedAt": "2021-08-17T21:01:08Z"
        }
      ]
    },
    {
      "number": 108,
      "id": "MDU6SXNzdWU5NzA1ODg2MDk=",
      "title": "What is the URL in errors?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/108",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "PPMAlert is:\r\n\r\n~~~\r\nstruct {\r\n  PPMTaskID task_id;\r\n  opaque payload<1..255>;\r\n} PPMAlert;\r\n~~~\r\n\r\nAnd the text says:\r\n\r\n> where `task` is the associated PPM task (this value is always known) and\r\n`payload` is the message. When sent by an aggregator in response to an HTTP\r\nrequest, the response status is 400. When sent in a request to an aggregator,\r\nthe URL is always `[aggregator]/error`, where `[aggregator]` is the URL of the\r\naggregator endpoint.\r\n\r\nWhere does the URL appear?",
      "createdAt": "2021-08-13T17:24:38Z",
      "updatedAt": "2021-08-17T21:32:15Z",
      "closedAt": "2021-08-17T21:32:15Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The URL comes from the PPM parameters of the task.",
          "createdAt": "2021-08-16T16:36:02Z",
          "updatedAt": "2021-08-16T16:36:02Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I mean where does it go. There is no slot in this structure for \"URL\"",
          "createdAt": "2021-08-16T17:04:01Z",
          "updatedAt": "2021-08-16T17:04:01Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Fixed.",
          "createdAt": "2021-08-17T21:32:15Z",
          "updatedAt": "2021-08-17T21:32:15Z"
        }
      ]
    },
    {
      "number": 109,
      "id": "MDU6SXNzdWU5NzA3MjM4NzI=",
      "title": "Add structure in AggregateReq/Resp to reflect when you are processing the same sub-batch",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/109",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "A batch of reports can be processed incrementally in a set of sub-batches. Each sub-batch may mean multiple RTs between leader and helper. Right now, this is hidden in the Prio or HH-specific pieces, but it probably should be manifest above.",
      "createdAt": "2021-08-13T21:37:06Z",
      "updatedAt": "2022-05-11T19:54:59Z",
      "closedAt": "2022-05-11T19:54:59Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, it seems to me that `AggregateResp` needs a boolean in it so that helper can indicate to leader whether more rounds are needed.",
          "createdAt": "2021-08-17T23:51:06Z",
          "updatedAt": "2021-08-17T23:51:06Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe this has been resolved with PrepareStepResult (indicating \"continued\" or \"finished\").",
          "createdAt": "2022-05-11T19:54:59Z",
          "updatedAt": "2022-05-11T19:54:59Z"
        }
      ]
    },
    {
      "number": 110,
      "id": "MDU6SXNzdWU5NzA3Mjk4MTQ=",
      "title": "TODO: Fix the bounds for length-prefixed parameters in protocol messages",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/110",
      "state": "OPEN",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": ".(E.g., `<23..479>` instead of `<1..2^16-1>`.)]",
      "createdAt": "2021-08-13T21:52:11Z",
      "updatedAt": "2024-06-05T17:05:03Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 111,
      "id": "MDU6SXNzdWU5NzA3MzA2MTM=",
      "title": "Do something about long-running requests",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/111",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [],
      "body": "It's easy to imagine that a request will take a long time, especially if you are using Collect and that has to trigger a bunch of work from the leader. We need some way of handling this.",
      "createdAt": "2021-08-13T21:54:07Z",
      "updatedAt": "2022-03-07T18:52:42Z",
      "closedAt": "2022-03-07T18:52:42Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This intersects with something we were discussing last week, which is whether or not `struct PPMParam` should have `HpkeConfig collector_config` in it, or a URL from which collector config can be fetched dynamically. We decided on the former to spare collectors of running an online HTTP server. However, if we adopt an asynchronous model for collect requests, then we might need collectors to be an HTTP server anyway so that the leader can notify them that a collect request is complete, in which case we should make collectors have a `key_config` endpoint like the other servers.",
          "createdAt": "2021-08-17T23:38:01Z",
          "updatedAt": "2021-08-17T23:38:01Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton and I were talking about this today. We agree that `CollectReq`/`CollectResp` definitely needs to be made asynchronous since in the `poplar1` case, servicing a `CollectReq` will definitely incur lots of expensive work between the aggregators (this is less likely in the `prio3` case because the aggregators aren't blocked on obtaining the aggregation parameter from the collector in that VDAF).\r\n\r\nWhat I'm not sure if is whether we should make all the interactions between the aggregators during preparation and verification asynchronous, and I'm not sure how to decide. It seems like it should be possible for leaders to scope each aggregate requests to a small enough number of shares that each one can reasonably be serviced synchronously.",
          "createdAt": "2022-01-22T01:56:22Z",
          "updatedAt": "2022-01-22T01:56:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "For collect requests, I'd also note here that asynchronicity is probably going to be important for `prio3`, as it's possible that the aggregation flow will fall behind the upload flow. (Imagine the helper can only handle 1000 reports/sec, but the clients are uploading 1million reports/sec.)\r\n\r\nAs for aggregate requests, I would expect these to be able to be completed fairly quickly (in a matter of seconds, in the worst case). Based on our experience so far, the most expensive part appears to be interacting with long-term storage, which is required for anti-replay and, if we take #174, looking up report shares. I think we can hold off on making these requests asynchronous until we've gotten enough deployment experience to determine if this is going to be a problem.",
          "createdAt": "2022-01-22T02:37:22Z",
          "updatedAt": "2022-01-22T02:37:22Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@martinthomson provided some interesting prior art that we should rely on when addressing this: https://mailarchive.ietf.org/arch/msg/ppm/dNf3eA-r6qGgeBZ9PhnHjJ1pSEc/",
          "createdAt": "2022-01-26T20:41:27Z",
          "updatedAt": "2022-01-26T20:41:27Z"
        }
      ]
    },
    {
      "number": 112,
      "id": "MDU6SXNzdWU5NzA3MzQ0NzQ=",
      "title": "Change PPMCollectResp to contain a list of shares",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/112",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "duplicate"
      ],
      "body": "We don't want to distinguish the helper from the leader and this will allow >1 helpers.\r\n\r\nWe can just force the leader to encrypt or have a cleartext version.",
      "createdAt": "2021-08-13T21:56:53Z",
      "updatedAt": "2021-09-08T02:33:11Z",
      "closedAt": "2021-09-08T02:33:10Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Subsumed by #117?",
          "createdAt": "2021-08-16T23:51:47Z",
          "updatedAt": "2021-08-16T23:51:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This was addressed in #135 ",
          "createdAt": "2021-09-08T02:33:10Z",
          "updatedAt": "2021-09-08T02:33:10Z"
        }
      ]
    },
    {
      "number": 113,
      "id": "MDU6SXNzdWU5NzA3Mzc4MDk=",
      "title": "Let's not call the random value \"jitter\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/113",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [
        "editorial"
      ],
      "body": "Because you'll also want a \"jitter buffer\" on the receiver side and that means something different.\r\n\r\nInstead of having a random value, why not just have time be to a nanosecond clock and then lexically sort between collisions?",
      "createdAt": "2021-08-13T22:05:40Z",
      "updatedAt": "2021-08-18T00:10:23Z",
      "closedAt": "2021-08-18T00:10:23Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Which will also help one remove duplicates.",
          "createdAt": "2021-08-13T22:05:51Z",
          "updatedAt": "2021-08-13T22:05:51Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, this is a bad name :)",
          "createdAt": "2021-08-16T16:38:43Z",
          "updatedAt": "2021-08-16T16:38:43Z"
        }
      ]
    },
    {
      "number": 114,
      "id": "MDU6SXNzdWU5NzA3Mzg3OTA=",
      "title": "Double check that leader and helper have the same shares",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/114",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "It's easy to see how this could get confused. Maybe we should have a hash over the input?",
      "createdAt": "2021-08-13T22:08:09Z",
      "updatedAt": "2021-08-26T15:17:07Z",
      "closedAt": "2021-08-26T15:17:07Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Nominally, the helper doesn't have to remember any shares ... ideally it only has to remember the timestamp/jitter-value of the last report it consumed in a given batch window. I wonder if that's sufficient to mitigate the issue you're describing?",
          "createdAt": "2021-08-17T01:16:52Z",
          "updatedAt": "2021-08-17T01:16:52Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Doesn't that just guarantee monotonicity?\r\n\r\nLike suppose we have this:\r\n\r\n```\r\nL->H: reports 1-10\r\nH->L: state1\r\nL->H: reports 11-20, state 1\r\nH->L: state 2\r\nL->H: reports 21-30, state 1\r\nH->L: state 2'\r\n```\r\n\r\nDoes something stop that?\r\n\r\n\r\n",
          "createdAt": "2021-08-17T01:22:31Z",
          "updatedAt": "2021-08-17T01:22:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "One way to detect if the state has been rewound is simply by using a counter to compute the nonce for encrypting the state.\r\n\r\n```\r\nL->H: reports 1-10\r\nH->L: decrypt state0 with nonce == 0, send state1 encrypted with nonce == 1\r\nL->H: reports 11-20, state 1\r\nH->L: decrypt state1 with nonce == 1; send state2 encrypted with nonce == 2\r\nL->H: reports 21-30, state 1\r\nH->L: attempt to decrypt state1 with nonce == 2, fail.\r\n```\r\n",
          "createdAt": "2021-08-17T01:32:59Z",
          "updatedAt": "2021-08-17T01:33:10Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Maybe you could describe what state you think the helper has to keep generally.\r\n",
          "createdAt": "2021-08-17T01:37:05Z",
          "updatedAt": "2021-08-17T01:37:05Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "For some context, we ran into this problem this week in Prio v2: ISRG's counterpart server fell far enough behind while processing batches of input shares from New Mexico that the two servers had inconsistent views of which shares' proofs had been validated at the time of scheduling an aggregation task, and so the two servers aggregated over inconsistent lists of inputs, which yields garbage that can't be reassembled into an output.\r\n\r\nMy intuition is that leader gets an explicit ack from helper on each `AggregateSubReq`, and so leader should be able to ensure that it only aggregates over those inputs that were acked by helper? However I think this gets much more complicated if we do #117, because then leader has to take the intersection of the sets of inputs acked by each helper, and then instruct helpers to only collect those shares that were acked by _all_ participating helpers.",
          "createdAt": "2021-08-17T23:45:42Z",
          "updatedAt": "2021-08-17T23:45:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Subsumed by https://github.com/abetterinternet/prio-documents/issues/141.",
          "createdAt": "2021-08-26T15:17:07Z",
          "updatedAt": "2021-08-26T15:17:07Z"
        }
      ]
    },
    {
      "number": 116,
      "id": "MDU6SXNzdWU5NzE5NTM0MjA=",
      "title": "Drop \"PPM\" prefix for all protocol messages",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/116",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Protocol messages are defined in the context of the PPM document, so the \"PPM\" seems redundant. I think we should just call things as they are, i.e., `TaskID` instead of `PPMTaskID`. ",
      "createdAt": "2021-08-16T17:31:33Z",
      "updatedAt": "2021-08-17T19:50:55Z",
      "closedAt": "2021-08-17T19:50:55Z",
      "comments": []
    },
    {
      "number": 117,
      "id": "MDU6SXNzdWU5NzIxODA0MzE=",
      "title": "Update protocol to generically allow >1 helper",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/117",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-16T23:48:36Z",
      "updatedAt": "2021-08-26T22:31:56Z",
      "closedAt": "2021-08-26T22:31:56Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "This seems to be a dupe fo #68 -- can we close?",
          "createdAt": "2021-08-18T15:19:04Z",
          "updatedAt": "2021-08-18T15:19:04Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Per discussion in the call today, I think this issue is about updating the various PDU definitions to allow for multiple helpers. For instance, `Param` currently has fields `Url leader_url` and `Url helper_url`, but it should instead have `Url aggregator_urls<1..lots>`. #68 is the more complex problem of extending the protocol to handle >1 helper, which probably involves more rounds of communication. I'm going to grab this issue because I ran into #133 which I think is related, so I'll fix both.",
          "createdAt": "2021-08-18T20:26:32Z",
          "updatedAt": "2021-08-18T20:26:32Z"
        }
      ]
    },
    {
      "number": 118,
      "id": "MDU6SXNzdWU5NzI5MzA2OTI=",
      "title": "Batch constraints are a bit funny",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/118",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "The current text has two batch constraints:\r\n\r\n```\r\n*  `batch_size`: The batch size, i.e., the minimum number of reports that are\r\n  aggregated into an output.\r\n* `batch_window`: The window of time covered by a batch, i.e., the maximum\r\n  interval between the oldest and newest report in a batch.\r\n```\r\n\r\nThis seems like it could have the result that collection is impossible. Suppose that we have:\r\n\r\n```\r\nbatch_size = 7200\r\nbatch_window = 3600\r\n```\r\n\r\nAnd reports come in at an average of 1/s. In this case it wouldn't be possible to satisfy these constraints because you would never be able to accumulate 7200 reports. More generally, it's not clear to me why ```batch_window``` should be a maximum rather than a minimum. How does that assist the helper/leader.\r\n\r\nISTM we should either have one constraint (probably ```batch_size```) or two but have them both be minimums. In either case, we probably want some way for the collector to learn when the limits are reached rather than having to poll.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "createdAt": "2021-08-17T17:52:51Z",
      "updatedAt": "2021-09-09T14:12:05Z",
      "closedAt": "2021-09-09T14:12:05Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> ISTM we should either have one constraint (probably batch_size) or two but have them both be minimums. In either case, we probably want some way for the collector to learn when the limits are reached rather than having to poll.\r\n\r\nHmm, I wonder if we can get away with a single constraint (batch_size) under the assumption that reports are totally ordered before aggregation? (Do we need to distinguish between intra- and inter-replay attacks in this case?) That would be simpler overall, I think.",
          "createdAt": "2021-08-17T21:20:59Z",
          "updatedAt": "2021-08-17T21:20:59Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Excitingly, the glossary says:\r\n\r\n```\r\n   Batch window:  The minimum time difference between the oldest and\r\n      newest report in a batch (in seconds).\r\n```\r\n\r\nSo we need to decide which it is.",
          "createdAt": "2021-08-17T23:12:44Z",
          "updatedAt": "2021-08-17T23:12:44Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Finishing up a PR For this now. Just to be clear, the intent was to enforce a *minimum* batch window, not a maximum. So I think the reference above is a typo.",
          "createdAt": "2021-08-20T01:11:28Z",
          "updatedAt": "2021-08-20T01:11:28Z"
        }
      ]
    },
    {
      "number": 130,
      "id": "MDU6SXNzdWU5NzM5MDkyMjM=",
      "title": "Cost implications of single leader model",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/130",
      "state": "CLOSED",
      "author": "hostirosti",
      "authorAssociation": "NONE",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "Assuming `Aggregators` and `Servers` are running on distinct cloud providers / hosting environments there is an additional egress cost incurred using the `Leader` model where one `Aggregator` coordinates input validation and data collection from all clients and then forwards data shares  / batches to respective `Aggregators` that are part of the setup.\r\n\r\n As of 2021-08-18 there are the following network costs of major cloud providers. *No claim of 100% accuracy*\r\n\r\nTL;DR Assuming `Aggregators` / `Servers` running on distinct providers using the Leader model we can assume $50 - $230 / TB added cost for egress network traffic.\r\n\r\n//TODO add how many input shares per TB approx.\r\n\r\n# Network Cost Comparison across providers\r\n\r\n## Ingress Network Cost\r\n*>>cost incurred by clients sending data to `Aggregator(s)`<<*\r\n*all prices per GB*\r\n\r\nAWS | Azure | GCP \r\n----|-------|------\r\n free | free | free \r\n\r\n## Egress Network Cost\r\n*>>cost incurred by sending data out leaving the provider (e.g. server communication, leader aggregator sending data to other aggregators<<*\r\n*all prices per GB*\r\n\r\nAWS | Azure | GCP\r\n-----|-------|------\r\n$0.05-$0.154<sup>1</sup> |   $0.0875 - $0.181<sup>2</sup> |  $0.045 - $0.23<sup>3</sup>\r\n\r\n<sup>1</sup> Tiered discount 1GB free, 1GB-10TB, 10TB-50TB, 50TB-150TB, 150TB+ \r\n<sup>2</sup> Tiered discount 5GB free, 5GB-10TB, 10TB-50TB, 50TB-150TB, 150TB-500TB, 500TB+\r\n<sup>3</sup> Tiered discount 0-1TB, 1-10TB, 10+TB Premium Tier starts at $0.08, Standard Tier goes down to $0.045 Tiered 0-10TB, 10-150TB, 150-500TB\r\n\r\n## Intra-Provider Network Cost\r\n*>>cost incurred by sending data within a provider between different regions<<*\r\n\r\n### Intra-Continental\r\n*all prices per GB*\r\nRegion | AWS | Azure | GCP\r\n---- | ---- | ---- | ----\r\nNA | $0.01-$0.02  |  $0.02 | $0.01\r\nEurope | $0.02 |  $0.02 | $0.02\r\nAsia | $0.09 | $0.08 | $0.05\r\nOceania | $0.098 | $0.08 | $0.08\r\nMEAA | $0.1105-$0.147 | $0.08 | $0.08\r\nSouth America | $0.138 | $0.16 | $0.08\r\n\r\n### Inter-Continental\r\n*all prices per GB*\r\nRegion | AWS | Azure | GCP\r\n---- | ---- | ---- | ----\r\nNA -> * | $0.01-$0.02 |  $0.05 | $0.08-$0.15<sup>1</sup>\r\nEurope -> * | $0.02 |  $0.05 | $0.08-$0.15<sup>1</sup>\r\nAsia, Oceania, Africa -> * | $0.086-$0.098 | $0.08 | $0.08-$0.15<sup>1</sup>\r\nSouth America -> * | $0.138 | $0.16 | $0.08-$0.15<sup>1</sup>\r\n\r\n<sup>1</sup> from/to Indonesia or Oceania2 $0.15 for GCP\r\n\r\n\r\n",
      "createdAt": "2021-08-18T17:40:23Z",
      "updatedAt": "2023-09-27T15:28:13Z",
      "closedAt": "2023-09-27T15:28:13Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Per today's design call, we agreed that we will stick with the leader model in the protocol for now. However, I am going to take a look at what the data egress costs are in Prio v2 relative to the amount of data we are moving, to provide some context on how much $$$ we are really talking about here. Further, I'm going to sketch out what a protocol variant that allows direct client to helper upload would look like, and how much cloud egress such a model would save, so that we have a better idea of how much money is saved.",
          "createdAt": "2021-08-18T18:04:03Z",
          "updatedAt": "2021-08-18T18:04:03Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "I would argue (in line with what I think is the current consensus) that having `Helpers` be minimal and stateless is preferable, however these egress costs will be significant. I propose we consider allowing the `Leader` run a `LeaderProxy` within each cloud provider, which could be specified by within the client, and could result in minimal egress costs. Here is a diagram (as best I could in ascii...), with the assumption that the `LeaderProxy` and `Helper` are within the same cloud, but top and bottom pairs are in different clouds:\r\n```\r\n                     +-------------+    +------------+\r\n            +-------->             |    |            |\r\n+--------+  |  +-----> LeaderProxy <---->   Helper   |\r\n|        |  |  |  +-->             |    |            |\r\n| Client +--|--+  |  +------^------+    +------------+\r\n|        |  |  |  |         |\r\n+--------+  |  |  |         |\r\n            |  |  |         |\r\n+--------+  |  |  |  +------v------+                    +-----------+\r\n|        |  |  |  |  |             |                    |           |\r\n| Client +--+  |  |  |    Leader   <--------------------> Collector |\r\n|        |  |  |  |  |             |                    |           |\r\n+--------+  |  |  |  +------^------+                    +-----------+\r\n            |  |  |         |\r\n+--------+  |  |  |         |\r\n|        |  |  |  |         |\r\n| Client +--|--|--+  +------v------+    +------------+\r\n|        |  |  |  +-->             |    |            |\r\n+--------+  |  +-----> LeaderProxy <---->   Helper   |\r\n            +-------->             |    |            |\r\n                     +-------------+    +------------+\r\n```\r\n\r\nOne added benefit of such an approach is the all of the egress cost is born by the leader, which further minimizes the responsibility and cost variance on the helpers.",
          "createdAt": "2021-11-12T23:44:41Z",
          "updatedAt": "2021-11-12T23:44:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I've been working on an implementation of PPM instantiated with `prio3`. In order to make this discussion more concrete, I took some measurements of the communication cost for various data types.\r\n\r\nThe table below contains results for 1000 reports. The first column is the data type (see https://docs.rs/prio/latest/prio/vdaf/prio3); the second column is the total amount of data uploaded by clients to the leader; the third column is the amount of data sent from the leader to the helper; and the final column is the amount of data data sent from the helper to the leader. Data rates exclude HTTP request framing.\r\n\r\n| data type | bytes processed | bytes sent | bytes received | \r\n| ---------- | -----------| ------------- | ------------ |\r\n| Prio3Count64 | 423.8 KB | 276.5 KB | 82.1 KB |\r\n| Prio3Histogram64 (10 buckets) | 837.9 KB | 280.4 KB | 86.0 KB |\r\n| Prio3Sum64 (32 bits) | 2.2 MB | 280.4 KB | 86.0 KB |\r\n\r\nExtrapolating, aggregating 1 million reports results in at most 273.8MB of egress, or about $0.06 at $0.23/GB.\r\n\r\nThe main thing to note here is that the amount of data uploaded by clients depends on the data type, but the amount of data sent from the leader to helper does not. This is because in `prio3` the helper's input share is always constant size, but the leader's depends on the data type. For `hits` on the other hand, the size of each input share is going to be linear in the length of the inputs. This is likely makes the concrete cost much higher, but we should measure it.\r\n",
          "createdAt": "2022-01-03T19:12:20Z",
          "updatedAt": "2022-01-03T19:12:20Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@eriktaubeneck wrote: \r\n> I propose we consider allowing the `Leader` run a `LeaderProxy` within each cloud provider, which could be specified by within the client, and could result in minimal egress costs. Here is a diagram (as best I could in ascii...), with the assumption that the `LeaderProxy` and `Helper` are within the same cloud, but top and bottom pairs are in different clouds: ...\r\n\r\nI'm not sure I see how this architecture helps with the state coordination problem. It seems like LeaderProxy just makes explicit a functionality that the Helper is going to have to implement it anyway. Why does making it explicit help?",
          "createdAt": "2022-01-03T19:24:53Z",
          "updatedAt": "2022-01-03T19:24:53Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "> I'm not sure I see how this architecture helps with the state coordination problem. \r\nAgreed, I don't believe it has an effect on state coordination. I'm only suggesting this in terms of minimizing cost.\r\n\r\nSuppose we have Agg1, Agg2, and Leader, which are in Cloud1, Cloud2, and Cloud3. If a message from Agg1 -> Agg2 needs to actually route through the Leader, i.e. Agg1 -> Leader -> Agg2, then that message would pay egress fees to both Cloud1 and Cloud2.\r\n\r\nIf instead we had a Leader Proxy running in Cloud1, the message could go from Agg1 -> Leader Proxy (Cloud 1) -> Agg2, which would only pay egress fees to Cloud1. It also has a nice site effect of consolidating all the egress fees onto the Leader.",
          "createdAt": "2022-01-04T18:49:14Z",
          "updatedAt": "2022-01-04T18:49:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Right now we don't have a situation where Agg1 would send a message to Agg2. In fact, for the moment, we don't support more than one helper. In my opinion, addressing this is situation is a bit premature.",
          "createdAt": "2022-01-04T21:34:10Z",
          "updatedAt": "2022-01-04T21:34:10Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "Got it. I misunderstood the added cost concern in this issue. Agreed that this is a premature optimization under current constraints. ",
          "createdAt": "2022-01-05T03:13:53Z",
          "updatedAt": "2022-01-05T03:13:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It seems like the ship has sailed on sending report shares directly to the Helper. However there are some other things we might do to minimize bandwidth:\r\n\r\n1. Avoid transmitting aggregation job in each job: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/405\r\n1. Avoid re-transmitting report shares across collection jobs: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/409",
          "createdAt": "2023-09-27T15:28:13Z",
          "updatedAt": "2023-09-27T15:28:13Z"
        }
      ]
    },
    {
      "number": 131,
      "id": "MDU6SXNzdWU5NzM5MTg2NDE=",
      "title": "What happens if one of the shares is corrupt?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/131",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We need some way for the helper to kick back an aggregate request with a bogus share without failing the entire batch.",
      "createdAt": "2021-08-18T17:53:21Z",
      "updatedAt": "2021-08-26T15:16:04Z",
      "closedAt": "2021-08-26T15:16:04Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "IMO this is the same as #114: we need to ensure that the leader and helper(s) are aggregating over the same set of reports. The leader already gets an `AggregateSubResp` from helper for each report (or doesn't), so it should be able to intersect the set of valid reports leader has with the set of valid report helper has and then issue an `OutputShareReq` that only references those reports. Currently `OutputShareReq` only has `batch_start` and `batch_end`. I think it needs either a list of report IDs, or perhaps a list of report IDs to exclude if that is more efficient. ",
          "createdAt": "2021-08-18T18:07:45Z",
          "updatedAt": "2021-08-18T18:07:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to this being a duplicate.\r\n\r\n@tgeoghegan's suggestion sounds a lot like what would be needed for the split-upload model that motivates #130. The main reason for the leader-only-upload model is that it avoids this cost by making the protocol simpler. The spec currently tries to solve this agreement problem by:\r\n1. Having the aggregators run an input-validation protocol.\r\n2. Having the leader hold the protocol state so that, at any given moment, it knows what set of reports are valid.\r\n3. Ensuring that each report is aggregated at most once by enforcing a total ordering on reports.\r\n\r\nWe can speculate about failure scenarios and whether these measure are sufficient --- as we already have done, quite a lot! --- but from where I stand it's not clear yet that there's a problem to solve here. I suggest that we hold off on specifying anything new until we're confident that it's necessary.",
          "createdAt": "2021-08-20T02:01:57Z",
          "updatedAt": "2021-08-20T02:01:57Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, now that I see there is an error, this seems like it should work OK.",
          "createdAt": "2021-08-20T02:38:07Z",
          "updatedAt": "2021-08-20T02:38:07Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Shall we close this in favor of https://github.com/abetterinternet/prio-documents/issues/141?",
          "createdAt": "2021-08-26T00:53:12Z",
          "updatedAt": "2021-08-26T00:53:12Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Seems like this, #141, and #114 all address the same problem.",
          "createdAt": "2021-08-26T02:00:33Z",
          "updatedAt": "2021-08-26T02:00:33Z"
        }
      ]
    },
    {
      "number": 133,
      "id": "MDU6SXNzdWU5NzQwMjgwMzk=",
      "title": "How does the leader know which `EncryptedInputShare` in a report is which?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/133",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "We have this definition:\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  Time time;\r\n  uint64 nonce;\r\n  Extension extensions<4..2^16-1>;\r\n  EncryptedInputShare encrypted_input_shares<1..2^16-1>;\r\n} Report;\r\n<...>\r\n* `encrypted_input_shares` contains the encrypted input shares of each of the aggregators.\r\n```\r\nThe intent is to eventually allow an arbitrary number of aggregators, but how is the leader meant to know which `EncryptedInputShare` goes to which aggregator? Is share 0 understood to be the leader's, and share 1 is the helper's?",
      "createdAt": "2021-08-18T20:23:47Z",
      "updatedAt": "2021-08-26T22:32:12Z",
      "closedAt": "2021-08-26T22:32:12Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, that's 0 and 1. But 3 and 4? LOL. \r\n\r\nMaybe we should do:\r\n\r\n```\r\nstruct {\r\n   AggregatorId aggregator;\r\n   EncryptedInputShare encrypted_input_shares<1..2^16-1>;\r\n} AnInputShare;\r\n```\r\n\r\nWhere ```AggregatorId``` is  TBD.\r\n",
          "createdAt": "2021-08-18T22:25:46Z",
          "updatedAt": "2021-08-18T22:25:46Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Ooh, vectors of labeled tuples are much better than my idea of requiring consistent ordering of per-aggregator values across `Report`, `Param` and `CollectResp`",
          "createdAt": "2021-08-18T23:20:00Z",
          "updatedAt": "2021-08-18T23:20:00Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is (going to be) resolved by #135.",
          "createdAt": "2021-08-23T20:37:46Z",
          "updatedAt": "2021-08-23T20:37:46Z"
        }
      ]
    },
    {
      "number": 138,
      "id": "MDU6SXNzdWU5NzczMjcyNzU=",
      "title": "Aggregator roles and key diversification",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/138",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "Currently, the derivation of HPKE context entangles a constant for `server_role`, which is 0x01 for the leader and 0x00 for the helper. Some iterations of PR #135, which aimed to generalize message definitions to support more than one helper, introduced a notion of an aggregator ID or role, and then used that role in the key derivation. That notion wound up being a little more complicated than what needed to be addressed in that PR, but we should still consider whether and how we want to formalize the notion of an aggregator's role and whether it should be involved in encryption key derivation.",
      "createdAt": "2021-08-23T18:57:06Z",
      "updatedAt": "2022-05-11T18:59:27Z",
      "closedAt": "2022-05-11T18:59:27Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not sure if this should go in label parking-lot. The server role constants we have now are sufficient for the deployments that will realistically materialize in the near or medium term, and I think we could define server roles in the future in a manner that is compatible with the existing constants.",
          "createdAt": "2021-08-23T19:01:50Z",
          "updatedAt": "2021-08-23T19:01:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm in favor of the generalization, but am fine with putting this in the parking lot.",
          "createdAt": "2021-08-26T00:51:51Z",
          "updatedAt": "2021-08-26T00:51:51Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "One important question we must address is the maximum number of aggregators in a protocol instantiation. We are already encountering places where [aggregators and clients must agree on the representation of the ID](https://github.com/abetterinternet/libprio-rs/pull/76).",
          "createdAt": "2021-09-14T21:23:50Z",
          "updatedAt": "2021-09-14T21:23:50Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We've long since introduced a formal `Role` enumeration whose layout was chosen to allow adding more helpers in the future. Closing.",
          "createdAt": "2022-05-11T18:59:27Z",
          "updatedAt": "2022-05-11T18:59:27Z"
        }
      ]
    },
    {
      "number": 139,
      "id": "MDU6SXNzdWU5Nzg1NjkzNjk=",
      "title": "Clarify encoding, endianness of multi-byte values in HPKE application info and AAD strings",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/139",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We specify HPKE context construction and sealing like so:\r\n\r\n```\r\nenc, context = SetupBaseS(pk, \"pda input share\" || task_id || server_role)\r\npayload = context.Seal(time || nonce || extensions, input_share)\r\n```\r\n\r\nWe should be explicit about the encoding and endianness of these values. Since we use [TLS presentation language](https://datatracker.ietf.org/doc/html/rfc8446#section-3) on the wire, I think the natural thing is to use that encoding when constructing these strings. So `time` (which is a uint64 counting seconds since epoch start) should be big endian, `extensions` should be the TLS presentation language encoding of the `struct Extension`, and so on.",
      "createdAt": "2021-08-24T23:50:40Z",
      "updatedAt": "2021-12-30T00:54:23Z",
      "closedAt": "2021-12-30T00:54:23Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "TLS-syntax specifies the byte encoding of anything represented in it, including endianness.",
          "createdAt": "2021-12-29T22:15:04Z",
          "updatedAt": "2021-12-29T22:15:04Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "At the risk of being tediously pedantic: I think we should explicitly state that the TLS syntax encoding of various structs is what should go into application info and AAD strings, to rule out the possibility of implementations just `memcpy`ing a C struct in there, or a JSON string or something. I'm not sure what the IETF standard of pedantry is here though.",
          "createdAt": "2021-12-29T22:58:50Z",
          "updatedAt": "2021-12-29T22:58:50Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I tend to think this is too pedantic, but if necessary, I would just say \"serialized\".",
          "createdAt": "2021-12-29T23:07:06Z",
          "updatedAt": "2021-12-29T23:07:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree this is too pedantic, and I don't see a need to add the word \"serialized\".",
          "createdAt": "2021-12-29T23:25:08Z",
          "updatedAt": "2021-12-29T23:25:08Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Fair enough, I am closing this issue then.",
          "createdAt": "2021-12-30T00:54:23Z",
          "updatedAt": "2021-12-30T00:54:23Z"
        }
      ]
    },
    {
      "number": 140,
      "id": "MDU6SXNzdWU5Nzg1ODI5NjU=",
      "title": "Specify how AEAD ciphertext and tag are represented in protocol messages",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/140",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "HPKE AEADs emit a ciphertext and a tag. HPKE doesn't specify for us how those two values should be represented, so we need to add some text explaining how the ciphertext and tag are combined into `EncryptedInputShare.payload` and `EncryptedOutputShare.encrypted_output_share` (we should probably also harmonize those field names because yuck).",
      "createdAt": "2021-08-25T00:20:40Z",
      "updatedAt": "2021-08-25T19:25:06Z",
      "closedAt": "2021-08-25T19:25:05Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is a non-issue: having re-read the HPKE spec as well as RFC5116 and RFC8439, there are unambiguous descriptions of how to represent `(ciphertext, tag)` for all the AEAD algorithms supported by HPKE ([AES-128-GCM and AES-256-GCM](https://datatracker.ietf.org/doc/html/rfc5116#section-5.1) and [ChaCha20Poly1305](https://datatracker.ietf.org/doc/html/rfc8439#section-2.8). So we do not need to specify it further at the PPM layer.",
          "createdAt": "2021-08-25T19:25:05Z",
          "updatedAt": "2021-08-25T19:25:05Z"
        }
      ]
    },
    {
      "number": 141,
      "id": "MDU6SXNzdWU5Nzk0NzMzODI=",
      "title": "Recovering after batch mismatch",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/141",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "ietf118"
      ],
      "body": "Note from today's call: the CFRG documents need to specify that after the aggregate phase, the helper and leader need to agree to which shares are valid. Depending on the crypto, this may require the leader to tell the helper which ones were valid, if the helper doesn't get the proof result.",
      "createdAt": "2021-08-25T17:41:27Z",
      "updatedAt": "2023-11-07T13:34:02Z",
      "closedAt": "2023-11-07T13:34:01Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe @cjpatton wants to address this.",
          "createdAt": "2021-08-25T18:05:23Z",
          "updatedAt": "2021-08-25T18:05:23Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Yep, I was just making a note so we didn't forget.\n\nOn Wed, Aug 25, 2021 at 11:05 AM Tim Geoghegan ***@***.***>\nwrote:\n\n> I believe @cjpatton <https://github.com/cjpatton> wants to address this.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/issues/141#issuecomment-905757569>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIJSHEOWSCKNGDPVW2TT6UWG7ANCNFSM5CZUDJBA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n",
          "createdAt": "2021-08-25T18:14:51Z",
          "updatedAt": "2021-08-25T18:14:51Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Wood and I spoke and decided he would be the best one to take this issue.",
          "createdAt": "2021-08-26T00:54:10Z",
          "updatedAt": "2021-08-26T00:54:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Options discussed so far:\r\n1. Have the helper wait for an ACK from the leader before aggregating its output share\r\n2. If the helper fails to respond to the request for some reason, have the leader \"rewind\" the helpers state, either by replaying the previous helper state (may impact anti-replay) or with a special \"compensation request\", which has the helper deduct the last output share form the total (Robert's idea).\r\n3. Run the protocol with multiple sets of helpers so that, if a fault occurs in one run, another run can be used.",
          "createdAt": "2021-10-12T15:00:12Z",
          "updatedAt": "2021-10-12T15:00:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood pointed this out, which should be useful: https://en.wikipedia.org/wiki/Two-phase_commit_protocol",
          "createdAt": "2021-10-12T20:22:53Z",
          "updatedAt": "2021-10-12T20:22:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As of DAP-01, there is a checksum in the AggregateShareReq that's used to confirm that the Aggregators agree on the set of aggregated reports. This is sufficient for \"error detection\", but we might want to do something for \"error correction\". I'm going to re-purpose this issue and put it in the parking lot.",
          "createdAt": "2022-09-13T02:31:39Z",
          "updatedAt": "2022-09-13T02:31:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 we decided to close this without doing anything. We already have error detection, and batch mismatch should be exceptionally rare. It's also not clear right now how to do error correction.",
          "createdAt": "2023-11-07T13:34:02Z",
          "updatedAt": "2023-11-07T13:34:02Z"
        }
      ]
    },
    {
      "number": 144,
      "id": "MDU6SXNzdWU5ODE1MzI5MzM=",
      "title": "Need LICENSE.md",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/144",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Right now this is empty. I propose that we do CC-0. Assuming people agree, I'll update LICENSE.md",
      "createdAt": "2021-08-27T19:25:48Z",
      "updatedAt": "2021-12-08T15:29:26Z",
      "closedAt": "2021-12-08T15:29:26Z",
      "comments": [
        {
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "body": "CC0 seems fine, since the I-D will eventually just get licensed under IETF IPR rules anyway.",
          "createdAt": "2021-09-08T17:07:28Z",
          "updatedAt": "2021-09-08T17:07:28Z"
        },
        {
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "body": "Or we could do something like this: https://github.com/yaronf/I-D/blob/main/LICENSE.md (however, I'm not sure what the best practice is, I don't see anything about it in RFC 8874, and even repositories like https://github.com/quicwg/base-drafts don't have a LICENSE.md file).",
          "createdAt": "2021-09-08T20:14:26Z",
          "updatedAt": "2021-09-08T20:14:26Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "What I'm trying to do here is have our initial submission actually have\nbroader terms than the IETF terms.\n\nOn Wed, Sep 8, 2021 at 1:14 PM Peter Saint-Andre ***@***.***>\nwrote:\n\n> Or we could do something like this:\n> https://github.com/yaronf/I-D/blob/main/LICENSE.md (however, I'm not sure\n> what the best practice is, I don't see anything about it in RFC 8874, and\n> even repositories like https://github.com/quicwg/base-drafts don't have a\n> LICENSE.md file).\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/issues/144#issuecomment-915538252>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIL2Q2HQXP7XXRRSV6LUA6725ANCNFSM5C6EWILQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n",
          "createdAt": "2021-09-08T21:22:54Z",
          "updatedAt": "2021-09-08T21:22:54Z"
        },
        {
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "body": "Makes sense. Some folks seem to think that CC0 / PD / Unlicense doesn't stick in certain locales because you have to assert copyright first. CC-BY feels weird, too, because the question is \"by whom\" (just the spec authors?). I'll ask someone at Mozilla about it.",
          "createdAt": "2021-09-08T21:36:56Z",
          "updatedAt": "2021-09-08T21:36:56Z"
        },
        {
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "body": "If CC0 doesn't work, our resident expert at Mozilla suggested that we might consider WHATWG licensing (which is likely palatable to the relevant stakeholders):\r\n\r\n> This work is licensed under a Creative Commons Attribution 4.0 International License. To the extent portions of it are incorporated into source code, such portions in the source code are licensed under the BSD 3-Clause License instead.",
          "createdAt": "2021-09-09T16:17:22Z",
          "updatedAt": "2021-09-09T16:17:22Z"
        }
      ]
    },
    {
      "number": 146,
      "id": "MDU6SXNzdWU5ODMzNDU2NTc=",
      "title": "How should servers figure out which task a message pertains to?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/146",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "Aggregators are expected to have an endpoint `hpke_config` from which they vend the HPKE public key and parameters so that clients may encrypt the contents of reports. There are no parameters to the `hpke_config` request, so there is no opportunity for an aggregator to provide a different config based on the task in play.\r\n\r\nNow, the `hpke_config` endpoint is relative to the aggregator's base endpoint URL, obtained from the per-task parameters, so an aggregator could simply provide base URLs that have the task ID in their path and use different key material on that basis. But all the other protocol messages like `Report` or `CollectReq` explicitly include task ID, suggesting that aggregators are supposed to be able to service multiple tasks from a single endpoint. Something feels inconsistent there.",
      "createdAt": "2021-08-31T01:24:49Z",
      "updatedAt": "2022-05-11T22:26:15Z",
      "closedAt": "2022-05-11T22:26:15Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we could change the endpoint to `[aggregator]/hpke_config/<task_id>`. We would probably have to redefine task ID to make sure it's safe to use in URL construction (URL encoded byte strings look terrible).",
          "createdAt": "2021-08-31T01:26:36Z",
          "updatedAt": "2021-08-31T01:26:54Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Good catch, and I like your suggested fix.",
          "createdAt": "2021-08-31T01:26:58Z",
          "updatedAt": "2021-08-31T01:26:58Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I could swear we talked about this a long time ago. Anyway, I am putting this in `parking-lot` for now because I don't think any progress is blocked on it, and it strikes me as the kind of thing that we will go back and forth on a lot during the IETF process. ",
          "createdAt": "2021-08-31T01:40:18Z",
          "updatedAt": "2021-08-31T01:40:18Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think the real issue here is that when handling a message, collectors, leaders and helpers need to know which task it pertains to in order to select the appropriate HPKE context (among other parameters). We should decide whether the task ID appears in protocol messages or whether it can be inferred from the endpoint and be consistent about it. Retitling accordingly.",
          "createdAt": "2022-01-21T02:11:16Z",
          "updatedAt": "2022-01-21T02:11:29Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "nit: IMO it would be more consistent to put the task ID in a `/hpke_config` request into the request body -- all other request/response pairs include the task ID in the request body, rather than including it in the request path.\r\n\r\n(There's no technical downside to putting the task ID in the path, this is only a \"consistency\" concern.)",
          "createdAt": "2022-04-01T04:09:16Z",
          "updatedAt": "2022-04-01T04:09:16Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "I think the issue with putting the task ID for a /hpke_config request in the body is that we have to switch from GET to POST, and lose cache-ability.",
          "createdAt": "2022-04-01T04:11:44Z",
          "updatedAt": "2022-04-01T04:11:44Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "That's a good point -- it's still pretty unsatisfying that this one request/response pair will have to be structured in this way, while all other request/response pairs (Upload, Aggregate, AggregateShare, Collect) use a different structuring.\r\n\r\nHowever, given that some clients will literally be web browsers, where HTTP caching is \"free\", I think it's worthwhile to keep HTTP caching semantics working.",
          "createdAt": "2022-04-01T16:26:06Z",
          "updatedAt": "2022-04-01T16:26:06Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(FWIW, this would be a great place to use [HTTP QUERY](https://www.ietf.org/id/draft-ietf-httpbis-safe-method-w-body-02.html) which is \"GET but with a body\", except that (a) it's still not standardized and (b) even if it were, it's still new enough that many HTTP libraries won't have good support for it.)",
          "createdAt": "2022-04-01T17:25:37Z",
          "updatedAt": "2022-04-01T17:25:37Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "One final thought for now: if we do stick to a GET request, we should consider putting the task ID for an `/hpke_config` request in a query parameter (e.g. `/hpke_config?task_id=foo`) rather than including it as part of the request path. Otherwise, every other request type will duplicate the task ID in both the request path & the request body.",
          "createdAt": "2022-04-01T20:56:57Z",
          "updatedAt": "2022-04-01T20:56:57Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "The `/hpke_config` endpoint has been changed to take the task ID as a query parameter, and all other messages include a task ID in the body. Should we close this out?",
          "createdAt": "2022-05-11T21:51:57Z",
          "updatedAt": "2022-05-11T21:51:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #236 ",
          "createdAt": "2022-05-11T22:26:15Z",
          "updatedAt": "2022-05-11T22:26:15Z"
        }
      ]
    },
    {
      "number": 148,
      "id": "MDU6SXNzdWU5OTIyOTUyNzA=",
      "title": "rename this repository",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/148",
      "state": "CLOSED",
      "author": "bdaehlie",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "I think we should rename this repository since we aren't using the term Prio to describe it any more. Consistent naming of resources helps with internal and external communication about this work.",
      "createdAt": "2021-09-09T14:26:33Z",
      "updatedAt": "2021-09-09T19:39:39Z",
      "closedAt": "2021-09-09T19:39:39Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood suggested `ppm-specification`. If nobody objects I will rename this repository this afternoon.",
          "createdAt": "2021-09-09T15:40:06Z",
          "updatedAt": "2021-09-09T15:40:06Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Done!",
          "createdAt": "2021-09-09T19:39:39Z",
          "updatedAt": "2021-09-09T19:39:39Z"
        }
      ]
    },
    {
      "number": 150,
      "id": "I_kwDOFEJYQs47dnPe",
      "title": "Can `AggregateReq`/`VerifyReq`s span multiple batch windows, and what is the scope of a `helper_state` blob?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/150",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Section 4.3 discusses the `helper_state` opaque blob, which is carried across successive `AggregateReq/Resp` messages as well as `OutputShareReq`. However we don't discuss the scope of a helper state blob. I think the only way it makes sense is if the leader maintains a helper state blob for each PPM task, but the text should be explicit about this. ",
      "createdAt": "2021-09-16T00:04:22Z",
      "updatedAt": "2022-05-11T18:57:54Z",
      "closedAt": "2022-05-11T18:57:54Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@ekr points out that we need to think about parallelism here. If, as I asserted, `helper_state` is per-task, then that means leader needs to guarantee that at most one `AggregateReq` is in flight for any task, to prevent concurrent modifications to the helper's state blob. We might want to constrain the aggregate protocol in a way that enables finer-grained state blobs to permit multiple aggregate protocol runs to occur in parallel. ekr's suggestion is to require that an `AggregateReq` only contain reports from a single batch interval, at which point the scope of a helper state blob can be (task, batch interval) which enables much greater parallelism.",
          "createdAt": "2021-09-23T00:47:26Z",
          "updatedAt": "2021-09-23T00:47:26Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "`helper_state` is gone as of #185, and aggregation job IDs provide more obvious answers for parallelizing input preparation.",
          "createdAt": "2022-05-11T18:57:54Z",
          "updatedAt": "2022-05-11T18:57:54Z"
        }
      ]
    },
    {
      "number": 153,
      "id": "I_kwDOFEJYQs48NCFj",
      "title": "Clarify need for client-provided timestamps & replay protection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/153",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Clients in the PPM send a client-generated `(timestamp, nonce)` tuple with their upload requests, and encrypt their reports using a nonce that is derived from this `(timestamp, nonce)` tuple. [section 4.2.2]\r\n\r\nThe aggregators use the `(timestamp, nonce)` tuple as a unique identifier for an individual report, to enable an \"anti-replay\" mechanism described in section 4.4.2. The reasoning for this anti-replay mechanism is described as:\r\n\r\n> Using a report multiple times within a single batch, or using the same report in multiple batches, is considered a privacy violation.\r\n\r\nI argue that this mechanism does not practically stop any of the actors from submitting some arbitrary report multiple times in a single batch (i.e. \"ballot-stuffing\"): malicious clients can effectively replay any of their reports by re-submitting the report content re-encrypted with a new `(timestamp, nonce)` tuple; malicious servers can effectively pose as a client and perform the described \"malicious client\" attack themselves.\r\n\r\nBut per discussion with timg@, the purpose of the anti-replay mechanism is not to stop a malicious client/server from doing arbitrary ballot-stuffing; it is to protect a malicious leader from using the same legitimate *client-provided* report multiple times, which could enable certain attacks on client privacy.\r\n\r\nSeparately, there is currently nothing in the spec placing any requirements or server validations on the timestamp value. A misconfigured or malicious client could send a \"poisoned\" report with a timestamp of `MAX_UINT64`. As the leader must send reports to the helper ordered by `(timestamp, nonce)` [sections 4.3.1, 4.4.2], once this report was sent to the helper as part of an aggregation request the leader would no longer be able to submit any additional reports (and if it tried, the helper would silently filter them).\r\n\r\nSeparately (again), message deduplication based on a total ordering of message timestamps will lead to messages that are received in a delayed fashion being dropped, since the leader will no longer be able to include them in aggregation requests. (The same effect would be seen if a misconfigured/malicious client sent a message timestamped forward to the end of the current batch; this is very similar to the \"poisoned report\" issue above, but somewhat more inherent to the ordering-based deduplication mechanism.)\r\n\r\nProposals:\r\n* Update the explanation of the anti-replay mechanism to describe the intended protection more explicitly, i.e. that it covers only replay of messages sent by a client, with replay attempted by the server (I think).\r\n* Think through and add server validation on the client-submitted timestamp. For example, we might decide that only reports in the current batch, or a previous batch falling within a \"grace window\", are allowed. (I am unsure if this would be acceptable for Heavy Hitters or other PPM algorithms that require a more \"online\" query capability.)\r\n* Consider switching away from an ordering-based deduplication system entirely, as it depends on ordering information that cannot be reliably provided by the clients & because it will cause delayed messages to be dropped. Whatever replaces the ordering-based deduplication mechanism will likely require more storage than the ordering-based mechanism, but this might be mitigated by bounding what reports can be accepted by a time or `batch_id` window.\r\n* Consider switching clients from reporting a timestamp to reporting a `batch_id` counter. The current timestamp-based report could be mapped to a batch ID by something like `timestamp / batch_window_duration` (though using a counter opens up the possibility of other methods of determining the current `batch_id`). Reports should still be deduplicated by the `(batch_id, nonce)` tuple. (The ordering-based deduplication mechanism would no longer work.)\r\n",
      "createdAt": "2021-09-28T18:00:13Z",
      "updatedAt": "2021-10-25T19:54:13Z",
      "closedAt": "2021-10-25T19:54:13Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I like how the linked PR solves this. Before closing this issue, however, we might consider being a bit more prescriptive, along the lines of https://datatracker.ietf.org/doc/html/rfc8446#section-8.2 (thanks @ekr for the link)",
          "createdAt": "2021-10-06T16:31:06Z",
          "updatedAt": "2021-10-06T16:31:06Z"
        }
      ]
    },
    {
      "number": 155,
      "id": "I_kwDOFEJYQs48Vf9H",
      "title": "Authentication of Collector's query",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/155",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "During execution of the collect protocol, the leader is responsible for relaying some parameters of the collect request to the helper, represented by the opaque `output_param` in `CollectReq` and `OutputShareReq` messages introduced by #154. That parameter will always be null/empty/absent for Priov3, but in Hits, it would contain the string prefixes that the collector wants a count of in the result set (and who knows what else in yet-undefined VDAFs).\r\n\r\nIt's currently possible for the leader (or any actor that can tamper with messages on the network) to insert arbitrary `output_param` into `OutputShareReq` messages. @cjpatton reckons [this is a threat to soundness](https://github.com/abetterinternet/ppm-specification/pull/154#discussion_r718917818). We should think about whether this enables the leader to undermine soundness in any ways beyond the capabilities the threat model already grants it, and if so, introduce collector->helper authentication+integrity. ~Confidentiality~Privacy may not be needed on here because the `output_param` should be identical for helper and leader.",
      "createdAt": "2021-09-30T14:43:08Z",
      "updatedAt": "2023-10-19T22:20:08Z",
      "closedAt": "2023-10-19T22:20:08Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Note that we already protect helper->collector messages with HPKE. Since both sides already advertise HPKE configs, one solve here would be to use that to tunnel a mutually authenticated channel through the leader.",
          "createdAt": "2021-09-30T14:44:46Z",
          "updatedAt": "2021-09-30T14:44:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This is misrepresenting the problem slightly. I think the key points are:\r\n1. For soundness we assume both aggregators are honest. Regardless, a network attacker can currently impersonate the leader to the helper because the leader is not authenticated and violate soundness. This is outside of the model for VDAF, but is one of our security considerations for the PPM protocol. This needs to be addressed.\r\n2. For privacy (note that this is not the same thing as confidentiality) we assume just one of the aggregators is honest. The dishonest aggregator may act arbitrarily, including forging the output parameter or the helper's encrypted input share itself. Authentication of the channel can't help us here.\r\n\r\nAs far as the mechanism for leader authentication: auth-mode for HPKE is a promising direction. One thing we need to think through is whether KCI attacks matter for our setting, since HPKE auth-mode is vulnerable to these: https://www.ietf.org/archive/id/draft-irtf-cfrg-hpke-12.html#sec-properties\r\n\r\n",
          "createdAt": "2021-09-30T15:10:53Z",
          "updatedAt": "2021-09-30T17:28:28Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Another option for leader authentication is client certificates for TLS.",
          "createdAt": "2021-09-30T17:29:09Z",
          "updatedAt": "2021-09-30T17:29:09Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "The text implies that we are doing mutual authentication, but it should be hoisted into the protocol definition\r\n```faithfully as specified. The system also assumes that servers communicate over\r\nsecure and mutually authenticated channels. In practice, this can be done by TLS\r\nor some other form of application-layer authentication.\r\n```",
          "createdAt": "2021-10-08T20:05:52Z",
          "updatedAt": "2021-10-08T20:05:52Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "What's discussed here is the risk of the leader presenting forged collect request/aggregate share request parameters to the leader. We have a similar problem in the other direction, which is that when satisfying a request to a collect job URI, a leader could throw away the aggregate share computed by the helper, fabricate two totally synthetic aggregate shares that combine into the leader's chosen aggregate result, and then present those to the collector, which has no way of verifying that either share came from the helper.\r\n\r\nWhile discussing this on the ppm@ietf.org mailing list, @cjpatton (citing ekr) observed that we can mitigate that risk by having collectors fetch each aggregate shares directly from each aggregator, enabling it to use TLS to verify each server's identity. This is a good way to solve this attack without adding new cryptography to DAP (e.g., an in-band signature over aggregate shares), though it might introduce new difficulties with ensuring the two aggregators produce aggregate shares over the same set of reports.",
          "createdAt": "2022-08-26T17:48:45Z",
          "updatedAt": "2022-08-26T17:48:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "When we ask ourselves what games the Leader can play by exploiting the lack of an authenticated channel between Collector and Helper, there are two buckets of concerns:\r\n1. Privacy: Is there attack that leads to disclosing information about measurements that the attacker can't learn otherwise?\r\n2. Robustness: Is there an attack that leads the Collector to compute the wrong aggregate result?\r\n\r\nI don't think we have a privacy issue here. There is definitely a way to attack robustness, assuming the Leader is actively attacking the protocol. However, there are other ways, like tweaking its own aggregate share.\r\n\r\nI don't see an obvious way of defeating our goals of privacy or robustness, at least in the threat model for VDAFs where for robustness we assume both Aggregators run the protocol correctly. That said, having the Collector communicate with both Aggregators directly seems like a reasonable defense-in-dpeth measure. If anyone wants this feature, please send a PR.",
          "createdAt": "2023-10-13T20:46:11Z",
          "updatedAt": "2023-10-13T20:46:11Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "For reference, here is the discussion that @tgeoghegan refers to above: https://mailarchive.ietf.org/arch/msg/ppm/3G_WM7L49L1wz0LK9snx7KCSQng/",
          "createdAt": "2023-10-13T20:47:30Z",
          "updatedAt": "2023-10-13T20:47:30Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I feel comfortable not addressing this in the protocol since nobody has articulated a privacy attack, and since we discuss this in the security considerations.",
          "createdAt": "2023-10-18T22:02:43Z",
          "updatedAt": "2023-10-18T22:02:43Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Agreed. Closing.",
          "createdAt": "2023-10-19T22:20:08Z",
          "updatedAt": "2023-10-19T22:20:08Z"
        }
      ]
    },
    {
      "number": 158,
      "id": "I_kwDOFEJYQs484ixL",
      "title": "Diagrams",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/158",
      "state": "OPEN",
      "author": "coopdanger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "To make this document more understandable for those uninitiated in the IETF community, it would be helpful to add some ladder diagrams or a state machine diagram or something early in the document to show the overall flow of exchanges between the different entities involved.",
      "createdAt": "2021-10-08T20:55:06Z",
      "updatedAt": "2023-10-24T22:04:31Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 161,
      "id": "I_kwDOFEJYQs497hbc",
      "title": "Securely provisioning `vdaf_verify_key`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/161",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "The [VDAF spec](https://datatracker.ietf.org/doc/html/draft-patton-cfrg-vdaf-00) describes how to generate `verify_params` for aggregators in `prio3` and `poplar1`. The verification parameters MUST be kept secret from other clients, which means that they may need to be rotated either if some aggregator's verification parameter is exposed or if a sufficient number of inputs are processed under some verification parameter (this would take a huge number of inputs to be a problem).",
      "createdAt": "2021-10-28T22:49:33Z",
      "updatedAt": "2023-02-08T00:44:55Z",
      "closedAt": "2023-02-08T00:43:42Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "It may suffice to discuss this abstractly in security considerations and otherwise declare rotation of task parameters to be out-of-band with the PPM protocol, as we already do for initial parameter establishment.",
          "createdAt": "2021-10-28T22:50:27Z",
          "updatedAt": "2021-10-28T22:50:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this is going to need to be specified. If it's done incorrectly, it could become an attack vector.\r\n\r\nBoth Prio v1 [[CGB17]] and Poplar [[BBCGGI21]] require per-report randomness for validating the recovered output shares. For robustness, it is important that the aggregators agree on fresh randomness that is unpredictable to the client. Otherwise it might be possible for a malicious client to garble the aggregate result.\r\n\r\nFor privacy the picture is murkier. The key question is whether a malicious aggregator can, by controlling the random input the helper consumes, cause the helper to leak information about the measurement. It's possible that the answer depends on the protocol:\r\n- For Prio v1 [[CGB17]], it seems to be just fine for the leader to just pick the randomness and send it to the helper. (See [[CGB17], Section 4.2, Step 3a].)\r\n- For Poplar [[BBCGGI21]], the process by which the shared randomness is established is unspecified (See [[BBCGGI21], Appendix C.4.].) It may be OK for the leader to just pick it, but this is not guaranteed.\r\n- For schemes based on techniques in [[BBCGGI19]], including `prio3` in the VDAF draft, as well as the code running in ENPA, the situation is similar: The distributed ZKP system described in [[BBCGGI19], Section 6.2] makes use of an \"ideal coin-flipping functionality\" (see Definition 6.3), which in practice would have to be realized by an actual protocol. Incidentally, I believe ENPA effectively realizes this ideal coin-flipping functionality by having a third party choose the randomness. (Can you confirm, @tgeoghegan?)\r\n\r\nFor PPM we know we'll need a procedure for establishing a shared secret among the aggregators, which will be used to derive per-report randomness by the VDAF. What's not clear right now is what security properties this procedure needs to have in order for PPM to meet its privacy goals. Either (1) the procedure is irrelevant to privacy, i.e., the share secret may be chosen by the adversary, or (2) we need a protocol that realizes the ideal coin flipping functionality described in [[BBCGGI19], Definition 6.3]. Or, perhaps there's something in between.\r\n\r\nIf (1), then we could just have the leader run the VDAF setup procedure and send the shared secret to the helper over a secure channel. If (2), then we'll have to do something more sophisticated. [[BBCGGI19], Section 6.1] points to some papers we can explore. As a straw man, could it be as simple as having each aggregator commit to some initial key material, then derive the shared secret from key material provided by each aggregator? (Something along the lines of https://en.wikipedia.org/wiki/Commitment_scheme#Bit-commitment_in_the_random_oracle_model?)\r\n\r\ncc/ @henrycg in case you have thoughts here or can point us in the right direction.\r\ncc/ @chris-wood since we spoke about this recently.\r\n\r\n[CGB17]: https://crypto.stanford.edu/prio/paper.pdf\r\n[BBCGGI19]: https://eprint.iacr.org/2019/188.pdf\r\n[BBCGGI21]: https://eprint.iacr.org/2021/017.pdf",
          "createdAt": "2021-12-30T02:07:34Z",
          "updatedAt": "2022-01-11T17:40:04Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "The commit-then-open coin-flipping protocol you mention is pretty simple and is general enough to cover the randomness needs of Prio, Heavy Hitters, and probably any future schemes as well. The only downside is that it adds another round of interaction between the servers.\r\n\r\nIf you want the leader to be able to unilaterally pick the randomness, let me know and I can check back through the Heavy Hitters paper to make sure that that would be safe in the HH context. I suspect it would be fine, but I would want to check carefully.",
          "createdAt": "2021-12-31T10:08:23Z",
          "updatedAt": "2021-12-31T10:08:23Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think my preference would be to take (2), since it's more conservative. However, it would be great to know if (1) is a possibility. We'd be much obliged if you could take a careful look at the HH paper and see if this will work. While at it, it would be great to know if [[BBCGGI19](https://eprint.iacr.org/2019/188), Theorem 6.6] falls apart if we replace the ideal coin-flipping functionality with the leader choosing the randomness unilaterally.",
          "createdAt": "2021-12-31T16:28:54Z",
          "updatedAt": "2021-12-31T16:28:54Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "Okay, I will add this to my queue. It may take a week or two, but I will follow up once I have taken a closer look.",
          "createdAt": "2022-01-01T09:05:47Z",
          "updatedAt": "2022-01-01T09:05:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Incidentally, I believe ENPA effectively realizes this ideal coin-flipping functionality by having a third party choose the randomness.\r\n\r\nIn ENPA, a random value is chosen for each report by the ingestion servers that get input shares from clients and batch them up to be forwarded to aggregators.",
          "createdAt": "2022-01-03T20:56:52Z",
          "updatedAt": "2022-01-03T20:56:52Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "I just read back through the Poplar (S&P'21) and BBCGGI19 papers and chatted with my co-authors. The bottom line, barring and bugs in our thinking, is:\r\n\r\n- For Poplar, the situation is as with Prio v1: the leader can choose the randomness and this has no effect on the zero-knowledge property of the proof system.\r\n- For BBCGGI19, if you are using the proof system of Example 5.6 with the compiler of Section 6.2, then again the leader can safely choose the randomness used to check the proof. (If you use some other base proof system\u2014not the one from Example 5.6 nor the ones like it\u2014then it might not be safe for the leader to choose the randomness.)\r\n\r\nYuval Ishai, a co-author on both of these papers, pointed out that the Fiat-Shamir trick (see Section 6.2.3 of BBCGGI19) applies to all three of these proof systems: Prio v1, Poplar, and BBCGGI19. So, you could potentially use Fiat-Shamir to avoid the need for the leader or anyone else to choose the randomness. The Fiat-Shamir trick only works if you have set up the underlying proof system to have soundness error ~2^{-128}, so if you are using a 64-bit modulus then Fiat-Shamir may not be useful.",
          "createdAt": "2022-01-11T14:05:15Z",
          "updatedAt": "2022-01-11T14:05:15Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> * For BBCGGI19, if you are using the proof system of Example 5.6 with the compiler of Section 6.2, then again the leader can safely choose the randomness used to check the proof. (If you use some other base proof system\u2014not the one from Example 5.6 nor the ones like it\u2014then it might not be safe for the leader to choose the randomness.)\r\n\r\nI'm fairly certain what we've implemented meets this criterion, but I want to be really careful here, since what we've implemented diverges somewhat from what's in the paper. (@henrycg, you and I spoke over email about the differences.) Let me spell this out so we're all on the same page.\r\n\r\nThe prio3 VDAF is designed to work with any [1.5-round, public-coin IOP](https://cjpatton.github.io/vdaf/draft-patton-cfrg-vdaf.html#name-fully-linear-proof-flp-syst). We refer to this primitive simply as an FLP (\"Fully Linear Proof\") system in the VDAF spec. [Our concrete FLP implementation](https://docs.rs/prio/latest/prio/pcp/index.html) is based on Theorem 4.3 and includes a couple generalizations. Namely:\r\n\r\n* The validity circuit takes an optional random input, which admits constructions like the SIMD circuit of Theorem 5.3. (The random input is called the \"joint randomness\" in the VDAF spec. It is generated by applying the Fiat-Shamir trick of Section 6.2.3.)\r\n* The validity circuit may contain multiple gadgets as suggested in Remark 4.5.\r\n\r\nIncidentally, while these changes were already suggested by BBCGGI19, they are significant enough to warrant a fresh security proof. Note that this FLP is not yet specified in the VDAF draft -- I plan to do so in the coming weeks. That way we'll have a concrete target for analysis.\r\n\r\n \r\n> Yuval Ishai, a co-author on both of these papers, pointed out that the Fiat-Shamir trick (see Section 6.2.3 of BBCGGI19) applies to all three of these proof systems: Prio v1, Poplar, and BBCGGI19. So, you could potentially use Fiat-Shamir to avoid the need for the leader or anyone else to choose the randomness. The Fiat-Shamir trick only works if you have set up the underlying proof system to have soundness error ~2^{-128}, so if you are using a 64-bit modulus then Fiat-Shamir may not be useful.\r\n\r\nIn the prio3 VDAF, we already do the Fiat-Shamir trick for the joint randomness in order to avoid interaction between the client and aggregators during the upload phase. It's a nice observation that we could extend this to also encompass the randomness used by the aggregators. (We call this the \"query randomness\" in the VDAF spec.) However, it may not be worth the extra computational cost for the client, since it doesn't save latency in the aggregation phase.\r\n\r\nIncidentally, we have not yet implemented a large enough field: See https://github.com/abetterinternet/libprio-rs/issues/106.\r\n\r\nCan you say a bit more about how this would work for Poplar? My understanding is that, since the client doesn't know in advance what candidate prefixes will be used to query its IDPF shares, it has know way of generating the challenge randomness that will be used for the secure sketch.\r\n ",
          "createdAt": "2022-01-11T21:00:35Z",
          "updatedAt": "2022-01-11T21:54:57Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "> Can you say a bit more about how this would work for Poplar? My understanding is that, since the client doesn't know in advance what candidate prefixes will be used to query its IDPF shares, it has know way of generating the challenge randomness that will be used for the secure sketch.\r\n\r\nThe idea for Poplar (roughly) is that the servers would derive the randomness used in the sketch by hashing their shares of the client's submission. Since the sketch-verification process does not require the servers to interact with the client, I think it's okay that the client does not know at the time of submission exactly which vector the servers will sketch. It is certainly possible that I m missing something here.\r\n\r\nI agree that using Fiat-Shamir here may not be worth the extra cost and complexity.",
          "createdAt": "2022-01-12T08:19:48Z",
          "updatedAt": "2022-01-12T08:19:48Z"
        },
        {
          "author": "schoppmp",
          "authorAssociation": "NONE",
          "body": "@cjpatton and I have been discussing this in cjpatton/vdaf#18, and I think we should review carefully whether it would make sense to rotate `verify_params` more often. More concretely, I believe it would be good to generate the verification randomness only *after* a set of client shares to verify has been agreed on, and to re-generate it for every such set of shares.\r\n\r\nThe reason is that in a real deployment, it might be easier to ensure that the right code is being run (i.e., the aggregation server behaves semi-honestly), than to ensure confidentiality of the aggregation server's state. Now if we fix the verification randomness in advance, an attacker learning it could then create a malicious client share that would pass the verification.",
          "createdAt": "2022-01-28T12:02:38Z",
          "updatedAt": "2022-01-28T12:02:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree that PPM needs to make sure that frequent key rotation for VDAFs needs to be as ergonomic as possible.",
          "createdAt": "2022-01-28T15:59:06Z",
          "updatedAt": "2022-01-28T15:59:06Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "We should resolve rotation of `verify_param` for PPM soon, now that we are moving forward with implementation. AFAICT there is currently no specification-compliant way for `verify_param` to be rotated.\r\n\r\nSpecifically, given that we are working in a distributed environment, we would need to support multiple versions of `verify_params` live at once for a single `task`. There are two basic strategies for rotating secrets in a distributed environment that I have encountered before, but I have concerns with both of them:\r\n\r\n1) Include a \"`verify_param` version identifier\" value that determines which \"version\" of the `verify_param` to use. AFAICT nothing like this is currently specified anywhere in PPM or VDAF. This would be a simple/straightforward solution, but would increase communication costs somewhat & would need to be specified.\r\n\r\n2) Try all of the `verify_param`s we know about. However, given the way that `verify_param` is used as part of `prep_init` to transform an input share into an initial preparation state, I'm not sure this is feasible: a helper would not know if the `verify_param` it chose was correct until after the next round of communication with the leader, since \"correct\" in this case means \"the same as the `verify_param` chosen by the leader\". And if the helper chose an incorrect `verify_param`, I believe this would lead to a transition to an `INVALID` state in the leader's state machine, so the helper couldn't try another `verify_param`.\r\n\r\nBarring a third option (or an error in analysis of the two options above), I think this needs to be addressed at the specification level.",
          "createdAt": "2022-04-15T18:43:53Z",
          "updatedAt": "2022-04-15T18:43:53Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> @cjpatton and I have been discussing this in [cjpatton/vdaf#18](https://github.com/cjpatton/vdaf/issues/18), and I think we should review carefully whether it would make sense to rotate `verify_params` more often. More concretely, I believe it would be good to generate the verification randomness only _after_ a set of client shares to verify has been agreed on, and to re-generate it for every such set of shares.\r\n\r\nThere is currently some text in the VDAF spec that suggests that `public_param` held by the clients needs to match with the `verify_param`:\r\n\r\n> CP The public_param is intended to allow for protocols that require the Client to use a public key for sharding its measurement. When rotating the verify_param for such a scheme, it would be necessary to also update the public_param with which the clients are configured. For PPM it would be nice if we could rotate the verify_param without also having to update the clients. We should consider dropping this at some point. See https://github.com/cjpatton/vdaf/issues/19.\r\n\r\nI think this is incompatible with the suggestion to generate `verify_params` after receiving the client report. But, if we want to go this way, perhaps the right answer is to drop the `public_param`? It's not used by either `prio3` or `poplar1`; and from the perspective of PPM, it would make things easier by allowing rotation of `verify_params` without coordination with the clients.",
          "createdAt": "2022-04-15T18:52:00Z",
          "updatedAt": "2022-04-15T18:52:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The spec has changed a lot since we last touched this issue, so I'll add a quick update here: The relevant parameter is the `vdaf_verify_key`, which is assumed to be exchanged out-of-band. Ideally this parameter can be chosen arbitrarily (even adversarially) without impacting privacy. We (me and @hannahdaviscrypto) are working on a formal analysis that will rule this in or out for Prio3/Poplar1. We'll update this thread when we have more to say here. In any case, for robustness it's important that `vdaf_verify_key` be a uniform random string unknown to the clients, so it might be useful to spell something here.",
          "createdAt": "2022-09-13T02:43:10Z",
          "updatedAt": "2022-09-13T02:43:10Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Ideally this parameter can be chosen arbitrarily (even adversarially) without impacting privacy.\r\n\r\nIf this turns out to be true (for all VDAFs?), we could consider having the leader generate a value for `vdaf_verify_key` on a per-aggregation-job basis, treating it as a piece of state that needs to live only as long as the aggregation job is \"active\". Perhaps the leader would communicate the key to the helper as a new field in `AggregateInitializeReq`.\r\n\r\nThis would allow `vdaf_verify_key` to be removed from the long-lived task parameters, which would be handy as one step towards allowing automated provisioning of tasks. I think it would also solve rotation concerns.\r\n\r\n\r\nedit: a few additional thoughts:\r\n\r\n* This suggestion also assumes that output shares recovered with differing `verify_key` values can be aggregated into a single aggregate result. I think this is true, but can't find an explicit confirmation of this in the VDAF specification. (if not, I suppose a similar approach could be used, but we'd need to specify `verify_key` at a per-batch level instead of per-aggregation-job)\r\n* We could potentially save a few bytes on-the-wire if we found a way to safely derive the `vdaf_verify_key` from other information already shared between the aggregators, instead of explicitly transmitting the `vdaf_verify_key` from leader to helper.",
          "createdAt": "2022-09-13T17:00:57Z",
          "updatedAt": "2022-09-13T17:23:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> If this turns out to be true (for all VDAFs?), ...\r\n\r\nWe would have to make this determination on a per-VDAF basis. (A security proof for Prio3/Poplar1 does not necessarily imply anything for an arbitrary VDAF.)\r\n\r\n> ... we could consider having the leader generate a value for `vdaf_verify_key` on a per-aggregation-job basis, treating it as a piece of state that needs to live only as long as the aggregation job is \"active\". Perhaps the leader would communicate the key to the helper as a new field in `AggregateInitializeReq`.\r\n>\r\n> This would allow `vdaf_verify_key` to be removed from the long-lived task parameters, which would be handy as one step towards allowing automated provisioning of tasks. I think it would also solve rotation concerns.\r\n\r\nThis would be really nice I think :) However it would require evaluating a candidate VDAF in the same security model as for Prio3/Poplar1. (We should have this model spelled out soon, in fact.)\r\n\r\n>     * This suggestion also assumes that output shares recovered with differing `verify_key` values can be aggregated into a single aggregate result. I think this is true, but can't find an explicit confirmation of this in the VDAF specification. (if not, I suppose a similar approach could be used, but we'd need to specify `verify_key` at a per-batch level instead of per-aggregation-job)\r\n\r\nOnly the prep phase depends on the verify key, so this shouldn't be a problem.\r\n\r\n>     * We could potentially save a few bytes on-the-wire if we found a way to safely derive the `vdaf_verify_key` from other information already shared between the aggregators, instead of explicitly transmitting the `vdaf_verify_key` from leader to helper.\r\n\r\nThis direction seems frought with foot-guns to me. 16 additional bytes per aggregation flow is reasonable.\r\n\r\n",
          "createdAt": "2022-09-13T17:44:30Z",
          "updatedAt": "2022-09-13T17:44:30Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Also, @schoppmp pointed out in the security considerations of the VDAF draft that @branlwyd's design would have some security benefit: If the verify key is chosen ONLY AFTER the clients uplooad their reports, then there's no way for this to be leaked ahead of time. (See Phillipp's comment above.)",
          "createdAt": "2022-09-13T17:46:58Z",
          "updatedAt": "2022-09-13T17:47:20Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah -- reviewing @schoppmp's comments on this issue, I think my proposal matches what Phillipp proposed in [their Jan 28 comment](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/161#issuecomment-1024149840):\r\n\r\n> More concretely, I believe it would be good to generate the verification randomness only after a set of client shares to verify has been agreed on, and to re-generate it for every such set of shares.\r\n\r\nCredit to Phillipp if this idea does turn out to be workable. :-)",
          "createdAt": "2022-09-13T18:09:26Z",
          "updatedAt": "2022-09-13T18:09:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Based on [this analysis](https://ia.cr/2023/130), it appears to be safe for the Leader to pick the VDAF verification key unilaterally, at least Prio3 and Poplar1. However we have to be somewhat careful: See https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/407 for the follow up. I'm going to close this issue, since we now have a better idea of what we need.",
          "createdAt": "2023-02-08T00:43:42Z",
          "updatedAt": "2023-02-08T00:44:55Z"
        }
      ]
    },
    {
      "number": 163,
      "id": "I_kwDOFEJYQs4-XTJo",
      "title": "Aggregators should inform collector of contribution count in collect response",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/163",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Collectors will want to know how many inputs are included in the aggregations they receive, for a number of reasons: they may wish to compute a mean over a privately aggregated sum, or track metrics on how many clients are providing inputs. struct `CollectResp` should include a count of how many contributions went into an aggregation.",
      "createdAt": "2021-11-05T22:55:01Z",
      "updatedAt": "2022-08-10T19:24:04Z",
      "closedAt": "2022-08-10T19:24:04Z",
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "The linked PR only added a `report_count` to the `AggregateShareReq` structure, which communicates it to the helper. I'm reopening this issue because the collector does not yet receive this information. In addition to the statistics-related motivation above, a recent PR to the VDAF spec, cfrg/draft-irtf-cfrg-vdaf#95, expects the number of reports to be available when unsharding.",
          "createdAt": "2022-07-11T20:47:34Z",
          "updatedAt": "2022-07-11T20:47:34Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "This was completed in #287.",
          "createdAt": "2022-08-10T19:24:04Z",
          "updatedAt": "2022-08-10T19:24:04Z"
        }
      ]
    },
    {
      "number": 166,
      "id": "I_kwDOFEJYQs4-u8xP",
      "title": "Consider making the Leader a non-aggregator",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/166",
      "state": "CLOSED",
      "author": "eriktaubeneck",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "In section 2.1: \"The leader is responsible for coordinating the protocol. It receives the encrypted shares, distributes them to the helpers, and orchestrates the process of computing the final measurement as requested by the collector.\"\r\n\r\nThen, in section 6.1.3: \"The leader is also an aggregator, and so all the assets, capabilities and mitigations available to aggregators also apply to the leader.\"\r\n\r\nFor a given _Measurement API_, which is implemented by a client provider, we assume that the client provider will need provide some sets of approved _aggregators_, which are trusted to be non-colluding. By requiring the _leader_ to be an _aggregator_, we limit who can actually be a leader.\r\n\r\nConsider a _Measurement API_ implemented by a web browser. In that case, I could imagine both the following scenarios:\r\n\r\n1. The site itself wants to act as the leader, collecting the encrypted reports themselves.\r\n2. The site delegates to a service provider, collecting the encrypted reports on that sites behalf (similar to current analytics tools.)\r\n\r\nFor scenario 1, it seems very unlikely that any site which wishes to act as a _leader_ for themselves would be able to be approved, as that would be a huge undertaking.\r\n\r\nFor scenario 2, this seems like it would limit competition in this space and potentially give a great deal of benefit to early approved services.\r\n\r\nGiven this, I propose downgrading the responsibility of the leader to just the storage and coordination role, and not imposing the currently required trust into the _leader_. This would be a fairly significant change to the current draft of the proposal, so before undertaking such changes and providing a PR, I'd like to first what the reaction to this idea is.",
      "createdAt": "2021-11-13T00:25:29Z",
      "updatedAt": "2022-01-21T23:54:03Z",
      "closedAt": "2022-01-21T23:54:03Z",
      "comments": [
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think that this worth considering.  The current formulation of roles is a little bit biased toward having a single entity that does most of the functions, with a secondary that performs only the necessary additional steps required to avoid collusion.\r\n\r\nThe current model has:\r\n* a leader that gathers inputs, coordinates aggregation, and assembles the results\r\n* a helper (or helpers) that participate in aggregation\r\n\r\nErik is describing a model where there is a less trusted entity that gathers inputs and assembles results, but does not participate in aggregation.  That entity might coordinate the aggregation process also, though I don't know if that is necessary, depending on what that coordination entails.  If the aggregation functions can talk amongst themselves, that might be more efficient.  Moreover, it might be more trustworthy that way as the (untrusted) coordination function does not need to see intermediate values; in Hits at least, that would leak some information.\r\n\r\nUnfortunately, this creates a different set of protocols than what is in the current draft.  You could cut things differently and avoid some (or most) of the complication by allowing this untrusted entity to gather reports and pass them to a leader for aggregation.  That leaves a few unnecessary redundancies in the proposed model.  For instance, you don't need the added complexity of encrypting the output shares toward the collector, because you can rely on TLS instead.\r\n\r\n@eriktaubeneck, I think that when you say \"storage\" you refer only to the phase where individual submissions are gathered prior to running the aggregation protocol.  There is another storage component to this system, which is the anti-replay or budgeting component that ensures that single measurements can't be used too many times.  That part can't be given to an untrusted entity.",
          "createdAt": "2021-11-15T00:38:52Z",
          "updatedAt": "2021-11-15T00:38:52Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> Unfortunately, this creates a different set of protocols than what is in the current draft. You could cut things differently and avoid some (or most) of the complication by allowing this untrusted entity to gather reports and pass them to a leader for aggregation. That leaves a few unnecessary redundancies in the proposed model. For instance, you don't need the added complexity of encrypting the output shares toward the collector, because you can rely on TLS instead.\r\n\r\nI don't believe that this is correct. The idea here is to avoid any entity other than the collector learning the true value. I don't see how the architectuer Erik proposes would remove that need.",
          "createdAt": "2021-11-15T00:42:04Z",
          "updatedAt": "2021-11-15T00:42:04Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I understand the goal.  I wanted to maintain that goal.\r\n\r\nIf you consider an entity that is coordinating and collecting, the shares of the result can be sent to that entity directly rather than using HPKE and routing them via a leader (which at this point is just an unnecessary intermediary).",
          "createdAt": "2021-11-15T00:49:46Z",
          "updatedAt": "2021-11-15T00:49:54Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "That puts way more complexity and need for real-time activity on that entity than with the current model, so I think this would be a regression.\r\n",
          "createdAt": "2021-11-15T00:57:24Z",
          "updatedAt": "2021-11-15T00:58:06Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "There are three different issues being discussed here:\r\n\r\n1. What are the properties of the party (currently the leader) who collects the data from the client?\r\n2. What party(s) maintain state for anti-reply and DP budgeting?\r\n3. How are results delivered to the collector so that no other party learns the true value.\r\n\r\nI'm primarily proposing a change for (1), as @martinthomson suggests. Thanks for clarifying. Collecting real time data from clients at scale is a non-trivial task, and it seems preferable to have as much flexibility and optionality for eventual collectors who are using this API. Removing the need for trust in the party promotes more competition, and thus more options for pricing, up time, robustness, etc. Dropping data from a client would result in unrecoverable measurement loss (whereas a failed PPM measurement could likely be rerun.)\r\n\r\n(2) is worth its own issue, though if we are comfortable with a single party managing the longer term state, that could still be compatible with this architecture. The functionality for (1) could be separated out, and called something like \"consumer\".\r\n\r\nFor (3), I generally agree with @ekr that minimizing complexity for the aggregators is preferable. Compared to the rest of the protocol, wrapping the result in a HPKE doesn't add a great deal of overhead. In general, it seems ideal to shift as much cost onto the leader (wrt the economics I mention above), and this also plays into egress costs, as described in #130.\r\n\r\nEDIT: does add overhead > doesn't add overhead.",
          "createdAt": "2021-11-15T03:30:27Z",
          "updatedAt": "2021-11-15T16:23:07Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Erik: I don't understand your point (3).\r\n\r\nFirst, in any situation where one party stores the reports prior to doling them out to the aggregators, they need to be HPKE encrypted, which means that the aggregators need to be able to do HPKE. Any given aggregation will require far more reports than the aggregate value and so wrapping the result in HPKE will be a very small fraction of the HPKEs that the aggregator does. What's the additional overhead here?\r\n\r\nSecond, I think it's potentially reasonable to introduce *another* entity which orchestrates the computation (as the leader does now), without having the cryptographic keys, but: (1) we certainly want to preserve the ability of a service to offer the same model as now in which the collector issues requests and gets answers without participating in the protocol (2) It's not actually clear to me what the security and privacy properties of this entity would be. At minimum, it seems likely that it would be able to cause the system to produce false results.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2021-11-15T12:39:18Z",
          "updatedAt": "2021-11-15T12:39:18Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "> Erik: I don't understand your point (3).\r\n\r\nUgh, typo. It *doesn't add* a great deal of overhead. I agree it's better to route through the leader, vs having aggregators connect directly to the collector, as @martinthomson suggested.\r\n\r\n> (1) we certainly want to preserve the ability of a service to offer the same model as now in which the collector issues requests and gets answers without participating in the protocol\r\n\r\nAgreed. In general, I don't see any limits on a single party participating in the protocol in more than role, so long as there are multiple non-colluding parties participating as aggregators.\r\n\r\n> (2) It's not actually clear to me what the security and privacy properties of this entity would be. At minimum, it seems likely that it would be able to cause the system to produce false results.\r\n\r\nI will take an attempt at describing this in more detail on this issue. My goal would be to design it such that the collector needs to trust this new _consumer_ entity, since they could affect the robustness of the result, but that the clients wouldn't need to trust the _consumer_ because they can't violate the client/end-user privacy.",
          "createdAt": "2021-11-15T16:18:13Z",
          "updatedAt": "2021-11-15T16:18:28Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I just saw this issue from Erik's email on the list. I wanted to point to a similar discussion (https://github.com/abetterinternet/ppm-specification/issues/64) we had a while back on allowing this via considering an intermediary aggregator as another \"client\" and supporting batch uploads to the leader / aggregators. I think these issues could probably be merged since they are addressing the same underlying goal.\r\n",
          "createdAt": "2022-01-19T17:13:59Z",
          "updatedAt": "2022-01-19T17:13:59Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "Thanks for linking @csharrison. Agreed that these seem to be addressing the same underlying goal. #64 is closed, but its outcome is unclear to me. Do you think it's still worth working on achieving this underlying goal?",
          "createdAt": "2022-01-19T17:34:32Z",
          "updatedAt": "2022-01-19T17:34:32Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Thanks for linking @csharrison. Agreed that these seem to be addressing the same underlying goal. #64 is closed, but its outcome is unclear to me. Do you think it's still worth working on achieving this underlying goal?\r\n\r\nThe issue isn't closed (just the PR https://github.com/abetterinternet/ppm-specification/pull/78). I still agree on the goal.\r\n\r\ncc @cjpatton ",
          "createdAt": "2022-01-19T17:59:24Z",
          "updatedAt": "2022-01-19T17:59:24Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "> The issue isn't closed (just the PR #78). I still agree on the goal.\r\n\r\nWhoops, misread! ",
          "createdAt": "2022-01-19T18:36:49Z",
          "updatedAt": "2022-01-19T18:36:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "FWIW, I don't think anyone strongly objects to allowing multiple reports per upload request. This is where we left it previously: https://github.com/abetterinternet/ppm-specification/pull/78#issuecomment-880096898\r\n\r\n",
          "createdAt": "2022-01-19T21:31:08Z",
          "updatedAt": "2022-01-19T21:31:08Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "In the current text, the closest we get to discussing anything like the _consumer_ is [this section](https://github.com/abetterinternet/ppm-specification/blob/main/draft-gpew-priv-ppm.md#anonymizing-proxies-anon-proxy) where we suggest using [OHTTP](https://datatracker.ietf.org/doc/draft-ietf-ohai-ohttp/) to put a server between clients and the aggregators. Besides stripping identifying information like IP addresses, such a server could also do any number of things that are useful in a specific deployment but can't or shouldn't be specified in PPM, like verifying vendor specific attestations from clients to increase confidence that reports are genuine.\r\n\r\nAs I recall, the goal was to avoid explicitly prescribing how such a server should work in PPM, just to reduce the complexity of the protocol. I still hope OHTTP/OHAI are good answers here, but I also don't think there's anything in the protocol that precludes arbitrary proxy solutions from being deployed. So long as the PPM leader is receiving well-formed reports from the proxy, then the protocol spoken between the proxy and the client can be anything. The exception of course is that the individual reports should be encrypted to the aggregators' HPKE keys by the real clients, but I don't think there's any way for PPM aggregators to prevent a proxy from transcrypting reports (of course, the threat model already concedes that a malicious client can leak its own inputs, and transmitting them in the clear to a proxy counts as leaking).\r\n\r\nAnyway, my opinion here is that we should formalize the notion of allowing multiple reports per upload requests to allow efficient batching of reports in client proxies and that we should discuss and illustrate but perhaps not specify how client proxies would be involved in PPM.",
          "createdAt": "2022-01-19T23:39:36Z",
          "updatedAt": "2022-01-19T23:53:32Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to @tgeoghegan, with one caveat: It may be useful for aggregators to do some sort of attestation of the clients, potentially even through an ingestor proxy. (See https://github.com/abetterinternet/ppm-specification/issues/89.) But this is out-of-scope of the core protocol right now. We included \"upload extensions\" with the report in order to enable this (see section {{upload-extensions}}), which may or may not be sufficient.",
          "createdAt": "2022-01-19T23:52:17Z",
          "updatedAt": "2022-01-19T23:52:17Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@eriktaubeneck given the recent discussion on the list, I'm wondering if it's possible to close this and open an issue for tracking the various deployment scenarios folks are considering. Besides that, is there anything else still unresolved in this thread?",
          "createdAt": "2022-01-21T17:44:43Z",
          "updatedAt": "2022-01-21T17:44:43Z"
        },
        {
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "body": "@cjpatton that sounds like a good plan to me. I will close this now and open up a new thread next week.",
          "createdAt": "2022-01-21T23:54:03Z",
          "updatedAt": "2022-01-21T23:54:03Z"
        }
      ]
    },
    {
      "number": 173,
      "id": "I_kwDOFEJYQs5AvW_R",
      "title": "addressing collusion",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/173",
      "state": "CLOSED",
      "author": "npdoty",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [
        "parking-lot"
      ],
      "body": "The privacy guarantees that motivate this protocol work require non-collusion: if servers collude, all the individual data can be retrieved. As was raised by a few folks at the BOF, methods for addressing collusion or providing users some confidence about non-collusion will be important for evaluating the utility for privacy of this work.\r\n\r\nWe might want to adjust the charter to not exclude discovery/selection, as that might benefit from hooks to help users choose servers operated by different parties. Or, we might include in the scope some group discussion or recommendation of ways to provide user confidence in non-collusion, even if that won't be defined in the protocol. Or, if there's an implied dependency on some other project to establish co-operative but non-colluding servers, we should make that explicit. Finally, it might be useful to describe a threat model and the implications of colluding servers, to make it easier to evaluate these proposals against alternatives that rely on trusting a server to minimize collected data.",
      "createdAt": "2021-12-21T19:21:50Z",
      "updatedAt": "2023-10-19T22:17:48Z",
      "closedAt": "2023-10-19T22:17:48Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> We might want to adjust the charter to not exclude discovery/selection, as that might benefit from hooks to help users choose servers operated by different parties. \r\n\r\nI think it might be interesting for the PPM WG to eventually consider this, but I don't think it should be in scope for the PPM RFC. My worry is that there's a huge number of ways for a client to agree on PPM task parameters with an aggregator and then authenticate for subsequent API interactions, and trying to describe them would dilute the focus of the PPM RFC. Discovery, especially, is a thorny problem, and I think we'd quickly end up reinventing the X.500 directory. To be clear, it'd be really nice to have something like the X.500 directory out there, but even if it's possible or a good idea to build that, I think it's outside of the scope of the PPM WG and protocol.\r\n\r\n> Or, we might include in the scope some group discussion or recommendation of ways to provide user confidence in non-collusion, even if that won't be defined in the protocol. Or, if there's an implied dependency on some other project to establish co-operative but non-colluding servers, we should make that explicit.\r\n\r\nI think it's important to remember that the point of PPM isn't to increase a user's confidence that any individual server will comply with the protocol. The point is to reduce the probability of compromise by increasing the number of participants who must defect and collude for privacy to be compromised. So in fact PPM concedes that it's not practical to get guarantees about the behavior of a remote server.\r\n\r\nLet's suppose there did exist some scheme that lets a client have confidence in the behavior of a remote server. Then I wouldn't need an algorithm like Prio at all: I would simply apply the scheme to a conventional telemetry system and have it prove to users that it never leaks information (I should credit this observation to Mariana Raykova, who explained it to me when I suggested specifying just the kind of mechanism you're suggesting).\r\n\r\nFurther, attestation schemes usually rely on some kind of trusted computing base (TCB) outside of the system being measured, like a TPM, ARM TrustZone or Intel SGX. Besides the degree to which these complicate the trust model (e.g., if TrustZone is used, I now have to trust ARM, the SoC manufacturer and the OEM that fused a software signing key into the SoC -- and we haven't thought about multi-tenant virtualization yet), I don't think we want to impose such hardware requirements on PPM implementations.\r\n\r\nFinally, I'm not convinced that there are yet any standards for attestation mature enough to be used in a protocol like PPM. What schemes exist today are usually intended to allow a vendor's backend to trust devices manufactured by a vendor, which is a very different proposition than allowing end users to trust an arbitrary remote server. I am aware that there's an [IETF working group](https://datatracker.ietf.org/wg/rats/about/) working in this space. Once that standard becomes mature, I think it'd be worth mentioning, but it's use in PPM implementations would be orthogonal to the PPM protocol itself: attestation tokens or bindings, just like any other kind of authentication, could be included in the HTTP requests in the PPM protocol, or provided to the helper in the [upload extensions](https://github.com/abetterinternet/ppm-specification/blob/main/draft-gpew-priv-ppm.md#upload-extensions-upload-extensions).\r\n\r\n> Finally, it might be useful to describe a threat model and the implications of colluding servers, to make it easier to evaluate these proposals against alternatives that rely on trusting a server to minimize collected data.\r\n\r\nWe do have a [threat model](https://github.com/abetterinternet/ppm-specification/blob/main/draft-gpew-priv-ppm.md#threat-model) section in the document which includes a section discussing [aggregator collusion](https://github.com/abetterinternet/ppm-specification/blob/main/draft-gpew-priv-ppm.md#aggregator-collusion), though all it says is that if all servers collude, the system provides no privacy at all. We could mention that adding more aggregators helps to mitigate these threats (because more aggregators means more servers have to collude), but the protocol is currently only designed to support exactly two aggregators.",
          "createdAt": "2021-12-29T20:52:05Z",
          "updatedAt": "2021-12-29T20:52:05Z"
        },
        {
          "author": "npdoty",
          "authorAssociation": "NONE",
          "body": "> I think it's important to remember that the point of PPM isn't to increase a user's confidence that any individual server will comply with the protocol. The point is to reduce the probability of compromise by increasing the number of participants who must defect and collude for privacy to be compromised. So in fact PPM concedes that it's not practical to get guarantees about the behavior of a remote server.\r\n> \r\n> Let's suppose there did exist some scheme that lets a client have confidence in the behavior of a remote server. Then I wouldn't need an algorithm like Prio at all: I would simply apply the scheme to a conventional telemetry system and have it prove to users that it never leaks information (I should credit this observation to Mariana Raykova, who explained it to me when I suggested specifying just the kind of mechanism you're suggesting).\r\n> \r\n> Further, attestation schemes usually rely on some kind of trusted computing base (TCB) outside of the system being measured, like a TPM, ARM TrustZone or Intel SGX. Besides the degree to which these complicate the trust model (e.g., if TrustZone is used, I now have to trust ARM, the SoC manufacturer and the OEM that fused a software signing key into the SoC -- and we haven't thought about multi-tenant virtualization yet), I don't think we want to impose such hardware requirements on PPM implementations.\r\n\r\nMy understanding is that the privacy advantage of PPM schemes is that it provides privacy without attestation of server behavior *by* splitting up the computation to servers that are known to be operated by different parties. So I'm not suggesting that we include in scope or rely on some perfect attestation of server behavior (I agree, it would make the PPM protocol unnecessary) but that we do consider mechanisms for choosing servers operated by different parties.\r\n\r\nIf there's no reason at all for a user to believe that the PPM system is operated by multiple non-colluding parties, then I don't see how it's different for the user than relying on the trustworthiness of a single telemetry server.",
          "createdAt": "2021-12-29T22:42:48Z",
          "updatedAt": "2021-12-29T22:42:48Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I am opposed to any work to define server selection or discovery at this time. All of the applications I am aware of that people actually intend to deploy involve software vendors selecting (or filtering) the servers, so this doesn't seem helpful.\r\n\r\nThe reason that it's different for the user is that they can *see* which servers the client is using and make their own decision about whether that's good enough. As a general matter, this is the kind of assurance users have about the behavior of any software they run.\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2021-12-29T23:10:31Z",
          "updatedAt": "2021-12-29T23:14:55Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> If there's no reason at all for a user to believe that the PPM system is operated by multiple non-colluding parties, then I don't see how it's different for the user than relying on the trustworthiness of a single telemetry server.\r\n\r\nOooh, that's an interesting point and I see now how it is distinct from a server attesting to the measurements of its code. We do require clients to connect to aggregators over HTTPS, which means they get a server identity in the form of the TLS certificate. At the risk of being glib, I'd say that if the web PKI hasn't figured out a better solution than OV and EV certs to prove to clients that `foo.com` and `bar.com` aren't the same real-world entity, then I don't know if we can do better.",
          "createdAt": "2021-12-29T23:12:11Z",
          "updatedAt": "2021-12-29T23:12:11Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": " I agree with Tim and EKR that working on server selection is premature at this time.",
          "createdAt": "2021-12-29T23:49:25Z",
          "updatedAt": "2021-12-29T23:49:25Z"
        },
        {
          "author": "npdoty",
          "authorAssociation": "NONE",
          "body": "Implementer pre-selection or filtering of servers, transparency and authentication of servers, and audits based on that transparency -- these are indeed ways to address the threat of collusion.\r\n\r\nIf there's agreement among potential implementers that selection/discovery won't be used (at least for now), then it's fine to exclude that from WG scope. But I would suggest that we should discuss the ways collusion will be addressed, in case it has impacts on the protocol design.",
          "createdAt": "2022-01-03T22:12:07Z",
          "updatedAt": "2022-01-03T22:12:07Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Collusion is now described in the updated security considerations. Technical means to address collusion are definitely out of scope here, as they are for protocols whose properties depend on similar non-collusion assumptions (OHTTP, MASQUE, Privacy Pass, etc). I think we can close this out.",
          "createdAt": "2023-10-19T22:17:48Z",
          "updatedAt": "2023-10-19T22:17:48Z"
        }
      ]
    },
    {
      "number": 176,
      "id": "I_kwDOFEJYQs5BMBlP",
      "title": "\"Heavy Hitters\" is now \"Poplar\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/176",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "@henrycg has informed me that the authors of [[BBCGGI21](https://eprint.iacr.org/2021/017.pdf)] have given their protocol a proper name so as to avoid confusing conflating the protocol with the problem it solves. We ought to adopt the name in the PPM and [VDAF](https://github.com/cjpatton/vdaf) specs. ",
      "createdAt": "2022-01-04T19:06:37Z",
      "updatedAt": "2022-01-06T18:48:37Z",
      "closedAt": "2022-01-06T18:48:37Z",
      "comments": []
    },
    {
      "number": 180,
      "id": "I_kwDOFEJYQs5B0TPi",
      "title": "Aggregators required to store nonce sets indefinitely",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/180",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Since #169, aggregators are required to store report nonces for batch intervals that have not yet been collected. This is necessary in order to prevent replay attacks. The problem is that nothing guarantees that a batch interval will be collected in a timely manner: In theory, it's possible that aggregators will have to store nonce sets indefinitely. To prevent this, there needs to be a way for the aggregators to agree on when to stop accepting reports for a given batch interval.\r\n\r\nThe strawman solution is to allow each aggregator to, at its own discretion, ignore a report if it can't confirm if the report was replayed or not. The problem is that this potentially leads to unnecessary data loss.",
      "createdAt": "2022-01-15T00:13:29Z",
      "updatedAt": "2022-09-15T19:20:43Z",
      "closedAt": "2022-09-15T19:20:43Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I propose resolving this with #338.",
          "createdAt": "2022-09-13T02:25:56Z",
          "updatedAt": "2022-09-13T02:25:56Z"
        }
      ]
    },
    {
      "number": 183,
      "id": "I_kwDOFEJYQs5CEUiE",
      "title": "Interaction of anti-replay and flexibility",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/183",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Reading through the latest changes on anti-replay it seems like we are foregoing a bunch of flexibility that we may want.\r\n\r\nHere's a simple example:\r\nSuppose that I have a measurement that i want to slice by geographic regions. At the moment this is impossible because the anti-replay mechanism requires only one set of aggregations for a given time window. As @cjpatton pointed out to me privately, you can have a different task for each region, but:\r\n\r\n1. This is going to be a pain if you want to change your slicing policy later\r\n2. It mostly precludes crosstabs and drilldown \r\n\r\nAt minimum, I think we want an anti-replay design to guarantee that any given input share is aggregated at most once but that allows overlapping windows. Eventually, I expect we're going to want to allow multiple aggregation but in restricted ways (otherwise drilldown and crosstabs are hard).",
      "createdAt": "2022-01-19T18:33:08Z",
      "updatedAt": "2023-10-18T15:05:09Z",
      "closedAt": "2023-10-18T15:05:09Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree that flexibility is generally useful, but we need to be careful to not allow any sort of overlapping windows to violate report privacy. We need a better way to reason about what's permissible and what's not here. ",
          "createdAt": "2022-01-19T22:58:03Z",
          "updatedAt": "2022-01-19T22:58:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree that the ability to filter aggregates by some attribute (region, user agent, etc.) is a useful feature, and I agree that protocol changes will likely be needed to support it. I'm not sure what this has to do with anti-replay, however:\r\n\r\n> At minimum, I think we want an anti-replay design to guarantee that any given input share is aggregated at most once but that allows overlapping windows. Eventually, I expect we're going to want to allow multiple aggregation but in restricted ways (otherwise drilldown and crosstabs are hard).\r\n\r\nA collect request may specify any batch interval the collector wants, so long as the conditions in {{batch-parameter-validation}}. This allows one to \"drill down\" on clients with certain attributes while still maintaining some privacy invariants.\r\n\r\nBut to emphasize @chris-wood's point, these conditions may not be sufficient. A more careful analysis is needed (especially for applications that use DP).",
          "createdAt": "2022-01-19T23:47:19Z",
          "updatedAt": "2022-01-19T23:47:19Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": ">  I'm not sure what this has to do with anti-replay, however:\r\n\r\nThe following text forbids overlapping batches, which are necessary for both drilldown and crosstabs\r\n\r\n> In addition, for any report whose nonce contains a timestamp that falls in a batch interval for which it has completed at least one aggregate-share request (see Section 4.3.2), the helper MUST send an error messsage in response rather than its next VDAF message. Note that this means leaders cannot interleave a sequence of aggregate and aggregate-share requests for a single batch.\r\n\r\n",
          "createdAt": "2022-01-19T23:50:59Z",
          "updatedAt": "2022-01-19T23:50:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah, indeed it does. In fact, this seems like it prevents any report from being aggregated more than once (even as required for protocols like Poplar). This needs to be fixed regardless.",
          "createdAt": "2022-01-19T23:57:08Z",
          "updatedAt": "2022-01-19T23:57:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "So I think the following is what we want. the check needs to be enforced by both the leader and helper (though at different times), so I'll try to speak in terms of a generic aggregator.\r\n\r\nBefore it processes a report share, the aggregator first checks if the report pertains to a batch that has already been collected.  This is the case if the aggregator is the leader and has completed a collect request for a batch containing the report, or if the aggregator is a helper and has completed an aggregate-share request for a batch containing the report (see {{aggregate-share-request}}). It also checks if the report has been \"observed\". This is the case if the report was used in a previous aggregate request.\r\n                               \r\n* If the report has not yet been observed but is contained by a batch that has   been collected, then the aggregator returns an error rather than process the report share further. This prevents additional reports from being aggregated after its batch has already been collected.              \r\n                               \r\n* If the report has been observed but has not been collected, then it returns an error rather than continue processing. This  prevents a report from being used more than once in a batch.\r\n\r\nIn particular, we *only* process a report if either it has been observed *and* has been collected *or* it has never been observed and is not contained by a batch that has been collected.\r\n\r\nI believe this allows overlapping batches to be collected without interfering with anti-replay.",
          "createdAt": "2022-01-21T22:02:25Z",
          "updatedAt": "2022-01-22T00:57:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Superseded by #489.",
          "createdAt": "2023-10-18T15:05:09Z",
          "updatedAt": "2023-10-18T15:05:09Z"
        }
      ]
    },
    {
      "number": 185,
      "id": "I_kwDOFEJYQs5COzIl",
      "title": "Replace helper state blob with an aggregation job UUID",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/185",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "chris-wood"
      ],
      "labels": [],
      "body": "@cjpatton and I were debating the [helper state blob](https://abetterinternet.github.io/ppm-specification/draft-gpew-priv-ppm.html#name-helper-state). Now that helpers have to store all the report nonces and are also likely to be expected to store their own report shares, I argue that we should get rid of it: we don't need it and it entails a bunch of complicated security considerations. Helpers need to encrypt their state so leaders can't see it, need to protect themselves against leaders replaying old state blobs and they need to version it so that a helper can deal with a blob constructed by an older version of itself.\r\n\r\nChris' rebuttal is that even if you don't use the blob to store something like a set of nonces or a counter or a partial aggregate share, it's still useful as a key into the storage where the helper keeps its state. This is helpful for a helper running on something like a Cloudflare Worker, where ephemeral servers are launched in response to incoming messages, and so a different helper instance might handle the sequence of `AggregateInit` and `Aggregate` requests.\r\n\r\nWe agreed that we don't need an arbitrarily large opaque blob shipped back and forth between leader and helper to solve that problem. All you need is a unique value like a UUID so that a stateless helper can recover context from storage. This will let us delete a whole bunch of SHOULD and MAY discussion from the spec.",
      "createdAt": "2022-01-22T01:48:38Z",
      "updatedAt": "2022-05-10T14:21:24Z",
      "closedAt": "2022-05-10T14:21:24Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we also owe a definition of an \"aggregation job\", which is evolving in #179 ",
          "createdAt": "2022-01-22T01:55:35Z",
          "updatedAt": "2022-01-22T01:55:35Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Some discussion of the implications of helper state to parallelism is in #150.",
          "createdAt": "2022-04-29T00:55:19Z",
          "updatedAt": "2022-04-29T00:55:19Z"
        }
      ]
    },
    {
      "number": 187,
      "id": "I_kwDOFEJYQs5CV14J",
      "title": "Make sure error codes are enumerated in errors table",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/187",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "We have some error codes like `staleReport` that aren't enumerated in the table in `Message Transport.Errors`. We should bring that table up to date. While we're in there, I think `outdatedConfig` and `staleReport` aren't the right types, since outdatedness or staleness aren't the only reasons that an HPKE config or report, respectively, could be rejected.",
      "createdAt": "2022-01-24T18:58:00Z",
      "updatedAt": "2022-05-11T18:57:19Z",
      "closedAt": "2022-05-11T18:57:19Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The error table was brought up to date by #233.",
          "createdAt": "2022-05-11T18:57:19Z",
          "updatedAt": "2022-05-11T18:57:19Z"
        }
      ]
    },
    {
      "number": 188,
      "id": "I_kwDOFEJYQs5Cbc57",
      "title": "Consider allowing clients to upload mulitple reports in a single message to aggregators",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/188",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Right now, an upload message from a client to an aggregator contains a single report (or two shares of a single report, in #174). I suspect there will be cases where a client will want to upload multiple reports in one HTTP transaction, especially deployments that use an intermediate client/client proxy/batching client. I believe we will need to define a PPM protocol message that allows multiple reports to support this.",
      "createdAt": "2022-01-26T00:21:01Z",
      "updatedAt": "2022-01-26T20:38:08Z",
      "closedAt": "2022-01-26T20:38:07Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This sounds like a duplicate of https://github.com/abetterinternet/ppm-specification/issues/64?",
          "createdAt": "2022-01-26T02:10:00Z",
          "updatedAt": "2022-01-26T02:10:00Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "You're right, it is! I should have searched for existing issues before writing up a new one.",
          "createdAt": "2022-01-26T20:38:07Z",
          "updatedAt": "2022-01-26T20:38:07Z"
        }
      ]
    },
    {
      "number": 189,
      "id": "I_kwDOFEJYQs5DBlJI",
      "title": "fix VDAF reference",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/189",
      "state": "CLOSED",
      "author": "bdaehlie",
      "authorAssociation": "NONE",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "Would be helpful to readers if this reference was fixed in the Editor's Copy:\r\n\r\n[I-D.draft-cfrg-patton-vdaf]\r\n    \"*** BROKEN REFERENCE ***\". ",
      "createdAt": "2022-02-04T18:21:20Z",
      "updatedAt": "2022-04-27T22:02:55Z",
      "closedAt": "2022-04-27T22:02:55Z",
      "comments": []
    },
    {
      "number": 190,
      "id": "I_kwDOFEJYQs5DBmFO",
      "title": "reconsider reliance on strict ordering",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/190",
      "state": "CLOSED",
      "author": "bdaehlie",
      "authorAssociation": "NONE",
      "assignees": [
        "branlwyd"
      ],
      "labels": [],
      "body": "The PPM specification relies on strictly enforced ordering in a couple of places:\r\n\r\nExample 1: \"aggregator_endpoints: A list of URLs relative to which an aggregator's API endpoints can be found. Each endpoint's list MUST be in the same order. The leader's endpoint MUST be the first in the list.\"\r\n\r\nExample 2: \"encrypted_input_shares contains the encrypted input shares of each of the aggregators. The order in which the encrypted input shares appear MUST match the order of the task's aggregator_endpoints (i.e., the first share should be the leader's, the second share should be for the first helper, and so on).\"\r\n\r\nThis technically solves the intended problem just fine as far as I can tell but it's a scheme that's prone to implementation mistakes, and if a mistake is made it'll be harder to spot than it probably needs to be. I would consider some kind of explicit tagging of resources rather than relying on consistent ordering in lists.",
      "createdAt": "2022-02-04T18:26:05Z",
      "updatedAt": "2023-10-13T20:48:46Z",
      "closedAt": "2023-10-13T20:48:46Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is addressed by the changes in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393: because we now have exactly two aggregators, we can change structures like report to have explicit `leader_share` and `helper_share` fields instead of a vector of shares and these awkward semantics around ordering. We'll close this once #393 lands.",
          "createdAt": "2023-05-26T21:16:04Z",
          "updatedAt": "2023-05-26T21:16:04Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #393.",
          "createdAt": "2023-10-13T20:48:46Z",
          "updatedAt": "2023-10-13T20:48:46Z"
        }
      ]
    },
    {
      "number": 191,
      "id": "I_kwDOFEJYQs5DBzok",
      "title": "reconsider empty body requirement for status 200 responses",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/191",
      "state": "CLOSED",
      "author": "bdaehlie",
      "authorAssociation": "NONE",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-03"
      ],
      "body": "Section 4.2.2:\r\n\r\n\"Clients SHOULD NOT upload the same measurement value in more than one report if the leader responds with status 200 and an empty body.\u201d\"\r\n\r\nIs a non-empty body understood to signal an error condition based on some other context?\r\n\r\nWhat if the client got status 200 from the leader but a non-empty body? Should the client retry as this text suggests?\r\n\r\nI don't know enough about HTTP semantics to know for sure, but it seems like specifying an empty body here is unnecessary. It seems fine to specify that the leader should *send* an empty body for 200, but maybe be less strict on the consumption side and drop the empty body requirement?",
      "createdAt": "2022-02-04T19:34:22Z",
      "updatedAt": "2022-12-08T22:50:24Z",
      "closedAt": "2022-12-08T22:50:24Z",
      "comments": []
    },
    {
      "number": 192,
      "id": "I_kwDOFEJYQs5DB2Mk",
      "title": "standardize HTTP response wording",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/192",
      "state": "CLOSED",
      "author": "bdaehlie",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "All of the following are currently used to refer to 200 responses:\r\n\r\n* status 200\r\n* HTTP 200 OK\r\n* HTTP status 200\r\n* HTTP status 200 OK\r\n\r\nWould be nice to pick one and use it consistently.",
      "createdAt": "2022-02-04T19:48:23Z",
      "updatedAt": "2022-05-12T20:28:52Z",
      "closedAt": "2022-05-12T20:28:52Z",
      "comments": []
    },
    {
      "number": 195,
      "id": "I_kwDOFEJYQs5FFuP-",
      "title": "Collect requirements",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/195",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Let's start with an overview of the problem as I understand it. Currently, batch management is dealt with by dividing the time into a contiguous list of batch windows, each of size min_batch_duration epochs. This is illustrated below (T denotes the T\u2019th batch window):\r\n\r\n~~~\r\n|...| T - 1 |  T  | T + 1 |...|\r\n~~~\r\n\r\nEach report indicates an epoch that places it uniquely into one batch window. \r\n\r\nWhen Collectors request the aggregate shares for a batch window, they indicate the interval of time they are interested in collecting aggregate results. This request is constrained in that the interval must be a multiple of the batch window size (min_batch_duration), though it can exceed more than one batch window. For example, a Collector can request aggregate shares for a single batch window T (marked with asterisks), as follows:\r\n\r\n~~~\r\n|...| T - 1 |  T  | T + 1 |...|\r\n            *******\r\n~~~\r\n\r\nA Collector can also request aggregate shares for multiple, consecutive batch windows T and T+1 as follows:\r\n\r\n~~~\r\n|...| T - 1 |  T  | T + 1 |...|\r\n            ***************\r\n~~~\r\n\r\nFor privacy, each collect request must correspond to some minimum number of measurements, called the min_batch_size. If the aggregators do not have enough report shares to satisfy the request, they must respond with an error. \r\n\r\nIntiuitively, this requirement is meant to esnure that the collect requests cannot be abused to learn individual measurements contributed to an aggregate output. However, the current batch scheme doesn't abide by this requirement. To see why, assume there exists a batch window T in which min_batch_size reports fall. Moreover, assume that there exists only one report in the T+1 window. Assume further the Collector makes the following pair of requests:\r\n\r\n1. out1 = Collect(T, T+1)\r\n2. out2 = Collect(T)\r\n\r\nThis would be a reasonable sequence of requests if, for example, the Collector wanted to drill down to get data for a more fine-grained batch window. However, if both requests were satisfied given the current set of reports, the delta between out1 and out2 would allow the Collector to learn the value of the single individual report in the window T+1. In general, the protocol should prohibit privacy violations of this form.\r\n\r\nOne way to rule out such privacy violations is to require that each batch window covered by a collect request contain at least min_batch_size reports. (In the example above, the first collect request above would be prohibited because T+1 has just one report.) However, this is perhaps overly restrictive. We want queries to be flexible enough to allow one to slice batches differently, but this needs to be done such that privacy violations cannot occur. \r\n\r\nGiven the above, here\u2019s an attempt to formalize the batch query constraint requirement. Let Ci = Collect(Ti,..., Tn), i <= n denote a collect request for the batch interval Ti,...,Tn. For each Ci there exists a unique Si = {r | Ti,...,Tn} where |Si| >= B. That is, Si is the set of reports included in the aggregate covered by the interval Ti,...,Tn, and its size is at least B, where B is min_batch_size.\r\n\r\nBased on this, the requirement can simply be stated as follows. Given Ci,...,Cq collect requests, it must not be possible for the collector to reconstruct an aggregate of over a report set S* such that |S*| < B . The Collector can run any number set operations, including intersection, union, and difference, using the output sets Si,...,Sq.\r\n\r\nThere's a number of questions that might follow from this:\r\n\r\n- Is this the right requirement? If not, how should it be tweaked?\r\n- If so, should the protocol specify how to enforce it, or defer that to the implementations? If we assume that collect requests will always agree on their aggregate inputs, then maybe we can simply rely on this and each aggregator's own \"collect request validation implementation\" to ensure the batch constraints aren't violated. \r\n- If we do decide to specify its enforcement, how should aggregators check collect requests for correctness while supporting query flexibility? For example, as in #183, what if one wanted to include a \"space dimension\" with a query, e.g., \"give me the aggregate for all reports in time Ti,..,Tj for this given User-Agent\" or whatever, then one shouldn't be able to abuse this to violate the requirement above.\r\n\r\nThere may be other questions that fall out of this as well, but hopefully this gets the ball rolling.",
      "createdAt": "2022-03-04T00:59:18Z",
      "updatedAt": "2023-10-18T15:10:09Z",
      "closedAt": "2023-10-18T15:08:05Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Currently, we have a task parameter `max_batch_lifetime` defined as:\r\n> The maximum number of times a batch of reports may be\r\n>  used in collect requests.\r\n\r\nIn a task where `max_batch_lifetime` (which I prefer to think of as a privacy budget) is 1, the `(out1, out2)` attack is defeated because after doing `Collect(T, T+1)`, the aggregators will refuse to service `Collect(T)`, since the `max_batch_lifetime` for `T` has been consumed. However I'm not sure if that works for tasks where `max_batch_lifetime > 1`, as will be the case for poplar1 (which successive rounds of prefix queries are made by the collector).\r\n\r\nWhat if we require that once a batch window has been included in a collect request over `Ti...Tn`, it can only appear in later collect requests over supersets of `Ti...Tn`. i.e., after doing `Collect(T, T+1)`, `Collect(T, T+1, T+2)` is permitted, but `Collect(T)` is not. This would make the state tracking quite a bit more complicated for the aggregators, though.",
          "createdAt": "2022-03-04T23:06:55Z",
          "updatedAt": "2022-03-04T23:06:55Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Tim: Requiring that later collect requests to be either a superset or disjoint to each prior collect request wouldn't be sufficient to meet the requirement, as a privacy-attacking collector could just order their requests differently. If T had `min_batch_size` reports in it, and T+1 had one report, they could issue `Collect(T)`, followed by `Collect(T, T+1)`, and then take the difference to isolate the value of the lone report in T+1. While no aggregator under any rule would allow `Collect(T+1)`, we will have to apply more strict conditions to avoid this difference-of-aggregates attack.\r\n\r\nI have an intuitive argument that deciding whether a set of collect requests could produce an aggregate over S* such that |S*| < B could be computationally hard. If the VDAF in use is prio3, we can represent each collect request with a pair of vectors, one vector of the additive aggregates in the result, and one vector (\"hot encoded\") that signifies which epoch the results came from. Let the report vector have dimension equal to the number of epochs in the time period we're analyzing, (call it N) and each element will be 0 if the corresponding epoch isn't part of the collect request, or equal to the number of reports in the epoch if it is included, |{r | Ti}|. The total number of reports in a collect request is equal to the element-wise sum of this report vector, which is the taxicab metric. We know that for each report vector, this sum is greater than or equal to B, `min_batch_size`. A privacy-attacking collector can take a linear combination of the additive aggregate results from the collect requests, and we can take the same linear combination of the corresponding report vectors to track which reports are included, and with which multiplicity. Now, our requirement begins to look like a lattice problem. We start with a set of the report vectors with taxicab norm >= B, and we want to know whether the lattice they generate has any vectors with taxicab norm < B.\r\n\r\n(Caveats: I don't know whether lattice problems are as hard with the taxicab metric as with the typical length norm. If the attacker somehow arrives at the aggregate for 2\\*Ti, they could divide by a small multiple to recover it, and the lattice view of the problem would be insufficient. I'm not sure whether there exists a combination of collect requests for which an attacker could reconstruct 2\\*Ti but not Ti, or some other small multiple. These report vectors have a fairly restrictive structure, as they each have only one consecutive run of nonzero elements; this may make the lattice problem easier than the general case.)\r\n\r\nMoreover, here's a pathological example to consider: Let B=10, and let N=100. Say that T0 has one report, and T1 through T99 each have B reports. Our attacker first issues `Collect(T0, T1)`, `Collect(T1, T2)`, `Collect(T2, T3)`, ... `Collect(T98, T99)`. Each successive request covers either B+1 or 2B reports. While the one-sided set difference of `Collect(T0, T1)` and `Collect(T1, T2)` is only one report, the attacker can't recover a `prio3` aggregate of the lone report in T0, as it's confounded by the influence of T2. This continues through the rest of the collect requests on two epochs. Now, the collector issues a request `Collect(T99)`. If the aggregators do not reject this request, the collector will be able to violate privacy by back-calculating the value of the aggregate of `Collect(T0)`, with only one report, which should not be permitted.\r\n\r\nWith this pathological example in mind, defining a collect request acceptance rule that only checks over some fixed time horizon could be troublesome. And, since we have to consider differences of collect requests, each additional collect request we include in our analysis could up to double the number of subsets we look at. (no good if the number of collect requests considered can be arbitrarily large) I think we will have to make some trade-off to be less-than-maximally permissive in what sorts of patterns of collect requests we allow, so we can preserve privacy of reports in sparse epochs, while making the on-line decision to allow or reject a collect request simpler.\r\n\r\nOne simple rule that we could adopt: Every collect request must either not overlap any previous non-aborted collect requests, or have an identical start time and interval to all overlapping non-aborted collect requests. This would allow for what I imagine to be normal operation, continuously collecting over the minimum interval, and then retrying with longer intervals if incoming reports dip below `min_batch_size`. However, it would prohibit any attempts to \"temporally drill down\" in situations that would otherwise not violate privacy. Are there use cases which would be better served by that than collecting over smaller intervals and summarizing aggregates? Do we contemplate VDAFs which cannot be re-aggregated, like percentile calculations? (`poplar1` counts from separate aggregations could be combined, though they may miss some results that are just under the threshold in each interval, but would have been above in a combined interval)",
          "createdAt": "2022-03-11T18:56:07Z",
          "updatedAt": "2022-03-11T18:56:07Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Is there a use case for partially-overlapping collection intervals? As Tim notes, `prio3` doesn't allow this due to allowing only a single collect request for any given minimal batch window; my (possibly-incorrect) understanding of `poplar1` is that the repeated collect requests would in practice be done over an identical interval.\r\n\r\nThat is, could we get away with something like this (the same as what David suggests, IIUC): all collection intervals containing a given minimal batch window must be identical.\r\n\r\nThis disallows drilling down. In `prio3` we could mitigate this by keeping the minimal batch window as small as possible. I am not sure if we need drilling down for `poplar1`, and of course we don't know what VDAFs are coming.",
          "createdAt": "2022-03-14T17:07:16Z",
          "updatedAt": "2022-03-14T17:07:16Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Would it be a good middle-ground to always split the client space into disjoint sets? That would simplify the computation of what is allowed, as David described above, but would still allow some flexibility to drill-down into results, as long as the disjoint sets don't get too small.\r\n\r\nThe reasoning is that, given two sets A and B, with A \u2286 B, and \u03a3B = X, \u03a3A = Y, clearly the difference B\\A  has \u03a3B\\A = X-Y. So, to keep the privacy requirements, B\\A must have `min_batch_size` elements as well.\r\n\r\n(Edit: Note that drill-down would necessarily mean that aggregators have to store which sets they have used and to disable all sets containing a nonce when pruning old nonces.)",
          "createdAt": "2022-04-12T12:31:40Z",
          "updatedAt": "2022-04-18T10:54:00Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I was thinking about collect requirements for [Prio3Aes128Histogram](https://cfrg.github.io/draft-irtf-cfrg-vdaf/draft-irtf-cfrg-vdaf.html#name-prio3aes128histogram), and I'm now wondering if the definition and enforcement of collect protocol privacy requirements might depend on the VDAF in use, and thus some portion of this will have be defined at the VDAF layer.\r\n\r\nOur thinking thus far is that DAP aggregators would have to enforce a minimum number of contributions within a collect request's batch interval. The appeal of this approach is that it can be applied generically regardless of the VDAF in use and depends on a single shared parameter that's easy to apply. But suppose someone deploys Prio3Aes128Histogram, picks a reasonable minimum batch size, but happens to choose their bucket boundaries such that one of the buckets always contains a single contribution. Does that violate the privacy of the individual who reported that value?\r\n\r\nA straightforward fix here would be to say that an aggregator should refuse to release a Prio3Aes128Histogram aggregate share unless each bucket has either zero contributions or enough contributions to meet some a privacy threshold. But introducing special cases for VDAFs at the DAP layer seems wrong, so perhaps the enforcement of this kind of thing should be specified in the VDAF's [`out_shares_to_agg_share`](https://cfrg.github.io/draft-irtf-cfrg-vdaf/draft-irtf-cfrg-vdaf.html#name-aggregation-2) function, which would now optionally return an error if privacy requirements aren't met?",
          "createdAt": "2022-05-31T17:50:19Z",
          "updatedAt": "2022-05-31T17:50:19Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think this has to be handled not just at the VDAF side of things but in the task definition. The impact of the collected information on privacy and the leakage of the specific aggregration function are task specific and need to be be handled there.\r\n\r\nI am not sure how straightforward your fix would be, given that aggregators don't see how many entries there are in a bucket. So this would be another validation step. But actually a post-aggregation validation step might be a useful extension. \ud83e\udd14",
          "createdAt": "2022-05-31T19:03:22Z",
          "updatedAt": "2022-05-31T19:03:22Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "If we do view this as a privacy violation, it's a rather sticky one: as Simon points out, the collector is the first entity that would be able to determine how many reports any given bucket contains (since it unshards the aggregate shares into an aggregate result). But once the collector has computed the aggregate result it has exposed itself to the privacy-violating measurement, and we must trust that the collector will act unilaterally to do the right thing. In contrast, the other privacy protections such as `min_batch_size` are implemented by both the leader & the helper aggregators (i.e. two separate, non-colluding entities) and are implemented without revealing the privacy-violating measurements.",
          "createdAt": "2022-05-31T19:14:17Z",
          "updatedAt": "2022-05-31T19:14:17Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Here are two alternate collect interval rules, which would allow aggregators to efficiently validate requests, while both preventing difference-of-aggregates attacks and allowing the collector more flexibility than the \"non-overlapping or identical\" rule above.\r\n\r\nFor the first rule, let us add another task parameter that is the duration between \"fenceposts\". This duration should be a multiple of `min_batch_duration`. (or, it could be defined in terms of a number of `min_batch_duration` windows) Every timestamp that is a multiple of this duration is identified as a \"fencepost\" instant. Then, the rule is that collect intervals may not cross fenceposts, and all combinations of current and past collect intervals between two fenceposts should not reveal the aggregate over less than `min_batch_size` reports. Aggregators can check this by exhaustively trying all combinations of previously used collect request intervals between the fenceposts, short-circuiting with success if every `min_batch_duration` window between the fenceposts has more than `min_batch_size` reports, or other methods. The main downside of this rule is that it introduces another parameter, and it must be set to carefully balance competing concerns. If the fencepost interval is too small for actual report traffic, such that an entire fencepost interval has fewer than `min_batch_size` reports, then data will be irrevocably lost. If the fencepost interval is too large, relative to `min_batch_duration`, then collectors may inadvertently cause the aggregators to consume too much CPU, if they trigger exhaustive checking on too many intervals. It also limits the time spans that the collector can compute aggregates over, but if we assume parameters are well chosen, it would usually be possible to split a time span of interest at fenceposts, and do aggregation summarization outside of DAP.\r\n\r\nFor the second rule, we require that batch intervals are always a power-of-two multiple of `min_batch_duration`. We also require that batch intervals start at a timestamp that is a multiple of their duration. Now, the set of permissible batch intervals form an infinite binary tree, where the leaf nodes are all of the minimum sized intervals, and each level up combines two neighboring valid batch intervals. The rule the aggregators enforce is very simple: the batch interval's start time and duration must meet the above multiple and alignment rules, there must be `min_batch_size` reports inside the batch interval, and there must be `min_batch_size` reports inside the batch interval's sibling interval in the binary tree. We can identify this sibling interval's start time by taking the batch start time, dividing by the duration, XORing with 1, and multiplying by the duration again. The upside of this rule is that it makes the job of the aggregators very simple (we only care about the reports and not about previous collect requests, plus we only check report counts in two intervals). I can imagine that collectors could adaptively switch between requesting different collect interval widths as time marches on, so that they can always aggregate across the entire timeline, even if report counts dip. The downside is that it imposes fairly structured limitations on batch intervals.",
          "createdAt": "2022-06-23T16:17:42Z",
          "updatedAt": "2022-06-23T16:17:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "We've addressed at least the core privacy issue here. We might be able to squeeze more utility out of DAP with a fancier batch-boundary enforcement algorithm, but perhaps we should wait to work on this until we need to.",
          "createdAt": "2023-10-18T15:08:05Z",
          "updatedAt": "2023-10-18T15:08:05Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Related to #489.",
          "createdAt": "2023-10-18T15:10:08Z",
          "updatedAt": "2023-10-18T15:10:08Z"
        }
      ]
    },
    {
      "number": 202,
      "id": "I_kwDOFEJYQs5FjMSe",
      "title": "Do not require parsing aggregation messages before authenticating",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/202",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The interop target PR (#179) currently specifies that messages must be cryptographcially authenticated before they are interpreted:\r\n\r\n> The last 32 bytes of each message is an HMAC-SHA256 tag computed over the serialized message excluding the tag field itself. The key that is used is the agg_auth_key shared by the aggregators and configured for the given task.\r\n> \r\n> Upon receiving an AggregateReq message, an aggregator MUST verify the tag before interpreting the message's contents. If verification fails, it MUST abort and alert its peer with error \"invalidHmac\".\r\n\r\nThe HMAC key to use to perform verification is based on the task; the task is identified by an ID contained in the message. Therefore, currently the message must be interpreted before it can be authenticated. This is a problem because implementations both cannot follow the letter of the specification, and because requiring interpretation of message bytes before verification makes it more likely that implementations will be exposed to attacks from unauthenticated attackers based on bugs in the message parsing code.\r\n\r\nImplementations should not have to parse authenticated messages before performing authentication.\r\n\r\n\r\nDiscussing with @cjpatton, @chris-wood, & @divergentdave, two strawman solutions were identified:\r\n\r\n1) Include the task ID as an HTTP header. This is a relatively simple strategy, but would either move the task ID out of the message itself (losing binding of the message to a task), or duplicate the task ID in both the message & the header (increasing the amount of overhead per request).\r\n\r\n2) Ensure that the task ID is at a specific byte offset in all authenticated messages, similar to the current arrangement that the authentication tag is the last 32 bytes of each message. For example, the task ID could be arranged to be the first 32 bytes of the message. Then authentication would be: read the task ID as the first 32 bytes and retrieve the HMAC key; verify the content based on the tag as the last 32 bytes & the retrieved key; if successful, parse the message. This strategy allows the message to keep its binding to the task without requiring duplication of information, but is somewhat more complex to specify & implement.",
      "createdAt": "2022-03-11T20:42:41Z",
      "updatedAt": "2022-04-12T19:06:09Z",
      "closedAt": "2022-04-12T19:06:08Z",
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Here's a third proposal for how to handle HMAC authentication. I propose the following structure definitions:\r\n\r\n```\r\nstruct {\r\n  AggregationJobID job_id;\r\n  opaque agg_param<0..2^16-1>;\r\n  ReportShare seq<1..2^16-1>;\r\n} AggregateInitReq;\r\n\r\nstruct {\r\n  AggregationJobID job_id;\r\n  Transition seq<1..2^16-1>;\r\n} AggregateContinueReq;\r\n\r\nstruct {\r\n  AggregateReqType msg_type;\r\n  select (msg_type) {\r\n    agg_init_req:  AggregateInitReq;\r\n    agg_cont_req:  AggregateContinueReq;\r\n  }\r\n} AggregateReqInner;\r\n\r\nstruct {\r\n  TaskID task_id;\r\n  opaque inner<0..2^16-1>;\r\n  opaque tag[32];\r\n} AggregateReqOuter;\r\n\r\nstruct {\r\n  Transition seq<1..2^16-1>;\r\n} AggregateRespInner;\r\n\r\nstruct {\r\n  opaque inner<0..2^16-1>;\r\n  opaque tag[32];\r\n} AggregateRespOuter;\r\n\r\nstruct {\r\n  Interval batch_interval;\r\n  uint64 report_count;\r\n  opaque checksum[32];\r\n} AggregateShareReqInner;\r\n\r\nstruct {\r\n  TaskID task_id;\r\n  opaque inner<0..2^16-1>;\r\n  opaque tag[32];\r\n} AggregateShareReqOuter;\r\n\r\nstruct {\r\n  HpkeCiphertext encrypted_aggregate_share;\r\n} AggregateShareResp;\r\n\r\nstruct {\r\n  opaque inner<0..2^16-1>;\r\n  opaque tag[32];\r\n} AggregateShareRespOuter;\r\n```\r\n\r\nThe specification text would say in each case that the HMAC tag is over the encoding of the preceding (one or two) structure elements, and the HMAC key is selected based on the `TaskID` in either the request structure itself, or the request corresponding to the response. The byte string contained by `AggregateReqOuter.inner` encodes a message of type `AggregateReqInner`, same with `AggregateRespOuter.inner` and `AggregateRespInner`, `AggregateShareReqOuter.inner` and `AggregateShareReqInner`, `AggregateShareRespOuter.inner` and `AggregateShareRespInner`.\r\n\r\nDownsides: This way, the specification is a bit more verbose, messages will be two bytes longer for the additional length field, and various fields will have their effective length limits reduced slightly.\r\nUpsides: The text \"MUST verify the tag before interpreting the message's contents\" will be much easier to follow, and we could get more specific, stating that the contents of `opaque inner<0..2^16-1>` must not be interpreted before verifying the tag over it and (sometimes) the task ID. There is no interaction with the HTTP layer request or response (aside from its body).",
          "createdAt": "2022-03-11T22:39:03Z",
          "updatedAt": "2022-03-11T22:39:03Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "The interop target now specifies request messages in a way that allows the task ID to be extracted without parsing the entire application-specific message -- the task ID is now always the first 32 bytes, which matches strawman solution (2) above.\r\n\r\nWe might still consider moving authentication parameters to the HTTP Authentication header later, but as far as I'm concerned that's a \"nice-to-have\" & is definitely not necessary for the interop pilot. I'm closing this bug as, as far as I'm concerned, the issue is resolved.",
          "createdAt": "2022-04-12T19:06:08Z",
          "updatedAt": "2022-04-12T19:06:08Z"
        }
      ]
    },
    {
      "number": 209,
      "id": "I_kwDOFEJYQs5GVdU3",
      "title": "Alignment with RFC 7807 Problem Details",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/209",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-03"
      ],
      "body": "Some of the requirements and recommendations in section 3.1 conflict with the spirit of RFC 7807, which it references.\r\n\r\nThere is presently a requirement that:\r\n\r\n> The \"instance\" value MUST be the endpoint to which the request was targeted.\r\n\r\nThe \"instance\" key is defined as one of the standard members in RFC 7807, and it says:\r\n\r\n> \"instance\" (string) - A URI reference that identifies the specific occurrence of the problem.  It may or may not yield further information if dereferenced.\r\n\r\nAs a consequence of the current text, PPM aggregators would always provide the same URI in the \"instance\" field. I would recommend that we remove the MUST requirement here. Alternately, we could define a new extension member (say, \"endpoint\") and put the aggregator's endpoint in that member, if we still wish to have it as part of the problem details. (For what it's worth, RFC 7807 allows the \"type\" and \"instance\" members to be relative URIs, so there is already a baked-in assumption that problem detail consumers retain the request URL associated with a JSON document)\r\n\r\nSecond, there's a SHOULD recommendation that the \"detail\" field be populated, and no mention of the \"title\" field. These are both standard members, defined as follows:\r\n\r\n> \"title\" (string) - A short, human-readable summary of the problem type.  It SHOULD NOT change from occurrence to occurrence of the problem, except for purposes of localization (e.g., using proactive content negotiation; see [[RFC7231], Section 3.4](https://www.rfc-editor.org/rfc/rfc7231#section-3.4)).\r\n> \"detail\" (string) - A human-readable explanation specific to this occurrence of the problem.\r\n\r\nI think we should add a recommendation that, when one of the error types from Table 1 is used, the accompanying description from the table (or a localization thereof) should be provided in the \"title\" member. The \"detail\" member could then provide more particularized information, for example, a message mentioning the old and new configuration IDs involved in an `urn:ietf:params:ppm:error:outdatedConfig` error.\r\n\r\nThird, there is currently the following requirement for a PPM-specific extension member:\r\n\r\n> The problem document MUST also include a \"taskid\" member which contains the associated PPM task ID (this value is always known, see {{task-configuration}}).\r\n\r\nAssuming that an aggregator services multiple PPM tasks at one HTTP endpoint, there are edge cases where we may want to use `urn:ietf:params:ppm:error:unrecognizedMessage`, but not know the task ID, for example, if a request body is too short to encode a task ID. With the present text, I think aggregators would have to respond with a non-PPM error type, or just a 400 Bad Request response with an empty body.\r\n\r\nAs \"taskid\" is the only extension member we define on our problem types, I think we should clarify the MUST requirement here, and only require it when the \"type\" is one of the error types in the PPM URN namespace.\r\n\r\nFor the edge cases where \"taskid\" is not known we could relax the MUST requirement, and only require a \"taskid\" member when the PPM Task ID is known. Alternately, we could suggest that the error in such error cases be of type `about:blank` (the default type, indicating no additional semantics beyond that of the HTTP status code) and with an appropriate status line, like \"400 Bad Request\".",
      "createdAt": "2022-03-24T20:38:31Z",
      "updatedAt": "2022-12-08T23:44:58Z",
      "closedAt": "2022-12-08T23:44:58Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> As a consequence of the current text, PPM aggregators would always provide the same URI in the \"instance\" field. I would recommend that we remove the MUST requirement here.\r\n\r\n+1, I think this should be MAY.\r\n\r\n> I think we should add a recommendation that, when one of the error types from Table 1 is used, the accompanying description from the table (or a localization thereof) should be provided in the \"title\" member. The \"detail\" member could then provide more particularized information, for example, a message mentioning the old and new configuration IDs involved in an urn:ietf:params:ppm:error:outdatedConfig error.\r\n\r\nThis sound fine.\r\n\r\n> As \"taskid\" is the only extension member we define on our problem types, I think we should clarify the MUST requirement here, and only require it when the \"type\" is one of the error types in the PPM URN namespace.\r\n\r\n+1 to clarifying the edge case you point out, however I don't think this field should be required. MAY or SHOULD would be sufficient.",
          "createdAt": "2022-06-27T19:35:47Z",
          "updatedAt": "2022-06-27T19:35:47Z"
        }
      ]
    },
    {
      "number": 210,
      "id": "I_kwDOFEJYQs5GYNsz",
      "title": "DP: Bound contribution from a single client",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/210",
      "state": "OPEN",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Seeing @chris-wood present on this made it clear that there are no mechanisms that might prevent a single client from contributing to aggregates multiple times.  This is problematic for the application of DP, as the contribution of any client needs to be bounded in order to place a finite bound on the DP noise.\r\n\r\nThere is currently a fixation on having fixed-sized duration for tasks and batch limits, but neither of these help.",
      "createdAt": "2022-03-25T12:45:04Z",
      "updatedAt": "2023-10-24T22:07:26Z",
      "closedAt": null,
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Given that clients don't want to break their own privacy, how is this different from the Sybil attacks discussed in #211 ?",
          "createdAt": "2022-04-12T13:36:04Z",
          "updatedAt": "2022-04-12T13:36:04Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "The measures that prevent spoiling of results are incomplete without some protection against these attacks.  This isn't a privacy issue as much as it is a correctness one.  So maybe strike my original comments about DP protections.",
          "createdAt": "2023-04-27T23:18:19Z",
          "updatedAt": "2023-04-27T23:18:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "We do acknowledge already the possibility of a Sybil attack resulting in a \"poisoned\" aggregate result: https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-07#section-7.2\r\n\r\n@martinthomson does this text cover the attack? Do you think we should do more than acknowledge the attack? For a general approach to mitigating Sybil attacks see https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth",
          "createdAt": "2023-10-18T15:15:54Z",
          "updatedAt": "2023-10-18T15:15:54Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think that what you have there in S7.2 is OK.  Like in the paper you cite, I don't think that you can put meaningful protections in place for Sybil attacks.  That's not to say that some cases can't benefit from protections, so maybe an informative note about some potential approaches would be how to strike the balance.",
          "createdAt": "2023-10-18T21:38:43Z",
          "updatedAt": "2023-10-18T21:38:43Z"
        }
      ]
    },
    {
      "number": 211,
      "id": "I_kwDOFEJYQs5GYP5u",
      "title": "Sybil attack distinction",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/211",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "From dkg in Vienna: \r\n\r\n> maybe distinguish between the two kinds of sybil attacks as a sybil attack against the collector (stats poisoning), vs. a sybil attack against a reporting client (privacy violation)\r\n\r\n",
      "createdAt": "2022-03-25T12:54:51Z",
      "updatedAt": "2022-05-10T14:22:57Z",
      "closedAt": "2022-05-10T14:22:56Z",
      "comments": [
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It seems like there are a bunch of assumptions about Sybil attacks generally that are not well articulated yet.  This would certainly help.",
          "createdAt": "2022-03-25T13:40:22Z",
          "updatedAt": "2022-03-25T13:40:22Z"
        }
      ]
    },
    {
      "number": 215,
      "id": "I_kwDOFEJYQs5Hor_e",
      "title": "OHAI proxy instead of null shares",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/215",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "It has already been mentioned that whether a client submits reports leaks. Section 6.1.2.3.4 suggests submitting \"null\" reports to mask this fact. \r\n\r\nWouldn't it be easier to use an anonymizing proxy to hide client participation?\r\n\r\nAlso, in the case of a VDAF like heavy hitters the \"null\" shares would either be very obvious (random string) or create computational effort and distort the result (a semi-common prefix).",
      "createdAt": "2022-04-12T13:25:17Z",
      "updatedAt": "2022-05-06T06:49:23Z",
      "closedAt": "2022-05-06T06:49:23Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Our goal has been to support but not to require the use of an OHAI proxy, or any other kind of server that sits between the clients and the aggregators. There are many deployments that will want an ingestion server or a proxy for reasons ranging from client anonymization to authentication to availability, but other deployments won't want or need this.\r\n\r\nYou raise a good point about what it means to upload a null or empty report in a VDAF like Poplar1, though.",
          "createdAt": "2022-04-19T21:17:46Z",
          "updatedAt": "2022-04-19T21:17:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we should continue to support both, though the \"null\" report idea is a bit underspecified right now. For Poplar1, why send a random string? I would think that some fixed string (say, the all-zero string) would make more sense.",
          "createdAt": "2022-04-26T01:09:57Z",
          "updatedAt": "2022-04-26T01:09:57Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "If you pick all-zero that string will probably always end up in your result and will have to be part of every round of the computationally-expensive protocol.\r\n\r\nBut I agree that it's probably not a huge problem even if we have to define what the null shares are for each VDAF.",
          "createdAt": "2022-05-06T06:49:23Z",
          "updatedAt": "2022-05-06T06:49:23Z"
        }
      ]
    },
    {
      "number": 216,
      "id": "I_kwDOFEJYQs5HqSsO",
      "title": "PPM should consider specifying the acceptable HPKE parameters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/216",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "HPKE is parameterized by several different configuration choices (defined in https://www.rfc-editor.org/rfc/rfc9180.pdf, section 4): a \"key encapsulation method\", a \"key derivation function\", and an \"AEAD encryption algorithm\". The specification goes on to define several options for each of these choices in section 7.\r\n\r\nPPM currently does not constrain these values, so a specification-compliant PPM implementation may use any set of choices for these values. Different implementations will succeed at interop only if they share a common set of supported HPKE configuration values. This may lead general implementations to support as wide a variety of HPKE parameters as possible, leading to code bloat -- this is especially important in a Web setting.\r\n\r\nQuestions:\r\n* Should PPM seek to specify HPKE configuration parameters at all?\r\n* If so, what are the right parameters to specify? (The best solution might be something like \"minimally-compliant PPM implementations must support HPKE configurations X, Y, and Z; implementations may support other HPKE configurations as they wish\")",
      "createdAt": "2022-04-12T18:58:35Z",
      "updatedAt": "2022-06-27T18:14:57Z",
      "closedAt": "2022-06-27T18:14:57Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> PPM currently does not constrain these values, so a specification-compliant PPM implementation may use any set of choices for these values. Different implementations will succeed at interop only if they share a common set of supported HPKE configuration values. This may lead general implementations to support as wide a variety of HPKE parameters as possible, leading to code bloat -- this is especially important in a Web setting.\r\n\r\nAnother word for \"code bloat\" here is \"cryptographic agility\". Agility is problematic for the reasons you mention, as well as others. However it's also often necessary in many cases. For example, lots of TLS implementations prefer X25519 for the key exchange because it's the fastest option, but some deployments are stuck using P-256 for various reasons.\r\n\r\nThat said, you're right that we don't know if we'll need this sort of agility for PPM.\r\n \r\n> Questions:\r\n> \r\n>     * Should PPM seek to specify HPKE configuration parameters at all?\r\n\r\nMy $0.02: We probably will need to eventually.\r\n\r\n>     * If so, what are the right parameters to specify? (The best solution might be something like \"minimally-compliant PPM implementations must support HPKE configurations X, Y, and Z; implementations may support other HPKE configurations as they wish\")\r\n\r\nThere is a notion in IETF standards known as \"mandatory to implement (MTI)\" ciphersuites. We might consider defining MTI suites for HPKE, but I don't think this is particularly pressing. Plus, we're likely to have wildly different opinions about what suites are MTI.\r\n\r\nTaking a step back, it's completely reasonable for a given implementation of the protocol to have only limited agility or no agility at all. It does impact interop, but in practice there are likely to be a small set of suites that 99% of people implement. For PPM, I would suspect that everyone is going to have (X25519, HKDF-SHA256, AES-GCM).\r\n",
          "createdAt": "2022-04-26T01:07:07Z",
          "updatedAt": "2022-04-26T01:07:07Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I would recommend specifying a MTI suite while allowing other suites as needed for particular deployments, just [as we did for ECH](https://tlswg.org/draft-ietf-tls-esni/draft-ietf-tls-esni.html#name-compliance-requirements).",
          "createdAt": "2022-05-12T14:38:00Z",
          "updatedAt": "2022-05-12T14:38:00Z"
        }
      ]
    },
    {
      "number": 217,
      "id": "I_kwDOFEJYQs5HvhGi",
      "title": "Drop ordering requirement on successive Aggregate requests",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/217",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Currently, the successive aggregate requests in an aggregation job are required to include client report data (report shares/transitions) in a consistent order.\r\n\r\nExample language from the spec:\r\n> The sequence of Transition messages corresponds to the ReportShare sequence of the AggregateInitReq. The order of these sequences MUST be the same (i.e., the nonce of the first Transition MUST be the same as first ReportShare and so on).\r\n\r\n> Next, the helper processes the Transition messages from the leader. If any message appears out of order or has an unrecognized nonce, or if any two messages have the same nonce, then the helper MUST abort with error \"unrecognizedMessage\".\r\n\r\n\r\nThe justification I have heard for this requirement is that requiring ordering saves implementations from needing to do an `O(n lg n)` sort, where `n` is the number of client reports being processed. However, even without ordering, no sort or other `O(n lg n)` operation would be required from implementations. Specifically, the aggregator can construct a hashmap from report nonce to report share/transition (requiring `O(n)` time total); then, when matching report shares/transitions to stored state, do an `O(1)` map lookup per report, for a total of `O(n)` time spent doing lookups.\r\n\r\n\r\nAn upside to dropping the ordering requirement is a simpler specification which permits a simpler implementation with fewer error modes.\r\n\r\n\r\nA downside to dropping the ordering requirement is that, even though both ordered and non-ordered implementations can do matching of request parts to stored state in `O(n)` time, I suspect that the unordered implementation will take more wallclock time (e.g. have \"larger constants hidden by the big-O\") due to the hashmap operations. [Not measured yet.]\r\n\r\n\r\nQuestions:\r\n* Is there a justification other than avoiding an `O(n lg n)` sort to requiring consistency of ordering in aggregate requests?",
      "createdAt": "2022-04-13T18:23:59Z",
      "updatedAt": "2022-06-13T19:39:34Z",
      "closedAt": "2022-06-13T19:39:34Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(this is a consideration post-interop-pilot, most likely--unless all parties think a change is a good idea, I think the interop pilot keeping the ordering requirement is fine)",
          "createdAt": "2022-04-13T18:29:18Z",
          "updatedAt": "2022-04-13T18:29:18Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "You're right that O(n) is not hard to achieve. However, not all implementors necessarily have a geneirc HashMap structure (or even a generic quicksort algorithm) on hand. For example, imagine you were implementing this in C and couldn't use stdlib. The naive algorithm for matching a request to a response would take O(n^2) time, and that's might what you go with if you never expect n to be huge.\r\n\r\nThe current text follows this design principle: Do your best to avoid design choices that permit bad (i.e., inefficient) implementations. In particular, the goal of the current spec is to only allow one \"natural\" matching algorithm. Perhaps it would be helpful to spell this algorithm out.\r\n\r\nFWIW, this exact problem came up in the design of Encrypted Client Hello. (See https://github.com/tlswg/draft-ietf-tls-esni/issues/378.) There the motivation was to reduce the risk of bad implementations becoming DoS vectors. Here the risk is even more pronounced, since the number of reports (n) processed in a single run of the aggregation sub-protocol might be 100s or even 1000s. After all, this flow is about bulk processing of large numbers of reports.\r\n\r\n\r\n",
          "createdAt": "2022-05-04T18:37:40Z",
          "updatedAt": "2022-05-04T18:46:48Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Here's how we ended up solving the problem in ECH: https://github.com/davidben/draft-ietf-tls-esni/commit/f8e9df6c475f9268b3f0b9054c14f76ca9cbe9d7#",
          "createdAt": "2022-05-04T18:46:05Z",
          "updatedAt": "2022-05-04T18:46:05Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing this for now -- this is a minor request, and I think the arguments for keeping the ordering requirement are good. We can reopen if this question comes up again.",
          "createdAt": "2022-06-13T19:39:34Z",
          "updatedAt": "2022-06-13T19:39:34Z"
        }
      ]
    },
    {
      "number": 218,
      "id": "I_kwDOFEJYQs5H8C3a",
      "title": "Aggregation set agreement underspecified",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/218",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "It seems to me we are not entirely clear on how aggregators determine which set of reports to aggregate over.\r\n\r\nThe spec currently uses a checksum to verify that the sets are the same but isn't very specific about how the set is defined. If I understand correctly this is done using the `batch_interval` argument which only supports time as a selection parameter.\r\n\r\nI see a few issues here:\r\n1. We should define how aggregators agree on a set in case the checksum fails.\r\n2. `batch_interval` should probably be a `batch_filter` so it can also depend on version information, location, etc. (Of course it is difficult to define which information can be metadata and how much of a privacy concern it creates.) See #27 .\r\n3. `batch_filter` would have to be part of the `CollectReq` to support the drill-down feature discussed in other issues, e.g. #183 .",
      "createdAt": "2022-04-18T10:45:39Z",
      "updatedAt": "2023-10-18T15:27:48Z",
      "closedAt": "2023-10-18T15:27:48Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi Simon!\r\n\r\n> 1. We should define how aggregators agree on a set in case the checksum fails.\r\nRight now we treat this case as a fatal error. Doing more than can get complicated. For example, one could imagine a scheme whereby the aggregators try to determine the union of their two sets and exclude reports not in the union. \r\n\r\nBefore we get into attempting to recover data, I think we should first try to get a sense of how likely this error is to occur in a typical deployment.\r\n\r\n> 2. `batch_interval` should probably be a `batch_filter` so it can also depend on version information, location, etc. (Of course it is difficult to define which information can be metadata and how much of a privacy concern it creates.) See #27 .\r\n> 3. `batch_filter` would have to be part of the `CollectReq` to support the drill-down feature discussed in other issues, e.g. #183 .\r\n\r\nWe definitely want to support this feature, however I think solving #195 would be a prerequisite.\r\n\r\n\r\n",
          "createdAt": "2022-04-26T00:58:10Z",
          "updatedAt": "2022-04-26T00:58:10Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.2.1-12.3.1 already specifies how to handle failures for individual reports. https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-input-share-decryption specifies the error for decryption failure.",
          "createdAt": "2023-10-18T15:27:48Z",
          "updatedAt": "2023-10-18T15:27:48Z"
        }
      ]
    },
    {
      "number": 221,
      "id": "I_kwDOFEJYQs5IIsg9",
      "title": "Usage of HPKE `info` and `aad` fields (was \"HPKE: do not use both application_info & AAD.\")",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/221",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "branlwyd"
      ],
      "labels": [],
      "body": "Per https://www.rfc-editor.org/rfc/rfc9180.pdf section 8.1:\r\n> Applications that only use the single-shot APIs described in Section 6 should use the Setup info parameter for specifying auxiliary authenticated information. Implementations which only expose single-shot APIs should not allow applications to use both Setup info and Context aad or exporter_context auxiliary information parameters.\r\n\r\n\r\nPPM effectively uses the single-shot APIs; we should consider refactoring PPM's usage of HPKE to use only one of application_info & AAD. Following the advice of the HPKE RFC, we would want to use application_info.\r\n\r\nThis would permit slightly simpler implementations. I am not sure how much it would simplify the HPKE computations.",
      "createdAt": "2022-04-20T21:41:18Z",
      "updatedAt": "2022-05-21T10:57:14Z",
      "closedAt": "2022-05-21T10:57:14Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Should we then formally use the single shot HPKE APIs from [HPKE section 6.1](https://www.rfc-editor.org/rfc/rfc9180#name-encryption-and-decryption-2)?",
          "createdAt": "2022-04-20T21:57:04Z",
          "updatedAt": "2022-04-20T21:57:04Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "That would be my recommendation (though, somewhat confusingly given the advice quoted above, those APIs still allow specifying both application info & AAD).\r\n\r\nImplementations can & do already use the single-shot APIs, so I strongly suspect specifying the single-shot APIs will be feasible. [See [here](https://github.com/divviup/janus/blob/8ad7d31b3bded508ed31e65ee8782088fdfe719c/janus_server/src/hpke.rs#L197) & [here](https://github.com/divviup/janus/blob/8ad7d31b3bded508ed31e65ee8782088fdfe719c/janus_server/src/hpke.rs#L306).]",
          "createdAt": "2022-04-20T22:25:49Z",
          "updatedAt": "2022-04-20T22:26:20Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to using single-shot APIs, though I'm not sure I agree that using either info or add (but not both) is simpler. What's the reasoning?",
          "createdAt": "2022-04-21T01:08:58Z",
          "updatedAt": "2022-04-21T01:08:58Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "The reasoning is:\r\na) it follows the advice of the HPKE spec (quoted above)\r\nb) AFAICT, in a single-shot setting, application info & AAD serve exactly the same purpose, so using both is arbitrary/confusing/a potential source of mistakes. (in a multi-shot setting, application info is bound to all encryptions/decryptions done by the same context; AAD is bound to a single encryption/decryption)",
          "createdAt": "2022-04-22T16:12:55Z",
          "updatedAt": "2022-04-22T16:12:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "TBH I'm not sure I agree with the quoted guidance here. There is a concept in protocol design known as \"domain separation\", the goal of which is to provide some kind of binding of long-term secret key operations to the context in which they were used. Imagine, for example, that someone used an HPKE secret key for PPM and some other protocol. Suppose further that both are using the empty string for `info`. Then the security of our PPM deployment depends on how the other protocol uses the derived AEAD key, since in both protocols  may end up deriving the same AEAD key. One way to avoid creating this attack surface is to pick an `info` string that no other application is likely to choose so that derived keys are guaranteed to not collide (except with some negligible probability).\r\n\r\nThat said, I'd go for changing how we use `info` and `aad`. Something like this might be simpler:\r\n* Let `info` be a fixed string that identifies the protocol. It would be good if this string were relatively long, say the SHA-256 hash of \"ppm-00\".\r\n* Let `aad` encode the task ID, sender/receiver roles, etc.\r\n\r\n",
          "createdAt": "2022-04-22T16:50:29Z",
          "updatedAt": "2022-04-22T16:54:54Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I definitely agree that application info should include information allowing domain separation, for the reasons you mention. (Indeed, the current PPM spec includes the strings \"ppm input share\" or \"ppm aggregate share\" in its application infos, presumably for this purpose.)\r\n\r\nBut I think that's orthogonal to the question of whether we should also use AAD, since the application info could also include information other than the \"domain separation parameter\". Picking one example to make things concrete, a client upload request sets its application info to `Report.task_id || \"ppm input share\" || 0x01 || server_role` and its AAD to `Report.nonce || Report.extensions`. But it could just as easily set the application info to `Report.task_id || \"ppm input share\" || 0x01 || server_role || Report.nonce || Report.extensions` (i.e. the concatenation of the two previous strings) and not use AAD. (Or alternatively, it could place that string in the AAD and not use application info.)\r\n\r\nMy argument is that it's simpler to use only one of the parameters, since in the single-shot setting that PPM is using HPKE, the two parameters serve exactly the same purpose (i.e. including additionally-authenticated data).",
          "createdAt": "2022-04-22T17:59:46Z",
          "updatedAt": "2022-04-22T18:00:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As I see it, our goal is to make sure that decryption only succeeds if the sender and receiver agree on the \"context\", i.e., the task ID, sender/receiver role, nonce, extensions, and so on. This way a MITM cannot force the aggregator to interpret an input share incorrectly. Does this sound reasonable to you?\r\n\r\nFrom the point of view of this threat model, I don't think it's immediately clear that sticking the context in `info` or `aad` is equivalent. For example, if we stuff the context in `info`, then it influences the derivation of the AEAD key; but depending on the AEAD, it might be feasible for an attacker to find a find two keys that decrypt the same ciphertext. (See https://eprint.iacr.org/2020/1153.pdf.) More analysis will be needed to say for sure whether this weakness in the AEAD amounts to an attack against our protocol. In the meantime, sticking the \"context\" in `aad` seems like a more conservative choice to me.",
          "createdAt": "2022-04-22T18:22:33Z",
          "updatedAt": "2022-04-22T18:22:33Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Given your concern, your approach SGTM. It sounds like this may be a concern worth raising with the folks working on the HPKE spec, given the advice to \"use the Setup info parameter for specifying auxiliary authenticated information\", but I'm not going to chase this down currently.",
          "createdAt": "2022-04-22T20:02:02Z",
          "updatedAt": "2022-04-22T20:02:02Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "So I think there's two tasks for this issue:\r\n* Move task ID from `info` to `aad`\r\n* Use single-shot APIs for HPKE\r\n\r\nAnything else?",
          "createdAt": "2022-04-26T00:50:40Z",
          "updatedAt": "2022-04-26T00:50:40Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Yep, I think that's it.",
          "createdAt": "2022-04-26T16:14:59Z",
          "updatedAt": "2022-04-26T16:14:59Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(reopening for now as the fix PR has been reverted temporarily)",
          "createdAt": "2022-05-13T16:54:30Z",
          "updatedAt": "2022-05-13T16:54:30Z"
        }
      ]
    },
    {
      "number": 226,
      "id": "I_kwDOFEJYQs5IgPYh",
      "title": "Aggregation idempotency",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/226",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "From @tgeoghegan:\r\n\r\nSuppose we have a task with max_batch_lifetime = 1. Consider:\r\n\r\n1. Leader makes an AggregateShareReq for some batch interval.\r\n2. Helper services the request, and marks that batch interval as having been collected once.\r\n3. Helper attempts to transmit an AggregateShareResp to leader, but the message is truncated.\r\n\r\nNow, if the leader retries its AggregateShareReq, the helper will refuse the request because the batch interval's lifetime has been consumed. So, to allow the leader to retry this request, the helper has to be willing to resend previously computed aggregate shares.\r\n\r\nIssue from #223.",
      "createdAt": "2022-04-26T19:55:29Z",
      "updatedAt": "2023-06-16T21:51:34Z",
      "closedAt": "2023-06-16T21:51:34Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "This issue also affects the aggregation process: if an aggregation step fails in a similar way, the helper might believe it has \"stepped\" a VDAF preparation to step `N+1` while the leader still believes it is at step `N`.\r\n\r\nOne strawman solution would be having the helper store the \"current\" aggregation step & the \"previous\" aggregation step, then updating the PPM specification to have the leader indicate which \"step number\" it is on when sending aggregate requests (since VDAF does not & I suppose should not have to carry this information). I think this would make the aggregation steps effectively idempotent. The downsides are the increased storage cost to the helper & the increased communication cost -- can we do better?",
          "createdAt": "2022-05-02T18:54:40Z",
          "updatedAt": "2022-05-02T18:56:00Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe this got addressed in the DAP-04 HTTP API refactor, except that the `{{collect-aggregate}}` section doesn't explicitly mention that the helper should be willing to service multiple requests to `POST {helper}/tasks/{task-id}/aggregate_shares` for the same `batch_selector`. We'll use this issue to improve the language in DAP-05.",
          "createdAt": "2023-05-26T21:21:46Z",
          "updatedAt": "2023-05-26T21:21:46Z"
        }
      ]
    },
    {
      "number": 228,
      "id": "I_kwDOFEJYQs5IlwRs",
      "title": "Message serialization doesn't seeem fully specified",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/228",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The only language I can find in the document that prescribes serialization is in Section 1.2 (please point out if I missed anything):\r\n\r\n> This document uses the presentation language of [[RFC8446](https://datatracker.ietf.org/doc/html/rfc8446)].\r\n\r\nWe intend this statement to be prescriptive, i.e., encoding/decoding of messages is as defined in RFC8446. Another valid interpretation might be \"we define the fields of each message following the conventions of RFC8446, but leaving encoding/decoding unspecified\".\r\n\r\nI think it would help to refine this statement a bit. WDYTA:\r\n\r\n> This document uses the presentation language of RFC8446 to define messages in the PPM protocol. Encoding and decoding of these messages as byte strings also follows RFC8446.",
      "createdAt": "2022-04-27T20:36:22Z",
      "updatedAt": "2022-04-28T17:45:54Z",
      "closedAt": "2022-04-28T17:45:54Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "SGTM FWIW (this was our interpretation, and I think it makes sense to clarify)",
          "createdAt": "2022-04-27T23:41:41Z",
          "updatedAt": "2022-04-27T23:41:41Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, this seems like a fine clarification. I don't think it was under specified before, but more information for those unfamiliar with TLS-style encoding doesn't hurt. ",
          "createdAt": "2022-04-28T16:37:38Z",
          "updatedAt": "2022-04-28T16:37:38Z"
        }
      ]
    },
    {
      "number": 230,
      "id": "I_kwDOFEJYQs5IqG1-",
      "title": "Separate report shares from aggregation parameters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/230",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Currently, the aggregation process begins by the leader sending report shares and aggregation parameters _together_ to each helper. @ekr observes that this is not strictly necessary, as the verification process should discard any reports that are invalid in the end, and the leader can cause each helper to store bogus report shares indefinitely anyway. Splitting up sharing report shares and aggregation parameters might help some implementations, e.g., by allowing the leader to stream report shares to each aggregator before initiating aggregation. For Prio3, this isn't much helpful since aggregation can begin without the collector's input. For Poplar, this might be very useful. ",
      "createdAt": "2022-04-28T16:41:37Z",
      "updatedAt": "2023-05-26T21:25:35Z",
      "closedAt": "2023-05-26T21:25:34Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Maybe I'm confused. How is `agg_param` computed? I had thought it was constant for a given instance, because this document does not seem to define it as an output.\r\n",
          "createdAt": "2022-04-28T16:58:59Z",
          "updatedAt": "2022-04-28T16:58:59Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "`agg_param` is chosen by the collector, and sent to the leader in a `CollectReq`. For the case of Prio3, this parameter is the unit type, but for Poplar1, it will be a candidate prefix.\r\n\r\nSplitting up how report shares and aggregation parameters are sent will have an additional benefit for the Poplar1 case, in that it would allow the leader to send report shares to the helper only once, for reuse in multiple different aggregations with different candidate prefixes. This would be a significant bandwidth win.",
          "createdAt": "2022-04-28T17:05:01Z",
          "updatedAt": "2022-04-28T17:05:01Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not against splitting them up, but I was just making a much more trivial point, which was that the leader can send the shares to the helper prior to locally initializing. I thought there was some text here that said that, but now I can't find it so maybe I misread or maybe it changed.\r\n",
          "createdAt": "2022-04-28T17:14:28Z",
          "updatedAt": "2022-04-28T17:14:28Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm not against splitting them up, but I was just making a much more trivial point, which was that the leader can send the shares to the helper prior to locally initializing. I thought there was some text here that said that, but now I can't find it so maybe I misread or maybe it changed.\r\n\r\nSure, but the helper will only be able to decrypt them. It won't be able to begin processing them until it knows `agg_param` (at least in general). I suppose it might be useful to permit the helper to decrypt immediately to reduce the chance of the HPKE config getting rotated before it has a chance to decrypt.\r\n\r\nIn any case, I think this change is a good idea for the reason mentioned by @divergentdave.\r\n\r\n",
          "createdAt": "2022-05-04T18:23:53Z",
          "updatedAt": "2022-05-04T18:23:53Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Technically #405 is a dupe of this issue, but since #405 has more concrete design proposals in it, I'm going to reverse-dupe and close this one.",
          "createdAt": "2023-05-26T21:25:34Z",
          "updatedAt": "2023-05-26T21:25:34Z"
        }
      ]
    },
    {
      "number": 237,
      "id": "I_kwDOFEJYQs5JN3WZ",
      "title": "Decide which Task parameters might be mutable.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/237",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "Tasks are defined with a number of parameters; eventually, it might be helpful to determine which might be mutated (and any restrictions on how they might be mutated).\r\n\r\nOne initial thought is that any of the parameters that end up distributed to clients are likely effectively immutable, if only due to the challenge of updating all of the clients.\r\n\r\n[This fell out of a discussion on https://github.com/divviup/janus/pull/142.]",
      "createdAt": "2022-05-06T20:48:18Z",
      "updatedAt": "2023-06-16T21:51:20Z",
      "closedAt": "2023-06-16T21:51:20Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think for the time being we should plan on the all of the parameters of a config being immutable: If you need to change them role a new task.",
          "createdAt": "2022-06-27T19:32:34Z",
          "updatedAt": "2022-06-27T19:32:34Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "That works for now IMO. Some of the cryptographic parameters (e.g. `collector_config`, `vdaf_verify_key`) are considered to be part of the task configuration by DAP. Clients don't need to know about these parameters; to support rotating these parameters without needing to update all clients, we may eventually want to allow these to be mutated.",
          "createdAt": "2022-06-29T18:02:59Z",
          "updatedAt": "2022-06-29T18:02:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd for now we might just note that the collector config and the verification key might change over the course of a task's lifetime, but this is deployment specific. Everything else should be immutable.\r\n",
          "createdAt": "2022-09-13T01:24:07Z",
          "updatedAt": "2022-09-13T01:24:07Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe that VDAF-05's requirement that the VDAF verify key be set in all aggregators before any reports are prepared (https://mailarchive.ietf.org/arch/msg/ppm/h4ArBZTQ1H_dm0umwjT-gcXaBHg/) means that you can't rotate the VDAF verify key without making a new task. That leaves the collector HPKE config, but there again I think making a new task suffices, given that collector HPKE configs can likely be safely used for quite a long time.\r\n\r\nFor DAP-05, I think we should check if there's any language in the draft that suggests task parameters are immutable, and add a sentence as needed explicitly stating that they're immutable.",
          "createdAt": "2023-05-26T21:35:33Z",
          "updatedAt": "2023-05-26T21:35:33Z"
        }
      ]
    },
    {
      "number": 239,
      "id": "I_kwDOFEJYQs5JZo6U",
      "title": "Provide error handling guidance",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/239",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "The draft defines a list of problem document types to represent conditions such as an aggregate share request being rejected because it would violate privacy requirements. Some of these are fatal but in some cases, the sender of a request can try again. For instance, if a `CollectReq` is rejected because its `batch_interval` does not contain enough reports, then the collector could either wait a while for more reports to arrive or try again immediately with a bigger `batch_interval`. We should add guidance on which errors are non-fatal and what implementations can try to do when they encounter them.",
      "createdAt": "2022-05-10T17:14:05Z",
      "updatedAt": "2023-10-24T22:04:07Z",
      "closedAt": "2023-10-24T22:04:07Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Some discussion [here](https://github.com/ietf-wg-ppm/ppm-specification/pull/233#discussion_r861403548).",
          "createdAt": "2022-05-10T17:14:17Z",
          "updatedAt": "2022-05-10T17:14:17Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "We should also remove 'reportRejected': \"Report could not be processed for an unspecified reason.\" and replace it with something clearer. In Janus it is currently used for expired task, expired reports, repeated report ID and already collected batches.\r\n* Expired task \u2192 Client configuration error, should be updated.\r\n* Expired report \u2192 Client configuration error, maybe invalid clock. Probably not fixable.\r\n* Repeated report ID \u2192 Maybe a network issue. Report should be considered as sent. Probably fine.\r\n* Already collected batch \u2192 Mismatch between Client and Collector behavior is causing data loss.",
          "createdAt": "2023-08-14T10:19:49Z",
          "updatedAt": "2023-08-14T10:19:49Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "regarding `reportRejected`: I definitely don't object to adding more-specific error codes where it makes sense (though I'd lean towards adding codes only when the receiver of the error might make a programmatic logic decision based on the code). But I think we should also keep something as generic as `reportRejected`, as an aggregator is allowed to reject any report for any implementation-specific reason it chooses -- it'd be good to have a catchall error for this kind of rejection.",
          "createdAt": "2023-08-17T20:36:36Z",
          "updatedAt": "2023-08-17T20:36:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I see two considerations mentioned on this issue:\r\n1. Aborts leading to privacy violations\r\n2. Distinguishing between \"fatal\" and \"non-fatal\" aborts\r\n3. Refine the \"reportRejected\" abort\r\n\r\nAnything else?\r\n\r\n@tgeoghegan can you explain the privacy concern?\r\n\r\n@tgeoghegan what is your working definition of \"fatal\" here?",
          "createdAt": "2023-10-17T16:54:35Z",
          "updatedAt": "2023-10-17T16:54:35Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I mentioned privacy as a reason that an aggregator might refuse a collection request. I don't have a privacy concern with what errors are returned from DAP requests.\r\n\r\nA fatal error here is one that should cause the requestor to simply give up. For example, if a report upload fails because its timestamp falls into a range of time that's already been collected, then the client shouldn't retry later, it should just give up and throw away that report.\r\n\r\nA non-fatal error is one where the requestor could automatically retry the operation, perhaps differently, but without some intervention like fixing a bug or changing a task's configuration. For example, if a collection request fails because there aren't enough reports in the time interval, the collector could try again later, hoping that enough relevant reports will have arrived.\r\n\r\nI do not think we should be breaking `reportRejected` into multiple, more specific errors. I have been deliberately moving in the opposite direction of using `reportRejected` in more places unless, as Bran argued back in August, the error's recipient should do something specific. As ever, implementations are free to include as much explanation and context as they want in an error's body to help debugging, but the DAP protocol does not need to spell out error types that don't cause their recipients to behave in some specific manner.\r\n\r\nAFAIK we currently have good descriptions of what a requestor should do if it sees some error in the protocol text. I don't think there's anything more to do here.",
          "createdAt": "2023-10-19T14:40:28Z",
          "updatedAt": "2023-10-19T14:40:28Z"
        }
      ]
    },
    {
      "number": 240,
      "id": "I_kwDOFEJYQs5JZqZ7",
      "title": "Clarify or remove MUST requirement for leader to buffer reports",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/240",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "@chris-wood observes that we have an awkward MUST requirement on leaders buffering received reports: https://github.com/ietf-wg-ppm/ppm-specification/pull/233#discussion_r869306691",
      "createdAt": "2022-05-10T17:17:30Z",
      "updatedAt": "2022-05-11T19:27:25Z",
      "closedAt": "2022-05-11T19:27:25Z",
      "comments": []
    },
    {
      "number": 241,
      "id": "I_kwDOFEJYQs5JZsY-",
      "title": "Allow leader to explicitly cancel aggregate jobs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/241",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "It's possible for a leader to abandon an aggregate job before it's finished, and the helper never gets to find out. For instance, suppose leader sends `AggregateInitReq`, then helper responds with a malformed `AggregateResp`, maybe just because there is a bug in the client's message encoding routine. Upon receipt of the malformed message, the leader will cease preparation of the reports in that aggregate job. From the helper's point of view, there will forever be an unfinished aggregate job, awaiting the next `AggregateReq` from leader.\r\n\r\nThis introduces an ambiguity when helper handles `AggregateShareReq`. It's possible that the helper might have some abandoned aggregate jobs whose reports would have contributed to the `AggregateShareReq`. We might want the helper to abort here, because this could indicate that the leader has jumped the gun on starting the collect protocol, but we can't, because there's no way for the helper to know whether any of those jobs were intentionally abandoned by the leader.\r\n\r\nThis could be fixed by having the leader explicitly send an aggregate job cancellation message to the helper if it sees an `AggregateResp` it doesn't like.\r\n",
      "createdAt": "2022-05-10T17:25:33Z",
      "updatedAt": "2023-10-26T15:43:31Z",
      "closedAt": "2023-10-26T15:43:31Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is a special case or at least related to #141.\r\n\r\nAt the moment, the protocol includes report counts and a checksum in aggregate shares, which aims to let us detect how bad this problem is. We can decide whether it's worth introducing this based on what we learn from running the protocol.",
          "createdAt": "2022-05-10T17:26:20Z",
          "updatedAt": "2022-05-10T17:26:20Z"
        }
      ]
    },
    {
      "number": 248,
      "id": "I_kwDOFEJYQs5Jj_EX",
      "title": "No negotiation for HPKE Configs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/248",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The server only supplies one config, so what happens if the server would support X22519 and P-256? We can fix this by having the server supply multiple configs.",
      "createdAt": "2022-05-12T15:38:26Z",
      "updatedAt": "2022-12-08T21:38:13Z",
      "closedAt": "2022-12-08T21:38:13Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This is probably a good idea. We could just change the payload of the response to `GET /hpke_config` to a sequence of `HpkeConfig` messages rather than one.",
          "createdAt": "2022-06-27T19:31:33Z",
          "updatedAt": "2022-06-27T19:31:33Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As mentioned in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/303#discussion_r954175414, it might be useful to allow the client to advertise which algorithms it supports. This would increase the likelihood of successfully negotiating an a HPKE suite that both parties support.",
          "createdAt": "2022-08-24T18:50:27Z",
          "updatedAt": "2022-08-24T18:50:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@ekr should we try to tackle this in DAP-03?",
          "createdAt": "2022-09-13T01:05:21Z",
          "updatedAt": "2022-09-13T01:05:21Z"
        }
      ]
    },
    {
      "number": 255,
      "id": "I_kwDOFEJYQs5J3GMe",
      "title": "Consider specifying aggregator behavior on message-deserialization failures.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/255",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The DAP specification defines a number of error modes, which ultimately end up mapped to error codes defined in section 3.1.\r\n\r\nCurrently, the specified behavior does not describe how to treat message-deserialization failures, in any of client-aggregator, aggregator-aggregator, or collector-aggregator communications. We should consider specifying this.\r\n\r\nInitial thoughts:\r\n* Moving to `FAILED` (with a new error code?) or `INVALID` makes sense to me, for deserialization errors during the aggregation process.\r\n* It'd be very nice if one \"bad\" client report didn't fail the entire aggregation job, e.g. we should probably specify behavior that limits the \"blast radius\" of a badly-serialized message.\r\n* We could leave this unspecified; I suppose the risk is that different implementations might treat this error mode with different semantics (e.g. some will fail a single report, some will fail the entirety of the aggregation job).",
      "createdAt": "2022-05-17T21:34:17Z",
      "updatedAt": "2022-12-09T22:51:57Z",
      "closedAt": "2022-12-09T22:51:57Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(This fell out of discussion on https://github.com/divviup/janus/pull/167; FWIW, Janus will currently transition a report aggregation to `INVALID` if it fails to decode a message, though of course that's just one implementation's choice.)",
          "createdAt": "2022-05-17T21:37:47Z",
          "updatedAt": "2022-05-17T21:37:47Z"
        }
      ]
    },
    {
      "number": 259,
      "id": "I_kwDOFEJYQs5J78Wa",
      "title": "Checking `batch-collected` at time of aggregate initalization is insufficient",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/259",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "collecting a batch more than once"
      ],
      "body": "I think that there is a hole in our anti-replay requirements, for tasks with `max_batch_lifetime > 1`. The issue is that currently, we only require that aggregators check if reports fall in intervals that have already been collected during the leader and helper aggregate initialization, see section `input-share-batch-validation`. We need a similar check either when handling aggregation continuations or the collect flow.\r\n\r\nAs an example of what we should be disallowing, consider a task with `min_batch_size = 10, max_batch_lifetime = 2`. Let's say that all aggregations are done with the same aggregation parameter, whatever it is.\r\n\r\n- The leader sends out an AggregateInitializeReq for its first job, with ten reports.\r\n  - The input share validation checks succeed, as there have been no collect reqeusts yet.\r\n- The leader sends out an AggregateInitializeReq for its second job, with one report. This succeeds as well.\r\n- The leader sends an AggregateContinueReq for its first job, with `finished` transitions.\r\n- The leader sends an AggregateShareReq for an interval that covers all reports in both jobs.\r\n- The leader sends an AggregateContinueReq for its second job, with `finished` transitions.\r\n  - Note that we do not require the helper to do any checks against collect requests when handling aggregation continuations. (see `agg-continue-flow`)\r\n- The leader sends a second AggregateShareReq, identical to the previous request.\r\n  - Note that the checks in `batch-parameter-validation` pass.\r\n    - The batch includes eleven reports, so `min_batch_size` is satisfied.\r\n    - We assumed `max_batch_lifetime = 2`, and each report has been collected zero or one times thus far.\r\n  - Aside from the `max_batch_lifetime` condition, the `collect-aggregate` section does not impose any other checks or conditions requiring the helper to consider previous AggregateShareReq requests when deciding if a request is valid, nor does it require that identical requests always get the same response.\r\n\r\nAfter receiving the two aggregate shares produced above, the collector could compute the aggregate function over both the first ten reports and all eleven reports. For many functions, this would allow recovery of the eleventh input, breaking our privacy goal.\r\n\r\nThere is a paragraph that says:\r\n\r\n> After issuing an aggregate-share request for a given batch interval, it is an error for the leader to issue any more aggregate or aggregate-init requests for additional reports in the batch interval. These reports will be rejected by helpers as described {{agg-init}}.\r\n\r\nHowever, the referenced section only covers the \"aggregate-init\" part, and not aggregate continuation.",
      "createdAt": "2022-05-18T19:58:46Z",
      "updatedAt": "2024-05-21T20:58:53Z",
      "closedAt": "2024-05-21T20:58:53Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 I volunteered to implement multi-collection and resolve all related open issues.",
          "createdAt": "2023-11-08T15:07:34Z",
          "updatedAt": "2023-11-08T15:07:34Z"
        }
      ]
    },
    {
      "number": 260,
      "id": "I_kwDOFEJYQs5KCmAM",
      "title": "Clarify the domain in which aggregation job IDs must be unique",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/260",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-01"
      ],
      "body": "The current DAP specification says (section 4.3.1.1):\r\n```\r\nGenerate a fresh AggregationJobID. This ID MUST be unique within the context of the corresponding DAP task.\r\n```\r\n...which implies that the domain of uniqueness for aggregation job IDs is at least as large as all aggregation job IDs for the related task.\r\n\r\nBut then, the aggregate continue request definition is (section 4.3.2.1):\r\n```\r\nstruct {\r\n  AggregationJobID job_id;\r\n  PrepareStep prepare_shares<1..2^16-1>;\r\n} AggregateContinueReq;\r\n```\r\n... which implies that aggregation job IDs must be unique over all aggregation jobs (on any task), since the `task_id` is omitted.\r\n\r\nDepending on intent, I suggest one of the following edits:\r\n* Update section 4.3.1.1 to state that aggregation job IDs must be unique over all aggregation jobs (for any task the aggregator knows about).\r\n* Update section 4.3.2.1 to include a `TaskId` in the `AggregateContinueReq` message.",
      "createdAt": "2022-05-19T18:56:41Z",
      "updatedAt": "2022-06-23T14:31:16Z",
      "closedAt": "2022-06-23T14:31:16Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "My two cents: I really like keeping separate tasks isolated as much as possible, so I like the idea of requiring aggregation job IDs to be unique only over the domain of aggregation jobs for the related task. OTOH, this would require sending extra bytes with every `AggregateContinueReq`, which folks certainly might find unpalatable.\r\n\r\nedit: If we make aggregation job IDs globally unique, this would also make aggregation jobs the only thing other than tasks that are not identified per-task -- client reports are effectively identified by (task_id, report_nonce) currently. IMO this is another point in favor of explicitly maintaining that aggregation job IDs are unique only up to the task. This might be especially important for \"large\" implementations that may wish to shard over tasks -- if aggregation job IDs are globally unique, all shards must know about all aggregation job IDs, even those pertaining to tasks they don't know about. (edit: or, to put it another way, a sharded-by-task implementation would have to have a component that knows how to map aggregation job IDs to task IDs, which would require knowledge of all aggregation job IDs & their related tasks; IMO it'd be nicer if we shaped things so that implementations can shard over _something_, and sharding by task makes sense to me)",
          "createdAt": "2022-05-19T19:01:19Z",
          "updatedAt": "2022-05-19T22:01:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree this needs to be fixed and I prefer your second suggestion.",
          "createdAt": "2022-05-24T18:28:41Z",
          "updatedAt": "2022-05-24T18:28:41Z"
        }
      ]
    },
    {
      "number": 261,
      "id": "I_kwDOFEJYQs5KDCyc",
      "title": "Aggregate{Initialize,Continue}Resp should not include the aggregation job ID",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/261",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-01"
      ],
      "body": "`Aggregate{Initialize,Continue}Resp` both include the relevant aggregation job ID; but in both cases, the `Aggregate{Initialize,Continue}Req` includes the aggregation job ID, the response will always reflect the aggregation job ID that was included in the request, and a request is always paired with a response by means of being part of the same HTTP request/response. This looks to be unnecessary communication overhead--if that's accurate, I think we can just drop these fields.",
      "createdAt": "2022-05-19T20:49:20Z",
      "updatedAt": "2022-06-23T16:26:05Z",
      "closedAt": "2022-06-23T16:26:05Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I wonder if a better design is to pull the aggregation job ID out of the DAP structure and make it a query parameter for the URL, e.g., `[aggregator]/aggregate?aggregate-job-id=...`, for two reasons: (1) the URL isn't echoed in the response, achieving the desired property of this change, and (2) this avoids any caching headaches, since different each aggregate init request would have a different URL. Thoughts?",
          "createdAt": "2022-05-19T21:18:34Z",
          "updatedAt": "2022-05-19T21:18:34Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think caching is a non-issue, because requests to `/aggregate` are POSTs, and POSTs are not required to be idempotent (hence not cacheable). That said I kind of like the idea of hoisting the aggregation job ID into the request URL. However, I think it'd be appropriate to put the aggregation job ID in the path (i.e., `[aggregator]/aggregation_jobs/<base64urlencoded job ID>`), since the job ID is part of the resource being acted on.\r\n\r\nAt that point, I think we might want to revisit all the DAP endpoints. Currently, we have `/upload`, `/aggregate` and `/collect`, which are all verbs. HTTP urges us to think about [resources](https://datatracker.ietf.org/doc/html/rfc7231#section-2), which ought to be nouns, and we should let the HTTP method be the verb.\r\n\r\nSo instead of POST to `[leader]/upload`, maybe we want POST to `[leader]/reports/<report_nonce>`, and instead of `[helper]/aggregate`, we want `[helper]/aggregation_jobs/<aggregation job ID>` and so on. However all of that is far beyond the scope of this issue.",
          "createdAt": "2022-05-19T21:50:26Z",
          "updatedAt": "2022-05-19T21:50:26Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not so sure about the proposed benefit to caching -- the relevant requests are `POST`s, so they shouldn't be cached. And if we ever encounter an HTTP client that does cache `POST` requests, I don't including the IDs as query params will help: the `/aggregate` endpoint is used for all aggregation messages (initialization and continue), so we'd still run into potential caching headaches on aggregate continue requests since they'd be using identical URLs to the aggregate initialization requests.\r\n\r\nThat said, perhaps this change is desirable from the perspective of fitting HTTP semantics more closely. A few thoughts:\r\n1. Almost all messages include all relevant data in the HTTP body, rather than using the query parameter. We'd probably want to decide on a rule for what goes into query parameters vs the body, and apply it to all messages for consistency.\r\n2. The one place in the DAP spec that doesn't place all request information in the body is the `/hpke_config` request, which places the task ID as a query parameter. This is done because we want to use HTTP caching for these requests, which implies using `GET`, and `GET` can't reliably transmit a body. (IMO, if/when the HTTP `QUERY` verb is standardized+practically usable, it would be preferable to move the `/hpke_config` parameters to the request body, for consistency.)\r\n3. For aggregate initialization/continue messages specifically, IMO we'd want to move the task ID to the query parameter, too, since the task ID is effectively another part of the aggregation job identifier. (...depending on how the discussion in #260 goes.)\r\n4. Placing this ID in the body allows it to be transmitted raw/unencoded. Placing the ID in the query parameters would require some URL-safe encoding, such as base64url, which would cost a small but non-negligible number of bytes.\r\n\r\nAll that said, IMO I kinda like keeping all of the request parameters together in one place, and IMO the body is a slightly nicer place to put the request parameters than the URL query parameter (since we can transmit with whatever encoding we want, saving a few bytes).",
          "createdAt": "2022-05-19T21:52:26Z",
          "updatedAt": "2022-05-19T21:52:26Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, sorry, I forgot these were POSTs. ",
          "createdAt": "2022-05-19T21:53:01Z",
          "updatedAt": "2022-05-19T21:53:01Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I like Tim's idea of making the IDs part of the URL path if we move the IDs out of the request body, but also agree that a larger refactoring of the URL layout is beyond the scope of this issue.",
          "createdAt": "2022-05-19T21:55:36Z",
          "updatedAt": "2022-05-19T21:55:36Z"
        }
      ]
    },
    {
      "number": 264,
      "id": "I_kwDOFEJYQs5KP_-i",
      "title": "Consider unifying the top-level media type used for DAP messages.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/264",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "branlwyd"
      ],
      "labels": [
        "draft-01"
      ],
      "body": "Currently, DAP uses the following media types (defined in section 7.1):\r\n* `application/dap-hpke-config`\r\n* `message/dap-report`\r\n* `message/dap-aggregate-initialize-req`\r\n* `message/dap-aggregate-initialize-resp`\r\n* `message/dap-aggregate-continue-req`\r\n* `message/dap-aggregate-continue-resp`\r\n* `message/dap-aggregate-share-req`\r\n* `message/dap-aggregate-share-resp`\r\n* `message/dap-collect-req`\r\n* `message/dap-collect-resp`\r\n\r\nNote that the first uses the `application` top-level type, while the remainder use the `message` top-level type. For consistency, I suggest we use the same top-level type for all messages.\r\n\r\nWhich type is more appropriate? Per [RFC 2046](https://www.rfc-editor.org/rfc/rfc2046.html):\r\n* `application` is for \"either uninterpreted binary data or information to be processed by an application\"\r\n* `message` is for \"an encapsulated message\"; all of the motivating examples (see section 5.2) are around encaspulating one message format inside another, or fragmenting a message into multiple chunks. [RFC 6838](https://www.rfc-editor.org/rfc/rfc6838.html) provides further guidance that \"multipart and message are composite types; that is, they provide a means of encapsulating zero or more objects, each one a separate media type.\"\r\n\r\nGiven that DAP messages are not a method of encapsulating a message of a different type, I suggest that `application` is the more appropriate top-level type to use. This is also supported by the rather large number of `application` media types vs the relatively tiny number of `message` media types in [IANA's media types list](https://www.iana.org/assignments/media-types/media-types.xhtml).\r\n\r\n(this issue fell out of discussion on https://github.com/divviup/janus/pull/183)",
      "createdAt": "2022-05-23T21:20:22Z",
      "updatedAt": "2022-06-23T14:21:04Z",
      "closedAt": "2022-06-23T14:21:04Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Agreed, `application` seems better.",
          "createdAt": "2022-05-24T18:30:36Z",
          "updatedAt": "2022-05-24T18:30:36Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "The `message/` types were chosen in PR #128 on a tentative basis, and marked with \"OPEN ISSUE: Solicit review of these allocations from domain experts.\"\r\n\r\nBased on the encapsulation reasoning, I agree that `application` be most appropriate for our media types.",
          "createdAt": "2022-05-24T19:03:49Z",
          "updatedAt": "2022-05-24T19:03:49Z"
        }
      ]
    },
    {
      "number": 266,
      "id": "I_kwDOFEJYQs5KfhBk",
      "title": "Align spec with vdaf-01",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/266",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-01"
      ],
      "body": "The next VDAF draft is out. It includes some minor changes that we'll need to adopt here.\r\nhttps://datatracker.ietf.org/doc/draft-irtf-cfrg-vdaf/01/\r\n",
      "createdAt": "2022-05-26T16:19:19Z",
      "updatedAt": "2022-07-11T18:37:37Z",
      "closedAt": "2022-07-11T18:37:37Z",
      "comments": []
    },
    {
      "number": 267,
      "id": "I_kwDOFEJYQs5Kpdhx",
      "title": "Specify handling of agg_param",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/267",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "There is an agg_param in AggregateInitializeReq and in CollectReq and in AggregateShareReq.\r\nIIUC CollectReq.agg_param may need to be forwarded as AggregateInitializeReq.agg_param or - depending on the VDAF - later as AggregateShareReq.agg_param.\r\n\r\nISTM the logic needs to be specified to implement DAP correctly.",
      "createdAt": "2022-05-30T08:29:30Z",
      "updatedAt": "2022-07-30T02:34:08Z",
      "closedAt": "2022-07-30T02:34:07Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, both the agg_param and batch_interval need to match. Is there anything that needs to change in order to make this so?",
          "createdAt": "2022-05-31T15:16:49Z",
          "updatedAt": "2022-05-31T15:16:49Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Well, if the agg_param in all three (CollectReq, AggregateInitializeReq, AggregateShareReq) must be the same the leader cannot start aggregating before the collector sends the request. Which is an optimization that was requested, right?\r\n\r\nI think there are two valid scenarios:\r\n1. AggregateInitializeReq.agg_param can be determined by the leader, e.g. because it is empty. In this case the leader can start aggregating ahead of time, without a request. Later AggregateShareReq.agg_param will be set to CollectReq.agg_param.\r\n2. AggregateInitializeReq.agg_param does need information from the collector. So it is set to CollectReq.agg_param. Presumably AggregateShareReq.agg_param can then be empty.\r\n\r\n",
          "createdAt": "2022-05-31T15:29:50Z",
          "updatedAt": "2022-05-31T15:29:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Right, these are the two valid scenarios. For (2.) the current use case is Prio3, where the agg_param is always \"\" and aggregation can begin prior to receiving a collect request. For (1.) the envisioned use case is Poplar1, where the agg parameter encodes a set of candidate prefixes. In general these cannot be known until the agg result for a previous Poplar1 evaluation has been collected.",
          "createdAt": "2022-05-31T16:01:17Z",
          "updatedAt": "2022-05-31T16:01:42Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "So we need a flag in the Task to determine which scenario applies, right?",
          "createdAt": "2022-05-31T16:05:40Z",
          "updatedAt": "2022-05-31T16:05:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Unless this impacts interop, I don't think the spec needs normative text to address this. I think the text we have in the overview is sufficient.",
          "createdAt": "2022-05-31T16:12:58Z",
          "updatedAt": "2022-05-31T16:12:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Given no guidance on how to resolve this, and because it hasn't been touched in a while, I'm going to close this issue.",
          "createdAt": "2022-07-30T02:34:07Z",
          "updatedAt": "2022-07-30T02:34:07Z"
        }
      ]
    },
    {
      "number": 270,
      "id": "I_kwDOFEJYQs5Ls3zt",
      "title": "Consider specifying problem type for \"unrecognized aggregation job\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/270",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Similarly to the \"unrecognizedTask\" error token, a request might reference a nonexistent/unknown aggregation job. It may be useful to specify an \"unrecognizedAggregationJob\" or similar error token for this case.",
      "createdAt": "2022-06-13T23:04:34Z",
      "updatedAt": "2022-07-07T12:28:18Z",
      "closedAt": "2022-07-07T12:28:18Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I noticed this as well. In fact, there are a few error cases we're not covering right now. I wonder if we really *need* to cover them all? In any case I am not opposed to adding this one.",
          "createdAt": "2022-06-27T19:28:57Z",
          "updatedAt": "2022-06-27T19:28:57Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we probably don't need to specify many errors; my thinking, at least, is that we only _need_ to specify the error types that, when communicated, will cause a difference in behavior on the part of the receiver which is relevant to interoperability. (Certainly, implementations might choose to have more granular errors to e.g. ease debuggability, but this can be left implementation-specific.)\r\n\r\nI'm not sure if that's the right bar to justify a new error code; it makes sense to me since it follows the principle of \"specify minimal behavior to allow interoperability.\"\r\n\r\nPerhaps we should eventually decide what justifies an error code being specified, and do a larger pass over the error codes to (de-)specify things that are not needed?",
          "createdAt": "2022-06-30T18:30:03Z",
          "updatedAt": "2022-06-30T18:30:03Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(in any case, the general principle of what should/should not be an error condition is probably outside the scope of this issue. I'm happy enough trying to specify unrecognizedAggregationJob for now; if you'd like to specify any of the other error cases you mention, I can do so as well.)",
          "createdAt": "2022-07-05T20:40:39Z",
          "updatedAt": "2022-07-05T20:40:39Z"
        }
      ]
    },
    {
      "number": 271,
      "id": "I_kwDOFEJYQs5L1mLK",
      "title": "Consider enforcing min_batch_size check in aggregator",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/271",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "`min_batch_size` as part of task parameters is distributed out of band by aggregators. At the moment there's no protocol level support to guarantee `min_batch_size` is not maliciously reduced by a dishonest leader. For use cases that work with central differential privacy, the `min_batch_size` is chosen based on an epsilon-dp guarantee over a certain batch. A dishonest leader can send a smaller `min_batch_size` to helper to break a strong dp guarantee.\r\n \r\nOne possible solution is to include parameters like `min_batch_size` (and other privacy parameters, for eg. epsilons in differential privacy) in `Extension`, which will become part of AAD therefore avoid tempering, and enforcing aggregators to check the out-of-band `min_batch_size` matches the ones included in every `Extension` field for the same task. Aggregator can also implement a one-off calculation on the task parameters to verify the `min_batch_size` is reasonable with other provided privacy parameters. For example, given a local differential privacy epsilon e_0, target central differential privacy epsilon e_c, and a min_batch_size N,  we can calculate whether e_c can be achieved  by aggregating e_0 over N reports, by using various shuffling amplification analysis.",
      "createdAt": "2022-06-15T14:41:02Z",
      "updatedAt": "2022-11-01T23:09:45Z",
      "closedAt": "2022-11-01T23:09:45Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "`min_batch_size` is distributed out-of-band, which means that the helper and leader are considered to both have the value configured separately (i.e. without a chance for one of the aggregators modify the value that is configured for the other aggregator).\r\n\r\n`min_batch_size` is also evaluated separately and independently by each aggregator for each collect operation. The leader's check is specified in [Collection Initialization](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-collection-initialization) & the helper's check is specified in [Collection Aggregation](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-collection-aggregation). (In both cases, the relevant section refers to the [Validating Batch Parameters](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-validating-batch-parameters) section which specifies the `min_batch_size` check directly.)\r\n\r\nGiven the above, I believe a malicious leader cannot reduce `min_batch_size`, either at time of task initialization or at time of collection. (A malicious leader can certainly skip its own `min_batch_size` check, but as long as the helper does not collude, a small batch will still be caught by the helper's check.)\r\n\r\n> Aggregator can also implement a one-off calculation on the task parameters to verify the `min_batch_size` is reasonable with other provided privacy parameters.\r\n\r\nThis is an interesting idea & worthy of consideration IMO -- AFAIK, the DAP spec currently does not speak to what values of `min_batch_size` are reasonable.",
          "createdAt": "2022-06-16T18:11:03Z",
          "updatedAt": "2022-06-16T18:11:03Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think the problem is DAP doesn't specify how task parameters are distributed out-of-band. If the task parameters is sent to helper from a malicious leader before task initialisation, then the helper would check against a wrong `min_batch_size` later.\r\n\r\nHaving said that, my above suggestion of checking against a `min_batch_size` in AAD also assumes client have the right task parameters.\r\n\r\nOn the other hand, I see there is an open issue in [Differential privacy](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-differential-privacy):\r\n```\r\n[OPEN ISSUE: While parameters configuring the differential privacy noise (like specific distributions / variance) can be agreed upon out of band by the aggregators and collector, there may be benefits to adding explicit protocol support by encoding them into task parameters.]\r\n```\r\nThese DP parameters will face the same problem: how do we guarantee they are configured honestly.\r\n\r\nI agree we should consider adding `min_batch_size` calculation from epsilon-dp parameters. There are well researched paper about calculating `min_batch_size` based on local DP from each client and a desired central DP on the batch. This _may_ need to be VDAF specific though.",
          "createdAt": "2022-06-16T20:10:04Z",
          "updatedAt": "2022-06-16T20:10:04Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Regarding the values of `min_batch_size`, there is an explicit statement in \"[6.4 Batch Parameters](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-batch-parameters)\":\r\n\r\n> This document does not specify how to choose minimum batch sizes.\r\n\r\nI think there also used to be a somewhat obvious statement that if `min_batch_size` is 1 there is no privacy but I cannot find it anymore.\r\n\r\nI assume specification is avoided because there might be corner cases where low values make sense, e.g. some tasks might have a batch size of 2 because plausible deniability is enough. I would also like to see some guidance/examples on this but probably in the VDAF spec.",
          "createdAt": "2022-06-17T09:11:00Z",
          "updatedAt": "2022-06-17T09:11:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As @simon-friedberger points out, the DAP draft doesn't try to define a set of configuration parameters that are sufficient for all deployments. Indeed, this is something we expect to be deployment specific, particularly when DP is desired.\r\n\r\n@wangshan it sounds like what you're after here is a means of opting out of a task if its configuration does not provide a sufficient level of privacy (for some definition of \"privacy\"). I think some part of this mechanism is inherently deployment-specific, but it might be useful to specify some protocol logic that is likely to be common to multiple deployments.\r\n\r\nOne question is whether this mechanism should be implemented during the \"online\" phase of the protocol, as you suggest (i.e., during upload, aggregation, or collection) or \"offline\" at configuration time. Offline seems preferable to me, given that this is always treated as a fatal error.\u00a0However since we haven't yet specified how tasks are configured, it's not clear that we can define any behavior here.\r\n\r\nSuppose each Aggregator implemented an endpoint, e.g., `POST /add_task`, to which the Collector posted task configurations. We could allow the aggregator to \"abort\" this request with a specific error, e.g., `insufficientPrivacy`. Would this be sufficient?",
          "createdAt": "2022-06-27T19:25:56Z",
          "updatedAt": "2022-06-27T19:25:56Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think that it makes sense to have the `min_batch_size` set by the client and covered by the HPKE AAD. There's no way for the client to verify or enforce that either aggregator actually honored the value covered by the report AAD, so I don't think it would mitigate any of the currently possible attacks.\r\n\r\nFurther, I believe that in order to undermine the `min_batch_size` privacy parameter, you need both/all aggregators to collude -- it doesn't do leader any good to emit aggregate shares over a single report unless helper computes an aggregate share over that same report. If you admit colluding aggregators, then you're outside of DAP's threat model: if they're already colluding, then they might as well just send each other decrypted input shares.\r\n\r\nI think the same applies to aggregators using the wrong epsilon on an aggregate share -- it won't help unless both aggregators subvert the protocol together.\r\n\r\n>Suppose each Aggregator implemented an endpoint, e.g., `POST /add_task`, to which the Collector posted task configurations. We could allow the aggregator to \"abort\" this request with a specific error, e.g., `insufficientPrivacy`. Would this be sufficient?\r\n\r\nI don't think we can specify an `/add_task` endpoint in DAP without opening the can of worms of bringing task parameter negotiation into the scope of the protocol. ",
          "createdAt": "2022-06-27T20:15:21Z",
          "updatedAt": "2022-06-27T20:15:21Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> @wangshan it sounds like what you're after here is a means of opting out of a task if its configuration does not provide a sufficient level of privacy (for some definition of \"privacy\"). I think some part of this mechanism is inherently deployment-specific, but it might be useful to specify some protocol logic that is likely to be common to multiple deployments.\r\n\r\nYes. I agree complete mitigation will likely be deployment-specific. \r\n\r\n> Further, I believe that in order to undermine the min_batch_size privacy parameter, you need both/all aggregators to collude\r\n\r\n@tgeoghegan that's only the case after task initialization. But the parameters can be corrupted at task initialization stage, I imagine it's very common for the leader's organisation to drive the task, if helpers just receive task parameters from leader, then they are unable to verify if the parameters from leader can guarantee any privacy for the task.  ",
          "createdAt": "2022-06-30T12:09:06Z",
          "updatedAt": "2022-06-30T12:09:30Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This issue essentially captures the transparency and parameter enforcement part of issue https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/290",
          "createdAt": "2022-07-22T15:15:09Z",
          "updatedAt": "2022-07-22T15:15:09Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I don't think that it makes sense to have the min_batch_size set by the client and covered by the HPKE AAD. There's no way for the client to verify or enforce that either aggregator actually honored the value covered by the report AAD, so I don't think it would mitigate any of the currently possible attacks.\r\n\r\n@tgeoghegan I think the threat model here necessarily assumes that at least one aggregator is honest. I think you're right that if both were dishonest, including `min_batch_size` in Report has no benefit. But if at least one is honest, then doesn't the client have assurance that at least one aggregator checked it and acted accordingly? ",
          "createdAt": "2022-07-22T15:17:51Z",
          "updatedAt": "2022-07-22T15:17:58Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think there are three issues being conflated here:\r\n\r\n1. A malicious leader reducing `min_batch_size` which, it was assumed that leader, helpers and collector agreed on this value out of band so this should be impossible.\r\n2. Aggregators actually enforcing `min_batch_size` (what the title says) which they always had to do.\r\n3. Clients actually knowing what `min_batch_size` will be enforced which is the transparency part.\r\n\r\nI think 3. is a great idea, 1. should probably be discussed in #290 and 2. is a misunderstanding.",
          "createdAt": "2022-07-28T16:20:59Z",
          "updatedAt": "2022-07-28T16:20:59Z"
        }
      ]
    },
    {
      "number": 272,
      "id": "I_kwDOFEJYQs5L20ft",
      "title": "Increase Ciphertext size limit",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/272",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Current HPKE Ciphertext size is limited to 2 bytes, about 65KB, many use cases will need more than this, Prio for eg. doesn't have this limit. Can we change this to 4 bytes  so the size can be `<1..2^32-1>`?\r\n\r\n```\r\nstruct {\r\n  HpkeConfigId config_id;    // config ID\r\n  opaque enc<1..2^16-1>;     // encapsulated HPKE key\r\n  opaque payload<1..2^16-1>; // ciphertext\r\n} HpkeCiphertext;\r\n```",
      "createdAt": "2022-06-15T19:48:27Z",
      "updatedAt": "2022-08-04T19:32:26Z",
      "closedAt": "2022-08-04T19:32:26Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@wangshan I'm not opposed to bumping this limit since it's a painless change, but what use cases would require individual reports to contain more than 65KB of data? Could you please propose a PR that lifts the limit from 2^16 to 2^32?",
          "createdAt": "2022-06-23T14:34:45Z",
          "updatedAt": "2022-06-23T14:35:04Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@chris-wood will do.\r\nUse case wise, 65KB can only represent 4B int vector of 16K dimension, if we encode correlations in individual reports then it can easily be breached, for eg. if I want to see histogram of errors by locales, let's say I have 100 error codes and 200 locales, that's already 20K dimension in one hot encoding.",
          "createdAt": "2022-06-24T11:18:57Z",
          "updatedAt": "2022-06-24T11:18:57Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "PR created: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/279",
          "createdAt": "2022-06-24T14:50:34Z",
          "updatedAt": "2022-06-24T14:50:34Z"
        }
      ]
    },
    {
      "number": 273,
      "id": "I_kwDOFEJYQs5L6ZxK",
      "title": "Collect without interval",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/273",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "If the collector is not interested in time and interval, but simply want to collect aggregation in a batch that meets `min_batch_size`. Then can the protocol support a 'batch-based collection' instead of current interval-based?\r\n\r\nConsider a Prio usecase, when `max_batch_lifetime == 1`, collector will only need to collect the aggregation so far, with a batch size B >= `min_batch_size`. This can be orchestrated by the leader, which can implement a counting service to track batch size for a task, once it reaches `min_batch_size`, leader sends AggregateShareReq to collect helper's `aggregate_share` and return to collector.\r\n\r\nThis requires a new id to tie agg-flow with agg-share-flow. For example, in addition to `agg_job_id`, leader can send a unique `batch_id` in every AggregateReq. At collect time, leader use the same `batch_id` to collect `output_share` in helper (helper can still proactively aggregate output_shares to `aggregate_share`, since there are no more batch windows, helper can store `aggregate_share` by `agg_job_id`, or accumulate all aggregation jobs' `output_share` to one `aggregate_share`, and store it with `batch_id`). Illustrated as following:\r\n\r\n```\r\n|<------------------------ batch_id1 ----------------->| <== AggregationShareReq\r\n|    agg_job_id1     |   agg_job_id2   |   agg_job_id3 | <== AggregateReq\r\nT0                                                    Tm <== Time\r\n```\r\nHere [T0, Tm] is the time takes to aggregate `min_batch_size` number of reports, it has no relationship with `min_batch_duration` or collector interval.\r\n\r\nAs this issue pointed out: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/195, to avoid privacy leak, each batch window must contain at least `min_batch_size` reports, otherwise attacker can find ways to slice intervals to break privacy guarantee. But if the protocol does require each batch window meets `min_batch_size`, then the collect interval itself is no longer useful, since the duration that takes to meet `min_batch_size` is the smallest duration that can be queried. Therefore, it seems to make sense to base collection entirely on batch size, not interval.\r\n",
      "createdAt": "2022-06-16T13:55:49Z",
      "updatedAt": "2022-08-19T21:56:19Z",
      "closedAt": "2022-08-19T21:56:19Z",
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "In this case the aggregators no longer need to worry about `min_batch_duration`. Leader can store last know Tm to filter out late arrivals if needed.",
          "createdAt": "2022-06-16T13:59:51Z",
          "updatedAt": "2022-06-16T13:59:51Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This seems similar to my suggestion in #218 of having a `batch_filter` to allow more flexible specification of subsets of reports.",
          "createdAt": "2022-06-17T08:29:29Z",
          "updatedAt": "2022-06-17T08:29:29Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Yes, `batch_filter` sounds like a more generic name that can incorporate both batch interval and batch id. \r\n\r\nBut I think allowing advanced filter can open up privacy concerns. When slicing the batches, one not only have to make sure the sliced batch meets all privacy guarantees, but also all the deltas with previously slices. We may find a way to do this with time intervals, but adding other metadata like region will make it extremely hard. These metadata will likely be related to client measurements, so I think it's better to encode such information in the measurement, or designing different tasks for them. \r\n\r\nThere are use cases where drill down or fine-grained filtering is not needed, usr simply want aggregate results with privacy guarantee.",
          "createdAt": "2022-06-17T12:01:23Z",
          "updatedAt": "2022-06-17T12:01:23Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Agreed, previous discussion on this topic is mainly here: #195 ",
          "createdAt": "2022-06-17T12:35:45Z",
          "updatedAt": "2022-06-17T12:35:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this would be useful. Let me check if I understand the protocol changes that are required:\r\n* The task config indicates whether collection is interval-based or batch-based.\r\n* For batch-based tasks, the collector specifies a batch ID rather than a time interval. Perhaps batch IDs are generated deterministically (e.g., first batch is batch ID = 0, second is batch ID = 1, and so on) so that the Leader can pre-aggregate reports for VDAFs that permit this (i.e., when the aggregation parameter is known). \r\n* For batch-based tasks, an aggregate result corresponds to a unique set of exactly `min_batch_size` measurements. Reports are sorted into to batches by the leader.",
          "createdAt": "2022-06-27T20:05:42Z",
          "updatedAt": "2022-06-27T20:05:42Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> For batch-based tasks, an aggregate result corresponds to a unique set of exactly min_batch_size measurements. Reports are sorted into to batches by the leader.\r\n\r\nI think exact `min_batch_size` maybe too strong, >= should be sufficient, so implementations can control how close to `min_batch_size` they want",
          "createdAt": "2022-06-30T12:24:48Z",
          "updatedAt": "2022-06-30T12:24:48Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Per discussion in IETF 114, let's aim to support this in the next version of the draft, with a simpler (but equivalent) version of the existing query validation. ",
          "createdAt": "2022-07-28T15:22:58Z",
          "updatedAt": "2022-07-28T15:22:58Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Looking at the implementation strategy for #297, I think there is a different approach which would be less disruptive to the existing design/provide better unification between the different query types, allow better precision w.r.t the number of reports per batch in the new `fixed-size` case, add less complexity to the helper, and give the leader less power to choose how reports are mapped into batches. (I suggested this in a review comment on #297, but @simon-friedberger correctly pointed out that this discussion would be less confusing in a GH issue.)\r\n\r\nThe high-level idea is: the current approach in #297 for `fixed-size` tasks is to bind aggregation jobs to batches (identified by an opaque \"batch ID\") at time of creation of the aggregation job. Instead, we could identify batches in `fixed-size` tasks by a batch interval (as is currently done with `time-interval` task types), dropping the concept of an opaque batch ID entirely. Another way of stating this idea is that, rather than introducing a new query type, we view this new feature as choosing how batch intervals are decided: `time-interval` generates batch intervals on a fixed cadence, `fixed-size` generates batch intervals dynamically based on the rate of incoming reports.\r\n\r\n* This approach would allow unifying almost all of the functionality/struct definitions between the `fixed-size` & `time-interval` task types. The largest difference would be in batch validation: `time-interval` batch intervals would be considered valid if the interval's `start` & `duration` are multiples of `min_batch_duration` (i.e. same as today), `fixed-size` batch intervals would be considered valid if they contain exactly[1] `min_batch_size` reports. (This is one part of the suggested design that adds complexity to the helper -- both the leader & the helper would need to implement this batch validation logic.)\r\n* This also allows reports in a single aggregation job to be matched to different batches, which sidesteps some hairy technical concerns ([PR discussion](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297#discussion_r939781474)) around whether a leader can generate aggregation jobs that are guaranteed to neither underflow `min_batch_size` nor overflow `max_batch_size` without having to stop generating aggregation jobs at batch boundaries until enough existing aggregation jobs complete to confirm that the batch won't be too small. (This doesn't change how a helper would operate; all the helper needs to do is verify that the chosen batches are of the correct size as suggested in the previous point.)\r\n* #297 suggests adding functionality to allow the collector to ask the leader what the next batch is. We would still need this for `fixed-size` tasks, since the batch intervals are based on the rate of incoming reports which the collector does not have insight into. In the interest of keeping the task types unified, this could also be implemented for `time-interval` task types. (All of this complexity falls onto the leader only.)\r\n* #297 also changes when reports are no longer accepted: the previous semantics were \"reports are no longer accepted once a collect request is received\", the new semantics are \"reports are no longer accepted once results are collected\". I think we still want this change [2], since otherwise the \"tell-me-what-the-next-batch-is\" functionality from the previous point couldn't be used safely without potentially cutting off a batch prematurely. (I think this change would also need to be made to the helper, but IMO it's no more complex than the existing requirement.)\r\n\r\nI touch on a few more points in the [original PR comment](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297#issuecomment-1212692348) but this captures the big idea.\r\n\r\n[1] #297 includes `min_batch_duration` and `max_batch_duration` to provide the leader some leeway in producing acceptably-sized batches. I think this is no longer required as the leader can always choose an interval to contain an exact number of reports (assuming we include a way to break timestamp ties, which is possible but probably a little messy).\r\n\r\n[2] Somewhat off-topic from the issue at hand, but I think allowing collect requests to be made before a batch interval is complete is valuable for performance reasons, too. Tasks using a VDAF with a trivial (i.e. `()`) aggregation parameter can incrementally aggregate reports as they arrive. This isn't currently possible for tasks with a nontrivial aggregation parameter, since the aggregation parameter to use is not communicated to the aggregators until the collect request is made. But if the collect request arrives before the batch is complete, we can start incrementally aggregating reports at that point. This, of course, assumes that the collector knows the aggregation parameter before the batch interval is complete -- seems like a fair assumption but may not always be true?",
          "createdAt": "2022-08-12T22:05:10Z",
          "updatedAt": "2022-08-12T22:05:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd I think it's time to flesh out your idea in a PR. I would suggest cribbing off of #297, i.e., creating a new branch based on the current one.\r\n\r\nMy main requirements are as follows:\r\n- It should be possible for query types to provide [Property (3.)](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297/files#r941851766).\r\n- For query types with a max batch size (e.g., `fixed-size`), we want to make sure to avoid \"throwing away\" reports due to constrains imposed by batch intervals. I'm not exactly sure how to achieve this, but maybe it'll become clear once the PR is spelled out.\r\n-  I would like to be able to de-couple storage requirements from aggregation rate. I.e, regardless of whether the Leader runs 100 reports per aggregation job or 10,000, I should be able to \"pre-aggregate\" output shares in the same way.",
          "createdAt": "2022-08-15T16:14:30Z",
          "updatedAt": "2022-08-15T16:17:13Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd @cjpatton Choosing batch-interval dynamically is not ideal because:\r\n\r\n1). As @junyechen1996 and others have mentioned, the coarse-grained timestamp would prevent aggregators from effectively defining intervals in `fixed-size` query. For e.g. a task with timestamp precision of 1hr cannot have a dynamic interval less than 1hr. If the min_batch_size is achieved in 10min, then the use case cannot be supported, unless we throw away all reports after `min_batch_size`, which can be very wasteful.\r\n\r\n2). In the `fixed-size` query, time and order don't matter, aggregators generally don't care about Report timestamps (other than the ones in distant future or past), any late-arrivals can be added to the next batch if another collect request is received. This allows the implementations more flexibility \r\n\r\n> whether a leader can generate aggregation jobs that are guaranteed to neither underflow min_batch_size nor overflow max_batch_size without having to stop generating aggregation jobs at batch boundaries until enough existing aggregation jobs complete to confirm that the batch won't be too small. \r\n\r\nI think this can be worked out by leader itself. It is the leader's responsibility to create a `AggregateShareReq` only when `min_batch_size` amount of reports has been verified successfully (I believe in interval query the leader is responsible for only creating AggregateShareReq after the current time has elapsed the current interval end). If not, then leader should initialise another `{agg-init}` with a new AggregationJob containing the number of ReportShares missing from `min_batch_size`. \r\nThis is to say an `AggregationJob` is bound to a batch ID at `{agg-init}`, but the full list of `AggregationJob`s are only clear when `{collect-flow}` happened. Note the newly created `AggregationJob` may interleaves reports with another batch ID, but it's ok as long as no time interval is involved. \r\n\r\nOn a more general term, I'd prefer we support different collect requirements explicitly and separately at this stage, once we know more about their behaviour and the challenges in implementation, we can consider unifying if necessary. Otherwise we may find ourselves unable to support use case of one type without breaking the other type.",
          "createdAt": "2022-08-15T23:08:10Z",
          "updatedAt": "2022-08-15T23:08:10Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton we should define what exactly does \"(3.) a query determines a unique batch\" means.\r\n\r\nFrom the PR:\r\n\r\n> These difficulties beg the following essential question: How important is property (3.)? Without it, the Leader is capable of sorting reports into batches as it pleases. For example:\r\nThe Leader can batch reports by User-Agent, location, and so on. This is is still possible with (3.), but this would at least force the Leader to \"throw away\" reports from clients that are not in the target group, but pertain to the query.\r\nThe Leader can do a kind of weak stuffing attack. Stuffing attacks ares still possible with (3.), but again, it would at least force the Leader to throw away any reports that pertain to the query.\r\n\r\nThis is a valid concern but since with pure interval based collection leader can still do this, I don't see what addressing this can improve. Also worth pointing out that if the task is protected by differential privacy, and client is sending information like user-agent in the clear, then the task should accept that group batches by the same user-agent is still privacy preserving as long as `min_batch_size` is met. ",
          "createdAt": "2022-08-15T23:15:12Z",
          "updatedAt": "2022-08-15T23:15:12Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for your feedback; I have sent #300 which hopefully makes the approach I am suggesting clear. To respond to a few of the concerns raised in this issue:\r\n\r\n> 1). As @junyechen1996 and others have mentioned, the coarse-grained timestamp would prevent aggregators from effectively defining intervals in fixed-size query. For e.g. a task with timestamp precision of 1hr cannot have a dynamic interval less than 1hr. If the min_batch_size is achieved in 10min, then the use case cannot be supported, unless we throw away all reports after min_batch_size, which can be very wasteful.\r\n\r\nThe randomness is used as a timestamp tie-breaker, so a `fixed-size` task can create many batches for the same timestamp value if necessary due to rounding. I think that, if the Leader waits to allow collection of a batch to account for clock skew + timestamp truncation, no reports will ever need to be dropped.\r\n\r\n\r\n> It is the leader's responsibility to create a AggregateShareReq only when min_batch_size amount of reports has been verified successfully (I believe in interval query the leader is responsible for only creating AggregateShareReq after the current time has elapsed the current interval end). If not, then leader should initialise another {agg-init} with a new AggregationJob containing the number of ReportShares missing from min_batch_size.\r\n\r\nThis only works if the batch intervals are allowed to overlap, as you note later in your comment. Other than that, this is somewhat inefficient: a few failed report aggregations can delay a batch from being collectable until another aggregation job is run, and the Leader will only realize it needs to run another aggregation job once the prior aggregation jobs have completed. This \"extra\" aggregation job is also fairly likely to be quite small compared to the normal batch sizes in use.\r\n\r\n\r\n> On a more general term, I'd prefer we support different collect requirements explicitly and separately at this stage, once we know more about their behaviour and the challenges in implementation, we can consider unifying if necessary.\r\n\r\nThe current approach in #297 (IMO) effectively creates two similar-but-separate protocols embedded inside the top-level DAP protocol. The approach I suggest in #300 achieves the same overall goal (i.e. generate batches as soon as `min_batch_size` is hit) by adding a few small, mostly-orthogonal features to the existing protocol. In my experience, the latter approach leads to a more maintainable & extensible design.\r\n\r\n\r\n> This is a valid concern but since with pure interval based collection leader can still do this, I don't see what addressing this can improve.\r\n\r\nThe Leader can only do this if they throw away any reports that don't meet their criteria. Open issue #130 may end up with the client sending report shares directly to the leader & helper; if so, at that point the Helper will be able to detect if the Leader is dropping reports.",
          "createdAt": "2022-08-17T03:04:28Z",
          "updatedAt": "2022-08-17T06:31:49Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "How large is the timestamp rounding (i.e. truncation) duration? If it has to be too large the delay would be problematic; I'm not sure what is generally considered large enough to protect privacy.",
          "createdAt": "2022-08-17T06:30:47Z",
          "updatedAt": "2022-08-17T06:30:47Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> This is a valid concern but since with pure interval based collection leader can still do this, I don't see what addressing this can improve. \r\n\r\nThe difference is that with interval based collection the leader is restricted to splitting into:\r\n* everybody before\r\n* victim timestamp + everybody in the same `batch_interval.start` up to `batch_interval.duration` (which must be divisible by `min_batch_duration`)\r\n* everybody after\r\n\r\nWith fixed-size-batch the leader can just insert `min_batch_size-1` reports with the same timestamp as the victim and it works.",
          "createdAt": "2022-08-17T12:24:31Z",
          "updatedAt": "2022-08-17T12:25:00Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> How large is the timestamp rounding (i.e. truncation) duration? If it has to be too large the delay would be problematic; I'm not sure what is generally considered large enough to protect privacy.\r\n\r\nFor our use case it'll have to be hours.",
          "createdAt": "2022-08-17T15:03:55Z",
          "updatedAt": "2022-08-17T15:03:55Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@simon-friedberger what is the attack in this case, if we are talking about sybil attack, then I don't see what's the difference in the two types: for interval query, leader can generate fake reports with timestamp in `(batch_interval.start, batch_interval.duration)`; in fixed-size query, leader can generate fake reports with any timestamp; How the collection happens doesn't affect leader's ability to be malicious, am I missing something?",
          "createdAt": "2022-08-17T19:04:30Z",
          "updatedAt": "2022-08-17T19:04:30Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd \r\n> The randomness is used as a timestamp tie-breaker, so a fixed-size task can create many batches for the same timestamp value if necessary due to rounding. I think that, if the Leader waits to allow collection of a batch to account for clock skew + timestamp truncation, no reports will ever need to be dropped.\r\n\r\nOne reason to support fixed-size collection is to deliver an aggregation result as soon as the batch size condition is met. If leader has to wait then it might defeat this purpose. In my example above, if I know the task will usually aggregate `min_batch_size` in 10min, but due to privacy constraints the most precise timestamp I can have is in 1 hour, then I'll have to wait for 1hr to get a batch which would have been available after 10min. Whether to drop the extra reports is kind of a secondary concern.\r\n\r\n> This only works if the batch intervals are allowed to overlap, as you note later in your comment. Other than that, this is somewhat inefficient: a few failed report aggregations can delay a batch from being collectable until another aggregation job is run, and the Leader will only realize it needs to run another aggregation job once the prior aggregation jobs have completed. This \"extra\" aggregation job is also fairly likely to be quite small compared to the normal batch sizes in use.\r\n\r\nTrue, but my point was the interval doesn't matter, so there's no real \"overlap\". Besides, I think it's better for privacy if the chronological order of reports are not preserved inter- or intra-batches, when slicing on time window is not required. We can work around the inefficiency above by allowing leader to collect slightly more than `min_batch_size` but still less than `max_batch_size`, to minimise the need of \"extra\" aggregation jobs. After all, verification failure should be rare unless the system is under attack.\r\n\r\n> The current approach in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297 (IMO) effectively creates two similar-but-separate protocols embedded inside the top-level DAP protocol.\r\n\r\nThat's indeed my preference, the advantage is adopters that are only interested in one query type don't need to worry about the other one so much. I'm not against unifying implementation details, but I think it's better to have two distinct sub-protocols that support the two use cases well, than one sub-protocol that has to compromise. At this point, I think I should go read your PR :)",
          "createdAt": "2022-08-17T19:42:37Z",
          "updatedAt": "2022-08-17T19:42:37Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I think that the need to round timestamps on the order of hours likely disallows the approach in #300, since that approach may delay batches for a period of time up to the timestamp rounding factor. I'm going to retract that PR & add a few comments on #297.",
          "createdAt": "2022-08-17T20:23:01Z",
          "updatedAt": "2022-08-17T20:23:01Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> for interval query, leader can generate fake reports with timestamp in `(batch_interval.start, batch_interval.duration)`; in fixed-size query, leader can generate fake reports with any timestamp; How the collection happens doesn't affect leader's ability to be malicious, am I missing something?\r\n\r\n@wangshan I don't think you are. It's a minor difference in the power of the attacker. In the first case other reports in the interval have to either be dropped or will add noise. In the second the attack can be executed more precisely.\r\nAs stated on the mailing list, I don't think this has a big impact either.\r\n\r\n> One reason to support fixed-size collection is to deliver an aggregation result as soon as the batch size condition is met. If leader has to wait then it might defeat this purpose. In my example above, if I know the task will usually aggregate `min_batch_size` in 10min, but due to privacy constraints the most precise timestamp I can have is in 1 hour, then I'll have to wait for 1hr to get a batch which would have been available after 10min. Whether to drop the extra reports is kind of a secondary concern.\r\n\r\nIs this coherent? Even if the timestamp is 1 hour for privacy, the correct timestamp is probably in the last 10 minutes...",
          "createdAt": "2022-08-17T20:39:29Z",
          "updatedAt": "2022-08-17T20:39:29Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Is this coherent? Even if the timestamp is 1 hour for privacy, the correct timestamp is probably in the last 10 minutes...\r\n\r\nAt least one privacy violation I can think of requires knowing the clock skew between client & aggregator. Specifically, a malicious leader could bucket reports by apparent clock skew between client timestamp & server timestamp. Then if they learn a client device's clock skew, they can correlate the reports in the relevant buckets as likely having been generated by that device.\r\n\r\nRounding timestamps to a large enough degree protects against this attack, since all reports will fall into the same bucket. (Though I think if the malicious actor could control when a client submitted reports, they might be able to figure out bounds on the client's clock skew by triggering a report upload near a rounding boundary and seeing how the client rounds the timestamp.) And even if we know the report arrived at the server in the last 10 minutes, that doesn't tell us what the clock skew between client & aggregator is.\r\n\r\nThat said, I'm curious if there are other privacy violations to consider here -- has the threat model around the client timestamp been discussed somewhere already?",
          "createdAt": "2022-08-17T21:09:16Z",
          "updatedAt": "2022-08-17T21:09:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The main concern about timestamps -- or any other report metadata -- is that it creates a fingerprinting surface that a Helper can use to control the reports that end up in the batch. Of course it should be noted that the Leader also has this capability, and more.\r\n\r\nMy primary concerns about #300 (as it's currently spelled) are more operational:\r\n1. As @wangshan mentioned, latency if `time_precision` is large. The amount of storage Aggregators need depend on this value; the larger the better.\r\n2. The PR allows us to \"borrow\" reports from a previous time interval to fill the next batch, which is good. But in order to do so, I would have to store output shares individually, rather than in aggregate, in order to be able determine at aggregation time which shares fall into the batch range. Unless I'm missing something? cc/ @branlwyd ",
          "createdAt": "2022-08-17T23:50:37Z",
          "updatedAt": "2022-08-17T23:56:50Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd I originally brought this up in this issue: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/274\r\n\r\nThe threat I was referring to is the following: if there is a proxy between client and aggregator (like the one described in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/294), and the proxy is owned by the leader, (for e.g leader's edge server). Assuming we encrypt the report from client to the proxy and to aggregator (so no one other than leader sees the timestamp), if the timestamp is precise, then an attacker from leader with access to the input of the proxy can look up the time packages arrived, and figure out which client the report is coming from (maybe with help of other info like size of the package). Rounding timestamp makes this a lot harder. \r\n\r\nIn the fixed-size query case, the main purpose of the timestamp is to allow aggregators filter out very old (or very future) reports.\r\n",
          "createdAt": "2022-08-18T01:32:39Z",
          "updatedAt": "2022-08-18T01:32:39Z"
        }
      ]
    },
    {
      "number": 274,
      "id": "I_kwDOFEJYQs5L6jUI",
      "title": "Consider supporting coarse-grained timestamp in Nonce",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/274",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Not sure if DAP enforces precision of `Nonce.Time`:\r\n\r\n```\r\nTime uint64; /* seconds elapsed since start of UNIX epoch */\r\n```\r\n\r\nBut with fine-grained timestamp, it is easier for attacker to associate a report with the user sent that report. For example, if timestamp is accurate to second, and leader is compromised, attacker can compare users that uploaded at a given second (perhaps from a routing server) and reports' timestamp, then figure out if a particular user has contributed to a task. With a coarse-grained timestamp, however, we can minimise this threat with a secure routing server. Anti-replay can still be satisfied with the UUID part of Nonce.\r\n\r\n\r\n",
      "createdAt": "2022-06-16T14:25:47Z",
      "updatedAt": "2022-07-07T12:21:41Z",
      "closedAt": "2022-07-07T12:21:41Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "This is a valid concern. One reasonable change would be to modify the report timestamp to align with the batch window start (min_batch_duration), and then bump the size of the nonce to offset the increase change of collisions. Specifically, this would mean we update the Nonce like so:\r\n\r\n~~~\r\nstruct {\r\n  Time time; // set to min_batch_duration for current batch window\r\n  uint8 rand[32]; // or 16 bytes, whatever we're comfortable with for the sake of minimizing collisions\r\n} Nonce;\r\n~~~\r\n\r\n@wangshan, would this work? @ekr, @tgeoghegan, @cjpatton, thoughts?",
          "createdAt": "2022-06-22T18:41:35Z",
          "updatedAt": "2022-06-22T18:41:35Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@chris-wood I think this should work. ",
          "createdAt": "2022-06-24T11:35:15Z",
          "updatedAt": "2022-06-24T11:35:15Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Great! I'll prep a PR. ",
          "createdAt": "2022-06-24T13:23:07Z",
          "updatedAt": "2022-06-24T13:23:07Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I would add the `min_batch_duration` can also be of any length, so aligning Nonce with it will make it coarse-grained for use cases where `min_batch_duration` is large, say in hours, but won't be as effective for very small `min_batch_duration`, but I think that should probably be addressed separately.",
          "createdAt": "2022-06-24T14:53:13Z",
          "updatedAt": "2022-06-24T14:53:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm happy with this change, though 16 bytes will be sufficient since clients are supposed to choose this at random. (Note that we chose 32 bytes for the various \"IDs\" (i.e., task ID, aggregation-job ID) because we anticipated some deployments using a hash of something as the ID.",
          "createdAt": "2022-06-27T15:50:33Z",
          "updatedAt": "2022-06-27T15:50:33Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm happy with this change, though 16 bytes will be sufficient since clients are supposed to choose this at random. \r\n\r\nYep \ud83d\udc4d ",
          "createdAt": "2022-06-27T17:03:14Z",
          "updatedAt": "2022-06-27T17:03:14Z"
        }
      ]
    },
    {
      "number": 276,
      "id": "I_kwDOFEJYQs5MFYra",
      "title": "Add metadata definition",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/276",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "It would be convenient for testing purposes if we could just collect some random data and also send that data as metadata for comparison.\r\n\r\nSince we already discussed that we might need other metadata for slicing I think it would be good to add a definition for it. Could probably be as simple as saying \"there are `n` opaque metadata bytes with `n`, it's meaning and usage defined by the task\".",
      "createdAt": "2022-06-20T07:18:20Z",
      "updatedAt": "2022-07-04T09:26:48Z",
      "closedAt": "2022-07-04T09:26:48Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@simon-friedberger Hmm, I'm not following this issue. It sounds like implementations can just decide what sort of metadata they might include, if I understand what you mean by metadata, and throw it in an extension. Do you have a specific change to the spec in mind? ",
          "createdAt": "2022-06-23T14:33:50Z",
          "updatedAt": "2022-06-23T14:33:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with Chris W., it seems like the `extensions` field of the `Report` could be used for this purpose.",
          "createdAt": "2022-06-27T16:18:29Z",
          "updatedAt": "2022-06-27T16:18:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Also, if we're specifically talking about enabling correctness testing (i.e., verifying that the aggregates that come out of the pipeline match the real sums over the original inputs), then we've had luck with deterministically deriving inputs from a timestamp (or in the case of DAP, the report nonce), so that the collector can re-derive the inputs without any additional communication channel between protocol participants.",
          "createdAt": "2022-06-27T20:54:38Z",
          "updatedAt": "2022-06-27T20:54:38Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@simon-friedberger given the above, are we OK to close this issue, or do you think there are spec changes that need to happen?",
          "createdAt": "2022-06-29T14:50:48Z",
          "updatedAt": "2022-06-29T14:50:48Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Seems good to me!",
          "createdAt": "2022-07-04T09:26:48Z",
          "updatedAt": "2022-07-04T09:26:48Z"
        }
      ]
    },
    {
      "number": 278,
      "id": "I_kwDOFEJYQs5Mc-nN",
      "title": "Resource oriented HTTP API (was: Conveying task and job IDs)",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/278",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-04"
      ],
      "body": "Issue #261 touched on how task and job parameters are conveyed between aggregators across request and response messages. This issue tracks the more general question of how we convey these parameters. They could be conveyed in application messages as they are now, in URLs as query parameters, or even headers. \r\n\r\n@mnot, if you have cycles, I'd be very interested to hear what you think.\r\n\r\n_Originally posted by @BranLwyd in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/261#issuecomment-1132242994_",
      "createdAt": "2022-06-23T16:28:54Z",
      "updatedAt": "2023-01-30T18:10:17Z",
      "closedAt": "2023-01-30T18:10:17Z",
      "comments": [
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "What's the deployment model? In other words, if I wanted to deploy this protocol, would I:\r\n\r\na) spin up a new, dedicated server and treat it like a black box\r\nb) create new resources on an potentially existing server that are specific to this protocol\r\nc) apply the protocol to potentially existing resources on a server\r\n\r\n?",
          "createdAt": "2022-06-23T23:32:56Z",
          "updatedAt": "2022-06-23T23:32:56Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I think (a) is going to be the most common deployment model here. ",
          "createdAt": "2022-06-23T23:43:39Z",
          "updatedAt": "2022-06-23T23:43:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Based on my experience implementing the current draft, I think it would be great to move the task ID to either a header or the query string of the URL. The latter might be preferable since it's more consistent with `/hpke_config`. I'm agnostic about the aggregation-job ID.\r\n\r\nThe problem solved by moving the task ID to either the URL or a header is the following: Before parsing a message it's useful to first make sure that the message is authentic: https://github.com/cloudflare/daphne/blob/main/daphne/src/roles.rs#L421\r\n\r\nHowever to authenticate a DAP message it's necessary to at least parse the task ID in order to look up the bearer token: https://github.com/cloudflare/daphne/blob/main/daphne/src/auth.rs#L84-L87",
          "createdAt": "2022-06-27T18:46:42Z",
          "updatedAt": "2022-06-27T18:46:42Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "Will it ever be necessary to send these requests in an unmodified browser (either by directly entering the Urls into the location bar, or using forms or javascript)?\r\n\r\nIf not, it's largely a matter of stylistic preference. \r\n\r\nURL parameters are visible in the location bar (and in logs); their semantics are determined by the resource you're interacting with. So, if you use them you should be approaching the protocol as an exercise in resource modelling -- i.e., \"this resource takes those parameters, and does that\".\r\n\r\nThe body is similar to URL parameters in terms of scope of semantics, but needs to be identified with a media type. Also it's not visible, and not available on some request methods, and its processing is tied to the request method to some degree (most loosely with POST).\r\n\r\nHeaders have a (purportedly) universal scope -- but it's OK if they have no applicability to most resources. They go on all messages, and keep the URL clean. If you use a header, take a look at [Structured Fields](https://httpwg.org/specs/rfc8941.html).\r\n\r\nDoes that help?",
          "createdAt": "2022-06-29T05:33:30Z",
          "updatedAt": "2022-06-29T05:33:30Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Will it ever be necessary to send these requests in an unmodified browser (either by directly entering the Urls into the location bar, or using forms or javascript)?\r\n> \r\n> If not, it's largely a matter of stylistic preference.\r\n\r\nOK, great! These will almost never be sent in an unmodified browser, so I'm glad this boils down to preference.\r\n\r\n> URL parameters are visible in the location bar (and in logs); their semantics are determined by the resource you're interacting with. So, if you use them you should be approaching the protocol as an exercise in resource modelling -- i.e., \"this resource takes those parameters, and does that\".\r\n> \r\n> The body is similar to URL parameters in terms of scope of semantics, but needs to be identified with a media type. Also it's not visible, and not available on some request methods, and its processing is tied to the request method to some degree (most loosely with POST).\r\n> \r\n> Headers have a (purportedly) universal scope -- but it's OK if they have no applicability to most resources. They go on all messages, and keep the URL clean. If you use a header, take a look at [Structured Fields](https://httpwg.org/specs/rfc8941.html).\r\n> \r\n> Does that help?\r\n\r\nIndeed it does, and I have one final question. Some of these APIs will be called with the expectation that the client is authenticated. Right now, we're using bearer tokens for experimentation purposes, since they're simple to configure and easy to check. In the future, we might require the underlying channel to be mutually authenticated via mTLS or whatever's relevant. However, there's another variant wherein the request is authenticated using [message signatures](https://httpwg.org/http-extensions/draft-ietf-httpbis-message-signatures.html). In this case, does the choice of URL parameter vs header vs request body matter much? My understanding of the request signature design is that _all_ contents of the request, including the URL, any headers, and body are all authenticated, so my intuition suggests that this again boils down to stylistic preference. Is that right?\r\n\r\n@tgeoghegan, assuming the above is accurate, how do you feel about using URLs for things like task IDs, job IDs, etc, and keeping the rest of the version-specific DAP stuff in the body? If and when DAP is versioned, I would expect the content types of the messages to change, but I can't imagine the concepts of task ID or job ID changing much. In other words, the latter seem like invariants (for lack of a better term), so sticking them in URL parameters allows them to be handled separately from things that might change by version or VDAF algorithm. (This relates to how we choose to indicate the VDAF algorithm in use, which is currently done by just tying it to the task ID.)",
          "createdAt": "2022-06-29T10:44:13Z",
          "updatedAt": "2022-06-29T10:44:13Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "AFAIK the HTTP message signature stuff has evolved from the AWS request signature scheme, which does cover headers, query params and bodies, so you're right that we are flexible in that respect. The one gotcha is that not all HTTP methods allow bodies (crucially, GET requests can't have one), so that may nudge us towards using headers or query params.\r\n\r\nAll that being said -- I think we should plan to refactor the HTTP API endpoints so that they are oriented around the protocol's entities/nouns instead of actions/verbs. I started sketching that idea [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/261#issuecomment-1132239527). That change will be pretty disruptive, so I think I want to target it for the draft we'd submit to IETF 115 (114 being right around the corner).\r\n\r\n",
          "createdAt": "2022-06-29T19:48:33Z",
          "updatedAt": "2022-06-29T19:48:33Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Agreed \ud83d\udc4d Let's use this issue to hash that initial out idea a bit further.",
          "createdAt": "2022-06-29T20:00:17Z",
          "updatedAt": "2022-06-29T20:00:17Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 114's PPM session, we discussed rewriting the HTTP API to be more resource oriented and align with the relevant BCPs ([slides 8-9](https://datatracker.ietf.org/meeting/114/materials/slides-114-ppm-whats-new-for-dap-00)). As I noted above, I'd like to get that done for the next draft, so I've retitled this issue to focus it on the work I plan to do.",
          "createdAt": "2022-08-03T18:26:23Z",
          "updatedAt": "2022-08-03T18:26:23Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "There are numerous places in DAP where we mandate specific HTTP status codes in responses, but [RFC 9205 recommends against this](https://httpwg.org/specs/rfc9205.html#using-http-status-codes). We should revisit prescriptions about responses and make sure that any protocol-critical error information is conveyed in a problem document type, and otherwise allow implementations to use whatever 4xx or 5xx HTTP status they want.\r\n\r\nRFC 9205 also suggests [a format for example HTTP messages](https://httpwg.org/specs/rfc9205.html#specifying-the-use-of-http), which we should use where possible.",
          "createdAt": "2022-08-24T20:13:30Z",
          "updatedAt": "2022-08-24T20:13:30Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this is still doable for IETF 115, but it looks like we'll release draft-02 before this is ready, so punting to draft-03.",
          "createdAt": "2022-08-25T21:09:05Z",
          "updatedAt": "2022-08-25T21:09:05Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "[RFC 7807](https://www.rfc-editor.org/rfc/rfc7807.html) has specific suggestions on how to communicate error details which might be interesting.",
          "createdAt": "2022-08-26T06:13:41Z",
          "updatedAt": "2022-08-26T06:13:41Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "Note that it's currently being revised - see https://github.com/ietf-wg-httpapi/rfc7807bis",
          "createdAt": "2022-08-26T06:18:50Z",
          "updatedAt": "2022-08-26T06:18:50Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Also related: We have \"outdatedConfig\" but we seem to be missing \"unrecognizedConfig\" like we do for task IDs. Might be worth adding when fixing this.",
          "createdAt": "2022-08-26T08:18:18Z",
          "updatedAt": "2022-08-26T08:18:18Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "A potentially related question, @chris-wood is `POST` the right method in a resource oriented API, compared to PUT? ",
          "createdAt": "2022-10-17T14:39:12Z",
          "updatedAt": "2022-10-17T14:39:12Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> A potentially related question, @chris-wood is `POST` the right method in a resource oriented API, compared to PUT?\r\n\r\nPOST and PUT are both perfectly valid methods to use. The key distinction between them is that PUT is idempotent and POST is not. [From Mozilla's MDN web docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PUT): \"calling [PUT] once or several times successively has the same effect (that is no side effect), whereas successive identical [POST](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST) requests may have additional effects, akin to placing an order several times.\"\r\n\r\nThis is significant when you consider recovery from error cases: suppose a client sends a message to a server, but the response is lost due a network error. The client now does not know whether their request was handled and thus whether they can move to the next step in some protocol. If the request was idempotent (like a PUT), then it's OK for the client to re-send the request until they get a successful response, since they know that the server receiving the same request multiple times won't have caused duplicate work or other side effects. If the request was not idempotent, then the client needs some other means of figuring out what state the server is in and recovering (perhaps a GET on some other resource).",
          "createdAt": "2022-10-17T14:44:50Z",
          "updatedAt": "2022-10-17T14:44:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Removing the draft tag, as we're punting beyond 03.",
          "createdAt": "2022-11-05T00:13:09Z",
          "updatedAt": "2022-11-05T00:13:09Z"
        }
      ]
    },
    {
      "number": 283,
      "id": "I_kwDOFEJYQs5NKsnk",
      "title": "Clarify language around `report-replayed` report share error",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/283",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Currently, the `report-replayed` report share error is returned \"if the report has already been aggregated\" ([specification reference](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-input-share-validation)).\r\n\r\nSome VDAFs, like `poplar1`, will require repeatedly aggregating the same client reports with a varying aggregation parameter. I think the specification language would effectively block this use-case currently.\r\n\r\nI think the fix is to return a `report-replayed` error if the report has already been aggregated *with the same aggregation parameter*. Note that this change would not allow a malicious leader to potentially aggregate a different set of reports with a different aggregation parameter: the existing `batch-collected` error would be returned if a client attempted to upload another report after the first collection attempt.\r\n\r\n(Given the small size of the proposed fix, I'll send a PR shortly, but discussion on this issue/the WG mailing list takes precedence -- I'll update the PR to follow any changes from discussion.)",
      "createdAt": "2022-07-05T18:48:17Z",
      "updatedAt": "2022-07-11T18:38:02Z",
      "closedAt": "2022-07-11T18:38:02Z",
      "comments": []
    },
    {
      "number": 288,
      "id": "I_kwDOFEJYQs5OS3bN",
      "title": "Polling collect URI changes the Leader's state",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/288",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "When the Collector issues a collect request to the Leader, the Leader responds with a URI the Collector can poll later on to get the (encrypted) aggregate result. This is done via an HTTP GET to the collect URI returned by the Leader.\r\n\r\nMy understanding is that the Leader is free to remove the aggregate results from storage thereafter. (See https://github.com/cloudflare/daphne/issues/53 for discussion.) The spec doesn't require this request to be authenticated, so it's possible for someone to cause the Leader to delete this state before the Collector has a chance to fetch it.\r\n\r\nThis issue can be solved by:\r\n1. requiring the Leader to store results for some period of time;\r\n2. making the collect URI hard to predict (e.g., by sticking a random number in the URI string); or\r\n2. requiring authentication for this endpoint.\r\n\r\n(2.) seems easiest to implement, but may be tricky to spell. Any thoughts on this?",
      "createdAt": "2022-07-21T17:17:25Z",
      "updatedAt": "2022-08-16T17:33:30Z",
      "closedAt": "2022-08-16T17:33:30Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> My understanding is that the Leader is free to remove the aggregate results from storage thereafter.\r\n\r\nThat's not what [section 4.4.1](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-collection-initialization) says:\r\n\r\n> The leader MUST retain a collect job's results until the collector sends an HTTP DELETE request to the collect job URI, in which case the leader responds with HTTP status 204 No Content.\r\n>\r\n> [OPEN ISSUE: Allow the leader to drop aggregate shares after some reasonable amount of time has passed, but it's not clear how to specify that. ACME doesn't bother to say anything at all about this when describing how subscribers should fetch certificates: https://datatracker.ietf.org/doc/html/rfc8555#section-7.4.2]\r\n\r\nThat text is consistent with [@chris-wood's idea](https://github.com/cloudflare/daphne/issues/53#issuecomment-1191499038). So I argue the current situation is (1). We do have to solve the problem of how long a leader can wait before tossing collect results -- or maybe we don't, since ACME doesn't bother to. But we could use this issue to discuss that question.\r\n\r\nre: (2): I think we should avoid as much as possible imposing requirements on the construction of URIs and relative paths[^1]. An implementation should feel free to generate unpredictable collect job URIs (Janus randomly generates a v4 UUID) but I don't think DAP should mandate it, just so we don't have to write the text to spell it out.\r\n\r\nre: (3): I have a hard time imagining a deployment where the `/collect` endpoint and collect job URIs wouldn't be authenticated[^2], but my position on authn in DAP is that we shouldn't specify anything, allowing implementations to use any authn scheme they want that is composable with HTTP. At most we ought to state requirements (e.g., \"the channel between aggregators must be mutually authenticated and confidential\") but avoid mandating how those requirements are met (e.g., \"the aggregators must use mTLS\").\r\n\r\n[^1]: The precedent here is ACME, which actually doesn't specify URIs at all. Instead, implementations expose a [directory endpoint](https://datatracker.ietf.org/doc/html/rfc8555#section-7.1.1) which in turn guides clients to the URIs to hit to create orders, or accounts, or whatever. That said, we shouldn't do exactly what ACME does because (1) AFAIK the BCP that mandated this part of ACME's design has been revised and (2) forcing a directory lookup (that is, an extra HTTP round trip) every time an ACME subscriber wants to issue a new cert (roughly once per 90 days) is not so bad, but forcing a directory lookup every time a DAP client wants to upload a measurement seems wasteful.\r\n[^2]: For argument's sake: maybe a deployment wants to make the results of some aggregation public, but in such settings, the collector could combine the aggregate shares into results and then stick them into a CDN instead of making it the leader's problem. Not to mention that the collector is probably most capable of transforming aggregate results into some other format that's better suited for public consumption.",
          "createdAt": "2022-07-21T18:13:06Z",
          "updatedAt": "2022-07-21T18:13:06Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> re: (3): I have a hard time imagining a deployment where the /collect endpoint and collect job URIs wouldn't be authenticated[2](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/288#user-content-fn-2-cc44bdbddf183583de77b5b1e755da8e), but my position on authn in DAP is that we shouldn't specify anything, allowing implementations to use any authn scheme they want that is composable with HTTP. At most we ought to state requirements (e.g., \"the channel between aggregators must be mutually authenticated and confidential\") but avoid mandating how those requirements are met (e.g., \"the aggregators must use mTLS\").\r\n\r\nI agree with this. I don't think DAP should specify or even mandate authentication, for the reasons Tim mentioned, but also because the aggregate outputs are encrypted only to the intended collector. Giving those encrypted aggregate shares to an unauthorized party seems fine based on the properties of HPKE. ",
          "createdAt": "2022-07-22T12:49:59Z",
          "updatedAt": "2022-07-22T12:49:59Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "FWIW, the current DAP spec text:\r\n\r\n> The leader MUST retain a collect job's results until the collector sends an HTTP DELETE request to the collect job URI\r\n\r\nimplies (at least to my reading) that the leader must indeed retain the results until an explicit `DELETE` is sent from the collector [1]. I agree that it would be very nice if an aggregator could drop collect results eventually without requiring an explicit request from the collector to do so -- that's consistent with [Chris Wood's idea](https://github.com/cloudflare/daphne/issues/53#issuecomment-1191499038) but inconsistent (IMO) with the current specification text.\r\n\r\nIf I'm not missing anything, I can prep a small PR fixing this up.\r\n\r\n\r\n[1] To be very pedantic, following the [HTTP GET semantics](https://httpwg.org/http-core/draft-ietf-httpbis-semantics-latest.html#GET), the leader could choose to send a representation indicating that the results are gone; but strictly following the text of DAP, the leader would still be required to retain the results. Given that these results would be inaccessible from an outside observer, this is a somewhat academic distinction -- IMO another good reason to make a change here.",
          "createdAt": "2022-07-25T23:08:36Z",
          "updatedAt": "2022-07-25T23:08:36Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton we took a change here to improve the text discussing how long leaders should hold on to collect job results, and I think questions about authentication of the collect job URIs is captured by #293. Are there any outstanding concerns about whether polling collect job URIs causes state changes?",
          "createdAt": "2022-08-03T18:22:15Z",
          "updatedAt": "2022-08-03T18:22:15Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Nope :)",
          "createdAt": "2022-08-16T17:33:30Z",
          "updatedAt": "2022-08-16T17:33:30Z"
        }
      ]
    },
    {
      "number": 289,
      "id": "I_kwDOFEJYQs5OTrmJ",
      "title": "Allow hpke_config to be queried without task_id",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/289",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "In [HPKE Configuration Request](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/main/draft-ietf-ppm-dap.md#hpke-configuration-request-hpke-config), the `[aggregator]/hpke_config` endpoint will be called with a `task_id`, unrecognised `task_id` will result in a 404. \r\nThis requires clients (or any entity wishes to call `[aggregator]/hpke_config`) to know a task id in advance. This is unnecessary for deployments that want to share HPKE config across multiple tasks. Can we consider make `task_id` optional in this endpoint, when queried without `task_id`, aggregator returns either one or a list of `hpke_config`.",
      "createdAt": "2022-07-21T20:45:47Z",
      "updatedAt": "2022-09-01T19:54:39Z",
      "closedAt": "2022-09-01T19:54:39Z",
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This relates to issue https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/248",
          "createdAt": "2022-07-21T21:00:26Z",
          "updatedAt": "2022-07-21T21:00:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Works for me!",
          "createdAt": "2022-07-21T21:55:53Z",
          "updatedAt": "2022-07-21T21:55:53Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Agreed. This is sufficiently orthogonal from #248 and an unnecessary constraint on a deployment. ",
          "createdAt": "2022-07-22T12:52:07Z",
          "updatedAt": "2022-07-22T12:52:07Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "One thing to note here is that Aggregator needs to have the option of aborting instead of returning a response, since some deployments will want key separation across tasks.",
          "createdAt": "2022-07-22T18:40:26Z",
          "updatedAt": "2022-07-22T18:40:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this is text-ready. @wangshan would you mind writing a PR?",
          "createdAt": "2022-07-30T00:35:29Z",
          "updatedAt": "2022-07-30T00:35:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Also relates to #278, in that we can address both of these at once. I sorta buy that #248 is orthogonal, but I don't see why we wouldn't address that problems if we're dealing with this. I think it would save implementations some churn.",
          "createdAt": "2022-08-03T18:28:44Z",
          "updatedAt": "2022-08-03T18:28:44Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton yes aiming to have a PR this week",
          "createdAt": "2022-08-15T22:16:30Z",
          "updatedAt": "2022-08-15T22:16:30Z"
        }
      ]
    },
    {
      "number": 290,
      "id": "I_kwDOFEJYQs5OUEAQ",
      "title": "In-band task configuration and transparency",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/290",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Task configuration between aggregators currently is handled out-of-band, this requires leader and helper to agree some secure method to exchange task configuration prior to upload.\r\n\r\nAn alternative is to create task in-band and on-demand from `Report` or `ReportShare` received from clients. Assuming server sends a unique task_id and all the parameters required by [Task configuration](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/main/draft-ietf-ppm-dap.md#task-configuration-task-configuration) to the clients out-of-band (this is defined in client capability in current spec). Then clients can include these fields in report's `extension`, and upload them to servers along with reports. This is similar to an idea mentioned in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/271.\r\n\r\nUpon receiving a Report (leader) or ReportShare (helper) with unseen tuple of (task_id, extension), the aggregators create the task on-demand, then proceed to aggregate-flow as usual.\r\n\r\nThis can be optimised to avoid checking the tuple of (task_id, extension), by letting client create `task_id` based on `extension`. For example, the client can share some text that defines the use case, communicated to them out-of-band, then create `task_id` using `hash(\"shared text\" || extension)`.  This can be particularly useful if the \"shared text\" or part of extension is only available on the client side.\r\n\r\nThere are some nice features in this method:\r\n- By sending all task parameters to clients then return to the server, we add transparency to the task that clients participate in. Client has the choice to see what data has been collected with what parameters (like `min_batch_size`, `max_batch_lifetime`, or any differential privacy parameters if that's the privacy guarantee used). For some clients these task parameters can even be hardcoded on the client side (for e.g. on a mobile device) to avoid any tempering from server.\r\n- Because `extension` is used in HPKE's AAD, malicious leader cannot change the task parameters, for e.g reducing `min_batch_size`.\r\n- A \"task\" now means the same task_id and same task parameters (including parameters for choosing VDAF). We can guarantee reports from one task will only be aggregated with one VDAF. For the same reason, if a malicious client changes the task_id or task parameters, its report will be aggregated in a different task, with other poison reports from the same malicious attack. The \"good\" reports are not polluted. \r\n- It avoids any out-of-band task orchestration between leader and helper, which isn't defined by the spec yet.\r\n- The on-demand creation is easy to implement with streaming framework that has `groupBy` operator. In fact, task as an object doesn't have to exist in aggregators, it mainly becomes an identifier to group aggregations together.\r\n\r\nThe disadvantage is added overhead in `Report` and `ReportShare`. Some optimisation can be done by moving Extension out of `ReportShare` and into `AggregateInitReq`.\r\n\r\nNote that the parameters that are not necessarily tied to a task may still need to be exchanged out-of-band between aggregators, like collector hpke config (or even aggregator's hpke config, see https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/289). Secrets that should not be know by clients, like `vdaf_verify_key` must still be exchanged out-of-band.\r\n\r\n",
      "createdAt": "2022-07-21T22:33:43Z",
      "updatedAt": "2022-09-13T01:13:27Z",
      "closedAt": "2022-09-13T01:13:26Z",
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Sounds workable. The clients might have to tell the leader who the helper is supposed to be.\r\nAnd the aggregators would probably need some kind of restriction about which expensive computations to perform depending on client authentication. Otherwise malicious clients could invent new tasks which use expensive VDAFs and submit data for them.",
          "createdAt": "2022-07-22T09:55:46Z",
          "updatedAt": "2022-07-22T09:55:46Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "The crux of the issue here seems to be that DAP currently lacks task configuration transparency. It's possible for clients to be configured with a task ID and related parameters that they think corresponds to some reasonable level of privacy (e.g., with a large enough `min_batch_size`), where in actuality the privacy guarantees are weak. Would you agree with that summary, @wangshan? \r\n\r\nI don't know if in-band configuration is the best solution to this problem. @simon-friedberger points out a number of considerations that would complicate things. I need to think on this more. \r\n\r\nAlso, for what it's worth, in some previous version of this draft the task ID was derived from all relevant parameters in a manner similar to your proposal, i.e., `task_id = hash(\"shared text\" || extension)`, but we removed that because it imposed too much structure on the out of band configuration. ",
          "createdAt": "2022-07-22T13:06:31Z",
          "updatedAt": "2022-07-22T13:06:31Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Sounds workable. The clients might have to tell the leader who the helper is supposed to be.\r\n\r\nThis is definitely doable, the current task configuration parameters already include `aggregator_endpoints`\r\n\r\n> And the aggregators would probably need some kind of restriction about which expensive computations to perform depending \r\non client authentication. Otherwise malicious clients could invent new tasks which use expensive VDAFs and submit data for them.\r\n\r\nTrue, but without client authentication, this attack is possible anyway, without knowing the cost of computation. \r\n",
          "createdAt": "2022-07-22T14:10:04Z",
          "updatedAt": "2022-07-22T14:22:44Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> True, but without client authentication, this attack is possible anyway, without knowing the cost of computation.\r\n\r\nWell, if the tasks are defined out-of-band nobody can trigger a Poplar1 task if you only want to run Prio3 tasks. But I totally agree that it's a corner case and it has a trivial fix.",
          "createdAt": "2022-07-22T14:25:02Z",
          "updatedAt": "2022-07-22T14:25:02Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> The crux of the issue here seems to be that DAP currently lacks task configuration transparency. It's possible for clients to be configured with a task ID and related parameters that they think corresponds to some reasonable level of privacy (e.g., with a large enough `min_batch_size`), where in actuality the privacy guarantees are weak. Would you agree with that summary, @wangshan?\r\n> \r\n> I don't know if in-band configuration is the best solution to this problem. @simon-friedberger points out a number of considerations that would complicate things. I need to think on this more.\r\n\r\nYou summary about transparency is correct. But I'd say it's only half of the concern. The current spec does not specify how leader and helper can exchange task configuration safely. If task creation happens out-of-band, I can imagine at task initialisation, a rouge engineer from leader configs a task with weak privacy guanratee and sync that with helper, without anyone detecting it. With in-band configuration, client can detect such behaviour. Furthermore, client can implement sanity checks to verify the parameters indeed provide a strong guarantee. One advantage of addressing this with in-band configuration, is that it doesn't introduce a new API, therefore no new surface of attack. I'm happy to discuss this more.\r\n\r\nThe problem @simon-friedberger mentioned are definitely worth addressing, but I'd argue without client authentication, it's hard to protect malicious client from sending garbage data to the server regardless of how the task is configured.\r\n\r\n> Also, for what it's worth, in some previous version of this draft the task ID was derived from all relevant parameters in a manner similar to your proposal, i.e., `task_id = hash(\"shared text\" || extension)`, but we removed that because it imposed too much structure on the out of band configuration.\r\n\r\nInteresting, is there a commit/issue I can read about this decision? \r\n",
          "createdAt": "2022-07-22T14:49:33Z",
          "updatedAt": "2022-07-22T14:49:33Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> You summary about transparency is correct. But I'd say it's only half of the concern. The current spec does not specify how leader and helper can exchange task configuration safely. If task creation happens out-of-band, I can imagine at task initialisation, a rouge engineer from leader configs a task with weak privacy guanratee and sync that with helper, without anyone detecting it. With in-band configuration, client can detect such behaviour.\r\n\r\nI don't think in-band configuration is necessary for this. Rather, in-band _enforcement_ seems sufficient. In particular, if the client is given \"weak parameters,\" it could just opt to not use them. Instead, if the client is given \"good parameters\" and uses them, in-band enforcement would require the helper to check these parameters, and would fail if they didn't match that which were provided by the rogue leader. \r\n\r\nWhat I'm getting at is that in-band configuration seems separate from in-band enforcement. Would you agree or disagree with that claim?\r\n\r\n> Interesting, is there a commit/issue I can read about this decision?\r\n\r\nI don't recall if this was captured in an issue. Let me know if you really need it and I can look back in the history.",
          "createdAt": "2022-07-22T14:54:13Z",
          "updatedAt": "2022-07-22T14:54:13Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "To make things clear, there are two issues we are discussing:\r\n1. in-band enforcement and transparency, which is the topic of https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/271\r\n2. automatic on-demand task configuration.",
          "createdAt": "2022-07-22T15:16:52Z",
          "updatedAt": "2022-07-22T15:16:52Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Note that this removes the part of the protocol where leader, helpers and collector agree on parameters. \r\n\r\nSo, a collector - as the author of a client, which is probably the default case - can trivially create new tasks with e.g. `min_batch_size = 2` which will probably be too low for most collected data.\r\n\r\nOf course, as the author of a client, they could also just chose to ignore DAP entirely and send the value in the clear to a different telemetry server. So I don't think this fundamentally changes the attack model but it might still be bad for the \"trustability\" of DAP.",
          "createdAt": "2022-07-28T16:30:53Z",
          "updatedAt": "2022-07-28T16:30:53Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "A few thoughts:\r\n\r\n* Overall I like this idea. In addition to concerns about the trustworthiness of the leader, a generally-available DAP deployment will need to agree on task parameters with its co-aggregators. Having a standardized solution to this problem would make deploying DAP much easier in practice.\r\n\r\n* That said, I think the client is not well-positioned to specify some task parameters. IMO it would be unfortunate if some parameters are determined in-band and some parameters are determined out-of-band. Maybe we can find a solution to these issues? Specifically:\r\n\r\n  * `vdaf_verify_key` is private between the aggregators. (I think the leader can be trusted to generate this value & communicate it to the helper when a new task is introduced?)\r\n  * The leader-helper & leader-collector bearer tokens used for auth are private between the leader/helper & the leader/collector, respectively. (I think the leader could generate & inform the helper of the leader-helper auth token on task creation. The collector-leader token is trickier, since currently all leader-collector communication is initiated by the collector -- there is a chicken & egg problem of confirming who the correct collector is. Also, the bearer token is considered to be a temporary solution--the authentication method may change, which may change the considerations here.)\r\n  * `collector_config` is not private, but it will eventually need to be able to change (it is a public key & therefore must eventually be rotatable). This introduces complications not only because we want to identify tasks by `(task_id, task_parameters)`, but also because client updates are not atomic -- during rotation, for some time some set of (updated) clients will be advertising the rotated keyset, while another set of (non-updated) clients will be advertising the non-rotated keyset.\r\n\r\n* I have a slight preference that tasks continue to be identified by `task_id` rather than `(task_id, task_parameters)`. First, I don't think there is any real benefit to allowing multiple tasks to share the same ID (as long as the task ID space is large enough that accidental collision is negligible). Having a standard identifier shared between the aggregators will help practical debugging quite a lot. And I think at least some task parameters will eventually want to change (i.e. `collector_config`); not including the task parameters as part of the task identifier will make this easier. OTOH, I do think it is valuable to check & fail if the provided task parameters don't match what is configured for an existing task.\r\n\r\n* I am also somewhat dubious about the practical protections this provides to clients -- since the entity operating the collector & providing the client software will almost always be the same, I agree with Simon that they could just ignore DAP entirely and send the value in the clear. (Maybe I am mistaken about the deployment configuration that this is meant to protect?)\r\n\r\n* We are early-on enough in specifying DAP that these parameters could be communicated in an (optional?) field in the relevant messages, rather than as an extension. OTOH, this eases moving the task parameters to the aggregate initialization request--moving extensions is tricky because not every report for the same task is going to have the same set of extensions, nor will the extension ordering necessarily be the same.\r\n\r\n* I think DAP deployments will definitely want to include more information than just the raw task parameters. I imagine things like a collector identity, client authentication, additional task metadata (e.g. human-readable name/description), deployment-specific task parameters, etc. I would be OK implementing these as additional extensions on the client report.",
          "createdAt": "2022-08-03T01:37:08Z",
          "updatedAt": "2022-08-03T02:00:32Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> So, a collector - as the author of a client, which is probably the default case - can trivially create new tasks with e.g. min_batch_size = 2 which will probably be too low for most collected data.\r\n\r\nThis threat exists even without on-demand task configuration, the author of client can create a task of min_batch_size=2 out of band, and client would not know about it. But as long as we send such parameters to client side, such behaviour becomes much more detectable.\r\n\r\nIn general, like you said, DAP cannot prevent a collector/leader organisation from deliberately miss using or bypassing the DAP channel, but doing so requires some change on client and it's harder to hide than server-side work",
          "createdAt": "2022-08-10T22:14:25Z",
          "updatedAt": "2022-08-10T22:14:25Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> We are early-on enough in specifying DAP that these parameters could be communicated in an (optional?) field in the relevant messages, rather than as an extension. \r\n\r\nI think extension is designed to do this: as an optional field for report? cc @chris-wood \r\n\r\n> OTOH, this eases moving the task parameters to the aggregate initialization request--moving extensions is tricky because not every report for the same task is going to have the same set of extensions, nor will the extension ordering necessarily be the same.\r\n\r\nI'm curious why the extension can be different for the same task (other than key rotation in collector hpke config), and why the ordering may different?\r\n",
          "createdAt": "2022-08-10T22:24:18Z",
          "updatedAt": "2022-08-10T22:24:18Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@BranLwyd I agree `collector_config` and `vdaf_verify_key` shouldn't go to device, for the latter I think there were concerns whether it's privacy leak free if the key is chosen by the leader. I don't see how we can deliver these parameters other than OOB, perhaps an independent authority that leader and helper can query?\r\n\r\n>  First, I don't think there is any real benefit to allowing multiple tasks to share the same ID (as long as the task ID space is large enough that accidental collision is negligible). \r\n\r\nI agree with this too, if we can exclude parameters that may change during one task (like `collector_config`), then the reason for a task id to be shared by more than one tasks, is most likely due to task creation error or malicious attack. ",
          "createdAt": "2022-08-10T22:47:12Z",
          "updatedAt": "2022-08-10T22:47:12Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> > So, a collector - as the author of a client, which is probably the default case - can trivially create new tasks with e.g. min_batch_size = 2 which will probably be too low for most collected data.\r\n> \r\n> This threat exists even without on-demand task configuration, the author of client can create a task of min_batch_size=2 out of band, and client would not know about it. But as long as we send such parameters to client side, such behaviour becomes much more detectable.\r\n\r\nBut with an out-of-band agreement this can be handled during the agreement. If the collector says we want to collect user age with `min_batch_size = 2` and compute the median the aggregators can decline because it is a bad idea. If we are specifying that this will be handled automatically by software such a check cannot be done.",
          "createdAt": "2022-08-11T07:26:39Z",
          "updatedAt": "2022-08-11T07:26:39Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": ">  If the collector says we want to collect user age with min_batch_size = 2 and compute the median the aggregators can decline because it is a bad idea. If we are specifying that this will be handled automatically by software such a check cannot be done.\r\n\r\nCan we not implement the same check in aggregators when a task is being created on demand?",
          "createdAt": "2022-08-11T16:01:00Z",
          "updatedAt": "2022-08-11T16:01:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Overall I like the direction we're going with this thread. Like others, I'm most interested in the potential for streamlining task on-boarding. That said, lots of good points have been brought up about the potential for things to go wrong. I think my top concerns are:\r\n* In-band enforcement (I think @BranLwyd and @chris-wood raised this previously): What should we do if one party accepts a task config, but the other does not? In particular I'm thinking of the case where the Leader starts an aggregation job, but the Helper aborts due to the task config being unacceptable (for whatever reason). In practice, a couple of engineers from two different orgs will have to hop on a call and start debugging and we'll declare an incident. Meanwhile, a backlog of reports will start building up. Out-of-band task configuration reduces the chance of these sorts of surprises coming up.\r\n* We need to ensure that co-Aggregators can still do capacity planning. I think this means that there will have to be at least some \"global\" parameters that are picked out of band, e.g., min_batch_duration for time-interval tasks.\r\n\r\nI think this issue is ready for an initial draft PR. It would be helpful to see the details spelled out to see if there's anything not workable here.",
          "createdAt": "2022-08-11T21:31:16Z",
          "updatedAt": "2022-08-11T21:34:18Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Can we not implement the same check in aggregators when a task is being created on demand?\r\n\r\nThat depends, I don't think this kind of check can be codified because the aggregation system doesn't know the privacy implication of the data being processed. App usage time is probably less important than monthly income but both are just numbers to be averaged.\r\n\r\nI am fine if we want to ignore this but suggest that we clearly describe it as out of scope then.",
          "createdAt": "2022-08-12T11:27:11Z",
          "updatedAt": "2022-08-12T11:27:11Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 @simon-friedberger ",
          "createdAt": "2022-08-17T00:07:44Z",
          "updatedAt": "2022-08-17T00:07:44Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi folks, just wanted to update people here that @wangshan will be working on an extension (DAP's first!) to specify a mechanism for in-band task provisioning. I think the goal will be to get folks feedback in time for IETF 115 so that we can consider adopting the draft as a WG item.",
          "createdAt": "2022-08-24T21:35:14Z",
          "updatedAt": "2022-08-24T21:35:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing this issue as resolved. Here is the repo where we're working on this, if anyone is curious: https://github.com/wangshan/draft-wang-ppm-dap-taskprov\r\n\r\nTentative goal is to present this at IETF 115.",
          "createdAt": "2022-09-13T01:13:26Z",
          "updatedAt": "2022-09-13T01:13:26Z"
        }
      ]
    },
    {
      "number": 291,
      "id": "I_kwDOFEJYQs5OUKHp",
      "title": "Consider adding max_task_lifetime to force retire a task",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/291",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "wangshan"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "The current spec doesn't explain how to retire or delete a task. For some use cases, it's useful to have a hard task deadline, when expired, the task and all its aggregations must be made inactive and eventually deleted. This can be a new Duration type task parameter called `max_task_lifetime`, or `max_task_duration` to avoid similar named but int typed `max_batch_lifetime`.\r\n\r\nOther than the practical benefit, for clients that uploads repeatedly and periodically, this can add privacy benefit to limit the number of uploads from the same client to the same task (related to https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/210).\r\n\r\nPS. I feel like \"lifetime\" is more appropriate for a deadline than a max allowed count.",
      "createdAt": "2022-07-21T22:49:10Z",
      "updatedAt": "2022-09-15T18:19:32Z",
      "closedAt": "2022-09-15T18:19:32Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is a neat idea, especially in conjunction with the notion in #290 about clients being able to define tasks. We might think of a client sending task parameters to aggregators as implying the client's consent to those parameters, so having tasks expire and thus requiring clients to periodically re-affirm their consent to participate in a task is interesting.",
          "createdAt": "2022-07-29T15:07:34Z",
          "updatedAt": "2022-07-29T15:07:34Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I filed #299 forgetting about this issue. I think we should do this, and more see this as a minimum amount of time the aggregators must hang onto task-related data before feeling free to start discarding things. ",
          "createdAt": "2022-08-17T21:16:34Z",
          "updatedAt": "2022-08-17T21:16:34Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'm writing a PR for this feature, I realised there are two things this issue is trying to cover:\r\n\r\n1. The period that indicates a task is allowing participation. This is mainly a client-facing feature and is a **max** time.\r\n\r\n2. The **minimum** amount of time the aggregators must hang onto task-related data. This is more of an aggregator related feature.\r\n\r\nFor 1.  If we send task parameters to client for transparency, then client SHOULD check this timestamp before sending any queries to aggregators. To prevent malicious client from sending data to out of date task, aggregators should check the timestamp on these endpoint too. But this brings a lot of other issues: clock screw between client and aggregator, and clock screw between aggregators. On the collector side, aggregator cannot reject collect queries since the collect flow is asynchronous. Especially for interval based query and max_batch_lifetime > 1, collector may query a batch after the task itself is no longer active.\r\n\r\nBased on the current draft text, I agree with @chris-wood  that `max_task_lifetime` should be more about 2. It's more about retention of the task data and does not require any timestamp check on the task at each endpoint.   ",
          "createdAt": "2022-08-23T14:58:01Z",
          "updatedAt": "2022-08-23T14:58:01Z"
        }
      ]
    },
    {
      "number": 293,
      "id": "I_kwDOFEJYQs5Ovh_G",
      "title": "Revisit requirements and recommendations for request and response authentication",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/293",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "At IETF 114, we discussed how [draft-01](https://datatracker.ietf.org/doc/draft-ietf-ppm-dap/01/) contains inconsistent guidance and/or requirements on authentication. Some details can be found in [this slide deck](https://datatracker.ietf.org/meeting/114/materials/slides-114-ppm-whats-new-for-dap/), especially slide 7.\r\n\r\nDAP should figure out what the security requirements are for each of the channels between the protocol participants, and then decide what if any prescriptions to make.",
      "createdAt": "2022-07-28T15:03:45Z",
      "updatedAt": "2022-09-01T20:07:15Z",
      "closedAt": "2022-09-01T20:07:15Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "In my presentation at IETF 114, I argued that DAP should say as little as possible about authentication, in order to allow organizations deploying DAP to use whatever authn mechanism they already support for their HTTP APIs (e.g., maybe a vendor already operates a bunch of other APIs using OAuth 2 and so using that for DAP makes it easy for them to fit it into their existing permissions model).\r\n\r\n@ekr suggested that we should reach out to the HTTP WG for guidance here (which might prompt the HTTP WG to develop some guidance here).\r\n\r\nOne clear precedent to look at here is ACME, which requires the use of JWT for authentication of requests. We should talk to ACME operators in the wild (like Let's Encrypt) to see what they think.\r\n\r\n#155 discusses one part of this problem.",
          "createdAt": "2022-07-28T15:07:43Z",
          "updatedAt": "2022-07-28T15:08:27Z"
        }
      ]
    },
    {
      "number": 294,
      "id": "I_kwDOFEJYQs5Ovo9R",
      "title": "Protocol text should discuss ingestion servers/anonymizing proxies",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/294",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "If clients upload reports directly to the leader, then the leader learns things like the client's IP among other identifying metadata. This can be mitigated with the use of some kind of anonymizing proxy, like an [OHAI](https://datatracker.ietf.org/wg/ohai/documents/) server, provided the proxy is not colluding with the aggregator(s).\r\n\r\nHowever, it's not clear that every deployment will be able to use such a proxy, because the aggregators can't run it as a service, and perhaps the cost of deploying this additional actor is prohibitive.\r\n\r\nNonetheless, the protocol should discuss the privacy leaks that result from direct uploads and discuss how deployments might use anonymizing proxies.",
      "createdAt": "2022-07-28T15:19:08Z",
      "updatedAt": "2023-09-20T15:23:11Z",
      "closedAt": "2023-09-20T15:23:10Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "#64 discusses a related idea, or a specific way to address this, where uploads would go through the collector.\r\n\r\nAlso related to this is the notion of a proxy that would batch up reports, which could reduce the operational burden of running an aggregator (i.e., only the proxy needs to be highly available and receive millions of uploads from clients).",
          "createdAt": "2022-07-28T15:20:14Z",
          "updatedAt": "2022-07-28T15:20:14Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "batch up can be applied from the client as well, if client is allowed to send a fixed number of reports in a period of time. I feel batching is an implementation detail that helps scaling, not sure if it should be reflected on the protocol level.\r\n\r\nBut I agree adding description in the protocol about anonymizing, other than batching we can also introduce a jitter in the proxy to break ordering relaitonship.",
          "createdAt": "2022-08-10T18:37:01Z",
          "updatedAt": "2022-08-10T18:37:01Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> batch up can be applied from the client as well, if client is allowed to send a fixed number of reports in a period of time.\r\n\r\nThis wouldn't help with batching reports across many clients. The idea here is that instead of a leader having to receive a small message from each of a million clients, it could receive one large message from the batching proxy.\r\n\r\n> I feel batching is an implementation detail that helps scaling, not sure if it should be reflected on the protocol level.\r\n\r\nI wish this was the case, but I think at a minimum the protocol would have to define some kind of \"ReportList\" structure so that multiple reports could be transmitted in a single message.",
          "createdAt": "2022-08-10T18:48:09Z",
          "updatedAt": "2022-08-10T18:48:09Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I suppose different batching strategies could either require protocol support or not:\r\n1) Something like `ReportList` surely requires protocol support.\r\n2) OTOH, an `/upload` endpoint could be implemented as accepting a number of concurrent requests, then writing them all back to durable storage in a single batch/transaction (however that maps back to the durable storage system), before responding to any of the requests.\r\n\r\n(1) requires protocol support, but helps even if the performance-limiting factor is e.g. all of the TLS handshakes, message parsing, or some other artifact of receiving many small messages over TLS.\r\n\r\n(2) would give many of the advantages of batching without requiring protocol support, and would help most if the underlying performance limits are due to the limits of the durable storage system in use.",
          "createdAt": "2022-08-10T20:17:36Z",
          "updatedAt": "2022-08-10T20:17:36Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to adding text about the benefits of anonymization, and adding explicit reference to OHTTP (and maybe MASQUE).",
          "createdAt": "2022-08-17T00:12:41Z",
          "updatedAt": "2022-08-17T00:12:41Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm going to try to get this done for draft-02, hopefully early next week.\r\n\r\nEdit: I did not get to this in time, and am still unsure what I want to do about it (crucially, I'm not sure if we need a `ReportList` or not), so punting to draft-03.",
          "createdAt": "2022-08-25T21:08:04Z",
          "updatedAt": "2022-08-31T20:16:52Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan do you wanna try to take care of this for 03?",
          "createdAt": "2022-11-05T00:13:53Z",
          "updatedAt": "2022-11-05T00:13:53Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "For DAP-03, we can definitely throw in some text on using OHAI or MASQUE, perhaps in security considerations. I'm inclined to punt on discussing batching of reports and the corresponding API surface since we aren't yet sure that the protocol requires it to function.",
          "createdAt": "2022-11-07T14:55:52Z",
          "updatedAt": "2022-11-07T14:55:52Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "This is addressed in the recent security considerations rewrite, under [section 7.3, Anonymizing proxies](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-anonymizing-proxies).",
          "createdAt": "2023-09-20T15:23:10Z",
          "updatedAt": "2023-09-20T15:23:10Z"
        }
      ]
    },
    {
      "number": 295,
      "id": "I_kwDOFEJYQs5O2Jsl",
      "title": "Code points for VDAFs?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/295",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Right now the task config consists of:\r\n> A unique identifier for the VDAF instance used for the task, including the\r\n> type of measurement associated with the task.\r\n\r\nIf memory serves, this text predates the CFRG's adoption of the draft. It might be appropriate this point to pick codepoints for:\r\n* Prio3Aes128Count\r\n* Prio3Aes128Sum\r\n* Prio3Aes128Histogram\r\n* Poplar1\r\n\r\nDo folks have an opinion about where these codepoints should live? Following HPKE's lead, we would define them in the VDAF draft. We can prioritize this for VDAF-03, or decide that it's too early to worry about this.",
      "createdAt": "2022-07-30T00:33:42Z",
      "updatedAt": "2022-08-17T00:08:00Z",
      "closedAt": "2022-08-17T00:08:00Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The VDAF draft seems like the right place to define the codepoints. Once `draft-irtf-cfrg-vdaf` becomes an RFC (\ud83e\udd1e\ud83c\udffb), we will want new VDAFs beyond Poplar1 and the current Prio3 family to get defined in new CFRG documents. Ideally DAP should be able to use new VDAFs without any changes, errata or addenda to the DAP document(s), so the new CFRG documents should define the VDAF codepoints.",
          "createdAt": "2022-08-03T19:59:42Z",
          "updatedAt": "2022-08-03T19:59:42Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with @tgeoghegan.",
          "createdAt": "2022-08-04T19:19:37Z",
          "updatedAt": "2022-08-04T19:19:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "SGTM. We need codepoints in the VDAF draft for other reasons (see https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/102), so we can use those if/when needed by DAP. if you want to help move this along, please have a look at https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/104.",
          "createdAt": "2022-08-16T17:38:44Z",
          "updatedAt": "2022-08-16T17:38:44Z"
        }
      ]
    },
    {
      "number": 296,
      "id": "I_kwDOFEJYQs5O2MeX",
      "title": "Align with latest VDAF draft",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/296",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "This will be VDAF-02 or 03, depending on when we need to cut DAP-02.",
      "createdAt": "2022-07-30T01:26:45Z",
      "updatedAt": "2022-09-06T16:28:12Z",
      "closedAt": "2022-09-06T16:28:12Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "VDAF-03 is out, I will send a PR by end of week. https://datatracker.ietf.org/doc/draft-irtf-cfrg-vdaf/03/",
          "createdAt": "2022-08-24T22:46:09Z",
          "updatedAt": "2022-08-24T22:46:09Z"
        }
      ]
    },
    {
      "number": 299,
      "id": "I_kwDOFEJYQs5Pt9UJ",
      "title": "Add concept of \"minimum task lifetime\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/299",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Aggregators should keep task data around for as long as the task requires. Afterwards, they can garbage collect as needed.",
      "createdAt": "2022-08-12T16:47:41Z",
      "updatedAt": "2022-08-17T21:16:44Z",
      "closedAt": "2022-08-17T21:16:44Z",
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing in favor of #291.",
          "createdAt": "2022-08-17T21:16:44Z",
          "updatedAt": "2022-08-17T21:16:44Z"
        }
      ]
    },
    {
      "number": 301,
      "id": "I_kwDOFEJYQs5QEpxQ",
      "title": "Consider making batch IDs predictable",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/301",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "branlwyd"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "For fixed-size tasks, the Leader picks a batch ID and passes it to the Collector with the CollectResp. This is so that the batch can be queried later on with a different aggregation parameter (as required for Poplar1). Alternatively, we could arrange for batch IDs to be \"predictable\" so that the Collector doesn't have to store batch IDs.\r\n\r\n\r\n_Originally posted by @chris-wood in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/297#issuecomment-1219697601_",
      "createdAt": "2022-08-18T17:25:27Z",
      "updatedAt": "2022-09-15T21:35:05Z",
      "closedAt": "2022-09-15T21:35:05Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "With the current `next-batch` semantics, there is the possibility that a Collector crashing at an inopportune time might lead to a batch being lost permanently.\r\n\r\nIf batch IDs were predictable, this would be fixable in a fairly straightforward manner (from a [PR comment](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297#discussion_r948460390)):\r\n\r\n> Also, if it's acceptable to allow batch IDs to be enumerable (e.g. a counter, as suggested by others), I think a simpler idempotent design would be to have the Collector track which batch ID is next itself, and change next-batch to something like max-batch to allow Collectors to \"start up\" without knowing what the next batch is.",
          "createdAt": "2022-08-19T22:51:56Z",
          "updatedAt": "2022-08-19T22:51:56Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Here would be my suggested change for this: \r\n* Change type of `BatchID` from `uint8[32]` to `uint64`. Maybe call it `BatchNum`, since \"ID\" is always a 32-byte string elsewhere.\r\n* Have the Collector specify the `BatchID` in the collect request. Its choice of `BatchID` is arbitrary, except that the Aggregators are free to reject IDs that won't be ready for a while.\r\n* Allow the Leader to reject `BatchID`s that are too far into the future.\r\n\r\nAnything else to consider?\r\n\r\nI'll aim to have a PR up by end of week.",
          "createdAt": "2022-08-23T15:31:46Z",
          "updatedAt": "2022-08-23T15:34:09Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "You may wish to change `next-batch` to something like `current-batch` which causes the Leader to return the batch ID that is \"current.\" (Defined as something like \"the next batch that will be completed.\") This would allow a new Collector to start up with recent data, without needing out-of-band knowledge of the current batch ID.\r\n\r\nOTOH, I'm not sure if this `current-batch` functionality is necessary for any currently-planned deployment's needs; it's also possible to say that a Collector should simply start at the first Batch ID & iterate from there to find the latest Batch ID. In that case, the `next-batch` functionality could be dropped.",
          "createdAt": "2022-08-23T22:46:55Z",
          "updatedAt": "2022-08-23T22:46:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Partly solved by #304, we'll continue on in #342, which is more specific.",
          "createdAt": "2022-09-15T21:35:05Z",
          "updatedAt": "2022-09-15T21:35:05Z"
        }
      ]
    },
    {
      "number": 302,
      "id": "I_kwDOFEJYQs5QPXo6",
      "title": "Privacy Preserving measurement protocol",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/302",
      "state": "CLOSED",
      "author": "Qhassanx",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-08-22T10:54:02Z",
      "updatedAt": "2022-09-05T02:36:38Z",
      "closedAt": "2022-08-22T16:20:40Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(closing as there is no content in this bug; @Qhassanx please feel free to add a description & reopen if desired)",
          "createdAt": "2022-08-22T16:20:40Z",
          "updatedAt": "2022-08-22T16:20:40Z"
        },
        {
          "author": "Qhassanx",
          "authorAssociation": "NONE",
          "body": "Thanks!\nThis was a mistake.\nI am very unfamiliar with GitHub.  If there are any tutorials that I can access to become more familiar with basic functionality, please let me know.\n\n-qaid\n\n\nThis message was sent from an enhanced modular device with limited telephonic/satellitic capabilities.\n\n> On Aug 22, 2022, at 12:20 PM, Brandon Pitman ***@***.***> wrote:\n> \n> \ufeff\n> (closing as there is no content in this bug; @Qhassanx please feel free to add a description & reopen if desired)\n> \n> \u2014\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n> You are receiving this because you were mentioned.\n",
          "createdAt": "2022-09-05T02:36:37Z",
          "updatedAt": "2022-09-05T02:36:37Z"
        }
      ]
    },
    {
      "number": 306,
      "id": "I_kwDOFEJYQs5Qi6xx",
      "title": "Align DAP discussion of soundness/correctness/robustness and privacy with VDAF",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/306",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "[VDAF's Security Considerations](https://github.com/cfrg/draft-irtf-cfrg-vdaf/blob/main/draft-irtf-cfrg-vdaf.md#security-considerations-security) contains a nice definition and discussion of the notions of privacy (meaning protecting any individual client's measurement from being seen) and robustness (meaning that the aggregates delivered to a collector are accurate aggregations over the inputs).\r\n\r\nDAP's security considerations has definitions of privacy and correctness, and we refer to robustness and soundness in a few places. We should align our definitions with VDAF (or just refer to VDAF's definitions) and consistently use the term \"robustness\" throughout DAP.",
      "createdAt": "2022-08-25T19:01:48Z",
      "updatedAt": "2022-09-06T16:51:59Z",
      "closedAt": "2022-09-06T16:51:59Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "PR, please!",
          "createdAt": "2022-08-25T19:56:19Z",
          "updatedAt": "2022-08-25T19:56:19Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm going to get this done for draft-02, hopefully early next week.",
          "createdAt": "2022-08-25T21:07:47Z",
          "updatedAt": "2022-08-25T21:07:47Z"
        }
      ]
    },
    {
      "number": 310,
      "id": "I_kwDOFEJYQs5Q804k",
      "title": "Update info string prefixes for HPKE",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/310",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "",
      "createdAt": "2022-09-01T01:30:13Z",
      "updatedAt": "2022-09-06T16:23:29Z",
      "closedAt": "2022-09-06T16:23:29Z",
      "comments": []
    },
    {
      "number": 311,
      "id": "I_kwDOFEJYQs5Q_1Zv",
      "title": "Consider adding max_report_size to task parameters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/311",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Today the report payload size is limited to <1..2^32-1>\r\n\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  ReportMetadata metadata;\r\n  HpkeCiphertext encrypted_input_shares<1..2^32-1>;\r\n} Report;\r\n```\r\n\r\nIn practise, it's very unlikely for client to send report of GB size, large payload also increase performance burden of aggregators. It's likely that the aggregator that need to handle GB or x100MB size of payload to be designed very differently than aggregators only need to handle xMB size payload. We can add a field in task configuration to limit the size that aggregators can accept.\r\n\r\nAlternatively we can specify this limit on the aggregators or the whole DAP deployment, but I don't know if there is a way to communicate this capability.",
      "createdAt": "2022-09-01T14:07:46Z",
      "updatedAt": "2022-09-15T13:26:20Z",
      "closedAt": "2022-09-15T13:26:20Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think the maximum input size of a report will be a function of the VDAF in use. e.g., a report for `Prio3Aes128Sum` with `bits = 16` will always have the same size. I think the same is true for Poplar1, where you have to choose a bit length for the strings being aggregated.\r\n\r\nAny report that has a different size will be rejected during decoding (as @simon-friedberger and I learned during a test this week). If we introduce a size constraint at the DAP level, then I worry that we are introducing an opportunity for error by having a DAP-level `max_report_size` that disagrees with the report size dictated by the VDAF.\r\n\r\n",
          "createdAt": "2022-09-01T16:53:24Z",
          "updatedAt": "2022-09-01T16:53:24Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with @tgeoghegan, the expected size of the input shares is determined entirely by the VDAF and its parameters. This in turn is determined by the task config, so it should be sufficient to \"opt in\" to that task.\r\n\r\nThe same is true for Poplar1. In particular, each measurement is a bit string of some fixed length. For what it's worth, the public share + input shares is about 16.3 KB for 256-bit inputs.",
          "createdAt": "2022-09-01T20:05:30Z",
          "updatedAt": "2022-09-01T20:05:30Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@wangshan is there anything else you'd like to discuss for this issue or can we close it out?",
          "createdAt": "2022-09-13T01:06:15Z",
          "updatedAt": "2022-09-13T01:06:15Z"
        }
      ]
    },
    {
      "number": 314,
      "id": "I_kwDOFEJYQs5RRMCI",
      "title": "Editoral: Use underscores in enum variants instead of dashes",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/314",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We use dashes in enums:\r\n\r\n```\r\n  enum {\r\n    batch-collected(0),                                                             \r\n    report-replayed(1),\r\n    report-dropped(2),                                                              \r\n    hpke-unknown-config-id(3),\r\n    hpke-decrypt-error(4),\r\n    vdaf-prep-error(5),\r\n    batch-saturated(6),\r\n  } ReportShareError;\r\n```\r\n\r\nI'm not sure how got on this kick (might be my fault!), but I would prefer underscores, mainly for consistency with RFC 8446:\r\n\r\n```\r\n  enum {\r\n    batch_collected(0),                                                             \r\n    report_replayed(1),\r\n    report_dropped(2),                                                              \r\n    hpke_unknown_config_id(3),\r\n    hpke_decrypt_error(4),\r\n    vdaf_prep_error(5),\r\n    batch_saturated(6),\r\n  } ReportShareError;\r\n```\r\n\r\n1. Does anyone have a strong preference for dashes? \r\n2. Does anyone object to landing this in DAP-02?",
      "createdAt": "2022-09-06T15:25:32Z",
      "updatedAt": "2022-09-19T19:07:44Z",
      "closedAt": "2022-09-19T19:07:44Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "1. No. The argument for consistency with RFC 8446 is good enough for me.\r\n2. I think we could take this in draft-02 if we really wanted to, but since this change has no actual impact on implementations (the enum variant names never appear in encoded messages), I think we could err on the side of allowing a reasonable span of time for discussion in the WG and do this in draft-03.",
          "createdAt": "2022-09-06T16:49:11Z",
          "updatedAt": "2022-09-06T16:49:11Z"
        }
      ]
    },
    {
      "number": 315,
      "id": "I_kwDOFEJYQs5RRMo-",
      "title": "Update CL for DAP-01 -> DAP-03",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/315",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "",
      "createdAt": "2022-09-06T15:27:13Z",
      "updatedAt": "2022-09-19T18:59:15Z",
      "closedAt": "2022-09-19T18:59:15Z",
      "comments": []
    },
    {
      "number": 316,
      "id": "I_kwDOFEJYQs5RSy9A",
      "title": "Consider tying nonces to shares within a batch.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/316",
      "state": "CLOSED",
      "author": "hannahdaviscrypto",
      "authorAssociation": "NONE",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "collecting a batch more than once"
      ],
      "body": "DAP uses a checksum over the report nonces to make sure that the same set of reports are included in repeated batches, but it does not verify the consistency of the ciphertexts  and input shares themselves between collect requests. In situations where max_batch_lifetime >=2, it may be possible for a a malicious leader to change the input share for a given nonce between two collect requests over the same batch interval. Replays are currently prevented by putting the nonce in the AAD of HPKE for client reports; however this does not protect against nonce replays with changed input shares by malicious clients. This could be of particular concern for Poplar, where multiple collect requests are used to explore a binary tree that is expected not to change between aggregations. \r\n\r\nCould we consider additional verification strategies for honest aggregators/helpers to ensure that every repeated batch interval contains the same input shares as well as the same nonces?",
      "createdAt": "2022-09-06T22:31:29Z",
      "updatedAt": "2024-05-21T20:58:53Z",
      "closedAt": "2024-05-21T20:58:53Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is a really neat attack. I think this would be mitigated if we wind up doing direct uploads from client to helper (tracked in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/130). But unless there's some kind of authentication scheme in play, a leader can still race with the clients to submit reports with the same nonces but different contents, so I wonder whether this falls into the general category of Sybil attacks?",
          "createdAt": "2022-09-06T23:08:51Z",
          "updatedAt": "2022-09-06T23:08:51Z"
        },
        {
          "author": "hannahdaviscrypto",
          "authorAssociation": "NONE",
          "body": "Yes, I think direct uploads would absolutely solve the problem. \r\n\r\nI think this is definitely related to Sybil attacks but goes slightly further because the leader can replace honest reports or change malicious reports after they have already been collected one or more times in addition to its expected ability to impersonate arbitrary clients at the start of the protocol.",
          "createdAt": "2022-09-07T22:59:06Z",
          "updatedAt": "2022-09-07T22:59:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "An alternative to direct uploads is to ensure the input shares are sent precisely once (and not resent with each AggreagteInitializeReq). This would also go part of the way towards addressing #130, since it reduces communication cost by a factor of `max_batch_lifetime - 1`.",
          "createdAt": "2022-09-10T01:03:10Z",
          "updatedAt": "2022-09-10T01:03:10Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Replays are currently prevented by putting the nonce in the AAD of HPKE for client reports; however this does not protect against nonce replays with changed input shares by malicious clients.\r\n\r\n**First**, there is definitely something fishy in the spec regarding nonces.\r\nIMHO the leader can just discard reports with a repeated nonce but it's indeed not clear:\r\n\r\n> _To detect replay attacks, each aggregator keeps track of the set of nonces pertaining to reports that were previously aggregated for a given task. If the leader receives a report from a client whose nonce is in this set, it either ignores it or aborts the upload sub-protocol as described in [Section 4.3](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#upload-flow). A Helper who receives an encrypted input share whose nonce is in this set rejects the report as described in [Section 4.4.1.4](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#input-share-batch-validation)._\r\n\r\nWhat is this doing: \"previously aggregated for a given task\"? Shouldn't nonce repetition just be forbidden?\r\n\r\nISTM that the client is sending a report once hence a nonce against replay makes sense. But, if the helper is receiving  shares multiple times, a nonce isn't doing it's job anymore.\r\n\r\n**Second**, it's not clear how the actual batch validation is supposed to work:\r\n\r\n> [4.5.6. ](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.6)[Batch Validation](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-batch-validation)\r\n> \r\n> Before an Aggregator responds to a CollectReq or AggregateShareReq, it must first check that the request does not violate the parameters associated with the DAP task. It does so as described here.\r\n\r\nNote that the `CollectReq` goes to the leader who should be able to just discard repeated nonces and the `AggregateShareReq` goes to the helper but it doesn't actually contain the shares. Those had been sent with the `AggregateInitializeReq`. So something needs to be fixed here.\r\n",
          "createdAt": "2022-09-12T07:24:45Z",
          "updatedAt": "2022-09-12T07:24:45Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I would propose that we change the nonce usage to:\r\n\"[...] each aggregator keeps track of the set of nonces it has previously received, for the currently valid reporting period. If it receives a report whose nonce is in this set, [...]\". The part about the currently valid reporting period allows throwing away the list of nonces at some point.\r\n\r\nThis implies Chris' proposal of sending input shares only once.",
          "createdAt": "2022-09-12T07:36:31Z",
          "updatedAt": "2022-09-12T07:36:31Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> What is this doing: \"previously aggregated for a given task\"? Shouldn't nonce repetition just be forbidden?\r\n\r\nThe original text ensures that nonces only need to be unique inside of a task. While nonces _should_ be universally-unique (assuming clients construct them properly, i.e. by reading bytes from a CSPRNG), I'd prefer if the spec text only required aggregators to look for conflicts within a given task. (i.e. tasks should be _isolated_ from one another)\r\n\r\nI do like the idea of allowing aggregators to \"forget\" report nonces after awhile. The spec currently allows for this: aggregators can drop uploads that are too old (or for any other reason). Since the identifier of a report effectively includes the timestamp, this means aggregators can forget nonces that are attached to timestamps that are \"too old\". I think this is true for both `time-interval` & `fixed-size` tasks. (though I wonder if some of the wording around this was blurred with the introduction of `fixed-size` tasks, which split the timestamp out from the nonce)\r\n\r\n===\r\n\r\n> Second, it's not clear how the actual batch validation is supposed to work\r\n\r\nThe referenced \"batch validation\" section covers validation of batch parameters (number of reports, batch boundaries), and is evaluated during collection to ensure the batch being collected meets all of the requirements. Note that none of the checks require knowing the shares, only how many shares there are (and other information like batch parameters & task configuration).\r\n\r\nNonce replay protection is described in the [Anti-replay](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-anti-replay) section, and I believe it is referenced from appropriate places such that the aggregators have all the information they need to evaluate the protection.",
          "createdAt": "2022-09-12T17:40:49Z",
          "updatedAt": "2022-09-12T18:36:04Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@simon-friedberger I submitted https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/333 last week to cover the missing text about the leader rejecting duplicate nonces during the upload protocol.",
          "createdAt": "2022-09-12T18:29:32Z",
          "updatedAt": "2022-09-12T18:29:32Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd \r\n\r\n> The original text ensures that nonces only need to be unique inside of a task. \r\n\r\nAh, that's fine. I thought there was something more to the statement, e.g. \"previously aggregated\" not necessarily meaning \"received\" but that they have been used in an aggregation.\r\n\r\n> > Second, it's not clear how the actual batch validation is supposed to work\r\n> \r\n> The referenced \"batch validation\" section covers validation of batch parameters (number of reports, batch boundaries), and is evaluated during collection to ensure the batch being collected meets all of the requirements. Note that none of the checks require knowing the shares, only how many shares there are (and other information like batch parameters & task configuration).\r\n\r\nIt also includes the \"overlap check\":\r\n\r\n> _Finally, the Aggregator checks that the batch does not contain a report that was included in any previous batch. If this batch overlap check fails, then the Aggregator MUST abort with error \"batchOverlap\". For time-interval tasks, it is sufficient (but not necessary) to check that the batch interval does not overlap with the batch interval of any previous query. If this batch interval check fails, then the Aggregator MAY abort with error \"batchOverlap\"._\r\n\r\nI assume this is also based on the nonce. Correct?\r\n\r\n@tgeoghegan \r\n\r\n> @simon-friedberger I submitted #333 last week to cover the missing text about the leader rejecting duplicate nonces during the upload protocol.\r\n\r\nSeems reasonable, can we do the same for the helper?\r\n",
          "createdAt": "2022-09-12T19:58:00Z",
          "updatedAt": "2022-09-12T19:58:00Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> I assume this is also based on the nonce. Correct?\r\n\r\nI think this check actually doesn't require inspecting any particular nonces -- for `time-interval` tasks this is effectively checking that there is no overlap with the time intervals in previous collect requests[1]; for `fixed-size` tasks I think things are even simpler, there is no overlap condition since batches are arbitrarily identified & the mapping of reports to batches is controlled by the aggregators.\r\n\r\nThe wording \"checks that the batch does not contain a report that was included in any previous batch\" is somewhat unclear, though -- it could be fairly understood as saying that checking nonces is required, even though the particulars of how this check is implemented does not require examining nonces. Maybe a clarification is in order?\r\n\r\n[1] Though as the text notes, this is a sufficient but not necessary condition -- an aggregator implementation could also do a more precise check, allowing overlap in collect intervals as long as the overlapping portion of the intervals does not contain any reports. Determining if the overlapping portion of the intervals contains any reports would indeed require checking individual reports' timestamps, which is similar to having to check nonces. However, I'm not sure of a practical reason an implementation would want to do this.",
          "createdAt": "2022-09-12T20:11:35Z",
          "updatedAt": "2022-09-12T20:11:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 I volunteered to implement multi-collection and resolve all related open issues.",
          "createdAt": "2023-11-08T15:07:49Z",
          "updatedAt": "2023-11-08T15:07:49Z"
        }
      ]
    },
    {
      "number": 317,
      "id": "I_kwDOFEJYQs5Rdg45",
      "title": "Update HTTP Caching Ref",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/317",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I-D Nits complains of outdated reference for:\r\n\r\n    ** Obsolete normative reference: RFC 7234 (Obsoleted by RFC 9111)\r\nWe should update tit.",
      "createdAt": "2022-09-08T16:58:18Z",
      "updatedAt": "2022-09-09T18:00:01Z",
      "closedAt": "2022-09-09T18:00:00Z",
      "comments": []
    },
    {
      "number": 319,
      "id": "I_kwDOFEJYQs5RdjUz",
      "title": "Alphabetize Terms",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/319",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "This is purely editorial and would probably be done later, but as ekr always says \"PRs welcome.\"",
      "createdAt": "2022-09-08T17:06:07Z",
      "updatedAt": "2022-09-09T17:59:46Z",
      "closedAt": "2022-09-09T17:59:46Z",
      "comments": []
    },
    {
      "number": 321,
      "id": "I_kwDOFEJYQs5RdlNl",
      "title": "Add \"Helper\" to Term Section",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/321",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "All the other actors are in Figure 1 are included except Helper.",
      "createdAt": "2022-09-08T17:11:40Z",
      "updatedAt": "2022-09-09T18:00:31Z",
      "closedAt": "2022-09-09T18:00:31Z",
      "comments": []
    },
    {
      "number": 324,
      "id": "I_kwDOFEJYQs5Ri6RT",
      "title": "Size for Role, FixedSizeQueryType, ReportShareError, & PrepareStepResult",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/324",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Specify a size for Role, FixedSizeQueryType, ReportShareError, & PrepareStepResult.",
      "createdAt": "2022-09-09T17:13:52Z",
      "updatedAt": "2022-09-10T00:42:38Z",
      "closedAt": "2022-09-10T00:42:38Z",
      "comments": []
    },
    {
      "number": 328,
      "id": "I_kwDOFEJYQs5RjAMX",
      "title": "s4.4.1.5: agg_id",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/328",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Is there any reason that agg_id values can't match the rest of the document? In agg_id, leader is 0x00 and helper is 0x01, but everywhere else it's 0x01 and 0x02 for leader and helper, respectively.",
      "createdAt": "2022-09-09T17:33:48Z",
      "updatedAt": "2023-10-13T20:31:00Z",
      "closedAt": "2023-10-13T20:31:00Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is an impedance mismatch between the roles defined by DAP and the roles defined by VDAF. VDAF doesn't really have a notion of a client or a collector, and so the only roles are the aggregators, who get numbered from 0.\r\n\r\nIn DAP, we have those two additional roles, and the role identifiers are used in HPKE encryption, among other places. We could have used leader = 0, helper = 1 in DAP to line up with VDAF, but we decided to put the aggregators at the end of the role enum so that if we allow more aggregators at some point, their IDs will be continuous with the two existing ones.\r\n\r\nAt some point we'll have to make a decision about whether DAP will ever support more than two aggregators. Even if it does, that protocol may not be backward compatible with this DAP for myriad reasons, at which point reassigning the role IDs is a minor disruption.",
          "createdAt": "2022-09-09T18:10:55Z",
          "updatedAt": "2022-09-09T18:10:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'd be fine with re-assigning IDs. Another option would be to define `agg_id` to be `role - 2` or something so that we define the variable exactly once.",
          "createdAt": "2022-09-10T00:55:45Z",
          "updatedAt": "2022-09-10T00:55:45Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Another suggestion to resolve this impedance mismatch:\r\n\r\nCurrently the concept of \"roles\" are used for two purposes:\r\n(a) determining the source or destination of a given HPKE-encrypted message\r\n(b) mapping to an `agg_id` for relevant VDAF operations\r\n\r\nNotably, (a) requires being able to represent non-aggregator roles (i.e. client/collector), while (b) only requires that aggregator roles be represented.\r\n\r\nWe could instead:\r\n(a) assign each aggregator an `agg_id` value (e.g. leader = 0, helper = 1); this value is used directly for VDAF operations.\r\n(b) encode the HPKE role as a potentially-multibyte value. (e.g. aggregators are `0x00 || agg_id`; client is `0x01`; collector is `0x02`).\r\n\r\nThe upsides, as I see them, are that we avoid arbitrary math like `agg_id = role - 2`, and we can easily extend to more than two aggregators should we wish to in the future.\r\n\r\nMore opinionatedly, in my experience as an implementor, there are many places where a generic \"role\" (leader/helper/client/collector) is specified where actually we want to specify an aggregator (leader/helper). An implementation can avoid some unnecessary error-handling by splitting \"role\" from \"aggregator-specific role\". It might therefore make sense to take a similar strategy at the design level.\r\n\r\nThe most obvious downside is that we possibly use an extra byte for roles during HPKE encryption; I think this is marginal since the role is part of the (implicit) `info` parameter, i.e. this extra role byte does not translate to an extra ciphertext byte. I think it's OK to use a variable-length encoding here w/o risking an ambiguous message as the message is still unambiguously parsable, but please check this assertion. [edit: specifically, I think that the `info` parameter to HPKE operations, which is something like `\"dap-01 input share\" || sender_role || receiver_role`, is unambiguously parsable even if `sender_role` & `receiver_role` are variable-length but unambiguously parsable.]",
          "createdAt": "2022-09-12T22:35:06Z",
          "updatedAt": "2022-09-12T22:42:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Just to reiterate a requirement discussed on pr #335: We can't change the value of agg_id passed to VDAF.prep_init() without breaking compatibility with VDAF-03. In particular, for the leader this value MSUT be 0 and for the helper this value MUST be 1.\r\n\r\nIf you feel like VDAF needs to be designed in a way that makes this *not* a breaking change, please follow up on https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/40.",
          "createdAt": "2022-09-15T19:25:48Z",
          "updatedAt": "2022-09-15T19:25:48Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Now that we've settled on two Aggregators for DAP, I think the delta here is primarily superficial (To match the VDAF spec, we'd change 2 -> 0 and 3 -> 1.) I'm gonna close this issue, but I suppose if anyone feels strongly about aligning these things, please send a PR and I suspect we'd merge it.",
          "createdAt": "2023-10-13T20:31:00Z",
          "updatedAt": "2023-10-13T20:31:00Z"
        }
      ]
    },
    {
      "number": 329,
      "id": "I_kwDOFEJYQs5RjCc4",
      "title": "No public_share in a Report?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/329",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The presentation syntax is:\r\n\r\n    opaque public_share<0..2^32-1>;\r\n\r\nAre there ever going to be no public shares in a Report?",
      "createdAt": "2022-09-09T17:42:31Z",
      "updatedAt": "2022-09-14T22:57:11Z",
      "closedAt": "2022-09-14T22:57:11Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "EDITED: Yes, Prio3 has an empty public share. This is mainly needed for Poplar1. From the [VDAF change log](https://www.ietf.org/archive/id/draft-irtf-cfrg-vdaf-03.html#name-change-log):\r\n\r\n> Extend (V)DAF syntax to include a \"public share\" output by the Client and distributed to all of the Aggregators. This is to accommodate \"extractable\" IDPFs as required for Poplar1. (See [[BBCGGI21](https://www.ietf.org/archive/id/draft-irtf-cfrg-vdaf-03.html#BBCGGI21)], Section 4.3 for details.)",
          "createdAt": "2022-09-09T17:58:57Z",
          "updatedAt": "2022-09-09T18:01:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@seanturner if you want, we can add text that makes sure your question is answered. Otherwise I don't think there's any action to take here.",
          "createdAt": "2022-09-10T01:00:59Z",
          "updatedAt": "2022-09-10T01:00:59Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Are there ever going to be no public shares in a Report?\r\n\r\nI think the answer should be: \"Yes, Prio3 doesn't need public shares.\"",
          "createdAt": "2022-09-12T08:03:10Z",
          "updatedAt": "2022-09-12T08:03:10Z"
        },
        {
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Thanks for the update.",
          "createdAt": "2022-09-13T03:34:55Z",
          "updatedAt": "2022-09-13T03:34:55Z"
        }
      ]
    },
    {
      "number": 330,
      "id": "I_kwDOFEJYQs5RjFz1",
      "title": "IANA Consideration for Query Type",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/330",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "s4.1 includes the following:\r\n\r\n     Future specifications can introduce new query types as needed. \r\nThat means this I-D needs an IANA consideration to address registering a query type.",
      "createdAt": "2022-09-09T17:55:35Z",
      "updatedAt": "2022-09-12T13:44:01Z",
      "closedAt": "2022-09-12T13:44:01Z",
      "comments": []
    },
    {
      "number": 332,
      "id": "I_kwDOFEJYQs5RjcLY",
      "title": "Review s4.1",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/332",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "editorial",
        "draft-08"
      ],
      "body": "I was going to rework s4.1 because it repeats the statement about the parameters being defined in subsections below, but I wasn't sure that I would get the flow right.",
      "createdAt": "2022-09-09T19:33:33Z",
      "updatedAt": "2023-10-17T14:55:58Z",
      "closedAt": "2023-10-17T14:55:58Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree this could be improved. I'd be happy to review a PR, but let's perhaps wait for DAP-02 to be cut first?",
          "createdAt": "2022-09-10T00:54:11Z",
          "updatedAt": "2022-09-10T00:54:11Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "#504 removes the first.",
          "createdAt": "2023-10-13T20:33:53Z",
          "updatedAt": "2023-10-13T20:33:53Z"
        }
      ]
    },
    {
      "number": 334,
      "id": "I_kwDOFEJYQs5RkGvp",
      "title": "Aggregator behavior in case of unrecognized extension?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/334",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "It doesn't seem like we specify this right now. Ideally we would want that the Aggregator can safely ignore an extension it doesn't support. (And require extensions to be designed accordingly.) Is there any reason to do something else?",
      "createdAt": "2022-09-09T23:48:20Z",
      "updatedAt": "2022-10-19T15:27:48Z",
      "closedAt": "2022-10-19T15:27:48Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "We might eventually have a class of extensions that are \"critical\" in the sense that an aggregator that can't understand the extension should abort processing entirely. (motivating strawman example: maybe the extension details some extra privacy parameters/requirements, and the client would prefer that an honest aggregator give up rather than attempt to process a report if it does not implement those parameters/requirements)\r\n\r\nIf so, we might take a page from X.509 (e.g. TLS certificates): extensions could have a \"critical bit\". If an aggregator ever sees an extension it does not understand with the \"critical bit\" set, it must abort. (It may ignore non-critical extensions it does not understand.) The X.509 version of this functionality is detailed in [RFC 5280 section 4.2](https://datatracker.ietf.org/doc/html/rfc5280#section-4.2).\r\n\r\nOn the other hand, it's unclear that we need a mechanism like this currently, and it's simpler not to specify this behavior. Notably, TLS handshake messages have their own notion of extensions, and they do not include similar \"criticality\" information.",
          "createdAt": "2022-09-10T00:20:30Z",
          "updatedAt": "2022-09-10T00:21:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we should avoid \"critical bit\" semantics for as long as possible, given how this has played out in the web PKI. (Many TLS stacks ignore critical extensions.) I'm strongly in favor of adopting TLS-style semantics, which puts it on extensions to make sure they can be ignored safely.\r\n\r\nPut another way: If ever we are in a situation where we want an extension to be \"critical\", we should instead consider incorporating that extension into the core DAP protocol.",
          "createdAt": "2022-09-10T00:58:04Z",
          "updatedAt": "2022-09-10T00:59:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed in favor #369, which states the problem more clearly. Plus, Tim nicely laid out the options there.",
          "createdAt": "2022-10-19T15:27:48Z",
          "updatedAt": "2022-10-19T15:27:48Z"
        }
      ]
    },
    {
      "number": 340,
      "id": "I_kwDOFEJYQs5Rxa4l",
      "title": "Determining query type before parsing a request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/340",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "The query type is determined by the task config, which in turn is determined by the task ID. The task ID is encoded by the message, so it is necessary to parse the task ID before the Aggregator knows how to parse it.\r\n\r\nPR https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297 introduced the following problem: If an Aggregator implements multiple query types (\"time-interval\" and \"fixed-size\", say), then a message's format depends on the query type for the task. This means it is necessary to parse the task ID before the rest of the message.\r\n\r\nThis is perfectly implementable, but it has two downsides:\r\n1. This likely breaks abstraction boundaries in current implementations and will require refactoring. (For example, [here is how reports are handled in Daphne](https://github.com/cloudflare/daphne/blob/main/daphne/src/roles.rs#L251-L255). Notice that the message is parsed in one step.)\r\n2. Going forward, we'll need to make sure that the task ID always appears first in any DAP message so that it can be easily parsed. This is already true today, but we might want to add some text to Operational Considerations that spells this requirement out so that we minimize the chance of regression.\r\n\r\nHere's my question for folks: Do we want to try and do something about this? I can think of a couple of solutions:\r\n1. Add the query type byte to messages whose format depends on this. One downside is that it creates a new error case for the Aggregator to check, namely, that the query type indicated in the message matches the query type of the task. (In fact, we tried to avoid this in #297.)\r\n2. Move the task ID out of the message payload and into the URL query string, or perhaps the HTTP header. The only downside to this is that it complicates request authentication for deployments that want to sign the payload and bind it to the task config.  (This may be desirable from a security perspective.) One possible upshot is that it might put us in a better position to address #278. (@tgeoghegan please weigh in here!)",
      "createdAt": "2022-09-13T19:27:25Z",
      "updatedAt": "2022-09-15T18:20:00Z",
      "closedAt": "2022-09-15T18:20:00Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I noticed this issue while working on the implementation of \"fixed-size\". I think my preference would be to do nothing here, i.e., shift the burden to implementations and maybe make a note in Operational Considerations.",
          "createdAt": "2022-09-13T19:28:56Z",
          "updatedAt": "2022-09-13T19:34:28Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I thought about this too, but I was waiting to finish Janus' DAP-02 implementation to see how impactful it was. (implementation is still ongoing, I'll update this bug w/ my thoughts once we're closer to done)\r\n\r\nIf we do decide to do something, I think I'd prefer solution #2. We've had a few issues now of the form \"correctly parsing this message requires knowing the task ID, which requires parsing part of the message.\" This is technically feasible (as you note), but is fairly awkward from an implementation standpoint (IMO). Moving the task ID out from the message sidesteps this problem, and IMO fits well with #278.\r\n\r\nI think solution #1 would allow implementations to have simpler parsing logic, but I'm not sure this upside is worth the cost of an additional byte on the wire & an additional error case. Though I suppose this new error case exists either way -- but without a discriminator byte, the error case would most likely present itself as a message parsing failure. (e.g. \"message has too many/too few bytes\")",
          "createdAt": "2022-09-13T20:44:40Z",
          "updatedAt": "2022-09-13T20:44:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "After conferring with @wangshan, @MichaelScaria et al offline, we discovered another consideration here which makes me lean towards making protocol messages self-describing (solution (1.)).\r\n\r\nWe're working on an [extension that would change the semantics of the task ID](https://github.com/wangshan/draft-wang-ppm-dap-taskprov). At a high-level, an Aggregator that supports this extension uses information provided by the client (in a Report extension) to derive the task ID. This information includes, among other things, the query type to use for the task.\r\n\r\nThe problem is that decoding of the `AggregateInitializeReq` depends on the query type, which, when this extension is in-use, is not known until parsing the report extensions:\r\n\r\n```\r\n  struct {\r\n    TaskID task_id;\r\n    AggregationJobID job_id;\r\n    opaque agg_param<0..2^16-1>;\r\n    select (query_type) { /* BUG: This is not known until report_shares is parsed. */\r\n      case time-interval: Empty;\r\n      case fixed-size: BatchId batch_id;\r\n    };\r\n    ReportShare report_shares<1..2^32-1>;\r\n  } AggregateInitializeReq;\r\n  ~~~\r\n```\r\n\r\nIf we take solution (1.), then `query_type` is encoded by `AggreagtInitializeReq`, thus resolving this bug. There are other ways to resolve this. I can think of a few possibilities if there were an extension field in `AggregateInitializeReq`. However there are lots of things to consider for this sort of change, and I would prefer we wait until later.\r\n\r\nDoes anyone object to taking solution (1.)? I will have a PR ready for this by the end of the week. I would like to land this in DAP-02 if at all possible.",
          "createdAt": "2022-09-14T19:30:23Z",
          "updatedAt": "2022-09-14T19:30:23Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "PR is up, would appreciate review.",
          "createdAt": "2022-09-14T23:20:02Z",
          "updatedAt": "2022-09-14T23:20:02Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think that the right solution in the long run is Chris' #2, namely putting the task ID, and anything else needed to parse the request body, into the HTTP request path. But I'm OK with taking #341 for now in the interest of making `draft-02` implementable. Sure, we'll have to take the query type byte out of these messages later, but there are quite a few messages that currently have a `TaskID` or `AggregationJobID` member that will get moved up into the request path in draft-03 (\ud83e\udd1e\ud83c\udffb)\r\n\r\n> The only downside to this is that it complicates request authentication for deployments that want to sign the payload and bind it to the task config. \r\n\r\nI think there's plenty of prior art on including things like HTTP headers, path or query params in request signatures. See for instance [AWS v4 request signatures](https://docs.aws.amazon.com/general/latest/gr/sigv4-create-canonical-request.html), which explains how to canonicalize an HTTP request into a string that can be \"signed\" (well, it's an HMAC, but AWS insists that it's a signature).",
          "createdAt": "2022-09-14T23:58:41Z",
          "updatedAt": "2022-09-15T00:01:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Option (2.) doesn't quite solve my extension problem (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/340#issuecomment-1247211935), but I have no objection to moving the task ID (and maybe agg job ID) out of the message in any case. Seems to me like a way cleaner design that would likely solve other problems.",
          "createdAt": "2022-09-15T00:33:50Z",
          "updatedAt": "2022-09-15T02:18:46Z"
        }
      ]
    },
    {
      "number": 342,
      "id": "I_kwDOFEJYQs5R9oyF",
      "title": "Fixed-size queries: Discovery of batch IDs is unspecified",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/342",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "For tasks that support fixed-size queries, the Collector indicates in its `CollectReq` the ID of the batch it wishes to aggregate. At the same time, the Leader is responsible for assigning ID, and it is free to chose any `BatchID` it wishes. (It may, for example, choose them at random.) This is to allow the Leader to opportunistically aggregate reports as they arrive, rather than wait for a request from the Collector.\r\n\r\nThe current draft doesn't specify a means by which the Collector learns these batch IDs. This is instead left up to the deployment, which presents interop issues for Collectors and Leaders that are organizationally separated.\r\n\r\nWe explored a couple possible solutions:\r\n1. Require the batch IDs to be predicable (#301), for example, by requiring the Leader to use a counter for picking the next batch ID. The concern here is that this potentially complicates implementation of the Leader.\r\n2. \"next-batch\" functionality: Initially (#297) we had spelled out a method by which the Leader could request the \"next batch\" and the Leader would respond with a `BatchID`. This solves the problem, but creates another: It requires the Collector to remember batch IDs that it needs to query again.\r\n3. \"current-batch\" functionality (#308): Like \"next-batch\", except that it adds fault tolerance in case the Collector loses a recently used batch ID.",
      "createdAt": "2022-09-15T21:33:01Z",
      "updatedAt": "2022-10-25T16:12:49Z",
      "closedAt": "2022-10-25T16:12:49Z",
      "comments": [
        {
          "author": "bhalleycf",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Schemes like \"next-batch\" tend to require durable state on the server associated with some sort of stable client id, as the client might not connect for some time and need to catch up on multiple batches.  Also they tend to have race issues related to failure.  (E.g. get next id then crash, or even \"get next id, durably ack it, and then get hit by meteor and destroyed\").  Another issue is that if something bad happens to the cluster there's no signal that there has been discontinuity in the data to the Collector.  For this kind of thing a (generation, sequence) tuple is often good.   The generation is an arbitrary 64-bit number that is changed any time there is a discontinuity.  The sequence starts at 1 in each generation and increases strictly sequentially.  State is stored on the client(s).  If the client has no stored state, it says it has version (0, 0).  The server always reports the version it is returning, and starts with (current-generation, least-existing-sequence-number) if it gets a (0, 0) request.  Alternatively, there can be a way for the client to get the least existing version with a separate method.  There are no races with this scheme, no server state, and discontinuities are visible.  One possible enhancement for the single-Collector use case is to have an \"I am done with (generation, tuple)\" method in the API, so a Collector that has made the aggregate durable to its satisfaction can advise the aggregation system that it is ok to release resources (rather than waiting for them to expire or be force-deleted for size reasons).",
          "createdAt": "2022-10-05T19:22:01Z",
          "updatedAt": "2022-10-05T19:23:12Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Resolving this issue should resolve the redundancy between `struct BatchSelector` and `struct Query` described in #366 (which is closed as a dupe of this).",
          "createdAt": "2022-10-13T21:19:04Z",
          "updatedAt": "2022-10-13T21:19:04Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "One more data point here: For e2e tests in Daphne, I ended up implementing basically what amounts to a \"current-batch\" functionality. I'd say let's adopt Brandon's proposal and call it a day :) https://github.com/cloudflare/daphne/pull/134/files#diff-ad5c7b3e9dda27f4fbc34d6243f6cf199840aa205af84a0a0a8474469ff081d1",
          "createdAt": "2022-10-17T22:31:53Z",
          "updatedAt": "2022-10-17T22:31:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #313 ",
          "createdAt": "2022-10-25T16:12:49Z",
          "updatedAt": "2022-10-25T16:12:49Z"
        }
      ]
    },
    {
      "number": 349,
      "id": "I_kwDOFEJYQs5SKLyk",
      "title": "Consider reducing IDs from 32 bytes to 16",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/349",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "The draft defines the following types as `uint8_t[32]`:\r\n* TaskID\r\n* BatchID\r\n* AggregationJobID\r\n\r\nI believe the draft RECOMMENDS that most of these be generated randomly. However, the spec permits different choices here, as the main requirement is *uniqueness*, not necessarily randomness/unpredictability. For example, it is allowed (and perfectly reasonable) to set TaskID to the hash of some unique \"context\" agreed upon by all of the parties.\r\n\r\nI think 32 bytes is overkill for these purposes.\r\n\r\nWhy 32 bytes? Well, the idea is that an ID could be set to the SHA-256 hash of some string and rely on the collision resistance (CR) of SHA-256 for uniqueness. If we had to truncate the hash, say to 16 bytes, then we could no longer rely on CR for SHA-256, since finding a collision against truncated SHA-256 might be significantly easier.\r\n\r\nWhat's the impact of an ID collision?:\r\n* TaskID: An encrypted input share intended for one task might get decrypted and consumed in another. But if the VDAF is robust, then in the input share will get rejected with high probability. On the other hand, for DAFs (VDAF sans verification), aggregate results would get garbled.\r\n* BatchID: It's not clear which aggregate result to return the Collector.\r\n* AggregationJobID: Helper ends up iin a weird state, causing an aggregation job to fail.\r\n\r\nIn any case, it is still the responsibility of the parties in the protocol to make sure that all IDs are unique in the relevant context.  In particular, the impacts above are ruled out by a correct implementation.\r\n\r\nAssuming you buy this argument, the question that remains is: Why should we bother shortening this? We don't save much in terms of communication overhead. My primary consideration here is implementation: A 16-byte ID is much easier to stuff into a log than a 32-byte ID.",
      "createdAt": "2022-09-19T19:07:31Z",
      "updatedAt": "2022-12-08T21:28:54Z",
      "closedAt": "2022-12-08T21:28:54Z",
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton A related issue: report ID is defined differently than the others:\r\n```\r\nopaque TaskID[32];\r\nopaque BatchID[32];\r\nuint8 ReportID[16];\r\nopaque AggregationJobID[32];\r\n```\r\nshould it be a uint8 type or opaque as well",
          "createdAt": "2022-10-05T23:32:40Z",
          "updatedAt": "2022-10-05T23:32:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, that was intentional: ReportID is the new name for the 16-byte random nonce we had in the report metadata. We definitely don't need 32 bytes for this!\r\n\r\nEDIT: I have no idea if uint8 or opaque is preferred. I've heard both. cc/ @chris-wood, @ekr",
          "createdAt": "2022-10-06T00:52:37Z",
          "updatedAt": "2022-10-06T00:53:29Z"
        }
      ]
    },
    {
      "number": 350,
      "id": "I_kwDOFEJYQs5SOdPC",
      "title": "Rationale for 2-byte QueryType instead of 1?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/350",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "QueryType is 2 bytes:\r\n```\r\nenum {\r\n   reserved(0), /* Reserved for testing purposes */\r\n   time_interval(1),\r\n   fixed_size(2),\r\n   (65535)\r\n} QueryType;\r\n```\r\n\r\nDid we ever discuss a particular rationale for this? I can't imagine needing as many as 65,535 distinct query types; 1-byte (255 distinct types) is definitely sufficient.",
      "createdAt": "2022-09-20T14:29:38Z",
      "updatedAt": "2022-09-23T14:57:39Z",
      "closedAt": "2022-09-22T19:44:56Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, I agree, it seems like 255 kinds of queries ought to suffice.",
          "createdAt": "2022-09-20T14:46:44Z",
          "updatedAt": "2022-09-20T14:46:44Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "This is definitely a bike shed, but an extra byte here doesn't seems worth trimming. I would leave this as-is. \ud83e\udd37 ",
          "createdAt": "2022-09-20T16:01:42Z",
          "updatedAt": "2022-09-20T16:01:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree it's a bike shed. If someone wants to send a PR, feel free. Otherwise let's park this.",
          "createdAt": "2022-09-20T16:13:42Z",
          "updatedAt": "2022-09-20T16:13:42Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Every report will have the extra byte if in-band task configuration is used, so would be nice to have this.",
          "createdAt": "2022-09-22T17:00:43Z",
          "updatedAt": "2022-09-22T17:00:43Z"
        }
      ]
    },
    {
      "number": 355,
      "id": "I_kwDOFEJYQs5SU1t1",
      "title": "Rename `Nonce` to `ReportID`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/355",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "branlwyd"
      ],
      "labels": [
        "draft-02"
      ],
      "body": "Currently, the DAP specification has a number of opaque values used to identify some DAP entity:\r\n* `Nonce` (identifying a client report)\r\n* `TaskID` (identifying a task)\r\n* `AggregationJobID` (identifying an aggregation job)\r\n* `BatchID` (identifying a batch)\r\n\r\n`Nonce`'s name differs from the rest. I think this is because, historically, the nonce also included a time component. Now that the time component has been split out, I think we could rename `Nonce` to `ReportID` for clarity of purpose.\r\n\r\nThoughts?\r\n\r\n(and I probably don't need to say this, but there is no reason to push this into DAP-02 -- it could easily land in DAP-03 instead.)",
      "createdAt": "2022-09-21T16:23:22Z",
      "updatedAt": "2022-09-23T14:57:08Z",
      "closedAt": "2022-09-22T20:17:05Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yup, I think this is a good idea! I'm tempted to say let's stick this in 02, since it's editorial, but it's actually somewhat invasive because we use the term \"nonce set\" in so many places. If you want to send a PR, I'd review it. But we should make sure there's broad consensus before merging.",
          "createdAt": "2022-09-21T16:25:43Z",
          "updatedAt": "2022-09-21T16:25:54Z"
        }
      ]
    },
    {
      "number": 360,
      "id": "I_kwDOFEJYQs5SpEbD",
      "title": "`taskid` should be optional in problem documents",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/360",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Section 3.2 discusses how to construct HTTP problem documents. In particular, [this paragraph](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-3.2-4) states:\r\n\r\n>The problem document MUST also include a \"taskid\" member which contains the associated DAP task ID (this value is always known, see [Section 4.2](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#task-configuration)), encoded in Base 64 using the URL and filename safe alphabet with no padding defined in sections 5 and 3.2 of [[RFC4648](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#RFC4648)].\r\n\r\nIt's not true that the task ID is always known. [4.3.1](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.3.1-3) allows implementations to respond to requests to `/hpke_config` that don't have a `task_id` query parameter with an error, and in that scenario I don't think there is any sensible task ID value for the server to use. We should change this MUST to allow some problem documents to omit `taskid`.",
      "createdAt": "2022-09-26T18:03:05Z",
      "updatedAt": "2022-12-08T23:51:45Z",
      "closedAt": "2022-12-08T23:51:45Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We might also consider making the key `task_id` for consistency with the query parameter on `/hpke_config`.",
          "createdAt": "2022-09-26T18:04:14Z",
          "updatedAt": "2022-09-26T18:04:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this is text-ready.",
          "createdAt": "2022-09-27T01:52:29Z",
          "updatedAt": "2022-09-27T01:52:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Thinking about this further, I think this is actually a dupe of #278. Currently, we require that problem documents contain the `taskid` and an `instance` value that \"MUST be the endpoint to which the request was targeted\". The examples of `instance` values in RFC 7807 look like this:\r\n\r\n```\r\n{\r\n             \"type\": \"https://example.com/probs/out-of-credit\",\r\n             \"title\": \"You do not have enough credit.\",\r\n             \"detail\": \"Your current balance is 30, but that costs 50.\",\r\n             \"instance\": \"/account/12345/msgs/abc\",\r\n             \"balance\": 30,\r\n             \"accounts\": [\"/account/12345\",\r\n                          \"/account/67890\"]\r\n}\r\n```\r\nHere, the value of `instance` is a relative path pointing to the resource that caused the error. Our requirement that `instance` be the \"endpoint\" seems unhelpful. If I hit some API like `https://aggregator.example/dap-api/upload`, and it fails, and I get back a problem document with `\"instance\": \"https://aggregator.example/dap-api\"`, then that's not very helpful, because I already knew what URI I accessed when I made the request. It's also kinda annoying for implementations to have to know what public domain they're serving traffic from. For this reason, Janus always sets `\"instance\": \"..\"` since RFC 7807 allows relative URLs in `instance`.\r\n\r\nOnce we do #278, then I think we won't need a dedicated `taskid` (or `task_id`) field in problem documents anymore, because if the task ID is relevant to the request, then it'll appear in the relative path that we can put in `instance`. For example, a failed upload request might yield a problem document with `\"instance\": \"/tasks/abcdefghijklmnopqrstuvwxyz/reports/01234567890abcdef\"`, and a client can parse the task ID out of that.\r\n\r\nSo my plan here is to address #278, and in doing so, update the problem document text. If I don't get around to that for draft-03 for some reason, then the smaller change described here will suffice.",
          "createdAt": "2022-09-27T13:12:34Z",
          "updatedAt": "2022-09-27T13:15:49Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Since we're punting #278 past draft-03, I'm going to update the tag.",
          "createdAt": "2022-11-05T00:06:13Z",
          "updatedAt": "2022-11-05T00:06:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #389.",
          "createdAt": "2022-12-08T23:51:44Z",
          "updatedAt": "2022-12-08T23:51:44Z"
        }
      ]
    },
    {
      "number": 361,
      "id": "I_kwDOFEJYQs5SqIf1",
      "title": "Specify unrecognized task ID error explicitly in all APIs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/361",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "The current protocol text doesn't specify what participants should do when a task ID is not recognized, there are two places unrecognized task are mentioned:\r\n\r\n1. In the [Errors](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/main/draft-ietf-ppm-dap.md#errors) table, it has: \r\n\r\n> unrecognizedTask | An endpoint received a message with an unknown task ID.\r\n\r\n2. [`hpke_config` request](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/main/draft-ietf-ppm-dap.md#hpke-configuration-request-hpke-config) mentions \"404 Not Found and an error of type unrecognizedTask\", which seems to conflict with most implementation's choice of error code 400 (@cjpatton )\r\n\r\nThe text should specify error 400 with type `unrecognizedTask` is used in every API that receives unrecognized task ID.",
      "createdAt": "2022-09-26T21:57:40Z",
      "updatedAt": "2022-10-05T22:10:28Z",
      "closedAt": "2022-10-05T22:10:11Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This would be useful I think. Would you mind sending a PR, @wangshan?",
          "createdAt": "2022-09-27T01:51:50Z",
          "updatedAt": "2022-09-27T01:51:50Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton will do, create this issue to remind myself.",
          "createdAt": "2022-09-27T08:44:56Z",
          "updatedAt": "2022-09-27T08:44:56Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "PR #363 merged",
          "createdAt": "2022-10-05T22:10:28Z",
          "updatedAt": "2022-10-05T22:10:28Z"
        }
      ]
    },
    {
      "number": 362,
      "id": "I_kwDOFEJYQs5Sq5RA",
      "title": "DAP-02 has stricter anti-replay requirements than DAP-01",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/362",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-03"
      ],
      "body": "Anti-replay is enforced in [Section 4.4.1.4](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.1.4). Both DAP-01 and DAP-02 read:\r\n\r\n> Check if the report has already been aggregated with this aggregation parameter. If this check fails, the input share MUST be marked as invalid with the error report_replayed. This is the case if the report was used in a previous aggregate request and is therefore a replay.\r\n\r\nWhat's not fully specified, at least in this section, is what it means for a report to \"have been aggregated (with this aggregation parameter)\". For that we have been referring to the anti-replay section. [DAP-02, Section 4.5.7](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#name-anti-replay) reads:\r\n\r\n> To detect replay attacks, each aggregator keeps track of the set of report IDs pertaining to reports that were previously aggregated for a given task.\r\n\r\nThe report ID is defined as\r\n\r\n```\r\nuint8 ReportID[16];\r\n```\r\n\r\nOn the other hand, [DAP-01, Section 4.4.6](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-01.html#name-anti-replay) reads:\r\n\r\n> To detect replay attacks, each aggregator keeps track of the set of nonces pertaining to reports that were previously aggregated for a given task.\r\n\r\nThe nonce is defined as\r\n\r\n```\r\nstruct {\r\n  Time time;\r\n  uint8 rand[16];\r\n} Nonce;\r\n```\r\n\r\nThus in DAP-01 we are required to check for collisions of the timestamp+random value, whereas in DAP-02 we are required to check for collisions of just a random value (report ID). The latter is stricter, since it disallows repeated random values.\r\n\r\nAfter discussing some implementation stuff with @MichaelScaria, I realized that this may have been an unintended regression. From a security standpoint, I think the DAP-02 requirement is better, since it forces the client implementation to generate a fresh report ID each time it generates a report. Otherwise it is easier to fingerprint reports from the same client. \r\n\r\nDoes this violate assumptions in anyone's implementation? Where I can imagine this being problematic is if your implementation currently shards nonce sets into time windows. [Daphne 0.1.2](https://crates.io/crates/daphne) does this, but we have to amend this behavior to implement fixed-size queries anyway.\r\n\r\nReply to this thread if this causes an issue for you and you think we should reconsider this requirement.",
      "createdAt": "2022-09-27T02:24:15Z",
      "updatedAt": "2022-12-08T21:41:24Z",
      "closedAt": "2022-12-08T21:41:24Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "From Janus' perspective, I think we are OK with this change.\r\n\r\nJanus also breaks report sets into time windows -- I don't think this change will affect Janus' implementation of that functionality, though as you note this sharding behavior will change with `fixed-size` tasks.",
          "createdAt": "2022-09-30T18:04:06Z",
          "updatedAt": "2022-09-30T18:04:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm nearly done with the implemenation ... once the code is reviewed and landed I'll try to remember to write up how I ended up organizing data.",
          "createdAt": "2022-09-30T19:32:56Z",
          "updatedAt": "2022-09-30T19:32:56Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Seems fine to me. I can't think of a reason why the client would want to reuse a reportID with a different time for the same task.",
          "createdAt": "2022-09-30T21:12:28Z",
          "updatedAt": "2022-09-30T21:12:28Z"
        },
        {
          "author": "bhalleycf",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I am in favor of recommending that clients generate a good random id for every report AND for allowing the timestamp to be considered as part of the anti-replay strategy.  While an unbounded memory of seen report ids would indeed be excellent protection, in practice this requires unbounded storage of report ids, which is a practical problem for implementations.\r\n\r\nReplay of a client report can leak information, so we want to assure that a report is aggregated at most once.  A report has a report id and a timestamp, both generated by the reporting client.  Both are included in the authenticated additional data, and cannot be altered without the tampering being evident.\r\n\r\nFor the rest of this note, we will assume that the client is doing its part for its own privacy by choosing a new random report id for every report, and by having a clock is not hugely inaccurate.  Even if the clients are doing their part, we must still be concerned with the actions of other entities, such as a compromised leader, helper, or collector.  We also need to prevent third party manipulation, e.g. by replaying captured reports.  The use of the timestamp can help us here, because the timestamp cannot be altered by any of these parties without detection.  If at least one aggregator is honest and imposes the policy below, then we can use the timestamp to ease our storage burdens.\r\n\r\nLet\u2019s consider the timestamp in more detail.  For reporting accuracy we don\u2019t want to permit just any timestamp to be aggregated.  At any given time T by the server\u2019s clock, there are bounds we want to apply based on the timestamp.  Reports earlier than some time MinTime = T - MinDelta should be rejected as too old, and reports farther in the future than MaxTime = T + MaxDelta should be rejected for being so confused about the time.  We allow reports slightly in the future as we want to allow for clock-skew.  Call the closed interval [MinTime, MaxTime] for time T Allowed(T).\r\n\r\n```\r\n                      Allowed(T)\r\nToo Old [------------------------------ T --] Too Far In the Future\r\n        ^              \t                    ^\r\n        T - MinDelta                        T + MaxDelta\r\n        MinTime                             MaxTime\r\n```\r\n\r\nWe will discard reports outside of this interval when we receive them, and also when we process them.  We impose the \u201con reception\u201d policy to avoid queuing work that we\u2019d never do, and we impose on processing to prevent backlog and to allow the definition of LeastRetainedTime (below).\r\n\r\nAssume a monotonically non-decreasing system clock.  Whenever an aggregation begins, we durably record the current MinTime and associate it with the active state for the aggregation job, using appropriate locking in concurrent implementations.  Define the LeastRetainedTime as the minimum of the current MinTime and the recorded MinTime over all active aggregation jobs (if any).  It is impossible to aggregate any report before this time, as it will be rejected in any new job and isn\u2019t present in any existing job.  Garbage collection may safely remove any recorded report metadata with a timestamp before the LeastRetainedTime.\r\nWe now only have to remember report ids that are greater than or equal to the LeastRetainedTime.  If clients choose random ids with every report submitted, and there is at least one honest aggregator, then any given report cannot be replayed.\r\n\r\nNote that in cases where a server is so flooded with reports that it cannot process or remember them, then the DAP spec still has the \"report_dropped\" error available to ensure that reports are not aggregated when anti-replay cannot be assured.",
          "createdAt": "2022-11-02T19:23:32Z",
          "updatedAt": "2022-11-02T19:23:32Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @bhalleycf. Full disclosure: Bob is helping out with Daphne development. When he showed me this analysis, I was pretty convinced that relaxing this requirement would be safe and worth it to reduce burden on implementations. Although I'm not strongly opposed to keeping the stronger requirement if folks think it is useful.\r\n\r\nI think the only change that would need to be made is to [\"Anti-Replay\"](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.7):\r\n\r\n> To detect replay attacks, each aggregator keeps track of the set of ~report IDs~ **(report ID, timestamp) pairs** pertaining to reports that were previously aggregated for a given task",
          "createdAt": "2022-11-04T23:54:29Z",
          "updatedAt": "2022-11-04T23:54:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I also agree with Bob's analysis. I think we should make the change described by cjpatton [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/362#issuecomment-1304345127) in DAP-03.",
          "createdAt": "2022-11-07T22:15:39Z",
          "updatedAt": "2022-11-07T22:15:39Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "This argument is good. One point: while it certainly justifies including the timestamp in the overall anti-replay strategy, it does not necessarily justify checking `(report_id, timestamp)` rather than only `report_id` for the ID-based check. That is, the overall strategy could be:\r\n\r\n1. Check the timestamp; if it's out of the acceptable window, fail due to too old/too new.\r\n2. Check if `report_id` [or `(report_id, timestamp)`] is in the set of received reports; if it is, fail due to replayed-report.\r\n\r\nI would have a preference for doing the ID-based check on `report_id` only, since the report ID is the primary identifier of the report, but plausibly implementations may want to check timestamps as well (for example, if they bucket report IDs by timestamp).\r\n\r\nPerhaps we could phrase things such that either implementation is allowed? In practice, as long as the report IDs are properly generated as random values, the timestamp will be irrelevant to the ID-based check. If the leader/helper disagree on whether to check `report_id` or `(report_id, timestamp)`, the more restrictive check will be the one that is effectively in place.\r\n\r\nedit: another way of looking at this argument -- for the ID-based replay check, what justification is there for checking `(report_id, timestamp)` over, say, `(report_id, extensions)` or `(report_id, public_share)`?",
          "createdAt": "2022-11-08T06:17:44Z",
          "updatedAt": "2022-11-08T18:18:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @branlwyd edit: another way of looking at this argument -- for the ID-based replay check, what justification is there for checking (report_id, timestamp) over, say, (report_id, extensions) or (report_id, public_share)?\r\n\r\nI think if we assume that honest clients always choose their report ID at random, then any anti-replay check that includes the report ID ought to be sufficient. I agree we could write this in a way that makes this implementation-specific perhaps suggesting that (report_id, timestamp) is one particularly useful relaxation.",
          "createdAt": "2022-11-16T18:11:30Z",
          "updatedAt": "2022-11-16T18:11:30Z"
        }
      ]
    },
    {
      "number": 364,
      "id": "I_kwDOFEJYQs5SuZnn",
      "title": "Editorial: unify how errors are formatted",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/364",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Error types are either double quoted or code formatted, the text seems to encourage both:\r\n\r\n> In the remainder of this document, the tokens in the table above are used to\r\n> refer to error types, rather than the full URNs. For example, an \"error of type\r\n> 'unrecognizedMessage'\" refers to an error document with \"type\" value\r\n> \"urn:ietf:params:ppm:dap:error:unrecognizedMessage\".\r\n> \r\n> This document uses the verbs \"abort\" and \"alert with `[some error message]`\" to\r\n> describe how protocol participants react to various error conditions.\r\n\r\nBut these format are not consistent, some examples:\r\n> Helper MUST abort with error \"queryMismatch\"\r\n> aggregator MAY abort with an error of type `missingTaskID`\r\n\r\nIt would be much clearer if we choose one style and make it consistent across the text.",
      "createdAt": "2022-09-27T14:47:48Z",
      "updatedAt": "2022-09-30T15:29:45Z",
      "closedAt": "2022-09-30T15:29:32Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #363 ",
          "createdAt": "2022-09-30T15:29:45Z",
          "updatedAt": "2022-09-30T15:29:45Z"
        }
      ]
    },
    {
      "number": 366,
      "id": "I_kwDOFEJYQs5T776s",
      "title": "`struct BatchSelector` is redundant",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/366",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "In DAP-02, we define [`struct Query`](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.1):\r\n```\r\nenum {\r\n  reserved(0), /* Reserved for testing purposes */\r\n  time_interval(1),\r\n  fixed_size(2),\r\n  (255)\r\n} QueryType;\r\n\r\nopaque BatchID[32];\r\n\r\nstruct {\r\n  QueryType query_type;\r\n  select (Query.query_type) {\r\n    case time_interval: Interval batch_interval;\r\n    case fixed_size: BatchID batch_id;\r\n  }\r\n} Query;\r\n```\r\n\r\nWhich is then used in [`CollectReq`](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.5.1):\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  Query query;\r\n  opaque agg_param<0..2^16-1>; /* VDAF aggregation parameter */\r\n} CollectReq;\r\n```\r\n\r\nBut later, we define [`AggregateShareReq`](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.5.2) in terms of `struct BatchSelector`:\r\n\r\n```\r\nstruct {\r\n  QueryType query_type;\r\n  select (BatchSelector.query_type) {\r\n    case time_interval: Interval batch_interval;\r\n    case fixed_size: BatchID batch_id;\r\n  };\r\n} BatchSelector;\r\n\r\nstruct {\r\n  TaskID task_id;\r\n  BatchSelector batch_selector;\r\n  opaque agg_param<0..2^16-1>;\r\n  uint64 report_count;\r\n  opaque checksum[32];\r\n} AggregateShareReq;\r\n```\r\n\r\nIs there a reason we don't use `Query` in `AggregateShareReq`? `AggregateShareReq` and `CollectReq` should be very similar.",
      "createdAt": "2022-10-13T18:06:38Z",
      "updatedAt": "2022-10-13T21:18:09Z",
      "closedAt": "2022-10-13T21:18:08Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, Query and BatchSelector are the same struct. I wanted two different names because, depending on how we resolve #342, we may want to add additional fields to Query (e.g., \"current-batch\").",
          "createdAt": "2022-10-13T19:57:43Z",
          "updatedAt": "2022-10-13T19:57:56Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "That makes sense. It sounds like the resolution of #342 will either unify these two structs or introduce new fields that make them truly distinct, so I don't think we need another issue about this. I'll add a note in #342 and close this.",
          "createdAt": "2022-10-13T21:18:08Z",
          "updatedAt": "2022-10-13T21:18:08Z"
        }
      ]
    },
    {
      "number": 368,
      "id": "I_kwDOFEJYQs5T9HIk",
      "title": "SealBase is not defined",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/368",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-03"
      ],
      "body": "I only tried searching for it, but all I found were places that it was used.  This makes it difficult to determine which inputs are which.  Which is the AAD, which is the plaintext, and which is the nonce?  Please also *describe* the inputs in text.\r\n\r\nNote that at least one of the code blocks that use this is too wide at 76 characters.",
      "createdAt": "2022-10-13T23:16:43Z",
      "updatedAt": "2022-11-08T23:14:16Z",
      "closedAt": "2022-11-08T23:14:16Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "SealBase() and SealOpen() refer to the \"single-shot APIs\" from https://www.rfc-editor.org/rfc/rfc9180.html#name-single-shot-apis. I notice that we don't reference this explicitly. Would a reference suffice?",
          "createdAt": "2022-10-14T16:25:56Z",
          "updatedAt": "2022-10-14T16:25:56Z"
        }
      ]
    },
    {
      "number": 369,
      "id": "I_kwDOFEJYQs5ULz6U",
      "title": "What is the extension processing model",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/369",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Are unknown/unsupported extensions ignored?",
      "createdAt": "2022-10-17T23:55:46Z",
      "updatedAt": "2022-11-16T22:55:43Z",
      "closedAt": "2022-11-16T22:55:43Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "First off, just for clarity's sake, I'm assuming Martin is talking about [upload extensions](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#name-upload-extensions).\r\n\r\nI think there are three ways to solve this problem.\r\n\r\nFirst, unknown extensions could be ignored by the aggregators. I believe this is the intent of the existing text (though I acknowledge that the protocol text would need to be updated to make that clear). I think this would work for the two report extension use cases I'm aware of at the moment, which are client authentication to the helper and [taskprov](https://github.com/wangshan/draft-wang-ppm-dap-taskprov). In either case, the aggregator can ignore the extension, proceed with the protocol and I don't think anything disastrous happens. If the aggregator doesn't support client auth extensions, it simply proceeds with processing the report upload after applying whatever other security policy it wants to the report. If the aggregator doesn't support the taskprov extension, then the upload will fail gracefully and reliably because the aggregator doesn't recognize the task ID referenced by the report. This is simple and flexible, but it does mean that all extensions ever deployed in DAP need to be designed such that they can be gracefully ignored by aggregators.\r\n\r\nThe second way to handle this would be to use something like (ugh) X.509's extension criticality bit, which would appear alongside an extension and if set to true, would indicate to servers that they should abort processing of an upload with some error like `extensionNotSupported`, the idea being that further handling of the report without handling the extension is too dangerous. However, anytime I'm reaching out to (ugh) X.509 for ideas, I take that as a sign that maybe we're solving the wrong problem or doing something too complicated.\r\n\r\nBut I think the reason X.509 ends up needing a criticality bit is because at the time of issuance of a certificate, it's not known (and not knowable) what is the set of implementations of relying parties, servers, CT logs, audit tools, etc. that might ever see the cert, and so special care must be taken to account for the diversity of implementations. But in DAP, when a client is putting extensions into its reports, it's doing so with knowledge of the specific pair of aggregators to which it is uploading the report, so I think it's more reasonable to expect the client to know precisely what extensions the aggregators support. We have discussed having servers advertise what HPKE algorithms they support in #248, so advertising supported extensions is a minor step beyond that.\r\n\r\nSo the third option is to treat all extensions as critical, and require servers to abort if they don't recognize them. This is the least flexible, but I also think it's the easiest one to reason about, both for the DAP protocol and eventual extension authors.\r\n\r\nAll that said, I think I'll tentatively vote for all extensions implicitly being critical, but I'm open to having my mind changed.",
          "createdAt": "2022-10-18T15:05:17Z",
          "updatedAt": "2022-10-18T15:05:17Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I would prefer to make extensions ignored by default, as they are in TLS. That puts it to extension designers to make sure their extensions can be safely ignored. If an extension can't be safely ignored, we should consider merging it into the core protocol. See https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/334 for additional discussion.",
          "createdAt": "2022-10-18T15:17:23Z",
          "updatedAt": "2022-10-18T15:18:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "To be clear @martinthomson, the spec is not clear here yet, hence #334 :)",
          "createdAt": "2022-10-18T15:20:46Z",
          "updatedAt": "2022-10-18T15:20:46Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I apologize for splitting the discussion.\r\n\r\nI tend to side with @tgeoghegan's conclusions.  If nothing else, the aggregators can publish what they accept ahead of time, along with keys.",
          "createdAt": "2022-10-19T00:36:08Z",
          "updatedAt": "2022-10-19T00:36:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think my only concern with the third option is this: Suppose that, in some DAP deployment, the Client and Leader used some extension that is irrelevant to the Helper. For instance, imagine the Client produces some attestation (#89) that the report was generated on a Leader-trusted device. The Helper may not be able to do this check.\r\n\r\nI guess you could argue that both Aggregators *should* be doing this check, in which case option 3 is actually what we want? This would probably depend on the threat model for the extension.",
          "createdAt": "2022-10-19T15:17:45Z",
          "updatedAt": "2022-10-19T16:53:10Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "So are these extensions universal in the sense that the same extension applies to the data that is sent to all helpers (including the leader) or, as I believe is the case, the extensions are targeted and they are sent separately to each of the helpers?\r\n\r\nIn the latter case, the client can condition what it sends on what it knows about the specific helper that it is sending to.  Yes, that makes it more complicated for a client, but that is the nature of the trade-off.",
          "createdAt": "2022-10-19T22:40:23Z",
          "updatedAt": "2022-10-19T22:40:23Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> So are these extensions universal in the sense that the same extension applies to the data that is sent to all helpers (including the leader) or, as I believe is the case, the extensions are targeted and they are sent separately to each of the helpers?\r\n\r\nThe latter is what I'm imagining, yes.\r\n\r\n> In the latter case, the client can condition what it sends on what it knows about the specific helper that it is sending to.\r\n\r\nHmm that's a pretty good idea, though (1) it would require some protocol changes (currently the Leader \"re-transmits\" all of the extensions to each Helper) and (2) we would have to unbind the extensions from the HPKE ciphertext (currently the extensions are included in the AAD for encryption). (1) seems workable to me; (2) doesn't seem desirable from a security perspective. (Fiddling with extensions should result in the report being excluded, hence the current binding.)",
          "createdAt": "2022-10-25T01:04:18Z",
          "updatedAt": "2022-10-25T01:04:18Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thinking about it more, we don't have to do (2). Perhaps all we have to do is mark each extension with the identity of the intended consumer? Some extensions might be intended for the Leader, others for the Helper, others for both.",
          "createdAt": "2022-10-25T04:03:52Z",
          "updatedAt": "2022-10-25T04:03:52Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Why not move the extensions to the encrypted payload?  Sure, the client replicates extensions toward each helper, but you avoid a lot of this complexity.",
          "createdAt": "2022-10-25T04:24:39Z",
          "updatedAt": "2022-10-25T04:24:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "That could work, but I think we should avoid copying extensions. (People are worried about communication cost of DAP.)",
          "createdAt": "2022-10-25T14:57:00Z",
          "updatedAt": "2022-10-25T15:07:15Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'd be down for something like this:\r\n1. Treat Report.extensions as \"critical\", in the sense that any unrecognized extension triggers an abort.\r\n2. Add an extensions field to the plaintext for each Report.encrypted_input_share.payload (HPKE ciphertext). Alternatively, we could add a new field to Report.encrypted_input_share. Either way, the consumer treats these as critical. \r\n\r\n@tgeoghegan WDYT?",
          "createdAt": "2022-10-25T15:01:47Z",
          "updatedAt": "2022-10-25T15:05:58Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The idea being to (1) enable a client to send different extensions to either aggregator and (2) keep the extensions intended for the helper private from the leader? Yeah, seems like a good idea!",
          "createdAt": "2022-10-25T15:31:10Z",
          "updatedAt": "2022-10-25T15:31:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It would be great to work this out in DAP-03. I'll try to put together a PR for @martinthomson's suggestion next week, if no one else gets there first.",
          "createdAt": "2022-11-02T00:25:17Z",
          "updatedAt": "2022-11-02T00:25:17Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "PR is up, please have a look!",
          "createdAt": "2022-11-02T17:06:48Z",
          "updatedAt": "2022-11-02T17:06:48Z"
        }
      ]
    },
    {
      "number": 371,
      "id": "I_kwDOFEJYQs5VQPDf",
      "title": "Clarify encoding of AAD",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/371",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Currently, the AAD for input shares is defined as `task_id || metadata || public_share`. The variable `public_share` is narratively introduced in 4.4.1.3 as \"the public share sent to each aggregator\", referring to a part of the report share. Two implementations interpreted the construction of the AAD differently, as either [including](https://github.com/cloudflare/daphne/blob/61ab97f0c07f9f4bbdb13ec666cebd22febf12ec/daphne/src/vdaf/mod.rs#L184) or [not including](https://github.com/divviup/janus/blob/42873cddb666dce39d27a2342adc3d7232b14eda/core/src/hpke.rs#L37-L49) a 32-bit length suffix preceding the public share.\r\n\r\nWhile it would likely be okay to have a single variable-length byte array in the AAD without a length prefix, as the resulting AAD would still be unambiguous, I think it would be prudent to change this to explicitly include a length prefix, to simplify future changes.\r\n\r\nWe should define new structs for both kinds of AAD strings, and specify that the AAD is the result of encoding the corresponding struct. For example:\r\n\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  ReportMetadata metadata;\r\n  opaque public_share<0..2^32-1>;\r\n} ReportShareAAD;\r\n```",
      "createdAt": "2022-10-31T18:45:19Z",
      "updatedAt": "2022-11-09T16:23:12Z",
      "closedAt": "2022-11-09T16:23:12Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Places in DAP-02 `public_share` is used without a length prefix:\r\n* Section 4.3.2, in the AAD to `SealBase`.\r\n* Section 4.4.1.3, in the AAD to `OpenBase`.\r\n\r\nI think all of the places where we might unexpectedly not have a length prefix will live in the info or AAD parameters to `OpenBase` or `SealBase`, since I think all other encoding is specified via RFC8446 struct notation.\r\n\r\nI like the idea of specifying info/AAD values as encodings of an RFC8446 struct, as suggested in the OP: this would be unambiguous, and would lower the odds of regressions if we introduced additional variable-length fields.",
          "createdAt": "2022-11-01T17:32:21Z",
          "updatedAt": "2022-11-01T17:32:21Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I interpreted these sections as including the length prefix. The only section where we definitely can't pass the length prefix is https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.4.1.5, where the VDAF is invoked.",
          "createdAt": "2022-11-01T17:34:51Z",
          "updatedAt": "2022-11-01T17:34:59Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> We should define new structs for both kinds of AAD strings, and specify that the AAD is the result of encoding the corresponding struct\r\n\r\nI also support this approach in OP, I think all AAD string should be defined in structs and make it explicit that OpenBase and SealBase takes the encoding of such struct for AAD, that means AAD used in Aggregate share encrypt/decrypt in https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.5.4 should also have a struct definition.",
          "createdAt": "2022-11-02T22:52:47Z",
          "updatedAt": "2022-11-02T22:52:47Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Regarding the VDAF call, am I right TLS is not specified for the procedure calls with VDAF? so public_share can be any type/encoding that VDAF implementation understands, therefore how length of public_share is passed in the below call doesn't have to be constrained by how AAD is used.\r\n```\r\nprep_state = VDAF.prep_init(vdaf_verify_key,\r\n                            agg_id,\r\n                            agg_param,\r\n                            report_id,\r\n                            public_share,\r\n                            input_share)\r\n```",
          "createdAt": "2022-11-02T23:04:46Z",
          "updatedAt": "2022-11-02T23:04:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As input to VDAF, `public_share` is the opaque byte string carried by the `Report`, _excluding_ the prefix. Just as `input_share`, it is decoded as specified by the VDAF in use.",
          "createdAt": "2022-11-03T00:07:00Z",
          "updatedAt": "2022-11-03T00:07:00Z"
        }
      ]
    },
    {
      "number": 375,
      "id": "I_kwDOFEJYQs5Vd_08",
      "title": "How to handle duplicate extensions",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/375",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-03"
      ],
      "body": "As noted by @tgeoghegan in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/373#discussion_r1012218650, we need to decide how to handle duplicate extensions.\r\n\r\nOne simple idea is to treat duplicate extensions as decoding failures, i.e., abort with \"unrecognizedMessage\". \r\n      ",
      "createdAt": "2022-11-02T23:51:50Z",
      "updatedAt": "2022-11-16T22:58:28Z",
      "closedAt": "2022-11-16T22:58:28Z",
      "comments": []
    },
    {
      "number": 376,
      "id": "I_kwDOFEJYQs5VjDoW",
      "title": "Consider removing draft version from hardcoded strings like \"dap-02 input share\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/376",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The current text use some magic strings when calling `SealBase`, for example in [upload request](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/main/draft-ietf-ppm-dap.md#upload-request)\r\n\r\n```\r\nenc, payload = SealBase(pk,\r\n  \"dap-02 input share\" || 0x01 || server_role,\r\n  task_id || metadata || public_share, input_share)\r\n```\r\n\r\nThis assumes backward incompatibility across drafts, but it is plausible to have two or more drafts only contain backward compatible changes, especially if the client side of the protocol become more stable. Therefore it's better to remove \"dap-02\" from the string so the draft version is not hardcoded when calling SealBase, give client, leader, helper implementation a chance to use backward compatible draft versions. We can always change other part of the string to force backward incompatibility if needed.\r\n",
      "createdAt": "2022-11-03T21:01:27Z",
      "updatedAt": "2022-12-01T19:12:24Z",
      "closedAt": "2022-12-01T19:12:23Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Just to be clear, breaking backwards compatibility is very much the point: If there is a security vulnerability in DAP-X, then we want to make sure we can migrate to DAP-X+1 without allowing the attacker to force fallback to the previous version.\r\n\r\nI'd be open to only changing this as needed, as you suggest, but I think it would be better to plan your implementation so that you can upgrade as needed. In Daphne we have built a versioning mechanism that, when needed, will allow us to support multiple versions simultaneously. ",
          "createdAt": "2022-11-03T21:14:34Z",
          "updatedAt": "2022-11-03T21:14:50Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with Chris. We could introduce the notion of a \"security epoch\" and use that instead of the draft version, but then we'd have to decide every time we release a draft whether the changes are sensitive enough that it warrants bumping the epoch. That is a chore at best, and I suspect we'd wind up bumping the epoch every single time just to be safe anyway.",
          "createdAt": "2022-11-03T21:16:45Z",
          "updatedAt": "2022-11-03T21:16:45Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I get the motivation, but we can still achieve that by changing the string from something like \"dap input share\" to \"dap input share X+1\" on all participants when such vulnerability is detected, as long as one party is honest, even if all other parties refuse to upgrade, no privacy leak can happen.\r\n\r\nAlternatively, how about let these strings (or at least the one for input share) to be set as part of AAD, so the task creator can decide what the string should be.",
          "createdAt": "2022-11-03T22:33:45Z",
          "updatedAt": "2022-11-03T22:33:45Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with Tim. At some point in the future, we might be moving slowly enough that we want to keep the strings the same, but that seems like a future problem.",
          "createdAt": "2022-11-04T20:07:35Z",
          "updatedAt": "2022-11-04T20:07:35Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Closing this since the overwhelming response is to keep the draft version",
          "createdAt": "2022-12-01T19:12:23Z",
          "updatedAt": "2022-12-01T19:12:23Z"
        }
      ]
    },
    {
      "number": 377,
      "id": "I_kwDOFEJYQs5Vofnz",
      "title": "We need more than 16 bits for the agg parameter length prefix",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/377",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Poplar is likely going to need sequneces of candidate prefixes longer than 65K.\r\n\r\ncc/ @hannahdaviscrypto, @rosulek",
      "createdAt": "2022-11-04T21:52:26Z",
      "updatedAt": "2022-11-08T23:14:51Z",
      "closedAt": "2022-11-08T23:14:51Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "To match recent change of this nature, I would say go with a 32-bit length tag.",
          "createdAt": "2022-11-04T22:01:02Z",
          "updatedAt": "2022-11-04T22:01:02Z"
        }
      ]
    },
    {
      "number": 383,
      "id": "I_kwDOFEJYQs5W6g8S",
      "title": "Clarify handling of completed jobs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/383",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "[4.5.2. ](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.2)[Collection Aggregation](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-collection-aggregation) states \r\n\r\n> After issuing an aggregate-share request for a given query, it is an\r\n> error for the leader to issue any more aggregation jobs for additional\r\n> reports that satisfy the query. These reports will be rejected by\r\n> helpers as described [Section 4.4.1](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#agg-init).\r\n\r\nThis needs some clarification because it should only hold if a job has been successfully collected. If there was an error or a the collection has been cancelled it should be possible to retry or try again with different parameters.\r\n\r\nRegarding cancellation there is a statement in [4.5.1. ](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.1)[Collection Initialization](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-collection-initialization) saying:\r\n\r\n> The collector can send an HTTP DELETE request to the collect job URI, to which the leader MUST respond with HTTP status 204 No Content. The leader MAY respond with HTTP status 204 No Content for requests to a collect job URI which has not received a DELETE request, for example if the results have been deleted due to age. The leader MUST respond to subsequent requests to the collect job URI with HTTP status 204 No Content.\r\n\r\nWhich also needs to distinguish between collected jobs and deleted jobs to correctly treat further requests. If a job was deleted after collection the reports have been used, otherwise a new job could still use them.\r\n\r\n\r\n",
      "createdAt": "2022-11-21T15:53:39Z",
      "updatedAt": "2023-10-25T07:38:56Z",
      "closedAt": "2023-10-25T07:38:56Z",
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Istm that [4.4.1.4. ](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.1.4)[Early Input Share Validation](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-early-input-share-validatio) is making a distinction between \"aggregated\" and \"collected\" that doesn't match the constraints.\r\nA report has been \"used\" once the batch into which it has been aggregated has been collected. Until collection no information about the report can leak. I think we can remove (2.):\r\n\r\n> Check if the report has already been aggregated with this aggregation parameter. If this check fails, the input share MUST be marked as invalid with the error report_replayed. This is the case if the report was used in a previous aggregate request and is therefore a replay.\r\n\r\nAnd in [4.5.7. ](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.7)[Anti-replay](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-anti-replay) we can probably replace the word \"aggregated\" with collected.\r\n\r\nFor some VDAFs the act of aggregating might already leak information. In this case the report is \"used\" earlier but I still don't see why we need two different checks.",
          "createdAt": "2022-11-21T16:32:21Z",
          "updatedAt": "2022-11-21T16:32:37Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This has since been fixed in https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-input-share-validation by moving the rules for report re-use to the VDAF:\r\n\r\n> Check if the report may still be aggregated with the current aggregation parameter. This can be done by looking up all aggregation parameters previously used for this report and calling\r\nVdaf.is_valid(current_agg_param, previous_agg_params)\r\nIf this returns false, the input share MUST be marked as invalid with the error report_replayed.\r\n\r\n",
          "createdAt": "2023-10-25T07:38:56Z",
          "updatedAt": "2023-10-25T07:38:56Z"
        }
      ]
    },
    {
      "number": 384,
      "id": "I_kwDOFEJYQs5XegLU",
      "title": "Aggregation phase naming",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/384",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "We currently have AggregateInitializeReq/-Resp and AggregateContinueReq/-Resp containing the word \"Aggregate\" even though no aggregation is performed in these steps. In some places we also speak of verification, the VDAF draft calls it simply preparation.\r\n\"Verification\" would also be a natural choice. VDAF chose preparation because of the option to have a non-V DAF and because other tasks, like format conversion, are included as well.\r\n\r\nWe might want to adopt the term preparation as well. \r\n\r\nI've started a draft of a report flow here which might help with the discussion:\r\n\r\n![image](https://user-images.githubusercontent.com/103195467/204466721-96bd7139-c1a7-42e1-b91f-d9e9b3d0b054.png)\r\n\r\n[Editable Source](https://sequencediagram.org/index.html#initialData=A4QwTgLglgxloDsIAIDCAbKBTJAoUks8ISyAMliACZZj7jRyIoASW6wt9hTJKqAe3TosMCALq4wAIwEAPZAIButNJhwQAXAFlKAZwCuYLAFsNybbgzYkAWgB81jZoDKwTBCmyFy1U6SuAPoAjAAU2gCUADQugQBM4RFW6naOKVoAoggwYACewJ4y8ooqYGo2mdmBFNShsWGR0VkwgWwcdfGJSf4QDjU0YJoAqu4C1MgASljAEp64AMQAvIv9qgAWIHrIIMh6WCgCAGbIxjOQWyRUyADufFviuxAMyMCn4CDQAgjLuN4lqqtBoDkBsLrt9opjqdZnoADoIS43O7IB56J6QF5vMAfKBfXBYOSIKggaQiQBIRMh5pgAOZrCDU7G5ZAABSxOK+LNBWFwgL6lAGmgAIqI8gVkM1qvyOg0ItFkEoQJhiRAsMh0AIYIr0LkosgoAgoCgAGqCgCCADEefzaA42pxBqbqQysNSPlgAJIG6CKqAALywUwAjgYsGi9dl0AYqPrqeKqnaOglGrgQGIoAqVcg7Vxs2Bbex7UKRfkUBL+tKurqFUq3WqNVqdeHDcgTRbcLm+dRaJpHc7XSrPYaoD7-VM9MBcDRU9AM6rc7hcAIxdRo58EehMdN3muTgIDAgqHpdadKCqqFau3n7IDNMzaIcJCZkAgCShXlvsTu0dMLwN8xxu17Yx+ywQQkH1EMgxTNNZyzAsc3gq9c1ve9H2fV9NwIdkEEeH8O2va0HSdYC3TA6AEEg0MJynGDa3nHBz3xA8FwJIkSXJSkaTpBkQCZJc10VFljFsICXWw35il8MpASiZD3WOCA1lVJ49AAa22YQBGuLZ31sEBiLE8jY2hc5w3EeFpAMGBVP2I8USUtVCO2A8QUQ5BNRwxVjGoJkqAEeyPgAOnhd0UGMYMoGMLYEH8mABBMEx91gbDdU2OsEFjNK1i0lEBHhGhDn1ZSHMs6zbL1HTniOezlM2VTAu4LV2F-G0CMvHsDJAk5phhMz-NKmyID0dtEP-QtRK6kyhr65ABtsxrhHYSFDiYxjWMudisFsTioFpelGTQIQRDTDlmS5KwjtEcQrxvQRFrEKDp3TWtARam7CM0VAlOs7rg1DFBqygVdcje2wHDu47rs0ABmAAGaHcuQAArARpGQIYJndaCZ1rCGrokSkeJBv4pMO+6obxsQCZmYQthRtGMax-FCQ20ksApKldu4g6JuwzlNm5Xl7GQ3mVRcDZjCg7Hnszed8JvUWsHF8AAyoydKFo2XENW0HHEuqnBlCGmRCoCIychsdqI1nHM1emibdVSnrpYg8gA)",
      "createdAt": "2022-11-29T07:56:50Z",
      "updatedAt": "2023-10-19T22:16:03Z",
      "closedAt": "2023-10-19T22:16:03Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, this awkwardness in naming has been around for a while. It's tempting to reach for a term like \"aggregation\", except that suggests a function that takes _n_ items and produces 1. However, VDAF uses preparation to mean the transformation of a single input share into an output share, and then aggregation is reducing _n_ output shares to a single aggregate. We've also talked about using \"verification\", but unfortunately that verb isn't correct in all cases. In particular, the VDAF draft also defines DAFs, which drop the MPC zero knowledge proof verification (this is intended for settings where strong client authentication or attestation provides sufficient confidence in the integrity of input shares).\r\n\r\nWhat I do agree with is that DAP needs more explanatory text explaining what reports, aggregate shares and aggregate results are, and what the lifecycle of each of these things is. This is particularly awkward because the VDAF [preparation](https://cfrg.github.io/draft-irtf-cfrg-vdaf/draft-irtf-cfrg-vdaf.html#name-preparation-2) and [aggregation](https://cfrg.github.io/draft-irtf-cfrg-vdaf/draft-irtf-cfrg-vdaf.html#name-aggregation-2) steps both get run by DAP's aggregation sub-protocol, which is definitely confusing. I'm not sure that reaching for another synonym of \"aggregate\" would be less confusing, though.",
          "createdAt": "2022-11-29T22:26:52Z",
          "updatedAt": "2022-11-29T22:26:52Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Would you agree to the definitions in my diagram then?\r\n\r\nAs it stands, the aggregation sub-protocol ([4.4](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-verifying-and-aggregating-r)) does _both_ verification and aggregation. And the collection sub-protocol ([4.5](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-collecting-results)) does _both_ aggregation and collection.\r\n\r\nSo we split Aggregation into Preparation and Pre-Aggregation and make the Aggregation part of Collection explicit.\r\n\r\nI think this makes things less confusing because the pre-aggregation in the current Aggregation Sub-Protocol is only an optimization. So by splitting it, we don't have to use the name aggregation for things that possibly don't aggregate.\r\n",
          "createdAt": "2022-11-30T10:01:02Z",
          "updatedAt": "2022-11-30T10:01:02Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I agree the names are very confusing. I'm not sure Pre-aggregation should have its own flow, because it's not part of a sub-protocol.\r\n\r\n> We've also talked about using \"verification\", but unfortunately that verb isn't correct in all cases. In particular, the VDAF draft also defines DAFs, which drop the MPC zero knowledge proof verification\r\n\r\nVerification can still make sense for DAF, since helper needs to open the sealed report, if that can be counted as \"verify\". Preparation works too, but I wonder if we should keep DAP flow names and VDAF step names de-coupled so both can evolve without worrying about violating names in the other? \r\n\r\n\r\n\r\n",
          "createdAt": "2022-12-01T20:37:57Z",
          "updatedAt": "2022-12-01T20:37:57Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Given alignment on top of VDAF and the points @tgeoghegan raises above, I'm going to close this out on the basis that it's editorial and we don't have any better options at the moment.",
          "createdAt": "2023-10-19T22:16:03Z",
          "updatedAt": "2023-10-19T22:16:03Z"
        }
      ]
    },
    {
      "number": 385,
      "id": "I_kwDOFEJYQs5Xen5a",
      "title": "Pre-Aggregation into buckets",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/385",
      "state": "OPEN",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "editorial"
      ],
      "body": "We are indicating that batches can already start aggregating before the collector sends a query at least in three places:\r\n\r\n2.1\r\n> The leader distributes the shares to the helpers and orchestrates the process of verifying them (see [Section 2.2](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#validating-inputs)) and assembling them into a final measurement for the collector. Depending on the VDAF, it may be possible to incrementally process each report as it comes in, or may be necessary to wait until the entire batch of reports is received.\r\n\r\n4.4\r\n> Once a set of clients have uploaded their reports to the leader, the leader can send them to the helpers to be verified and aggregated. In order to enable the system to handle very large batches of reports, this process can be performed incrementally.\r\n\r\n4.5.2\r\n> Note that for most VDAFs, it is possible to aggregate output shares as they arrive rather than wait until the batch is collected. To do so however, it is necessary to enforce the batch parameters as described in [Section 4.5.6](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#batch-validation) so that the aggregator knows which aggregate share to update.\r\n\r\nOptional, but we might want to expand 4.5.2 and give this process a name and clarify what it means to validate a _bucket_ (is this the name we're using?) because some of the point in 4.5.6 don't really apply. For example, a bucket may, by definition, contain too few reports.",
      "createdAt": "2022-11-29T08:22:12Z",
      "updatedAt": "2024-06-13T16:52:16Z",
      "closedAt": null,
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Perhaps we should remove the mentioning of 4.5.6, since the \"bucket\" isn't a final batch and doesn't have to be valid at pre-aggregation stage.",
          "createdAt": "2022-12-01T19:19:41Z",
          "updatedAt": "2022-12-01T19:19:41Z"
        }
      ]
    },
    {
      "number": 394,
      "id": "I_kwDOFEJYQs5aaGye",
      "title": "Clients need to provide nonce when sharding measurements",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/394",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "In the upcoming VDAF-04, [`Vdaf.measurement_to_input_shares` now takes a `nonce` argument](https://github.com/cfrg/draft-irtf-cfrg-vdaf/blob/main/draft-irtf-cfrg-vdaf.md#sharding-sec-vdaf-shard). The intention is that clients will use the report ID they generate as the nonce, corresponding to [what we already do when we call `Vdaf.prep_init`](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-03#section-4.4.1.5). Once VDAF-04 is out, we will need to align DAP's discussion of report construction with it.",
      "createdAt": "2023-01-02T23:26:25Z",
      "updatedAt": "2023-02-27T22:12:36Z",
      "closedAt": "2023-02-27T22:12:36Z",
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I probably missed a discussion elsewhere but why are we adding this?",
          "createdAt": "2023-01-09T09:01:24Z",
          "updatedAt": "2023-01-09T09:01:24Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/117 explains why VDAF changed to take a random nonce during input sharding.",
          "createdAt": "2023-01-09T17:54:28Z",
          "updatedAt": "2023-01-09T17:54:28Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "More information here: https://mailarchive.ietf.org/arch/msg/cfrg/-errBjFRvCqi7KuAxoZwH6iY4Vc/",
          "createdAt": "2023-02-07T21:55:28Z",
          "updatedAt": "2023-02-07T21:55:28Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It seems this didn't make it into vdaf-4. Should we also push it to dap-5 then?",
          "createdAt": "2023-02-27T10:40:35Z",
          "updatedAt": "2023-02-27T10:40:35Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "VDAF-04 has the [necessary definition of `Vdaf.measurement_to_input_shares(measurement: Measurement, nonce: Bytes[Vdaf.NONCE_SIZE]) -> (Bytes, Vec[Bytes])`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf-04#name-sharding-2) so we're good here.",
          "createdAt": "2023-02-27T16:24:53Z",
          "updatedAt": "2023-02-27T16:24:53Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Ah, okay, I searched from the beginning and it's missing in 4.1. (Which is for DAFs)",
          "createdAt": "2023-02-27T20:12:26Z",
          "updatedAt": "2023-02-27T20:13:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah so far we don't see a need for providing a nonce for DAF sharding.",
          "createdAt": "2023-02-27T20:43:13Z",
          "updatedAt": "2023-02-27T20:43:13Z"
        }
      ]
    },
    {
      "number": 397,
      "id": "I_kwDOFEJYQs5bmgfX",
      "title": "Aggregate results should include the interval of time that the constituent reports span",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/397",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-04"
      ],
      "body": "[In DAP-03](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-03#section-4.5.2-13), the results of an aggregation are presented to the collector as:\r\n\r\n```\r\nstruct {\r\n  PartialBatchSelector part_batch_selector;\r\n  uint64 report_count;\r\n  HpkeCiphertext encrypted_agg_shares<1..2^32-1>;\r\n} CollectResp;\r\n```\r\n\r\nwhere `encrypted_agg_shares` is the encrypted form of [a VDAF's `AggResult` associated type](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf-03#name-constants-and-types-defined). In the case of a time interval task, the collector will have explicitly requested collection over some interval of time, but in the case of a fixed size task, that aggregate result will have been compiled from a set of reports chosen by the leader. So the collector can only infer that the reports are from no later than when the collect request came in and no sooner than when the collector got the previous batch.\r\n\r\nDAP should explicitly tell the collector what time interval is covered by a collected batch. ",
      "createdAt": "2023-01-17T18:11:45Z",
      "updatedAt": "2023-02-08T21:04:30Z",
      "closedAt": "2023-02-08T21:04:30Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "One way to do this would be to amend `PartialBatchSelector`. [In DAP-03](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-03#section-4.4.1.1-4), it is defined as:\r\n\r\n```\r\nstruct {\r\n  QueryType query_type;\r\n  select (PartialBatchSelector.query_type) {\r\n    case time_interval: Empty;\r\n    case fixed_size: BatchID batch_id;\r\n  };\r\n} PartialBatchSelector;\r\n```\r\n\r\nWe could add an `Interval` to the `fixed_size` case. However that is awkward because `PartialBatchSelector` is also used in `AggregateInitializeReq`, so we have to make sure that there's something cogent the leader could put there.\r\n\r\nAnother, simpler choice is to simply add an `Interval` field to `CollectResp`, but the downside there is that this information isn't useful in the time interval case, because the collector already knows what time interval it queried over.",
          "createdAt": "2023-01-17T18:13:21Z",
          "updatedAt": "2023-01-17T18:13:21Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm cool with adding an `Interval` with the minimum and maximum timestamp of all reports in the batch. I think overloading `PartialBatchSelector` is not the right approach, however, since this is meant to \"partially determine\" a batch of reports, where as the interval in our case is strictly informational.\r\n\r\nUnrelated follow question: if we define `CollectResp.interval` to be \"the minimum and maximum timestamp of reports in the batch\" then it's possible that `CollectReq.interval != CollectResp.Interval`. I think this is OK, and might even be useful.",
          "createdAt": "2023-01-31T14:38:51Z",
          "updatedAt": "2023-01-31T14:38:51Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to not including an `Interval` in `PartialBatchSelector`; instead, I think it should be placed directly in `CollectResp`.",
          "createdAt": "2023-01-31T22:49:26Z",
          "updatedAt": "2023-01-31T22:49:26Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Chris' point that the interval in `CollectResp` could be tighter than what was in `CollectReq` is compelling. I also now think we should put the `Interval` in `CollectResp` (or, well, [it's `Collection` now](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/77cb75a6480a1777cf5ef7c1c6d1d9d12f3c08b2/draft-ietf-ppm-dap.md?plain=1#L1617)). I'll draft this change and circulate it to the WG this week, as I would like to include it in draft-04, which already includes a bunch of message definition changes.",
          "createdAt": "2023-01-31T23:00:25Z",
          "updatedAt": "2023-01-31T23:00:58Z"
        }
      ]
    },
    {
      "number": 401,
      "id": "I_kwDOFEJYQs5c0t-w",
      "title": "Do we need mitigations against the Leader rewinding aggregation job state?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/401",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "#400 introduces a mechanism allowing the aggregators to recover from the loss of an aggregation job message. The idea is that the helper should be ready for the leader to send the same state transition twice.\r\n\r\nHowever, now we have to contend with the possibility of the leader sending two different sets of prepare messages for the same round of a VDAF, which might let the leader learn something about the input shares.\r\n\r\nFurther security analysis is needed to determine if there are real attacks here. If there are, one possible mitigation is for the helper to keep track of the messages received from the leader, and abort if it detects the leader trying to re-run an aggregation job round with new parameters.\r\n\r\nSee [previous discussion on this topic](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/400/files#r1082976904).",
      "createdAt": "2023-01-25T21:53:17Z",
      "updatedAt": "2023-10-24T21:47:10Z",
      "closedAt": "2023-10-24T21:47:10Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Another point of concern here is if the Leader can rerun the Helper with a previous aggregation parameter.",
          "createdAt": "2023-02-09T00:50:24Z",
          "updatedAt": "2023-02-09T00:50:24Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think the requirement for any retry mechanism would be that, if the body of the request does not change, then the server treats handles it as a retry; but if the body changes, the server should treat it as an attack. I believe that's exactly what we have, so I'm going to close this with no action.",
          "createdAt": "2023-10-24T21:47:10Z",
          "updatedAt": "2023-10-24T21:47:10Z"
        }
      ]
    },
    {
      "number": 402,
      "id": "I_kwDOFEJYQs5dRItT",
      "title": "VDAF verify key changes",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/402",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "VDAF-04 will include some new language about the generation of VDAF verify keys. DAP-04 will need to ensure that verify keys are handled appropriately. ",
      "createdAt": "2023-01-31T17:59:59Z",
      "updatedAt": "2023-01-31T18:18:53Z",
      "closedAt": "2023-01-31T18:18:53Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "It seems that the important security property is that the verify key be fixed at the time of task generation. The implication of this is that rotating a verify key means generating a new task. If that's the case, then maybe that should also be the answer for HPKE key rotation, as I'd be reluctant to define two different mechanisms.",
          "createdAt": "2023-01-31T18:00:47Z",
          "updatedAt": "2023-01-31T18:00:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Never mind, we already had #161 for this.",
          "createdAt": "2023-01-31T18:18:53Z",
          "updatedAt": "2023-01-31T18:18:53Z"
        }
      ]
    },
    {
      "number": 404,
      "id": "I_kwDOFEJYQs5dYC_V",
      "title": "DAP-04 chores",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/404",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "DAP-04 will need some routine changes:\r\n\r\n- [x] Update version tags for HPKE to `\"dap-04\"` (#424)\r\n- [x] Bump VDAF-03 to 05 (#429)\r\n- [x] Update change log (#430)\r\n- [x] Run spell check (#431)",
      "createdAt": "2023-02-01T18:16:03Z",
      "updatedAt": "2023-03-13T17:45:53Z",
      "closedAt": "2023-03-13T17:45:53Z",
      "comments": []
    },
    {
      "number": 405,
      "id": "I_kwDOFEJYQs5d3cH5",
      "title": "Decoupling the aggregation parameter from aggregation jobs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/405",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "collecting a batch more than once"
      ],
      "body": "Currently, the leader transmits aggregation parameters to the helper in an `AggregationJobInitReq`. Servicing a collection request can require arbitrarily many aggregation jobs (the leader chooses how many jobs to schedule), but all of them will have the same aggregation parameter value. This is fine for Prio3, because there is no aggregation parameter and thus it takes up no space in the wire message. But if a VDAF happens to have a large aggregation parameter (as is likely to be the case for Poplar1), then it's wasteful to retransmit the exact same aggregation parameter many times.",
      "createdAt": "2023-02-07T18:12:39Z",
      "updatedAt": "2024-05-21T20:58:54Z",
      "closedAt": "2024-05-21T20:58:54Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "One way forward here would be to add a new resource on the helper for aggregation parameters. So the leader would do something like:\r\n\r\n~~~\r\nPUT /tasks/{task-id}/aggregation_parameters/{agg-param-id}\r\n~~~\r\n\r\nAn `agg-param-id` would be a 16 byte unique identifier. The body of the request would contain `opaque agg_param<0..2^32-1>;`. Then, instead of having `agg_param` inline in `AggregationJobInitReq`, we would have:\r\n\r\n~~~\r\nstruct {\r\n  boolean has_agg_param;\r\n  select (has_agg_param) {\r\n    case true: AggregationParameterId agg_param_id;\r\n    case false: Empty;\r\n  };\r\n} AggregationParameterSpecifier;\r\n\r\nstruct {\r\n  AggregationParameterSpecifier agg_param;\r\n  PartialBatchSelector part_batch_selector;\r\n  ReportShare report_shares<1..2^32-1>;\r\n} AggregationJobInitReq;\r\n~~~\r\n\r\nMaybe we could also include an affordance for specifying an aggregation parameter value inline in `AggregationParameterSpecifier`.",
          "createdAt": "2023-02-07T18:17:37Z",
          "updatedAt": "2023-02-07T18:17:37Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "If we go this route, I think we should indeed allow specifying an aggregation parameter value inline, rather than special-casing on whether the VDAF has an aggregation parameter or not. This would be more flexible/general than special-casing on whether the VDAF has an aggregation parameter, without requiring more code complexity or bytes-on-the-wire. (and it would remove any possible error cases where `has_agg_param` is specified incorrectly)\r\n\r\nThat is, something like:\r\n```\r\nenum {\r\n  immediate(0),\r\n  by_ref(1),\r\n} AggregationParameterSpecifierType;\r\n\r\nstruct {\r\n  AggregationParameterSpecifierType type;\r\n  select (type) {\r\n    case immediate: opaque agg_param<0..2^32-1>;\r\n    case by_ref:    AggregationParameterId agg_param_id;\r\n  };\r\n} AggregationParameterSpecifier;\r\n```",
          "createdAt": "2023-02-07T21:56:12Z",
          "updatedAt": "2023-02-07T21:56:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd: SGTM!\r\n\r\n@tgeoghegan: One question the resource's scope: Is scoping by task sufficient? One thing that we have to be careful about is that we don't want to hold onto aggregation parameters longer than we absolutely have to. In the case of Poplar1, the prefix tree leaks more information about the measurements than the heavy hitters do. At the very least I think we'll need to include some language about expiring this resource.",
          "createdAt": "2023-02-08T00:15:47Z",
          "updatedAt": "2023-02-08T00:15:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think there is any narrower scope we could attach an aggregation parameter to. The only thing that would make sense is an aggregate share, but it's not guaranteed that an aggregate share will have been created by the time the leader starts scheduling aggregation jobs (otherwise you can't do eager Prio3 aggregation).\r\n\r\nI agree that some language about when aggregation parameters can be safely discarded is needed, but I'm not sure what threat you're describing. Are you concerned about a malicious aggregator leaking aggregation parameters to attack privacy? But if the aggregator is malicious, wouldn't it just ignore protocol language about discarding aggregation parameters?",
          "createdAt": "2023-02-08T19:15:45Z",
          "updatedAt": "2023-02-08T19:15:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not really describing an attack in our current threat model. This is more about defense-in-depth: intermediate aggregation parameters *are* leaky with respect to the batch of measurements (this is a well-known limitation of using IDPFs for heavy hitters), so we want to treat them as somewhat toxic assets. As an analogy, you wouldn't want to store plaintext input shares indefinitely, in case your server ever gets compromised.",
          "createdAt": "2023-02-08T19:40:47Z",
          "updatedAt": "2023-02-08T19:40:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this change is too ambitious for draft-ietf-ppm-dap-05, given the July 10 deadline. On top of that, the key motivation is Poplar1, and none of the DAP deployments I'm aware of have immediate plans to deploy that VDAF. Let's tackle this a little later.",
          "createdAt": "2023-06-26T21:56:53Z",
          "updatedAt": "2023-06-26T21:56:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 I volunteered to implement multi-collection and resolve all related open issues.",
          "createdAt": "2023-11-08T15:07:58Z",
          "updatedAt": "2023-11-08T15:07:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This change would require higher storage requirements and would add a bit of protocol complexity. (For instance, the aggregation parameter might arrive after an aggregation job.) Also, for heavy hitters, the aggregation parameters are privacy sensitive, so implementations would need to come up with a plan to prune aggregation parameters from storage after they're no longer needed.\r\n\r\nThe decision point is really whether the cost of re-transmission is so high that adding this complexity is worth it. Unfortunately, the cost here depends on the distribution of the inputs, since [the tree traversal depends on how many inputs have prefixes in common at each level of the tree](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf#section-9.3.1-2).\r\n\r\nThat said, we can estimate this cost by generating synthetic batch and computing the prefix tree for it. We have done so here: https://github.com/divviup/libprio-rs/pull/956\r\n\r\nUsing the same parameters as the performance analysis in the [Poplar paper](ia.cr/2021/017), we find that the largest aggregation parameter is about a kilobyte. Moreover, adjusting the size of the batch doesn't change this much: even for 100K measurements, the distribution doesn't change much.\r\n\r\nEven planning for an aggregation parameter that is 10x larger, the cost of re-transmitting with each aggregation job is easily outweighted by the cost of this change.",
          "createdAt": "2024-02-24T00:40:13Z",
          "updatedAt": "2024-02-24T00:40:13Z"
        }
      ]
    },
    {
      "number": 406,
      "id": "I_kwDOFEJYQs5d4p3e",
      "title": "What error should servers provide if someone tries to mutate an immutable resource?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/406",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "There are a few places in DAP where a client (in the HTTP sense, not the DAP sense) creates an immutable resource using a PUT request involving a unique identifier:\r\n\r\n- DAP clients create reports with a `report_id` in the body\r\n- DAP leaders create aggregation jobs with an `aggregation-job-id` in the path\r\n- DAP collectors create collection jobs with a `collection-job-id` in the path\r\n\r\nThese objects cannot be further mutated using a PUT request (e.g., you can't change the aggregation parameter on a collection job after creating it). The question is: what should the error look like? ",
      "createdAt": "2023-02-07T22:41:54Z",
      "updatedAt": "2023-06-21T16:45:17Z",
      "closedAt": "2023-06-21T16:45:17Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd flagged this awkwardness while reviewing the implementation in Janus [here](https://github.com/divviup/janus/pull/936#discussion_r1099171509). For now, we're going to use [HTTP 409 Conflict](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409). I think that might suffice for DAP. I'm not sure if we get anything from specifying another HTTP problem document type that has the same semantics as HTTP 409.",
          "createdAt": "2023-02-07T22:43:17Z",
          "updatedAt": "2023-02-07T22:43:17Z"
        },
        {
          "author": "jbr",
          "authorAssociation": "CONTRIBUTOR",
          "body": "`409 conflict` doesn't seem _wrong_, but it does seem more intended for content versioning. I would expect a [422](https://httpwg.org/specs/rfc9110.html#status.422) in that it's a syntactically valid but semantically invalid request",
          "createdAt": "2023-02-07T22:55:00Z",
          "updatedAt": "2023-02-07T22:55:00Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I see what you mean, but 422 lacks specificity. If none of the HTTP statuses fit neatly, maybe an HTTP 400 with a DAP-specific problem type is the way forward, then.\r\n\r\nThe case of mutating a report is already handled by the text in the [upload section](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-03#section-4.3.2-16), but we should still do something for collections and aggregation jobs.",
          "createdAt": "2023-02-08T01:35:11Z",
          "updatedAt": "2023-02-08T01:35:11Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "[RFC 9205's guidance on HTTP status codes](https://datatracker.ietf.org/doc/html/rfc9205#section-4.6-4) is to use the most general applicable HTTP status and allow more specific information to be provided in problem documents. So I think what we should do here is add DAP language explaining that mutating an aggregation job or a collection is an error, and let implementations decide what 4xx code to use.",
          "createdAt": "2023-05-26T21:44:55Z",
          "updatedAt": "2023-05-26T21:44:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Sorry to wait to chime in here until I've reviewed the PR. I think in principle I agree that we should try to follow HTTP semantics to the letter wherever possible, but the complexity of the solution (#471) makes me question the motivation somewhat.\r\n\r\nIf our intent is that you should always be able to retry a PUT (as long as the content hasn't changed), then I think that:\r\n1. We need to spell out a general mechanism that applies to all requests (the upload request counts here as well, right?) \r\n2. We need to work out the interaction of that mechanism with storage commitments.\r\n\r\nWith that in mind, I think it's important to ask that sticking to HTTP semantics is so important that it's worth the cost of adding a bunch of protocol logic to make it happen.",
          "createdAt": "2023-06-16T00:02:05Z",
          "updatedAt": "2023-06-16T00:04:04Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "The way I am reading this it's about which error to return not if the PUT should be possible. The problem with 409 is\r\n\r\n> This code is used in situations where the user might be able to resolve the conflict and resubmit the request.\r\n\r\nA 403 seems appropriate here. I definitely support @tgeoghegan 's suggestion to make it clear in a DAP error what the problem is. I'm not sure if we're doing anybody any favor if we leave the specific response code undefined, though.\r\n",
          "createdAt": "2023-06-19T09:50:12Z",
          "updatedAt": "2023-06-19T09:50:12Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> A 403 seems appropriate here. I definitely support @tgeoghegan 's suggestion to make it clear in a DAP error what the problem is. I'm not sure if we're doing anybody any favor if we leave the specific response code undefined, though.\r\n\r\nI believe we should abide by the guidance in RFC 9205/BCP 56, which recommend using the most general possible status code but allow implementations to be more specific if they wish. That makes sense to me, since in this case we don't recommend that clients do anything specific if they encounter any specific 4xx error.",
          "createdAt": "2023-06-20T16:33:58Z",
          "updatedAt": "2023-06-20T16:33:58Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "That's also reasonable, this \r\n\r\n> Instead, applications using HTTP should define their errors to use the most applicable status code, making generous use of the general status codes (200, 400, and 500) when in doubt. Importantly, they should not specify a one-to-one relationship between status codes and application errors, thereby avoiding the exhaustion issue outlined above.\r\n\r\nadvocates both for using \"the most applicable status code\" but also for using the general ones when in doubt. I guess this issue clearly signals doubt.",
          "createdAt": "2023-06-21T10:15:04Z",
          "updatedAt": "2023-06-21T10:15:04Z"
        }
      ]
    },
    {
      "number": 407,
      "id": "I_kwDOFEJYQs5d5EIP",
      "title": "VDAF security analysis: Requirements for the verification key",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/407",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-04"
      ],
      "body": "The analysis posted [on the CFRG mailing list](https://mailarchive.ietf.org/arch/msg/cfrg/-errBjFRvCqi7KuAxoZwH6iY4Vc/) describes the security consideration for DAP:\r\n\r\n> Some care is required in choosing the VDAF verification key. The\r\n> Aggregators may use any strategy they wish to exchange this key, but the\r\n> application MUST ensure that the key that is picked independently of the\r\n> reports. For DAP in particular, this means it is safe for the Leader to\r\n> pick the key and distribute it, but the Aggregators need to \"commit\" to the\r\n> verification key used for a given task and use the same key for the\r\n> duration of the task. (Rotation requires rolling the task configuration.)\r\n\r\nAssuming we agree this is an issue that needs to be addressed, what should we do about it?",
      "createdAt": "2023-02-08T00:23:22Z",
      "updatedAt": "2023-02-28T01:23:48Z",
      "closedAt": "2023-02-28T01:23:47Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": " One suggestion, due to @hannahdaviscrypto, is to derive the task ID from the verification key by applying a one-way function. That way there is no possibility of the task ID being picked before the key.\r\n\r\nBeing a bit more careful, we might use HKDF to derive both strings from a shared secret. Perhaps something like:\r\n```\r\n    prk = HKDF-Extract(salt, shared_secret)\r\n    task_id = HKDF-Expand(prk, \"task Id\")\r\n    vdaf_verify_key = HKDF-Expand(prk, \"vdaf verify key\")\r\n```",
          "createdAt": "2023-02-08T00:24:13Z",
          "updatedAt": "2023-02-08T00:24:13Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "```\r\nWhen modeling the\r\nfunction used to derive the challenge as a random oracle, it is sufficient\r\nto ensure that (i) the VDAF verification is fixed prior to the report being\r\ngenerated _and_ (ii) the nonce is chosen at random by the Client.\r\n```\r\nShould this be an `or`? I think we agreed that using a ROish derivation function including the client nonce would be sufficient, right?",
          "createdAt": "2023-02-08T08:00:55Z",
          "updatedAt": "2023-02-08T08:00:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Actually I intended \"and\" :)\r\n\r\nThis security consideration has to do with privacy. For Prio3, we need the \"query randomness\" generated by the Aggregators to be \"fresh\". (By \"fresh\" I mean it needs to be random and the adversary should not be able to influence this value.) Our security model for privacy allows the adversary to pick the verify_key: In order to ensure freshness of the query randomness used for a given report, it is sufficient to (i) pick verify_key prior to generating the report and (ii) choose a random nonce. If we didn't have (i), then attacker controls the query randomness by picking a favorable verify_key to prepare the report with: if we didn't have (ii), then query randomness is predictable to the attacker.",
          "createdAt": "2023-02-08T19:37:52Z",
          "updatedAt": "2023-02-08T19:37:52Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "But if the query randomness depends on the report nonce (as in `query_rand = SHA-3(nonce + verif_key + stuff)`) an attacker who controls verif_key can **not** pick a favorable verif_key because of the RO properties.\r\nRight?",
          "createdAt": "2023-02-09T00:58:15Z",
          "updatedAt": "2023-02-09T00:58:15Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Good question.\r\n\r\nSo the attacker knows `nonce` and `stuff`, and it can pick `verify_key` before the honest aggregator computes `query_rand`.\r\n\r\nThus, it can query the RO `SHA-3` with different `verify_key`s until it finds one for which the `query_rand` is \"favorable\". Does this make sense?\r\n\r\nIn other words, \"query_rand\" is not uniformly distributed, even though it was output by the RO! Its distribution is biased towards whatever property is favorable to the attacker. For instance, if for some reason `query_rand` is favorable if its first bit is `1`, then the attacker can easily arrange things so that the honest Aggregator computes `query_rand` that begins with `1`.\r\n",
          "createdAt": "2023-02-09T02:25:46Z",
          "updatedAt": "2023-02-09T02:25:46Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "If this is a problem, the problem also exists with 50% probability when using true randomness, so this shouldn't be an issue.",
          "createdAt": "2023-02-09T09:39:35Z",
          "updatedAt": "2023-02-09T09:39:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": " >  ... the problem also exists with 50% probability when using true randomness ....\r\n\r\nI'm just trying to give an example of how the attacker can bias the randomness. With more random oracle queries, it can fix the first 2 bits, 3 bits, ..., N bits, etc. It doesn't really matter. The point is: This possibility of this attack prevents a reduction to honest-verifier ZK of the FLP. (This is a standard assumption and is what is proven for the FLP we use in Prio3: See [2019/188](https://eprint.iacr.org/2019/188).)\r\n\r\nTo avoid this, we need all inputs to the random oracle, including `verify_key`, to be fixed before honest Aggregators compute `query_rand`.\r\n",
          "createdAt": "2023-02-09T15:15:19Z",
          "updatedAt": "2023-02-09T15:15:59Z"
        }
      ]
    },
    {
      "number": 408,
      "id": "I_kwDOFEJYQs5d5GXf",
      "title": "VDAF security analysis: Requirements for report processing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/408",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-04"
      ],
      "body": "The VDAF analysis [posted on the CFRG list](https://mailarchive.ietf.org/arch/msg/cfrg/-errBjFRvCqi7KuAxoZwH6iY4Vc/) suggests the following restrictions is needed:\r\n\r\n> Poplar1: Applications MUST ensure that reports are aggregated at most once\r\n> per-level. Repeated usage of the authenticator at a given level may lead to\r\n> subtle attacks on privacy. In particular, the current\r\n> \"max_batch_query_count\" mechanism of DAP [2, Section 4.5.6] is not\r\n> sufficient.\r\n\r\nBy \"level\" we mean the level of the IDPF tree. The basic idea is that the client generates some randomness for each level of the tree, and it is not safe to use this randomness more than once.\r\n\r\nRelatedly: \r\n\r\n> Prio3: Applications SHOULD ensure that each report is processed at most\r\n> once. It's probably safe to allow re-processing, but (1) this is not\r\n> captured in our model; (2) this gets hairy in terms of differential privacy\r\n> (see issue https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/94); and (3)\r\n> there doesn't seem to be a good reason to allow this. (It is explicitly\r\n> disallowed in DAP.)\r\n\r\nIn other words, the safest bet for Prio3 is to make sure we don't use a proof more than once.\r\n\r\nIn general, VDAFs may need to restrict how a report is processed based on the aggregation parameters with which it was processed in the past.",
      "createdAt": "2023-02-08T00:33:58Z",
      "updatedAt": "2023-02-27T22:11:42Z",
      "closedAt": "2023-02-27T22:11:42Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd suggested that the right place to enforce this is during report processing. We store nonces of previously processed reports; we could also store whatever information we need to restrict future aggregation parameters.\r\n\r\nI think the tricky part is how to spell this. I can think of a couple possible directions:\r\n1. Spell out requirements for Prio3 and Poplar1\r\n2. Try to describe things in terms of a generic VDAF (would require some plumbing in the VDAF document)\r\n\r\nThe first option is easier, but we'd have to update this text whenever we want a new VDAF.",
          "createdAt": "2023-02-08T00:40:09Z",
          "updatedAt": "2023-02-08T00:40:09Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we have to do (2), or DAP is falling short of its design goal to be generic over VDAFs.",
          "createdAt": "2023-02-08T19:16:46Z",
          "updatedAt": "2023-02-08T19:16:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Agreed. A third option is to take (1.) for the next draft, but plan on (2.) for a future draft. This would make our job a bit easier for the next VDAF draft!",
          "createdAt": "2023-02-08T19:32:58Z",
          "updatedAt": "2023-02-08T19:32:58Z"
        }
      ]
    },
    {
      "number": 409,
      "id": "I_kwDOFEJYQs5elLZa",
      "title": "Decouple transmission of report shares from leader to helper from aggregation job creation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/409",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "collecting a batch more than once"
      ],
      "body": "Currently, the leader transmits encrypted report shares to the helper inline in an `AggregationJobInitReq` message. This is inefficient in the face of VDAFs that allow querying the same batch of reports multiple times with different aggregation parameters. Given the current protocol, those report shares must be retransmitted from leader to helper each time. This could be a significant waste of bandwidth, depending on the size of report shares and the number of collections that will be run against a batch.",
      "createdAt": "2023-02-16T00:34:52Z",
      "updatedAt": "2024-05-21T20:58:54Z",
      "closedAt": "2024-05-21T20:58:54Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is akin to #405, and corresponds to [an `OPEN ISSUE` that's been in the protocol text for a while](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/89e5dc32be05ebd4eb032c7f6d0aaa7074c69b08/draft-ietf-ppm-dap.md?plain=1#L1180).",
          "createdAt": "2023-02-16T00:35:53Z",
          "updatedAt": "2023-02-16T00:35:53Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Solving this correctly is tricky: we don't want to force the leader to keep track of whether it has ever transmitted any report share to the helper, and in any case, we have to account for the possibility that the helper has garbage collected a report share between two runs of the collect protocol on a batch.\r\n\r\nAdditionally, we should consider allowing report shares to appear inline in an `AggregationJobInitReq` message, because it saves us a leader<->helper roundtrip in the Prio3 case.",
          "createdAt": "2023-02-16T00:37:14Z",
          "updatedAt": "2023-02-16T00:40:51Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Turns out this is the same as #230.",
          "createdAt": "2023-05-26T21:23:52Z",
          "updatedAt": "2023-05-26T21:23:52Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I conflated the questions of decoupling _report share_ transmission and _aggregation parameter_ transmission when I closed this. This issue is still relevant so I'm reopening it. This is particularly relevant in the context of Poplar1, or other VDAFs where the same reports may be aggregated multiple times. That means we need to study the performance of Poplar1 to understand whether we want to do this, especially as we are otherwise trying to converge DAP towards a stable state.",
          "createdAt": "2023-09-27T15:32:14Z",
          "updatedAt": "2023-09-27T15:32:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 I volunteered to implement multi-collection and resolve all related open issues.",
          "createdAt": "2023-11-08T15:08:06Z",
          "updatedAt": "2023-11-08T15:08:06Z"
        }
      ]
    },
    {
      "number": 413,
      "id": "I_kwDOFEJYQs5ffgJX",
      "title": "DoS possibility using Poplar under heavy hitter mode",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/413",
      "state": "CLOSED",
      "author": "SulemanAhmadd",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "SulemanAhmadd"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "The computation required by Poplar VDAF (for heavy hitter computation) is dependent on the number of bits in the measurement value. The bigger the measurement size, the more computation is needed by Poplar to create an aggregate results (as the number of rounds for aggregation increases superlinearly).\r\n\r\nIn a Sybil attack, malicious clients can craft dummy inputs by setting a very large measurement size which can cause heavy hitters computation to exhibit worst-case performance. This likely results in DoS vector for the Aggregation servers.",
      "createdAt": "2023-02-27T23:08:21Z",
      "updatedAt": "2023-06-26T23:25:19Z",
      "closedAt": "2023-06-26T23:25:19Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks Suleman! I think this needs a bit more context. Poplar1 requires each measurement to have same fixed length, which all parties (including the Clients) need to agree on prior to VDAF execution. Can you clarify what you mean by \"very large\"? No measurement size can exceed the fixed length.",
          "createdAt": "2023-02-27T23:31:24Z",
          "updatedAt": "2023-02-27T23:31:24Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "CONTRIBUTOR",
          "body": "When deciding the value of \"fixed length\" for the measurements, care must be taken that DAP deployment can handle heavy hitter computation using Poplar1. The larger the configured length of the measurement, the more computation is required.\r\n\r\nI'll expand on by giving an example:\r\n\r\nConsider the scenario when we use variable length domain names as the measurement values for Poplar1. We can set the length of each measurement to be fixed at 255 bytes (255 is the maximum number of characters allowed in a domain name by DNS) so that each domain name can fit inside the measurement value.\r\n\r\nThe domain name distribution has a long tail i.e. only a small number of domains are more than 100 characters. This means that for most cases we would have to encode the domain name and add some sort of padding to reach the 255 bytes fixed measurement size.\r\n\r\nWhen computing the heavy hitter for a batch of such measurements, an optimization we can add is to stop processing (or traversing the IDPF tree) when recovering a measurement if we have reached the padding region. Since, we know what the next set of values are, we do not need to traverse the entire IDPF tree for the computation. This has the implication that for smaller length domain names, It significantly reduces the cost of communication between aggregators and the steps required to recover a heavy hitter measurement.\r\n\r\nOn the other side, in case of Sybil attacks, malicious client can always use the maximum size domain names forcing the system to always operate on worst-case performance (the optimization cannot be applied) which can potentially hint towards a DoS vector.\r\n\r\n---\r\n\r\nBasically, the choice of length of measurement directly impacts the required computation required by a DAP deployment superlinearly when using Poplar1 VDAF. This might be more of a consideration for deployment rather a bug in the model.",
          "createdAt": "2023-02-28T19:28:59Z",
          "updatedAt": "2023-02-28T19:28:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks, this makes sense to me! I suggest we add some text in security considerations about this. I don't know if we yet have a DoS risk category, but probably not. Would you mind sending a PR?",
          "createdAt": "2023-02-28T21:07:51Z",
          "updatedAt": "2023-02-28T21:07:51Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Sure. I'll create a PR.",
          "createdAt": "2023-03-02T20:20:02Z",
          "updatedAt": "2023-03-02T20:20:02Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "#415 was long since merged, so I don't think there's anything left to do in this issue. Thanks again for flagging this problem and making a PR, Suleman!",
          "createdAt": "2023-06-26T23:25:19Z",
          "updatedAt": "2023-06-26T23:25:19Z"
        }
      ]
    },
    {
      "number": 414,
      "id": "I_kwDOFEJYQs5ff_1y",
      "title": "Consider binding the task ID to the VDAF verification key ",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/414",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "As of #411, the draft spells out requirements around exchanging the VDAF verification key. It includes the following suggestion:\r\n\r\n>  One way to ensure this is to include the verification key in a derivation of the task ID.\r\n\r\nThe idea being that by deriving the task ID verification key, we assure clients that the key was picked prior to the task being configured and the key doesn't change over the course of the task.\r\n\r\nWe might consider spelling out a scheme for this, in order to remove this security critical choice from deployments.\r\n\r\nSee https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/407 for more discussion",
      "createdAt": "2023-02-28T01:34:52Z",
      "updatedAt": "2023-10-23T20:16:42Z",
      "closedAt": "2023-10-23T20:16:42Z",
      "comments": []
    },
    {
      "number": 426,
      "id": "I_kwDOFEJYQs5gSEc9",
      "title": "metadata -> report_metadata renaming inconsistency",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/426",
      "state": "CLOSED",
      "author": "bhalleycf",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "The current version of the draft renames field \"metadata\" to \"report_metadata\" in the Report structure, but does not do the same renaming in the ReportShare and InputShareAad structures.  I think this naming should be consistent, though I don't have a preference between the alternatives.",
      "createdAt": "2023-03-08T14:14:50Z",
      "updatedAt": "2023-03-10T22:21:14Z",
      "closedAt": "2023-03-10T22:21:14Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for pointing this out! Can you send prepare a PR?",
          "createdAt": "2023-03-08T16:15:40Z",
          "updatedAt": "2023-03-08T16:15:40Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Fixed in #427 ",
          "createdAt": "2023-03-10T22:21:14Z",
          "updatedAt": "2023-03-10T22:21:14Z"
        }
      ]
    },
    {
      "number": 432,
      "id": "I_kwDOFEJYQs5hQRr7",
      "title": "Change PRG",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/432",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "If we reconsider our choice of PRG we should take into account that:\r\n1. The RNG is modeled as a random oracle in the proofs but this does not necessarily mean that we need a XOF. It might be more efficient to create a seed using something like SHA-3 (or -2) and then applying a fast AES based stream derivation.\r\n2. cSHAKE (which we are currently using) was only defined in NIST SP 800-185 not the original SHA-3 standard (FIPS 202) so it might not be available in some implementations. FIPS 202 has SHAKE which is also a XOF and we are already defining how to encode the binding string. We could simply define something similar to cSHAKE based on SHAKE. (Exact match is not possible due to non-matching constants.)",
      "createdAt": "2023-03-20T08:53:26Z",
      "updatedAt": "2023-03-21T14:43:16Z",
      "closedAt": "2023-03-21T14:43:16Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for summarizing this Simon!\r\n\r\nThis is more of a question for VDAF than for DAP, so I'd like to suggest we move it there.",
          "createdAt": "2023-03-20T15:53:20Z",
          "updatedAt": "2023-03-20T15:53:20Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton Maybe you or @divergentdave can do that? You should have write permissions for both repos, right?",
          "createdAt": "2023-03-21T11:03:49Z",
          "updatedAt": "2023-03-21T11:05:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Sure, I just thought you should raise it if you intend to push for this change. In any case it's a question about the underlying crypto, so I'll close this.",
          "createdAt": "2023-03-21T14:43:15Z",
          "updatedAt": "2023-03-21T14:43:15Z"
        }
      ]
    },
    {
      "number": 433,
      "id": "I_kwDOFEJYQs5iuGJm",
      "title": "editorial: type of AggregationJobContinueReq.round should be `uint16`, not `u16`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/433",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "",
      "createdAt": "2023-04-05T20:30:28Z",
      "updatedAt": "2023-07-03T18:08:34Z",
      "closedAt": "2023-07-03T18:08:34Z",
      "comments": []
    },
    {
      "number": 435,
      "id": "I_kwDOFEJYQs5jXBgr",
      "title": "Consider adding content type to POST <collection job> request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/435",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "In Daphne we use the media type to help guide request pre-processing, including parsing of resource IDs:\r\nhttps://github.com/cloudflare/daphne/blob/main/daphne_worker/src/config.rs#L885\r\n\r\nThe content type is missing from the POST <collection job> request, so we have to handle parsing the collection ID as a special case:\r\nhttps://github.com/cloudflare/daphne/blob/main/daphne_worker/src/lib.rs#L368-L382\r\n\r\nAdditional signaling would be help us reduce a little code complexity here.",
      "createdAt": "2023-04-13T19:01:34Z",
      "updatedAt": "2023-04-13T23:01:19Z",
      "closedAt": "2023-04-13T23:01:19Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "There is no body to a `POST` request on a collection job (used to poll the job's status), so I don't think it makes sense to include a content type. Do you mean the `PUT` request, used to create the job, whose body is `struct CollectionReq`?",
          "createdAt": "2023-04-13T19:44:55Z",
          "updatedAt": "2023-04-13T19:44:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I mean the POST actually. The PUT request already has the content type.",
          "createdAt": "2023-04-13T20:14:13Z",
          "updatedAt": "2023-04-13T20:14:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan convinced me offline that there is no sense in setting a content-type header for an empty body. We'll figure out some other way to deal with this.",
          "createdAt": "2023-04-13T23:01:19Z",
          "updatedAt": "2023-04-13T23:01:19Z"
        }
      ]
    },
    {
      "number": 436,
      "id": "I_kwDOFEJYQs5jXCau",
      "title": "Do we still need `max_batch_query_count`?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/436",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "collecting a batch more than once"
      ],
      "body": "This seems redundant now that w'ere now enforcing this restriction at the VDAF level:\r\n\r\n> Check if the report may still be aggregated with the current aggregation parameter. This can be done by looking up all aggregation parameters previously used for this report and calling\r\n>\r\n> `VDAF.is_valid(current_agg_param, previous_agg_params)`\r\n>\r\n> If this returns false, the input share MUST be marked as invalid with the error report_replayed.",
      "createdAt": "2023-04-13T19:04:38Z",
      "updatedAt": "2024-05-21T20:58:54Z",
      "closedAt": "2024-05-21T20:58:54Z",
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "We'll need to fix #259 as well before eliminating `max_batch_query_count`. (this just came up in another discussion) Otherwise, `max_batch_query_count = 1` is load-bearing in terms of preventing replaying batches with a different set of reports.",
          "createdAt": "2023-04-13T19:12:37Z",
          "updatedAt": "2023-04-13T19:12:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 I volunteered to implement multi-collection and resolve all related open issues.",
          "createdAt": "2023-11-08T15:08:14Z",
          "updatedAt": "2023-11-08T15:08:14Z"
        }
      ]
    },
    {
      "number": 437,
      "id": "I_kwDOFEJYQs5jXuli",
      "title": "Error URIs for \"bad request\" and \"internal server error\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/437",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The spec says problem details with a \"type\" other than those defined in the draft MAY be used, but MUST NOT overload the URN:\r\n\r\n> This list is not exhaustive. The server MAY return errors set to a URI other than those defined above. Servers MUST NOT use the DAP URN namespace for errors not listed in the appropriate IANA registry (see [Section 8.4](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-04.html#urn-space)).\r\n\r\nWhat's the motivation for this \"MUST NOT\"? Is it strictly necessary?\r\n\r\nI ask because there are a number of abort cases not covered by the existing URIs, and I'm not sure how to set the \"title\". There are two \"catch all\" aborts in Daphne right now:\r\n1. \"badRequest\": An error occurred that was triggered by the request.\r\n2. \"internalError\": The server encountered an unexpected error that it cannot recover from.\r\n\r\nI think any implementation of DAP is going to have to cover these cases.\r\n\r\nI have two questions:\r\n1. Should we codify these as URIs in the spec, i.e., define \"urn:ietf:params:ppm:dap:error:badRequest\"?\r\n2. If the answer to (1.) is \"no\", what are reasonable values for the types? \"badRequest\" without the prefix?",
      "createdAt": "2023-04-13T21:37:07Z",
      "updatedAt": "2023-04-13T22:12:01Z",
      "closedAt": "2023-04-13T22:11:04Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Bad request is just an HTTP 400, and internal error is just an HTTP 500. These are already well defined by HTTP. I don't think DAP would help anyone by redefining concepts from HTTP. [RFC 7807](https://www.rfc-editor.org/rfc/rfc7807#section-3.1) says that the `type` field is optional (\"When this member is not present, its value is assumed to be \"about:blank\".'), and DAP does not _require_ a problem document: \"When the server responds with an error status, it SHOULD provide additional information using a problem document\" ([1](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-04.html#section-3.2-2)).\r\n\r\nSo I think it's all right for a DAP aggregator to return HTTP 400 or HTTP 500 with no problem document at all, if no specific DAP error type applies.",
          "createdAt": "2023-04-13T21:55:26Z",
          "updatedAt": "2023-04-13T21:55:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ok, that's good enough for me. I'll just remove the type field for these cases.",
          "createdAt": "2023-04-13T22:10:57Z",
          "updatedAt": "2023-04-13T22:10:57Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "The reason for the `MUST NOT` is primarily forward compatibility, as the DAP draft is in charge of this URI namespace, and needs to be able to pick new types in it going forward freely. (Each error type that's defined gets to declare what set of additional attributes are expected, and in each case for DAP's error types, that's just the one for task ID) Catch-all \"bad request\" and \"internal error\" errors would be best served by `about:blank` or an absent type, as Tim mentions, but if there's a need for some other kind of custom error type, you could also define your own error type URI, for example with an `https://` URL you control. (The RFC recommends putting some documentation at that URL)",
          "createdAt": "2023-04-13T22:12:00Z",
          "updatedAt": "2023-04-13T22:12:00Z"
        }
      ]
    },
    {
      "number": 438,
      "id": "I_kwDOFEJYQs5jv2uo",
      "title": "Possible states of PrepareSteps sent in an aggregation continue request are ambiguous.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/438",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The spec says in the [helper continuation section](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.2.2-5.1) that helpers should be prepared for leaders to send PrepareSteps in the failed state.\r\n\r\n> If the status is failed, then mark the report as failed and reply with a failed PrepareStep to the Leader.\r\n\r\nIn the [leader continuation section](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.2.1-11), it says of the same continue request message that,\r\n\r\n> The prepare_steps field MUST be a sequence of PrepareSteps in the continued state containing the corresponding inbound prepare message.\r\n\r\nWe should be clear about whether non-`continue` PrepareStepStates are allowed in these requests. It might be useful to allow `failed` in `AggregationJobContinueReq` steps, because then it could carry a `ReportShareError`, so that the helper could get visibility of error codes, as the leader does. (The helper can already notice that a report share was filtered out from one request to the next, and infer that there was some error)",
      "createdAt": "2023-04-18T16:38:25Z",
      "updatedAt": "2023-10-24T21:48:53Z",
      "closedAt": "2023-10-24T21:48:52Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yup, we definitely need to clarify this, and I agree that an explicit signal of rejection is most useful. For what it's worth Daphne Helper will abort if it encounters a `failure` in the `AggregateContReq`: https://github.com/cloudflare/daphne/blob/main/daphne/src/vdaf/mod.rs#L755",
          "createdAt": "2023-04-18T16:43:01Z",
          "updatedAt": "2023-04-18T16:43:01Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree this merits clarification, and also vote for an explicit signal of rejection (i.e. the Leader sends Failed PrepareSteps to the Helper on error).",
          "createdAt": "2023-04-18T22:32:43Z",
          "updatedAt": "2023-04-18T22:32:43Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe this issue is obsolete, because the changes in #393 mean that it's no longer possible for `AggregationJobContinueReq` to contain a failure or share rejection message. However, what remains is the idea that the leader should explicitly signal preparation failure to the helper. I don't think we have a strong enough case for this yet, especially since deployments can use some means out of band of DAP to share error information between aggregators. So we should keep this open to eventually discuss an in-band mechanism for leader-to-helper error reporting, but I don't think there's anything to do for draft-ietf-ppm-dap-05 anymore (except to update a TODO which referenced the wrong issue number).",
          "createdAt": "2023-06-26T23:22:41Z",
          "updatedAt": "2023-06-26T23:22:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing with no action. For the moment we don't have a compelling reason for a fancier error handling mechanism. If we want to discuss this further, let's open a fresh issue with a more refined problem statement.",
          "createdAt": "2023-10-24T21:48:52Z",
          "updatedAt": "2023-10-24T21:48:52Z"
        }
      ]
    },
    {
      "number": 441,
      "id": "I_kwDOFEJYQs5kgBwh",
      "title": "URI template is confusing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/441",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": "The `{role}/resource_type/{resource-id}` template doesn't seem to hold.\r\n\r\n> A resource's path is resolved relative to a server's endpoint to construct a resource URI. Resource paths are specified as templates like:\r\n>\r\n> ```\r\n> {role}/resource_type/{resource-id}\r\n> ```\r\n>\r\n> `{role}` is one of the API endpoints in the task's aggregator_endpoints (see [Section 4.2](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-04#task-configuration)). The remainder of the path is resolved relative to the endpoint.\r\n>\r\n> DAP resource identifiers are opaque byte strings, so any occurrence of `{resource-id}` in a URL template (e.g., `{task-id}` or `{report-id}`) MUST be expanded to the URL-safe, unpadded Base 64 representation [...]\r\n\r\nThe literal string \"resource_type\" does not appear in the document.  Instead, we have things like `{aggregator}/hpke_config` (with no apparent `{resource-id}`) and `{leader}/tasks/{task-id}/reports`.\r\n\r\nIt seems like you might benefit from the base64url note as a general rule, but the idea that you might define a common template form is probably not right.  Maybe you can limit this text to an explanation that resources are identified using templates relative to a root URL that each aggregator defines.",
      "createdAt": "2023-04-27T04:59:23Z",
      "updatedAt": "2023-05-01T16:41:32Z",
      "closedAt": "2023-05-01T16:41:32Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @martinthomson! I agree this change makes sense.",
          "createdAt": "2023-04-28T19:44:50Z",
          "updatedAt": "2023-04-28T19:44:50Z"
        }
      ]
    },
    {
      "number": 442,
      "id": "I_kwDOFEJYQs5kgOdI",
      "title": "Replay attack requirements could be tighter",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/442",
      "state": "OPEN",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "editorial"
      ],
      "body": "It looks like the entirely of the replay protection requirements are buried in an implementation note in [Section 4.4.1.4](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.1.4-3.6.4.1).  The text here is pretty terse, so even though it is probably a good cue to someone who understands the problem pretty well, there are lots of bits missing:\r\n\r\n1. Normative language.  (Maybe this is covered by [this paragraph](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.1.2-2), but that is also buried.)\r\n2. An explanation of the attack that this covers.[*]\r\n3. Discussion of the use of timestamps, clock skew, and the associated trade-offs.\r\n\r\nThis probably justifies a section of its own.\r\n\r\n[*] Replay attack mitigation wouldn't be necessary if you submit the data directly (and don't retry failed uploads ... and avoid using TLS early data).  Therefore, honest clients that submit data directly to the leader are only exposed to the possibility that the leader defects and replays the submission.  So maybe only helpers need to perform validation under this architecture.\r\n\r\n",
      "createdAt": "2023-04-27T06:01:00Z",
      "updatedAt": "2023-11-01T16:44:33Z",
      "closedAt": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Martin wrote this issue against DAP-04. Section 4.4.1.4 is now [4.5.1.4 Input Validation](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-08#name-input-share-validation).\r\n\r\nThere are lots of references to 4.5.1.4 Input Validation from sections of normative text that specify the upload, aggregation and aggregate share sub-protocols. I believe we did this so that the document would have one canonical description of how to validate input shares, but it seems that the indirection is confusing to at least Martin (whose sense of what makes a good standard ought not be ignored), and even as an author, I find it difficult to decide how 4.5.1.4's language -- which is written in terms of an aggregator preparing inputs -- applies to the upload protocol.\r\n\r\nI think the path forward here is to break up 4.5.1.4 and \"inline\" its normative text into the different sections that currently refer to it. Then the upload section could explicitly discuss what should be done during that sub-protocol instead of making implementers reason through which parts of 4.5.1.5 are relevant. The downside is that we'd be duplicating some information (e.g., across the leader and helper descriptions of input validation) and admit the risk of disagreement between sections.",
          "createdAt": "2023-11-01T16:44:33Z",
          "updatedAt": "2023-11-01T16:44:33Z"
        }
      ]
    },
    {
      "number": 443,
      "id": "I_kwDOFEJYQs5kgPEb",
      "title": "Clients that submit multiple times",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/443",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "editorial"
      ],
      "body": "I don't see a [Client capability](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-7.1.1.2)  that addresses the possibility that a client might make multiple submissions.  Let's say Prio3Sum is in use, a client can't submit a value that exceeds the upload contribution limit, but it could submit multiple times.\r\n\r\nSee #89 for mitigations.",
      "createdAt": "2023-04-27T06:03:10Z",
      "updatedAt": "2023-10-19T22:12:04Z",
      "closedAt": "2023-10-19T22:12:04Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Good call, I think it's worth calling this out in the security considerations.",
          "createdAt": "2023-04-28T20:23:49Z",
          "updatedAt": "2023-04-28T20:23:49Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "We [now have text](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-capabilities-and-mitigation) that describes this attack, which landed as part of #488, so we can close this out.",
          "createdAt": "2023-10-19T22:12:04Z",
          "updatedAt": "2023-10-19T22:12:04Z"
        }
      ]
    },
    {
      "number": 444,
      "id": "I_kwDOFEJYQs5kgUT7",
      "title": "On demand collection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/444",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "It seems like the fixed cadence of collection isn't necessary, but the options are currently limited only by a need to have a demonstrated use case before adding options.\r\n\r\nI'm guessing, but the fixed time limit or fixed count might exist in order to cap the amount of state that the leader (or all aggregators) might need to retain to track and prevent replays[^1].  But then the timestamp in uploads should address that, allowing aggregation over arbitrary intervals.  A batching VDAF[^2] might have limits on how many reports can be gathered, just in terms of database size, I guess.\r\n\r\nAllowing on-demand collection would allow a collector the ability to trade off between timeliness of information and allowing more reports to accumulate.  Especially in the presence of DP protections (#19), this might allow the collector to control when it pays the DP noise cost of making a query.  For a VDAF that has a cutoff, like Poplar1, deferring a query might allow more information to be made available.\r\n\r\n[^1]: The text in the draft isn't very clear about replay attacks: #442.\r\n[^2]: I'm still of the opinion that a unified protocol makes very little sense here.  A Prio implementation carries all the complexity of Poplar in the protocol, which is non-trivial.",
      "createdAt": "2023-04-27T06:21:54Z",
      "updatedAt": "2023-10-19T22:08:25Z",
      "closedAt": "2023-10-19T22:08:25Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Can you clarify what you mean by \"fixed cadence of collection\" and \"on-demand collection\"?\r\n\r\nThe flexibility of collection is determined by the [query type](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.1) associated with the task:\r\n* For the \"time interval\" query type, the Collector picks a time interval and gets an aggregate of all reports with time stamps in that time interval (as long as there are a sufficient number of reports)\r\n* For the \"fixed size\" query type, the Collector gets an aggregate of a batch of reports chosen by the Leader (as long as the size of the batch is within the range specified by the task config)\r\n* Future query types specify their own batch selection criteria and degree of flexibility",
          "createdAt": "2023-04-28T20:38:36Z",
          "updatedAt": "2023-04-28T20:38:36Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "\"fixed cadence\" = \"time interval\".  This seems to be very rigid (fixed length intervals).\r\n\r\n\"fixed size\" seems to be bound to have a maximum size.  (The name also implies min=max, which doesn't appear to be the case.)\r\n\r\n> Future query types specify their own batch selection criteria and degree of flexibility\r\n\r\n\"on demand\" means that a collector can initiate collection when it wants, not bound by either time or number of reports.  I was thinking that maybe this could happen now as opposed to in the future.  \r\n\r\n\r\n",
          "createdAt": "2023-04-30T10:20:05Z",
          "updatedAt": "2023-04-30T10:20:05Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> \"fixed cadence\" = \"time interval\". This seems to be very rigid (fixed length intervals).\r\n\r\nThe intervals don't have to be fixed length. It can be any value subject to restrictions imposed by the [boundary check](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-boundary-check). As described [here](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-5.4.1-1), these are meant to reduce storage requirements of the Aggregators.\r\n\r\n> \"fixed size\" seems to be bound to have a maximum size. (The name also implies min=max, which doesn't appear to be the case.)\r\n\r\nYup, fixed size imposes a maximum batch size. The thinking here is that you'd want batches that are all \"roughly\" the same size, e.g., you might pick maximum batch size to be within some small constant factor of the minimum batch size.\r\n\r\nWhy allow max!=min? Our thinking at the time is that it may be difficult for the Leader to guarantee that it'll hit the target batch size every time. Implementation experience will hopefully tell us whether or not this is the case.\r\n\r\n> > Future query types specify their own batch selection criteria and degree of flexibility\r\n> \r\n> \"on demand\" means that a collector can initiate collection when it wants, not bound by either time or number of reports. I was thinking that maybe this could happen now as opposed to in the future.\r\n\r\nCertainly! It seems to me that this could be spelled out as a new query type. Do you agree?",
          "createdAt": "2023-05-01T16:16:16Z",
          "updatedAt": "2023-05-01T16:16:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Queries are defined [here](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.1).",
          "createdAt": "2023-05-01T16:27:57Z",
          "updatedAt": "2023-05-01T16:27:57Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Based on this, it seems to me like you could just as easily dispense with the idea of query types and instead define query bounds: minimum and maximums for both time and number of records.  Then, if maximums can be unbounded and minimums are zero you have covered a lot of ground.",
          "createdAt": "2023-05-01T23:40:48Z",
          "updatedAt": "2023-05-01T23:40:48Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Would the idea be that one could mix query \"types\" arbitrarily? For example, would the following sequence of queries be allowed:\r\n1. Aggregate the last 50 reports you saw.\r\n2. Aggregate all reports between 07:00 and 07:15 hours.\r\n3. (Hypothetical, not currently supported) Aggregate all reports with user agent \"Firefox\".\r\n\r\nThis might work, but we need to be careful. One requirement we have to meet is ensure that all batches are non-overlapping (i.e., no two batches contain the same report). This is meant to prevent [intersection attacks](https://docs.google.com/presentation/d/1cJEJ07nF_A0zWU49bQXedtrKLIUBqHrw7xzQZaR8ckg/edit#slide=id.g13d9afd0c43_0_171).\r\n\r\nAnother consideration is that we would like to avoid storing individual shares. The current restrictions on queries allow us to store these in aggregate instead.",
          "createdAt": "2023-05-02T02:44:55Z",
          "updatedAt": "2023-05-02T02:44:55Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I am thinking that this would be as simple as requesting collection based on the currently accumulated reports.  As long as the maximum wasn't already hit (in which case, the query might be answered immediately), asking for collection creates a barrier between queried reports and unqueried reports.  There might need to be a settling period to allow laggardly reports to be delivered if the cutoff is a time that is too close to the current time or there aren't enough reports to reach the minimum number. Once the set of reports is resolved though, the collection starts.  (You could also just outright reject queries that are too close to the current time or when there aren't enough uploads.)\r\n\r\nThis is closer to a query language, but it means that rather than having the query encompassed in a type, you express axes along which the query applies, and bounds on those axes.  As you say, you might have color=\"red\" as a query type, so that only red reports are included (as opposed to the colorless green ones).  That doesn't mean you couldn't have - as we are currently planning - a daily dump that is based on a {minimum, maximum} duration = 1d.  That has real benefits (you never have to guess where to accumulate).  But you can also set a minimum (in time or report count) and have no maximum, where people can decide when to consume the data (at least for circuits that can accumulate on the fly like Prio3Sum; other VDAFs might need caps).",
          "createdAt": "2023-05-02T06:10:05Z",
          "updatedAt": "2023-05-02T06:10:05Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@martinthomson given that QueryType is extensible, I'd propose that the sort of flexibility you have in mind here be written up and brought to the group as a new draft. I'm going to close this out on that basis.",
          "createdAt": "2023-10-19T22:08:25Z",
          "updatedAt": "2023-10-19T22:08:25Z"
        }
      ]
    },
    {
      "number": 445,
      "id": "I_kwDOFEJYQs5kgZQK",
      "title": "Clock skew and report inclusion in reports",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/445",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "For reports that are submitted to a task with a time interval query, what determines whether a report is included in an interval?  This isn't clear.\r\n\r\nAssuming that this is based on the value that clients assign to reports, then clock skew in clients will lead to two edge cases:\r\n\r\n1. Submissions for an interval might arrive well ahead of the actual start time.  This is only a problem if the leader (and maybe the helper) don't want to keep that report for the time remaining until the interval starts.\r\n2. Submissions for an interval might arrive well after the end time.\r\n\r\nThe latter is a problem if the report is delivered around the time that collection commences.  Especially if there are multiple actions in flight.\r\n\r\nLet's say that the interval ends and, 10 seconds later, collection is initiated.  At about the same time, the last few reports trickle in.  Which reports make it into the collection are hard to define.  The leader indicates the report count and that might match the report count at the helper - because the leader might concurrently be processing uploads - the set of reports that the leader includes might not match those used by the helper.  The checksum will detect this, but it seems like a trivially avoidable problem.\r\n\r\nThe leader could ensure that there is a time (based on some agreed allowance for skew) after which it does not accept reports.  After this, it might reject requests to collect until the reports it has received have been delivered to helpers.",
      "createdAt": "2023-04-27T06:41:15Z",
      "updatedAt": "2023-10-23T22:26:39Z",
      "closedAt": "2023-10-23T22:26:39Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Assuming that this is based on the value that clients assign to reports, [...]\r\n\r\nThat's correct.\r\n\r\n> [...] then clock skew in clients will lead to two edge cases:\r\n> \r\n>     1. Submissions for an interval might arrive well ahead of the actual start time.  This is only a problem if the leader (and maybe the helper) don't want to keep that report for the time remaining until the interval starts.\r\n\r\nRight. In fact, the Leader is empowered to reject such reports [in the upload flow](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.3.2-22). As is the Helper during the [aggregation flow](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-1.1-5.10).\r\n\r\n\r\n>     2. Submissions for an interval might arrive well after the end time.\r\n> \r\n> \r\n> The latter is a problem if the report is delivered around the time that collection commences. Especially if there are multiple actions in flight.\r\n> \r\n> Let's say that the interval ends and, 10 seconds later, collection is initiated. At about the same time, the last few reports trickle in. Which reports make it into the collection are hard to define. The leader indicates the report count and that might match the report count at the helper - because the leader might concurrently be processing uploads - the set of reports that the leader includes might not match those used by the helper. The checksum will detect this, but it seems like a trivially avoidable problem.\r\n> \r\n> The leader could ensure that there is a time (based on some agreed allowance for skew) after which it does not accept reports. After this, it might reject requests to collect until the reports it has received have been delivered to helpers.\r\n\r\n\r\nGood call out. We are aware of this problem (see [this Janus PR](https://github.com/divviup/janus/pull/1254)), and I believe at some point we had text the effect of \"the Leader MUST NOT request an aggregate share for a batch from the Helper until all pending aggregation jobs that pertain to the batch are completed\". However I can't find anything about this in the current version.\r\n\r\n",
          "createdAt": "2023-04-28T20:56:32Z",
          "updatedAt": "2023-04-28T20:56:32Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> the Leader MUST NOT request an aggregate share for a batch from the Helper until all pending aggregation jobs that pertain to the batch are completed\r\n\r\nFWIW, these are the semantics we're working towards implementing in Janus ([relevant issue](https://github.com/divviup/janus/issues/1283#issuecomment-1528089866)). I would be supportive of adding text to this effect to the spec, as AFAICT, a concurrent aggregation completion request & aggregate share request can inherently lead to the Leader & Helper falling out of sync in terms of which reports are included in a batch, in a way that can't be protected against by reasonable implementation strategies.",
          "createdAt": "2023-04-28T21:25:54Z",
          "updatedAt": "2023-04-28T21:30:32Z"
        }
      ]
    },
    {
      "number": 446,
      "id": "I_kwDOFEJYQs5kgZh2",
      "title": "Cheaper checksum",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/446",
      "state": "CLOSED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "ietf118"
      ],
      "body": "The current checksum algorithm is a little expensive.  It would be easier to just concatenate all of the report IDs and hash the resulting stream.  SHA-256 implementations usually have an IUF interface.",
      "createdAt": "2023-04-27T06:42:21Z",
      "updatedAt": "2023-11-07T13:46:11Z",
      "closedAt": "2023-11-07T13:46:11Z",
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "We need this checksum to not depend on the order of reports. The checksum gets updated with other accumulators as aggregation jobs finish (for the aggregate share, which is a field element vector, the number of reports included, and the time interval spanning all included report timestamps), and we need to tolerate the leader and helper adding contributions from different aggregation jobs in different orders. We could swap out SHA-256 for a cheaper function, and keep the same XOR accumulator structure.",
          "createdAt": "2023-04-27T14:44:16Z",
          "updatedAt": "2023-04-27T14:44:16Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Ah, that's right.  I wonder if XOR without SHA-256 is enough, given the construction requirements on report IDs.",
          "createdAt": "2023-04-27T23:16:08Z",
          "updatedAt": "2023-04-27T23:16:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not sure, given that clients may choose the report IDs maliciously (i.e., not randomly generated). If a batch mismatch occurs, the clients may be able to pick report IDs such that the Aggregators compute the same checksum. For example, imagine the Leader computes `checkusm = ID1 ^ ID2` and the Helper computes `chuksum = ID3 ^ ID4`, where `ID1 ^ ID2 = ID3 ^ ID4`.\r\n\r\nHashing each ID is meant to make this harder, [but even this may be insufficient](https://twitter.com/David3141593/status/1649415552463503360). It may be sufficient to ensure the clients can't predict the value of the checksum computed by either aggregator.",
          "createdAt": "2023-04-28T21:16:26Z",
          "updatedAt": "2023-04-29T00:08:04Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's keep in mind that when we introduced this hash, it wasn't designed as a security mechanism but rather a means of flushing out bugs in either implementations or the early drafts of the protocol itself. Point being: rather than reconsider which algorithms we use here, we might instead reconsider whether DAP needs this checksum at all. What we've said so far is that we need more operational experience with DAP to see how often the aggregators disagree on what reports belong to a given collection and I'm not sure we're there yet.",
          "createdAt": "2023-05-01T16:37:19Z",
          "updatedAt": "2023-05-01T16:37:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Fair enough. So far this has not been an issue, but the highest load we've (Daphne) tried so far is only around 500 reports/s.",
          "createdAt": "2023-05-01T16:43:48Z",
          "updatedAt": "2023-05-01T16:43:48Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "The checksum/report-count functionality was useful in catching some implementation bugs during Janus load-testing. In these tests we could also have determined the error by looking to the aggregate output, since the inputs were known; but in a real deployment, this would not be possible. (only the Collector is exposed to the final aggregate, and it can only check correctness insofar as it can estimate plausibility of the aggregate value it receives.)\r\n\r\nGiven that in most real deployments, the Aggregators will be run by different organizations than the Collector, it would be very nice if the Aggregators could determine if there was a report mismatch somehow before they sent bad data to the Collector, whose organization they may view as a client/subscriber to their service.",
          "createdAt": "2023-05-01T21:01:00Z",
          "updatedAt": "2023-05-01T21:01:00Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think we're all on the same page so just to be explicit: I see no need for this checksum from a privacy or robustness point of view.",
          "createdAt": "2023-06-19T11:15:41Z",
          "updatedAt": "2023-06-19T11:15:41Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@simon-friedberger I'm not sure we are on the same page here. @branlwyd suggests that keeping this checksum around to help catch failures in production is a useful tool. I can imagine some deployments wanting to keep this around indefinitely, whereas other deployments removing after they reach a certain level of confidence in their batch agreement logic. Perhaps we can split the difference by making checksum an optional value for aggregators?",
          "createdAt": "2023-10-19T21:33:29Z",
          "updatedAt": "2023-10-19T21:33:29Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "In our (Cloudflare) own internal experiments, we have triggered the condition that the checksum mechanism was designed to detect. Speaking as an operator, I want to make sure we keep this signal in the protocol.\r\n\r\nI'd be open to simplifying it, or making it optional as @chris-wood's PR proposes. However, we should also recognize that the cost of the checksum is quite low when we compare it to the heavier weight crypto that surrounds it (HPKE decryption, VDAF preparation).",
          "createdAt": "2023-10-19T23:19:19Z",
          "updatedAt": "2023-10-19T23:19:19Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm wary of adding optional or conditional behavior to the protocol. I think a per-task option for the checksum is the most we should ever do about this -- I definitely don't want any more complex in-band signaling to govern whether aggregators will include a checksum for a particular aggregate share.",
          "createdAt": "2023-10-23T17:32:46Z",
          "updatedAt": "2023-10-23T17:32:46Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@chris-wood To clarify: This may catch errors and be useful, but it is not necessary for the privacy guarantees nor for the robustness guarantees.\r\n\r\n",
          "createdAt": "2023-10-25T07:53:19Z",
          "updatedAt": "2023-10-25T07:53:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118, we decided to close this issue without action. Folks were leaning towards making it optional, but since it's relatively cheap we decided optimization here is mostly a bike shed.",
          "createdAt": "2023-11-07T13:46:11Z",
          "updatedAt": "2023-11-07T13:46:11Z"
        }
      ]
    },
    {
      "number": 448,
      "id": "I_kwDOFEJYQs5krHmD",
      "title": "Unique HpkeConfigId",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/448",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": ">    Aggregators SHOULD allocate distinct id values for each HpkeConfig in\r\n   a HpkeConfigList.  The RECOMMENDED strategy for generating these\r\n   values is via rejection sampling, i.e., to randomly select an id\r\n   value repeatedly until it does not match any known HpkeConfig.\r\n\r\n1. Why is this a SHOULD and not a MUST? What happens if you have duplicates?\r\n2. Why does this document take a position on how to generate it? Is there something wrong with just a counter?",
      "createdAt": "2023-04-28T19:17:48Z",
      "updatedAt": "2023-05-01T19:01:32Z",
      "closedAt": "2023-05-01T19:01:32Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "1. If there are duplicates then [the Aggregator is meant to use trial decryption to determine which HPKE config was used by the Client](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.3.2-18). Personally, I don't see a reason we should allow this. I think lifting the SHOULD to a MUST is a good idea.\r\n2. I don't think it needs to, a counter sounds just fine.\r\n\r\nStepping back, this language seems like it was lifted from the ECH spec, which has very different requirements for HPKE than we have here.",
          "createdAt": "2023-04-28T21:22:46Z",
          "updatedAt": "2023-04-28T21:22:57Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's make it a MUST.",
          "createdAt": "2023-04-28T22:45:42Z",
          "updatedAt": "2023-04-28T22:45:42Z"
        }
      ]
    },
    {
      "number": 449,
      "id": "I_kwDOFEJYQs5krIW3",
      "title": "Specify the types for each PDU",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/449",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "divergentdave"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "This document contains media type registrations in S 8.1 but afaict the PDUs don't themselves say what type they are.",
      "createdAt": "2023-04-28T19:20:45Z",
      "updatedAt": "2023-05-09T11:19:33Z",
      "closedAt": "2023-05-09T11:19:33Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "What does PDU stand for?",
          "createdAt": "2023-04-28T21:24:23Z",
          "updatedAt": "2023-04-28T21:24:23Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "protocol data unit (message).\r\n\r\nIt seems like some of the narrative text says what media type it is but others do not.\r\n\r\n\r\n\r\n",
          "createdAt": "2023-04-28T22:43:56Z",
          "updatedAt": "2023-04-28T22:43:56Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Got it, yes this would be helpful!",
          "createdAt": "2023-04-28T23:49:52Z",
          "updatedAt": "2023-04-28T23:49:52Z"
        }
      ]
    },
    {
      "number": 450,
      "id": "I_kwDOFEJYQs5krJQx",
      "title": "Use POST instead of PUT",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/450",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "ietf118"
      ],
      "body": "RFC 9110 says:\r\n\r\n> The fundamental difference between the POST and PUT methods is highlighted by the different intent for the enclosed representation. The target resource in a POST request is intended to handle the enclosed representation according to the resource's own semantics, whereas the enclosed representation in a PUT request is defined as replacing the state of the target resource. Hence, the intent of PUT is idempotent and visible to intermediaries, even though the exact effect is only known by the origin server.\r\n\r\nAs we are not replacing the resource at: `{leader}/tasks/{task-id}/reports` we should use `POST`\r\n",
      "createdAt": "2023-04-28T19:24:40Z",
      "updatedAt": "2024-02-06T00:46:27Z",
      "closedAt": "2024-02-06T00:46:27Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't have a well-formed view point here. I think from a protocol flow perspective the main thing we need is to be able to distinguish the initial message for an aggregation job (or collection job) with a continue (or poll) message. I think we already get this from media types but I'll check if/when we put a PR for this.\r\n\r\ncc/ @tgeoghegan",
          "createdAt": "2023-04-28T21:28:46Z",
          "updatedAt": "2023-04-28T21:28:46Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Right. I think this generally should not be encoded in methods.\r\n\r\n@martinthomson @mnot?\r\n\r\n\r\n\r\n",
          "createdAt": "2023-04-28T22:43:27Z",
          "updatedAt": "2023-04-28T22:43:27Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "Context?",
          "createdAt": "2023-04-30T08:58:02Z",
          "updatedAt": "2023-04-30T08:58:02Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The semantic I focused on when I selected PUT for these requests is idempotence. I did consider doing the REST thing of transferring the entire representation of the resources in requests and responses, but concluded that this is quite wasteful in the aggregation sub-protocol: there's no reason for the helper to echo report shares and aggregation parameters back to the leader.\r\n\r\nThat said, my main takeaway from having spent time on this during DAP-04 is that it doesn't _really_ matter what methods we use because we don't expect DAP to be executed by a generic HTTP client (e.g. a web browser) but rather by dedicated DAP implementations. That means that it's not the end of the world if play a little fast and loose with method semantics.\r\n\r\nLong story short: I don't object to using different methods here.",
          "createdAt": "2023-05-01T18:41:26Z",
          "updatedAt": "2023-05-01T18:41:26Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "If what you are doing is asking for a change to a resource, rather than its replacement PATCH works over PUT.  Idempotence is not guaranteed in that case, but it could be, depending on how you structure the request.  The same applies to POST, which could be idempotent and therefore safe to retry with that knowledge.",
          "createdAt": "2023-05-02T00:09:40Z",
          "updatedAt": "2023-05-02T00:09:40Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "One alternate resolution: we could keep using the `PUT` method, but change the URL to include the report ID (rather than it being part of the request body), i.e. `PUT /{leader}/tasks/{task-id}/reports/{report-id}`. (This was [previously considered](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/398#discussion_r1081893133) during the recent HTTP API redesign, but we ultimately reconsidered)",
          "createdAt": "2023-05-02T17:02:27Z",
          "updatedAt": "2023-05-02T17:02:27Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@divergentdave 's suggestion seems fine but I don't think we're getting anything out of the visibly idempotent behavior so we could just use POST. I would prefer this because\r\n\r\n> successful PUT of a given representation would suggest that a subsequent GET on that same target resource will result in an equivalent representation being sent in a [200 (OK)](https://www.rfc-editor.org/rfc/rfc9110.html#status.200) response.\r\n\r\nWhich doesn't really make sense for report submission. It would also make more sense to use POST for job creation.",
          "createdAt": "2023-06-19T10:24:54Z",
          "updatedAt": "2023-06-19T10:24:54Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This issue was discussed at IETF 118 but we did not reach a conclusion. The proposal was to keep the PUT, but add the report ID to the the resource URI. @mnot pointed out at the mic that \"resources\" always need a GET, which suggested to us that our attempts to define a \"resource-oriented\" API for DAP may have fallen short. I suggest we wait for @mnot's review of the draft before we take action here.",
          "createdAt": "2023-11-07T13:53:21Z",
          "updatedAt": "2023-11-07T13:53:21Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "Probably best to request a httpdir review.",
          "createdAt": "2023-11-17T03:57:25Z",
          "updatedAt": "2023-11-17T03:57:25Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/532, once it lands, will remove the last reason (AFAIK) for collection job polling to use a POST rather than a GET. That PR will leave the POST in place for now, but collection job polling semantics may be worthy of review by the httpdir folks.",
          "createdAt": "2023-11-28T18:50:33Z",
          "updatedAt": "2023-11-28T18:50:33Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We changed the request method for collection jobs in #544 and for report uploads in #543. I don't think there's anything else to do here.",
          "createdAt": "2024-02-06T00:46:27Z",
          "updatedAt": "2024-02-06T00:46:27Z"
        }
      ]
    },
    {
      "number": 451,
      "id": "I_kwDOFEJYQs5krMeI",
      "title": "What's the difference between a prep message and a prep share",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/451",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "~~~\r\n        | AggregationJobInitReq                               |\r\n      I | agg_param, helper_report_share, leader_prep_share_1 |\r\n     N1 |---------------------------------------------------->| I\r\n        |                                  AggregationJobResp | N1\r\n        |                   prep_msg_1, [helper_prep_share_2] | P1\r\n        |<----------------------------------------------------| [N2]\r\n~~~\r\n\r\nIn this diagram the helper sends both `prep_msg_1` and `helper_prep_share_2`, but the latter only in a multiround exchange? Why is this a distinction that needs to be made at the DAP level? It seems much more intuitive to just have the leader and helper each send opaque messages. This is especially confusing because Section 4.2.2.1 does not seem to make use\r\nof the helper's prep message at all, just its prep share.\r\n\r\nAs a separate note, having the messages numbered so that the helper's prep shares are even but its prep messages are odd is very confusing.\r\n\r\n\r\n",
      "createdAt": "2023-04-28T19:38:26Z",
      "updatedAt": "2023-05-31T23:05:05Z",
      "closedAt": "2023-05-31T23:05:05Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "As illustrated [in the VDAF doc](https://www.ietf.org/archive/id/draft-irtf-cfrg-vdaf-05.html#figure-6):\r\n![image](https://user-images.githubusercontent.com/3453007/235258388-ba8b3b1b-d9eb-4ef1-8ae7-0ca803ff1d3f.png)\r\n\r\n* \"prep share\" is the value emitted by each Aggregator at the start of each round of preparation\r\n* \"prep message\" is the result of combining the prep shares into the input for the next round\r\n\r\nConcretely, in Prio3 the \"prep share\" is the Aggregator's share of the \"verifier\": To check a proof, the verifier shares combined into the verifier, which is used to complete the proof check. For Poplar1 there are two rounds of \"prep shares\", each serving a specific mathematical function.\r\n\r\n> In this diagram the helper sends both `prep_msg_1` and `helper_prep_share_2`, but the latter only in a multiround exchange? \r\n\r\nYup. So `prep_msg_1` would be sent for any VDAF with at least one round of preparation, and `helper_prep_share_2` would be sent for any VDAF that has at least two rounds of preparation.\r\n\r\n> Why is this a distinction that needs to be made at the DAP level? It seems much more intuitive to just have the leader and helper each send opaque messages.\r\n\r\nThat might work. The reason I made the distinction is that it seems helpful for mapping the different fields of the message to points in the multi-party computation. In particular:\r\n* a prep share is an **output** of `VDAF.prep_next()`, but an **input** of `VDAF.prep_shares_to_prep()`\r\n* a prep message is an **input** of `VDAF.prep_next()`, but an **output** of `VDAF.prep_shares_to_prep()`\r\n\r\n> This is especially confusing because Section 4.2.2.1 does not seem to make use of the helper's prep message at all, just its prep share.\r\n\r\nSorry, which section? I can't find 4.2.2.1 in the [editor's copy](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.2.2.1).\r\n\r\n> As a separate note, having the messages numbered so that the helper's prep shares are even but its prep messages are odd is very confusing.\r\n\r\nI understand. It would be good to do something about this. In the current ping-pong proposal (#393), which this new overview text is meant to reflect, we end up handling multiple rounds of VDAF preparation in each aggregation flow request.\r\n\r\n",
          "createdAt": "2023-04-28T21:50:46Z",
          "updatedAt": "2023-04-28T21:50:46Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.4.2.1\r\n\r\n>   a prep share is an output of VDAF.prep_next(), but an input of VDAF.prep_shares_to_prep()\r\n    a prep message is an input of VDAF.prep_next(), but an output of VDAF.prep_shares_to_prep()\r\n\r\nThis seems like maybe a place to make an improvement.\r\n\r\nIn general, the less that we have to know about the internals of the VDAF the better.\r\n",
          "createdAt": "2023-04-28T23:07:36Z",
          "updatedAt": "2023-04-28T23:07:36Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "FWIW, I would be supportive of an update to the VDAF interface such that the usage in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393 is reduced back to \"send a single opaque message per report for each request/response\". The current approach has a few disadvantages:\r\n* Aggregators must handle multiple opaque VDAF messages per report with each round, which is more complex than necessary.\r\n* The DAP round no longer matches the VDAF round, which is a source of some confusion.\r\n\r\n(This assumes that such an update wouldn't make other use-cases of VDAF impossible, but I think the computation is effectively the same in terms of what is computed in which order. The difference would only be in how much is computed per call to a VDAF function.)",
          "createdAt": "2023-05-01T17:01:57Z",
          "updatedAt": "2023-05-01T17:01:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd if I understand your proposal correctly, this simplification is only possible for 2-aggregator VDAFs. Even though DAP is moving to 2-aggregator, the core VDAF document is not.\r\n\r\nInstead of changing the VDAF spec itself, perhaps you could propose \"wrapper\" methods for VDAF preparation that we could use here?",
          "createdAt": "2023-05-01T17:07:41Z",
          "updatedAt": "2023-05-01T17:08:11Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Oh, you're right -- this change would only work if VDAF were specified to 2 aggregators.\r\n\r\nA sketch of the functions we'd want:\r\n\r\n* DAP initialization: no change here; this is a fusion of the sequence of `prep_init`, `prep_next`. The input is the input share (and public share, etc), the output is the initial preparation state & preparation share. Used to implement leader & helper initialization. (I believe `libprio-rs` already implements its initialization functionality in this way.)\r\n* Step: a fusion of a sequence of `prep_next`, `prep_shares_to_prep`, `prep_next`. The input is a prep message and possibly a (next-round) prep share; the output is the equivalent of a #393 PrepareStep's (continued, finished, finish) cases -- possibly a prep message and, if so, possibly a next-round prep share. This is used to implement leader initialization, leader continuation, and helper continuation.\r\n* Helper post-initialization step: a fusion of a sequence of `prep_shares_to_prep`, `prep_next`. The input is a pair of prep shares; the output is a prep message and possibly a prep share. This is used to implement helper initialization.\r\n\r\n(note: the wrapper functions working out this way requires moving the `prep_next` currently described in #393 at the beginning of leader continuation to instead be described at the end of both leader initialization & continuation.)\r\n\r\nBut we should consider carefully if specifying these wrapper functions is actually a complexity win -- the additional layer of indirection might end up making things harder to understand, especially if specified in DAP. VDAF would want to consider the complexity of supporting two different interfaces, if we were to specify the change in VDAF.",
          "createdAt": "2023-05-01T20:12:09Z",
          "updatedAt": "2023-05-01T20:12:09Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> A sketch of the functions we'd want:\r\n\r\nThis sounds about right to me. One another note: In general, prep_shares_to_prep() is sensitive to the order of its inputs. In particular the first should be the Leader's and the second should be the Helper's.\r\n \r\n> But we should consider carefully if specifying these wrapper functions is actually a complexity win -- the additional layer of indirection might end up making things harder to understand, especially if specified in DAP.\r\n\r\nRight, but I think it's worth giving it a shot. I'll take a crack at this.\r\n\r\n\r\n> VDAF would want to consider the complexity of supporting two different interfaces, if we were to specify the change in VDAF.\r\n\r\nFor the moment we should assume (a) VDAF's interface won't change significantly and (b) VDAF will continue support more than two Aggregators.",
          "createdAt": "2023-05-01T21:11:52Z",
          "updatedAt": "2023-05-01T21:11:52Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "OK. One additional concern is around \"the wrapper functions working out this way requires moving the prep_next currently described in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393 at the beginning of leader continuation to instead be described at the end of both leader initialization & continuation\" -- I think phrasing things this way would require marginally more storage. (we'd end up storing a `prep_msg` and `prep_share`, rather than just a `prep_msg`, with each DAP round)\r\n\r\nI did things this way because, otherwise, the \"fused\" functions we end up with are much more complicated.",
          "createdAt": "2023-05-02T16:09:06Z",
          "updatedAt": "2023-05-02T16:09:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "PR is ready for review: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/461\r\n\r\nI don't think we can avoid the distinction between prep messages and prep shares, but we can at least encapsulate in a way that makes it irrelevant to a higher level understanding of the protocol. FWIW, the distinction *always* mattered, it's just that it was easier to brush under the rug in the previous aggregation flow. \"Ping-ponging\" a la #393 makes this distinction harder to brush aside.",
          "createdAt": "2023-05-02T23:19:31Z",
          "updatedAt": "2023-05-02T23:19:31Z"
        }
      ]
    },
    {
      "number": 452,
      "id": "I_kwDOFEJYQs5krN5X",
      "title": "Why should job IDs be pseudorandom?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/452",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [],
      "body": ">   1.  Generate a fresh AggregationJobID.  This ID MUST be unique within\r\n       the context of the corresponding DAP task.  It is RECOMMENDED\r\n       that this be set to a random string output by a cryptographically\r\n       secure pseudorandom number generator.\r\n\r\nThis seems unhelpful. Either these should be cryptographically random\r\nin which case this should be a MUST, or it doesn't matter, in which case\r\nwhy is this text here? How am I supposed to know.\r\n",
      "createdAt": "2023-04-28T19:44:51Z",
      "updatedAt": "2023-05-01T19:54:51Z",
      "closedAt": "2023-05-01T19:54:51Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The idea of the RECOMMENDATION here is to suggest a simple strategy for choosing IDs in a way that satisfies the requirement, which is uniqueness. A CSPRNG is certainly not required here.\r\n\r\nHow about instead of\r\n\r\n> It is RECOMMENDED that this be set to a random string output by a cryptographically\r\nsecure pseudorandom number generator.\r\n\r\nwe go with something like\r\n\r\n> The ID MAY be chosen at random.\r\n",
          "createdAt": "2023-04-28T21:55:20Z",
          "updatedAt": "2023-04-28T21:55:20Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I would propose just not saying how it's generated at all but rather that it must be unique.",
          "createdAt": "2023-04-28T22:58:01Z",
          "updatedAt": "2023-04-28T22:58:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "That works for me.",
          "createdAt": "2023-04-28T23:54:14Z",
          "updatedAt": "2023-04-28T23:54:14Z"
        }
      ]
    },
    {
      "number": 453,
      "id": "I_kwDOFEJYQs5krPj-",
      "title": "Why do duplicate report ids have error unrecognizedMessage",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/453",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "",
      "createdAt": "2023-04-28T19:51:08Z",
      "updatedAt": "2023-05-22T22:08:39Z",
      "closedAt": "2023-05-22T22:08:39Z",
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": ">    AggregationJobInitReq.report_shares are all distinct.  If two\r\n   ReportShare values have the same report ID, then the helper MUST\r\n   abort with error unrecognizedMessage.\r\n\r\nShouldn't this be a more specific error or just result in suppressing one of the shares?\r\n",
          "createdAt": "2023-04-28T19:51:49Z",
          "updatedAt": "2023-04-28T19:53:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I have no objection. IIRC we decided to abort because it is a stronger signal that something is going wrong, but I think just rejecting the report instead would work just fine.",
          "createdAt": "2023-04-28T22:09:45Z",
          "updatedAt": "2023-04-28T22:09:45Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I could go either way, but I think if it's going to be rejecting the whole message, it should be its own error.",
          "createdAt": "2023-04-28T22:42:07Z",
          "updatedAt": "2023-04-28T22:42:07Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I think one reason we decided to fail the entire message was that, if a Leader sends two different reports with the same ID, this indicates that the Leader itself is confused about the content of that report (as reports are primarily identified by report ID).\r\n\r\nHowever, the aggregation process uses the order of reports to assist with matching -- the semantics are that the same report aggregations will be referenced in the same order with each request, except that the Leader does not transmit any that have failed; matching is done by report ID -- so it's plausible that things might work out here. But failing just one of the reports will leave the other in place, which might plausibly create confusion for the Aggregators when they next attempt to match an incoming message's report IDs to the report IDs associated with their own state.\r\n\r\nI think if we don't fail the entire message, we should fail _both_ reports to avoid potential confusion matching request prepare steps to stored state in the aggregation process. (or, we could change the aggregation process to avoid this somehow -- we've chatted about not dropping errors from the list of preparation shares that are passed back and forth)\r\n\r\n(I don't have a strong opinion on whether we should have a new error or not -- `unrecognizedMessage` definitely isn't the error I would expect for this case, though it is sorta used as a grab-bag for low-level message parsing failures.)",
          "createdAt": "2023-04-28T23:02:13Z",
          "updatedAt": "2023-04-28T23:02:57Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't object to a generic message but then it should be like `invalidMessage`",
          "createdAt": "2023-04-28T23:30:25Z",
          "updatedAt": "2023-04-28T23:30:25Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with EKR that a new error type for this case is warranted. @tgeoghegan has pushed back on adding new error types in the past, so we should probably get his take as well.",
          "createdAt": "2023-04-28T23:57:21Z",
          "updatedAt": "2023-04-28T23:57:21Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we should only add new error types/codes if we expect the recipient of that error message to act differently when it sees it. I think that if we had both `invalidMessage` and `duplicateReport`, then client implementations would do exactly the same thing in either case (give up on uploading that particular report). Adding a new error code just means client implementations have to write more code and write more tests in exchange for no additional functionality. \r\n\r\nHowever I'm willing to defer to ekr's superior experience in protocol design; if you think that having more specific error types is sufficiently valuable for, say, debugging, then we can do that (though I'd also point out that RFC 7807 problem documents do allow implementations to include arbitrary extra information).\r\n\r\nI also agree that `invalidMessage` is a better name than `unrecognizedMessage`, if we decide to not add more error types.",
          "createdAt": "2023-05-01T18:35:43Z",
          "updatedAt": "2023-05-01T18:35:43Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "This is in the context of the aggregation flow, so the leader would have sent an aggregation job with a repeated report ID in it to the helper, and the question is what the helper's error type should be. I think the analysis is the same, as the leader should stop processing the job and not retry, since one of the aggregators has a programming error, or they are trying to speak incompatible protocol versions.",
          "createdAt": "2023-05-01T18:41:46Z",
          "updatedAt": "2023-05-01T18:41:46Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I actually don't feel strongly about how many errors there are. I'm mostly just advocating for a different name.",
          "createdAt": "2023-05-13T20:20:52Z",
          "updatedAt": "2023-05-13T20:20:52Z"
        }
      ]
    },
    {
      "number": 455,
      "id": "I_kwDOFEJYQs5krfYq",
      "title": "Handling missing state",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/455",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "> 1. Finally, if an Aggregator cannot determine if an input share is valid, it\r\n   MUST mark the input share as invalid with error `report_dropped`. For\r\n   example, if the Aggregator has evicted the state required to perform the\r\n   check from long-term storage. (See {{reducing-storage-requirements}} for\r\n   details.)\r\n\r\nIs this aggressive enough? I think we need to be careful that state exhaustion\r\ncan't be used to enable differencing attacks where only some inputs are removed.\r\nNeed to think more about this.",
      "createdAt": "2023-04-28T20:54:42Z",
      "updatedAt": "2024-06-05T17:06:19Z",
      "closedAt": "2024-06-05T17:06:19Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "What do you mean by \"differencing attacks\"?\r\n\r\nOne condition this text envisions is tracking report IDs for replay protection. We don't want to require Aggregators to keep this state forever; they should be allowed to drop report IDs for old reports, say, older than a week. Suppose an Aggregator just that. Then if it receives a report older than a week, it can't determine whether the report is replayed. Instead, it should reject the report to be on the safe side.",
          "createdAt": "2023-10-24T21:04:22Z",
          "updatedAt": "2023-10-24T21:04:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think there's anything to do here.",
          "createdAt": "2024-06-05T17:06:19Z",
          "updatedAt": "2024-06-05T17:06:19Z"
        }
      ]
    },
    {
      "number": 456,
      "id": "I_kwDOFEJYQs5kr5Mq",
      "title": "min_batch_size",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/456",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "> The query configuration specifies the minimum batch size, `min_batch_size`, and\r\nmaximum batch size, `max_batch_size`. The Aggregator checks that `len(X) >=\r\nmin_batch_size` and `len(X) <= max_batch_size`, where `X` is the set of reports\r\nin the batch.\r\n\r\nShouldn't this be *valid* reports in the batch?",
      "createdAt": "2023-04-28T22:41:40Z",
      "updatedAt": "2023-07-03T18:11:39Z",
      "closedAt": "2023-07-03T18:11:39Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah that's correct. We should be probably be more explicit.",
          "createdAt": "2023-04-28T22:59:33Z",
          "updatedAt": "2023-04-28T22:59:33Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's take this for draft-05 as it should be an easy fix.",
          "createdAt": "2023-05-01T18:18:49Z",
          "updatedAt": "2023-05-01T18:18:49Z"
        }
      ]
    },
    {
      "number": 459,
      "id": "I_kwDOFEJYQs5kyqtv",
      "title": "Align task-specific HPKE config with other task-specific resources",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/459",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "breaking",
        "ietf118"
      ],
      "body": "> It is a bit odd to have this structure.  I would have said global = `{aggregator}/hpke_config` and task-specific = `{aggregator}/tasks/{task-id}/hpke_config`.  Though I suspect that achieving consistency would be harder for the latter form, if you ever had to worry about that.\r\n\r\n_Originally posted by @martinthomson in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/454#discussion_r1181203600_\r\n            ",
      "createdAt": "2023-05-01T16:02:25Z",
      "updatedAt": "2023-11-07T13:50:46Z",
      "closedAt": "2023-11-07T13:50:46Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I considered this at the time of doing DAP-04 and felt that both `{aggregator}/hpke_config?task_id={task_id}` and this proposal feel unsatisfactory in the sense that they introduce an inconsistency across API endpoints. Without any other means of choosing between the two, I decided to just leave `hpke_config` as it was since I was already changing a ton of stuff.\r\n\r\nI also considered having the global case be `{aggregator}/tasks/<null task ID>/hpke_config`, where `<null task ID>` would be the encoding of a special reserved `TaskID` value. The advantage there is consistency, the downside is it's more complicated!\r\n\r\nI don't object to making the change in the issue description.",
          "createdAt": "2023-05-01T18:51:36Z",
          "updatedAt": "2023-05-01T18:51:36Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we should continue supporting both global and task-specific HPKE configs. I would prefer not trying to unify them by introducing some sort of \"null\" task, as this seems pretty messy. (I think I would just end up hard-coding the ID for the null task in the request routing logic.)\r\n\r\n@martinthomson's proposal strikes me as a strict improvement over the current status quo.",
          "createdAt": "2023-05-01T19:07:40Z",
          "updatedAt": "2023-05-01T19:07:40Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "One additional option would be to support _only_ `{aggregator}/tasks/{task-id}/hpke_config` for both per-task & global HPKE configuration settings -- this keeps things simple at the protocol level, at the cost of hiding from the clients if a given HPKE config is \"global\" or \"per-task\". (This might be viewed as more flexible for aggregator implementors, who are now free to choose whatever domain they wish for their HPKE configs.)\r\n\r\nWe could always add hints to the HPKE config's expected domains, too, if we still want to signal this to clients.",
          "createdAt": "2023-05-02T16:46:16Z",
          "updatedAt": "2023-05-02T16:46:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "In light of the discussion on https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/510, I think we should close this issue without doing anything. The consideration is purely aesthetic.",
          "createdAt": "2023-10-24T22:45:49Z",
          "updatedAt": "2023-10-24T22:45:49Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 we decided to close this issue without action. What we have right now is sufficient.",
          "createdAt": "2023-11-07T13:50:46Z",
          "updatedAt": "2023-11-07T13:50:46Z"
        }
      ]
    },
    {
      "number": 460,
      "id": "I_kwDOFEJYQs5ky0ag",
      "title": "Overhaul security considerations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/460",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-06"
      ],
      "body": "The security considerations section has lots of out of date content, especially in the threat modeling section that tries to match up actor capabilities with protocol mitigations (see for instance #443). We should review this section of the draft to bring it up to date.",
      "createdAt": "2023-05-01T16:39:27Z",
      "updatedAt": "2023-08-17T20:01:57Z",
      "closedAt": "2023-08-17T20:01:57Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "There's also some sections sprinkled throughout the text (like the various \"message security\" bits) that properly belong in security considerations.",
          "createdAt": "2023-06-13T22:16:46Z",
          "updatedAt": "2023-06-13T22:16:46Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think it would be good to add a bit more discussion regarding the document by Kyle: https://docs.google.com/document/d/1BdKOLrv0auHUEOo6YbCo6KaqfPC7leuzEfGPngoI_nk to #sybil.",
          "createdAt": "2023-08-02T10:59:06Z",
          "updatedAt": "2023-08-02T10:59:06Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> The Helper then encrypts agg_share under the Collector's HPKE public key as described in [Section 4.6.4](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#aggregate-share-encrypt), yielding encrypted_agg_share. Encryption prevents the Leader from learning the actual result, as it only has its own aggregate share and cannot compute the Helper's.\r\n\r\nThere is an assumption here that the collector will not share the result with the leader but it violates neither privacy nor robustness. It seems we are missing a requirement about aggregate output confidentiality.",
          "createdAt": "2023-08-02T10:59:53Z",
          "updatedAt": "2023-08-02T10:59:53Z"
        }
      ]
    },
    {
      "number": 466,
      "id": "I_kwDOFEJYQs5m5RnP",
      "title": "Leader's batch-size check during collection is insufficient.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/466",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "[DAP-04 Section 4.5.1 Collection Job Initialization](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-04.html#name-collection-job-initializati) says:\r\n> Upon receipt of a CollectionReq, the Leader begins by checking that it recognizes the task ID in the request path. If not, it MUST abort with error unrecognizedTask. Then, the Leader verifies that the request meets the requirements of the batch parameters using the procedure in [Section 4.5.6](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-04.html#batch-validation). If so, it immediately responds with HTTP status 201.\r\n\r\n\r\n[DAP-04 Section 4.5.6 Batch Validation](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-04.html#name-batch-validation) says:\r\n> Next, the Aggregator checks that batch contains a valid number of reports, as determined by the query type. If the size check fails, then the Aggregator MUST abort with error of type \"invalidBatchSize\".\r\n\r\n\r\nThe intent of this check is to avoid aggregating batches with too few reports. The correct number of reports to count is the number of successfully-aggregated reports, as this is the number of reports that would be reflected in the eventual aggregate received by the collector. However, it doesn't make sense to evaluate this check here for a few reasons:\r\n\r\n* The aggregation process is independent of the collection process, and may not yet be complete -- so at this point, the Leader has no way to know how many reports will eventually be included in the batch. (Indeed, for VDAFs with nontrivial aggregation parameters such as Poplar1, the aggregation process may not _start_ until after the collection request is received.)\r\n* The size of the batch may be lower-bounded by the number of reports that have already successfully completed aggregation, but this does not help to check a fixed-sized task's maximum batch size.\r\n\r\n(Even worse, it is arguably DAP-compliant to evaluate the \"number of reports\" check as \"reports received\" rather than \"reports successfully aggregated\". If so, a buggy/malicious Helper might be able to take advantage of this situation to generate aggregates over batches which do not meet the minimum batch size, breaking the desired privacy property. I suspect we should tweak the language of the batch size check to specify successfully-aggregated reports.)\r\n\r\nMy recommendation is to move the Leader's batch-size check to \"just before issuing an `AggregateShareReq` to the Helper\", as at this point the Leader can determine the exact number of reports that are aggregated & is therefore in position to accurately evaluate the batch size requirements. I think the Helper's check is fine, though as noted we might want to update wording such that aggregators are evaluating batch size as \"successfully aggregated reports\" rather than \"reports received\".",
      "createdAt": "2023-05-25T18:26:04Z",
      "updatedAt": "2023-07-05T23:09:04Z",
      "closedAt": "2023-07-05T23:09:04Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(for reference/an implementation example, a related issue from Janus: https://github.com/divviup/janus/issues/1414)",
          "createdAt": "2023-05-25T18:34:01Z",
          "updatedAt": "2023-05-25T18:34:01Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with Brandon here. The existing leader check at the time of creating collection jobs should at most be a SHOULD, but can only be a MUST at the time of constructing an aggregate share, and so for simplicity's sake I feel we should remove the batch size check at collection job creation time.",
          "createdAt": "2023-05-25T18:34:52Z",
          "updatedAt": "2023-05-25T18:34:52Z"
        }
      ]
    },
    {
      "number": 467,
      "id": "I_kwDOFEJYQs5oAtzy",
      "title": "Task ID generation advice is inconsistent",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/467",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "In DAP-04, [4.2 Task Configuration](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-04#section-4.2-3) says:\r\n\r\n> A TaskID is a globally unique sequence of bytes. It is RECOMMENDED that this be set to a random string output by a cryptographically secure pseudorandom number generator.\r\n\r\nBut then [7.7 Verification key requirements](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-04#name-verification-key-requiremen) says:\r\n\r\n> The verification key for a task SHOULD be chosen before any reports are generated. It SHOULD be fixed for the lifetime of the task and not be rotated. One way to ensure this is to include the verification key in a derivation of the task ID.\r\n\r\nNeither of these is a MUST, so this isn't impossible to implement. But it's not clear how implementations should weigh these (IMO) contradictory suggestions.",
      "createdAt": "2023-06-07T04:38:41Z",
      "updatedAt": "2023-06-13T00:39:06Z",
      "closedAt": "2023-06-13T00:39:05Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This is a problem in DAP-04, but it got fixed in eeb6264c.",
          "createdAt": "2023-06-13T00:39:05Z",
          "updatedAt": "2023-06-13T00:39:05Z"
        }
      ]
    },
    {
      "number": 470,
      "id": "I_kwDOFEJYQs5oodRd",
      "title": "Sync with latest VDAF draft",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/470",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "draft-05"
      ],
      "body": "Once the next draft of VDAF is published (likely to be 06), we need to align DAP with the changes it makes. Among other things:\r\n\r\n- Update text added in #393 to use correct references to ping-pong wrappers added in https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240",
      "createdAt": "2023-06-13T18:24:43Z",
      "updatedAt": "2023-06-20T23:38:43Z",
      "closedAt": "2023-06-20T23:38:43Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "VDAF-06 is out: https://datatracker.ietf.org/doc/draft-irtf-cfrg-vdaf/06/",
          "createdAt": "2023-06-16T01:57:12Z",
          "updatedAt": "2023-06-16T01:57:12Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Related: `ping_pong_start` in VDAF-06 has a nonce parameter, but not in this DAP version.",
          "createdAt": "2023-06-19T20:55:29Z",
          "updatedAt": "2023-06-19T20:55:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Related: `ping_pong_start` in VDAF-06 has a nonce parameter, but not in this DAP version.\r\n\r\nIt does, we use the DAP report ID as the VDAF nonce, which is explained in the text below.",
          "createdAt": "2023-06-20T15:44:39Z",
          "updatedAt": "2023-06-20T15:44:39Z"
        }
      ]
    },
    {
      "number": 472,
      "id": "I_kwDOFEJYQs5owDfX",
      "title": "Document extensions to and/or deviations from RFC 8446 presentation language",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/472",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "editorial",
        "ietf120"
      ],
      "body": "In #393, we define a union with an enum discriminant roughly like:\r\n\r\n```\r\nenum {\r\n  continue(0),\r\n  finished(1)\r\n  reject(2),\r\n  (255)\r\n} PrepareStepState;\r\n\r\nstruct {\r\n  PrepareStepState prepare_step_state;\r\n  select (PrepareStep.prepare_step_state) {\r\n    case continue:\r\n      ReportId report_id;\r\n      opaque payload<0..2^32-1>;\r\n    case finished:\r\n      ReportId report_id;\r\n    case reject:\r\n      ReportId report_id;\r\n      ReportShareError report_share_error;\r\n  };\r\n} PrepareStep;\r\n```\r\n\r\nThen later in protocol text, we define partial constructions of values like:\r\n\r\n```\r\nstruct {\r\n  PrepareStepState prepare_step_state = 2; /* reject */\r\n  ReportId report_id;\r\n  ReportShareError report_share_error;\r\n} PrepareStep;\r\n```\r\n\r\nThis notation is clear and certainly is more compact than a paragraph explaining to implementers what enum variant to use, but it conflicts with [section 3.7 of RFC 8446](https://datatracker.ietf.org/doc/html/rfc8446#section-3.7), which tells us that this is the notation for defining structs that contain fields whose values are fixed (I gather this was useful in TLS 1.3 for backward compatibility with older TLS versions).\r\n\r\nWe can resolve this by adding some text to \"Conventions and Definitions\", where we introduce the usage of RFC 8446 presentation language, explaining our extensions and deviations. There may also be some prior art in other TLS WG documents we can reference, but I haven't found it.",
      "createdAt": "2023-06-14T18:22:32Z",
      "updatedAt": "2024-07-27T17:08:53Z",
      "closedAt": "2024-07-27T17:08:52Z",
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Alternatively, can we use a different syntax to describe object, like the pseudo code used to describe HPKE and VDAF interaction. For e.g.\r\n```\r\n  return PrepareResp(report_id: ReportID, prepare_resp_state: PrepareRespState = reject, prepare_error: PrepareError);\r\n```",
          "createdAt": "2023-06-16T15:07:07Z",
          "updatedAt": "2023-06-16T15:07:07Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 to clarifying, and I like @wangshan's suggestion here. VDAF is just Python3, so to match it we'd do something like\r\n```python\r\nPrepareResp(report_id, prepare_resp_type=reject, prepare_error=vdaf_prep_error)\r\n```",
          "createdAt": "2023-06-16T17:41:13Z",
          "updatedAt": "2023-06-16T17:41:13Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "A Python constructor call could potentially do a lot of things, though. The original phrasing that just nails down a few bytes of the message is more clear about the simplicity.",
          "createdAt": "2023-06-19T09:31:07Z",
          "updatedAt": "2023-06-19T09:31:07Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We have not ever used Python notation in DAP, and I don't think we should introduce it just for this. It'd be much easier to document a couple extensions to/deviations from RFC 8446 presentation language.",
          "createdAt": "2023-06-20T15:27:12Z",
          "updatedAt": "2023-06-20T15:27:12Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I don't think we have to use full python syntax either, but the section in question is talking about instantiation of structs, instead of defining a struct type. I feel pseudo code communicates that well, since the purpose is to let people know what the message contains in different conditions, rather than how to encode/decode the message.\r\nIf we introduce deviations, don't we have to introduce new syntax to explain why in this case it's not a fixed value but an assignment?",
          "createdAt": "2023-06-21T14:53:12Z",
          "updatedAt": "2023-06-21T14:53:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Are there other deviations from TLS-syntax that we're aware of in the spec? cc/ @wangshan who has spent some time on this before.",
          "createdAt": "2023-10-25T23:27:06Z",
          "updatedAt": "2023-10-25T23:27:06Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "This kind of multi fields are not part of the spec either.\r\n```\r\n    case reject:\r\n      ReportId report_id;\r\n      ReportShareError report_share_error;\r\n```",
          "createdAt": "2023-10-26T21:41:37Z",
          "updatedAt": "2023-10-26T21:41:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ahh, yup, I think @suman-ganta could be right! Thanks, I'll add this to my slides for IETF 118.",
          "createdAt": "2023-10-31T11:49:38Z",
          "updatedAt": "2023-10-31T11:49:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I didn't find an example of this here, but I did in VDAF: https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf#section-5.8-6",
          "createdAt": "2023-10-31T11:52:31Z",
          "updatedAt": "2023-10-31T11:52:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 people were mostly inclined to clarify in the draft any discrepancies with TLS-syntax as they arise. @chris-wood offered an alternative option, which is to adopt QUIC-representation language, which may be expressive enough for our purposes. We decided to look at a PR and make a decision then if the wire change is worth the effort.",
          "createdAt": "2023-11-07T13:49:06Z",
          "updatedAt": "2023-11-07T13:49:06Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "[RFC 9420 2.1](https://datatracker.ietf.org/doc/html/rfc9420#name-presentation-language) has some prior art on documenting extensions to TLS PR. And actually we should check whether we want to adopt their optional and variable length vector notation.",
          "createdAt": "2024-05-09T18:06:42Z",
          "updatedAt": "2024-05-09T18:06:42Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "#556 addresses this to my satisfaction. To answer some specific points from above:\r\n\r\n- I don't think either of the extensions from MLS are useful for us: there's nowhere we would use `Optional<T>` (or at least nowhere that I think is more clear than what we have now), and I don't think the variable length vector encoding is worthwhile for us.\r\n- I don't think there's a strong enough case for rewriting everything into RFC 9000 syntax. Proponents of that approach are welcome to draft a PR, though.",
          "createdAt": "2024-07-06T00:08:46Z",
          "updatedAt": "2024-07-06T00:08:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Decision at IETF 120 is to merge @tgeoghegan's PR.",
          "createdAt": "2024-07-26T23:15:06Z",
          "updatedAt": "2024-07-26T23:15:06Z"
        }
      ]
    },
    {
      "number": 473,
      "id": "I_kwDOFEJYQs5owGVC",
      "title": "Inline `{{aggregation-job-validation}}` into the sections that reference it",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/473",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "`{{aggregation-job-validation}}` is referenced from the leader initialization and leader continuation sections. We did this to \"factor out\" common bits of protocol logic into a single place. However it only contains a single requirement, so on balance it probably does more harm to readability than it does improve maintainability of the document. We should inline that single bullet point into the sections that reference it.\r\n\r\nSee [previous discussion](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393#discussion_r1228710990).",
      "createdAt": "2023-06-14T18:30:38Z",
      "updatedAt": "2023-07-04T01:05:03Z",
      "closedAt": "2023-07-04T01:05:03Z",
      "comments": []
    },
    {
      "number": 478,
      "id": "I_kwDOFEJYQs5o-SKH",
      "title": "Order of string literals in SealBase",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/478",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I noticed for aggregation share, the string literal 0x00 is concatenated after server_role,\r\n```\r\nenc, payload = SealBase(pk, \"dap-04 aggregate share\" || server_role || 0x00,\r\n  agg_share_aad, agg_share)\r\n```\r\n\r\nFor others like PlaintextInputshare it's before server_role:\r\n```\r\nenc, payload = SealBase(pk,  \"dap-04 input share\" || 0x01 || server_role,\r\n  input_share_aad, plaintext_input_share)\r\n```\r\n\r\nIs there a reason behind this difference, or should we just unify them?",
      "createdAt": "2023-06-16T19:00:39Z",
      "updatedAt": "2023-09-29T13:29:00Z",
      "closedAt": "2023-09-29T13:29:00Z",
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "I think these are intended to be fixed roles, so `0x00` represent the collector, and `0x01` represents the client. The position is determined by which participant is the sender and which is the receiver, which explains why the order of the literals differs. We should probably clarify where these come from in the following prose. Alternately, we could replace the string literals with references to the enum's variants, but I'm not sure if there's accepted syntax for enum literals outside the context of a `select`.",
          "createdAt": "2023-06-16T19:09:43Z",
          "updatedAt": "2023-06-16T19:09:43Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I see, I guess this is mainly for debugging? I agree explain it somewhere in the text would be sufficient.",
          "createdAt": "2023-06-16T19:44:33Z",
          "updatedAt": "2023-06-16T19:44:33Z"
        }
      ]
    },
    {
      "number": 479,
      "id": "I_kwDOFEJYQs5pKBLP",
      "title": "Remove or replace prep_init and prep_continue with updated terminologies from VDAF.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/479",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Once VDAF updates its main interface, we should remove the mentioning of prep_init, prep_continue, etc, which becomes implementation details of VDAF and not longer require exposure in DAP.\r\n\r\nrelated VDAF issue (not exhaustive):\r\nhttps://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/255\r\nhttps://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/254\r\nhttps://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/252\r\nhttps://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/244",
      "createdAt": "2023-06-19T21:13:38Z",
      "updatedAt": "2023-08-31T10:56:24Z",
      "closedAt": "2023-08-31T10:56:24Z",
      "comments": []
    },
    {
      "number": 487,
      "id": "I_kwDOFEJYQs5qzYWw",
      "title": "Registry for PrepareError",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/487",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "[draft-priebe-ppm-dap-reportauth](https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth) (no I-D submitted yet) wants to define a new `PrepareError` type, but we do not have a registry for this, only the enum definition. In conjunction with setting up the necessary policy for `PrepareError` extensibility, we may want to expand the encoded size from one byte to two bytes. We are already using 4% of codepoints for DAP itself, and additional headroom would make managing this namespace easier.",
      "createdAt": "2023-07-06T16:14:08Z",
      "updatedAt": "2023-10-23T22:32:44Z",
      "closedAt": "2023-10-23T22:32:44Z",
      "comments": []
    },
    {
      "number": 489,
      "id": "I_kwDOFEJYQs5s6LsO",
      "title": "Labeling of reports for drill-down queries",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/489",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "feature"
      ],
      "body": "## Problem statement\r\n\r\nIssue #183 discusses the tradeoff between the anti-replay requirements in the collect sub-protocol and flexibility. In particular, ekr articulates a desire to enable cross tabulation and drilling down.\r\n\r\nAt a high level, the idea is that if you're measuring some metric across a population -- let's say it's the number of milliseconds it takes to perform some operation -- you might also wish to narrow that query to a subset of members based on what region of the world they are in.\r\n\r\nThe best thing you can do in DAP today is to create one task for each region your users are in. So you'd have tasks for `operation_latency_germany`, `operation_latency_turkey`, and so on. This solution isn't a complete dead end, but is unsatisfactory:\r\n\r\n- It forces you to commit to a strategy for constructing tasks ahead of gathering data.\r\n- Changing that strategy is hard (as it involves updating all the clients and then you have to figure out how to reconcile the data you get from clients on the new strategy with the long tail of clients on the old strategy).\r\n- Some of the tasks you create may end up with too few reports to be aggregatable. Maybe you can query over bigger and bigger time intervals to satisfy the minimum batch size, but then doing analysis across tasks can be challenging: for highly populated regions, you might have data with hourly granularity, but only monthly in others.\r\n\r\nIssue #183 is about that problem statement. In this issue, I want to sketch out one solution to that problem so we can explore its tradeoffs.\r\n\r\n## Solution outline\r\n\r\nDAP task parameters can optionally include a set of _labels_ that can be applied to each report uploaded in the task. Each label is a key-value pair. The keys are integers[^1][^2]. The values can be either strings or integers[^3]. The type of each label's values is also part of the task parameters.\r\n\r\nWhen uploading a report, Clients MAY include one or more of the labels[^4]. We extend [`struct ReportMetadata`](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-05#name-upload-request):\r\n\r\n```\r\nuint32 LabelKey;\r\n\r\nenum {\r\n  string(0),\r\n  integer(1),\r\n  <possibly more types>\r\n  (255)\r\n} LabelType;\r\n\r\nstruct {\r\n  LabelKey key;\r\n  LabelType label_type;\r\n  select (Label.label_type) {\r\n      string: opaque value<1..2^32-1>;\r\n      integer: int32 value;\r\n  }\r\n} Label;\r\n\r\nstruct {\r\n  ReportID report_id;\r\n  Time time;\r\n  Label labels<0..2^8-1>;\r\n} ReportMetadata;\r\n```\r\n\r\nAggregators then store the labels with the report shares. Then, when Collectors make queries, they can provide zero or more labels and values which instructs the Aggregators to only aggregate over the reports whose labels match every label enumerated in the query[^5]. We extend [`struct Query`](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-05#name-queries):\r\n\r\n```\r\nstruct {\r\n  QueryType query_type;\r\n  select (Query.query_type) {\r\n    case time_interval: Interval batch_interval;\r\n    case fixed_size: FixedSizeQuery fixed_size_query;\r\n  }\r\n  Label labels<0..2^8-1>;\r\n} Query;\r\n```\r\n\r\n### Batch validation\r\n\r\nA batch is now defined by both the existing query (i.e., the time interval or the batch ID for fixed case) and the labels specified by the collector. Otherwise, batch validation works as before:\r\n\r\n- Batches must meet the task's minimum batch size. If there are not enough reports with the label set requested by the collector, the collection job fails.\r\n- The task's max query count applies.\r\n- Batches may not overlap. If one query over time interval _t_ sets `country = Kenya`, and the next sets `color = blue`, then if any reports had `color = blue && country = Kenya`, the second query fails[^6].\r\n\r\n## Troubling implications\r\n\r\n### Label values are fingerprints\r\n\r\nThe whole point of DAP is to allow aggregation over sensitive values without revealing individual contributions, but the labels applied to reports may themselves be privacy sensitive. In particular, tasks may define so many labels that they allow high precision fingerprinting of clients by Aggregators, though at least Collectors wouldn't see the labels on individual reports.\r\n\r\nLabels and their values are opaque to DAP, so we could punt the problem to deployments and write a security consideration that labels should be chosen carefully. But maybe there's a clever cryptographic solution here? Could the label evaluation be done in MPC, or would that add more rounds to the protocol?\r\n\r\n### Breaks eager aggregation\r\n\r\nIn VDAFs that don't have aggregation parameters like the Prio3 family, Aggregators can eagerly aggregate reports ahead of any request from the Collector. This is no longer possible in the presence of labels because the Aggregators can't anticipate what labels the Collectors will query on (and if this were possible, then the Collector should have used distinct tasks in the first place and we wouldn't need labels at all).\r\n\r\nI suspect we can live with this problem, especially since deployments can opt out of labels and thus re-enable eager aggregation.\r\n\r\n### Queries in wrong order can lock Collector out of data\r\n\r\nWe started with the problem statement of enabling drill-down, which is to say, aggregating over a whole population, and then doing another query over a subset of the population. However due to collection anti-replay requirements this isn't possible: the query over the entire task would consume the `max_batch_query_count` for every report, and the subsequent query for some label value would fail.\r\n\r\nInstead, Collectors would have to make queries over the most specific combinations of labels and then combine those query results outside of DAP. For example, take `Prio3Count` over each of `country = [Kenya, Turkey, Germany]`, and then sum each of those together to get the count over the entire population.\r\n\r\nTo do better here, we'd have to relax the batch validation requirements. @simon-friedberger suggested that drill-down could be OK so long as the more specific queries yield disjoint sets, each of which is large enough to meet the task's minimum batch size. \r\n\r\nFor example, suppose we have a task with minimum batch size 100 and a single label that has 10 possible values. Further we have a time interval with 10,000 reports in it, and each value of the label has 1,000 values. Is it acceptable w.r.t. to allow one aggregation over the 10,000 reports, and then 10 more aggregations over each of the 10 label values?\r\n\r\nNow let's suppose the task has another label, each of which has 10 possible values, and that there are 100 reports with each combination of label values. Would 100 more aggregations over each of those combinations be acceptable?\r\n\r\n[^1]: What integer width should we use for the keys?\r\n[^2]: We could also make the keys strings, but nobody except the collector needs to know what the labels semantically actually are. A mapping of label key to some description can be maintained in the collector, and it's more efficient to encode an integer into reports than arbitrary length strings.\r\n[^3]: Do we need label value types besides these? Floating point numbers?\r\n[^4]: Should all the labels defined in a task be mandatory?\r\n[^5]: As written this implies that queries can only test for value equality, but we could design a richer query language that lets you do things like test string prefixes, suffixes, substrings, do integer comparisons, or do negations. I'm wary of taking on the design challenge of designing a sufficiently rich query language, but there is lots of prior art, such as [Apple's `NSPredicate`](https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/Predicates/AdditionalChapters/Introduction.html#//apple_ref/doc/uid/TP40001789).\r\n[^6]: This makes a strong case for including negations in the query language, so that the Collector's second query could express `color = blue && country != Kenya`.",
      "createdAt": "2023-07-28T23:05:21Z",
      "updatedAt": "2024-06-10T21:15:44Z",
      "closedAt": "2024-06-10T21:15:44Z",
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> ## Troubling implications\r\n> ### Label values are fingerprints\r\n> \r\n> The whole point of DAP is to allow aggregation over sensitive values without revealing individual contributions, but the labels applied to reports may themselves be privacy sensitive. In particular, tasks may define so many labels that they allow high precision fingerprinting of clients by Aggregators, though at least Collectors wouldn't see the labels on individual reports.\r\n\r\nEncoding different labels into creating many tasks has the same problem, although a reasonable limit on number of tasks does automatically limit the leakage. Still, given that we also don't specify what the minimum batch size would be I think it would be acceptable to let task creators decide here.\r\n\r\n> ### Breaks eager aggregation\r\n> In VDAFs that don't have aggregation parameters like the Prio3 family, Aggregators can eagerly aggregate reports ahead of any request from the Collector. This is no longer possible in the presence of labels because the Aggregators can't anticipate what labels the Collectors will query on (and if this were possible, then the Collector should have used distinct tasks in the first place and we wouldn't need labels at all).\r\n> \r\n> I suspect we can live with this problem, especially since deployments can opt out of labels and thus re-enable eager aggregation.\r\n\r\nAgreed, there is no reason not aggregate ahead of time if it can be determined that there are enough reports. For example because there are few enough labels to check all combinations.\r\n\r\n> ### Queries in wrong order can lock Collector out of data\r\n> We started with the problem statement of enabling drill-down, which is to say, aggregating over a whole population, and then doing another query over a subset of the population. However due to collection anti-replay requirements this isn't possible: the query over the entire task would consume the `max_batch_query_count` for every report, and the subsequent query for some label value would fail.\r\n\r\nTrue, although that's why we have a variable for max_batch_query_count instead of fixing it to 1. Some combination of assumptions about the sensitivity of data, the batch size and application of DP noise might simply make it acceptable to increase that limit.\r\n\r\n> Instead, Collectors would have to make queries over the most specific combinations of labels and then combine those query results outside of DAP. For example, take `Prio3Count` over each of `country = [Kenya, Turkey, Germany]`, and then sum each of those together to get the count over the entire population.\r\n> \r\n> To do better here, we'd have to relax the batch validation requirements. @simon-friedberger suggested that drill-down could be OK so long as the more specific queries yield disjoint sets, each of which is large enough to meet the task's minimum batch size.\r\n> \r\n> For example, suppose we have a task with minimum batch size 100 and a single label that has 10 possible values. Further we have a time interval with 10,000 reports in it, and each value of the label has 1,000 values. Is it acceptable w.r.t. to allow one aggregation over the 10,000 reports, and then 10 more aggregations over each of the 10 label values?\r\n> \r\n> Now let's suppose the task has another label, each of which has 10 possible values, and that there are 100 reports with each combination of label values. Would 100 more aggregations over each of those combinations be acceptable?\r\n\r\nLogically the answer is yes for this example but I agree with you that there is a general problem. The UX would be very confusing. The order of queries determines which can be answered.\r\n\r\n### max_batch_query_count\r\nI think the goal here was to limit usage of a particular report under the assumption that every time it gets used in an aggregation some information would leak. Is the fact that this is per batch instead of per report the result of an optimization to never treat individual reports in the aggregators or am I missing something?\r\n\r\n\r\n",
          "createdAt": "2023-07-31T15:35:46Z",
          "updatedAt": "2023-07-31T15:35:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Overall I'm supportive of addressing the problem and I think the proposal is going in the right direction.\r\n\r\nOne high-level question: How many variants of a label do we expect to see, and how often does the set of labels change? E.g.: for \"geo-location\" I would expect a a relatively large number of variants (continents? countries? ASes? ...) but that the set would not change frequently over time; on the other hand, for a \"user agent\" label I would expect a small number of active variants, but that the set would change with high frequency.\r\n\r\nRe labels as fingerprints: I think this is a real concern, at least as concerning as the timestamp. I don't think this is necessarily a deal breaker but it does require privacy considerations. To your question about whether the labels can be moved to MPC: I suspect the answer is \"yes\", but we have some design work to do. (Details below.) \r\n\r\n---\r\nWe want a VDAF for the following aggregation function $F$: for all reports with the same label set, compute some function $f$ of their inputs. I.e.,\r\n\r\n$F(\\ell, S = (m_1, \\ell_1), \\ldots, (m_N, \\ell_N))) = f( m' : (m', \\ell') \\in S, \\ell'= \\ell )$\r\n\r\nSuppose that $f$ is just the sum and each $m_i$ is equal to $0$ or $1$ (a la Prio3Count). Then this aggregation function can be computed with Poplar1 out-of-the-box. It's also straightforward to extend it to accommodate multiple labels. With a tweaked version of Poplar1, we could also make it so that the same report can be queried multiple times, up to some bounded number.\r\n\r\nIt should also be possible to extend the same basic approach to more sophisticated functions $f$, e.g., sum or histogram. but there is some design work to do. We're working on an idea for this but it hasn't been fleshed out fully yet and there is no guarantee it'll pan out. DM me if interested in the details. (cc/ @jimouris, @hannahdaviscrypto)\r\n",
          "createdAt": "2023-08-01T20:18:21Z",
          "updatedAt": "2023-08-01T20:18:41Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think we should flesh out this fingerprinting concern. \r\n\r\nVDAFs theoretically protect the measurements not the metadata. They require hiding those measurements among other measurements which potentially gets circumvented by fingerprinting.\r\n\r\nAssuming we add a lot of metadata to reports, for the sake of argument, we add a \"client_id\". At this point the leader can restrict aggregation to only values from a single client which is obviously very bad.\r\n\r\nThe leader already sees client IPs when they submit reports. Those are pretty good for fingerprinting. They also know the submission time, but given IP geofencing that probably doesn't add a lot. They know the time at which the client thinks it generated the report which also leaks.\r\n\r\nEven without any fingerprinting, the leader can restrict measurements to a single report plus min_batch_size - 1 reports that it made up.\r\n\r\nI agree that there is reason for concern. Tags would have to be applied carefully. This presumably reduces their utility. On the other hand. The leader currently sees client IPs which allow batching reports by region but the collector cannot use this.",
          "createdAt": "2023-08-02T12:59:37Z",
          "updatedAt": "2023-08-02T12:59:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": " I plan to pursue the \"move tags to MPC\" thing, in case it ends up being helpful. But overall I don't see a reason to block this, we just need to be careful (as @simon-friedberger points out). Perhaps it would be fruitful to constrain the cardinality of labels somehow? E.g., instead of an arbitrary string, we might require each lablel to be a 16-bit (or even 8-bit?) integer? This would be less flexible, but perhaps make it easier to reason about fingerprints.",
          "createdAt": "2023-08-02T19:46:48Z",
          "updatedAt": "2023-08-02T19:46:48Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not sure restricting the cardinality of labels helps against malicious use, in general: even if the label value is constrained to a single bit, the label could effectively be `is_this_the_user_I_am_targeting_for_deanonymization`.",
          "createdAt": "2023-08-03T17:32:34Z",
          "updatedAt": "2023-08-03T17:32:34Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "While you're correct the client would have to set that label and if you can get the client to do that you can probably get it to do other things to de-anonymize itself.",
          "createdAt": "2023-08-03T17:54:10Z",
          "updatedAt": "2023-08-03T17:54:10Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "From the POV of a realistic deployment, many users will be unaware of what labels are set by the client software that they use, and especially how those labels are computed; and it would be easy for a malicious entity providing the client software & operating the collector to decide on a much more subtle labeling system to allow deanonymization.\r\n\r\nI think labels inherently require trust in the entity providing the client/operating the collector.",
          "createdAt": "2023-08-03T18:19:15Z",
          "updatedAt": "2023-08-03T19:00:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi @simon-friedberger, please take a look at the Mastic VDAF, which attempts to solve part of this problem: https://datatracker.ietf.org/doc/draft-mouris-cfrg-mastic/",
          "createdAt": "2023-10-23T23:28:47Z",
          "updatedAt": "2023-10-23T23:28:47Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think we should avoid adding fingerprinting opportunities to the protocol.\r\n\r\n> The leader already sees client IPs when they submit reports. Those are pretty good for fingerprinting. They also know the submission time, but given IP geofencing that probably doesn't add a lot. They know the time at which the client thinks it generated the report which also leaks.\r\n\r\nThese shortcoming can be effectively addressed by other technologies like OHTTP & privacypass, for e.g. the reportauth ext: https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth/blob/main/draft-priebe-ppm-dap-reportauth.md proposes one solution.\r\n\r\nIf I understand correctly, the proposal is to include multiple labels in the same task so collector can filter by them? \r\nAlternatively, can we leave that out of MPC and treat as business logic? In DAP the basic unit is a task, all privacy guarantees will be made on a task's aggregation result (min_batch_size, future DP guarantee, etc.). This requirement can be resolved by task author designing the tasks to include all the labels. This essentially divide user population by cohorts, each designated by a `label`. This way, labelling is more explicit, especially if we address issue: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/500. If flexibility is a concern, we can use [taskprov](https://github.com/wangshan/draft-wang-ppm-dap-taskprov/blob/main/draft-wang-ppm-dap-taskprov.md#the-taskprov-extension-definition) to make these labels part of a task config and let client decide whether to opt-in with the labelled task or not. ",
          "createdAt": "2023-11-06T16:40:01Z",
          "updatedAt": "2023-11-06T16:40:01Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> > The leader already sees client IPs when they submit reports. Those are pretty good for fingerprinting. They also know the submission time, but given IP geofencing that probably doesn't add a lot. They know the time at which the client thinks it generated the report which also leaks.\r\n> \r\n> These shortcoming can be effectively addressed by other technologies like OHTTP & privacypass, for e.g. the reportauth ext: https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth/blob/main/draft-priebe-ppm-dap-reportauth.md proposes one solution.\r\n\r\nAgreed on OHTTP, but how does Privacy Pass help?\r\n\r\n\r\n> If I understand correctly, the proposal is to include multiple labels in the same task so collector can filter by them? Alternatively, can we leave that out of MPC and treat as business logic? In DAP the basic unit is a task, all privacy guarantees will be made on a task's aggregation result (min_batch_size, future DP guarantee, etc.). This requirement can be resolved by task author designing the tasks to include all the labels. This essentially divide user population by cohorts, each designated by a `label`. This way, labelling is more explicit, especially if we address issue: #500. If flexibility is a concern, we can use [taskprov](https://github.com/wangshan/draft-wang-ppm-dap-taskprov/blob/main/draft-wang-ppm-dap-taskprov.md#the-taskprov-extension-definition) to make these labels part of a task config and let client decide whether to opt-in with the labelled task or not.\r\n\r\nWhat's the difference between this proposal and what @tgeoghegan describes at the top of this issue? Taskprov _might_ make this easier (arguably), but it seems basically the same as \"spinning up a new task per-label\".\r\n\r\n",
          "createdAt": "2023-11-06T16:55:09Z",
          "updatedAt": "2023-11-06T16:55:09Z"
        },
        {
          "author": "bmcase",
          "authorAssociation": "NONE",
          "body": "I would like to +1 the usefulness of this functionality for histogram aggregation.  I'm new to PPM but have followed for understanding how we could use these VDAFs for protocols we're working on in the W3C PATCG.  In particular, I think these labeled secret shares are a great fit to work in Apple's Private Ad Measurement (PAM) proposal for publisher reports.  \r\n\r\nA couple thoughts on that are that \r\n1) the concern about labels being a fingerprinting vector doesn't apply in the same way in the PAM setting. There what would happen is an encrypted report would be sent back from an impression to a publisher and it is fine the publisher learns which impression that report corresponds to. The label(s) can be added server side before submitting to the MPC aggregation service. \r\n\r\n2) As for a report ending up in multiple queries because of different labels -- this would be something we'd like to intentionally support and have a DP budget layer that manages this.  In particular, one way this could work is that a report comes with some amount of DP budget and it can then be submitted (only once with replay protection) to the MPC aggregation to be included in multiple histograms but the budget requested across all those histograms needs to be less than the report has available.  If there are enough histograms in the output advanced DP composition becomes useful.  ",
          "createdAt": "2023-11-07T08:20:49Z",
          "updatedAt": "2023-11-07T08:20:49Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi @bmcase, great to have you.\r\n\r\n>     2. As for a report ending up in multiple queries because of different labels -- this would be something we'd like to intentionally support and have a DP budget layer that manages this.  In particular, one way this could work is that a report comes with some amount of DP budget and it can then be submitted (only once with replay protection) to the MPC aggregation to be included in multiple histograms but the budget requested across all those histograms needs to be less than the report has available.  If there are enough histograms in the output advanced DP composition becomes useful.\r\n\r\nActually there are two lines of work on DP right now.\r\n\r\nIn https://github.com/wangshan/draft-wang-ppm-differential-privacy we're working on fleshing out our considerations for DP and specifying some standard (non-MPC) DP mechanisms to use with DAP.\r\n\r\nIn the Slack, we've also been discussing generating the noise in MPC (each Aggregator computes a secret share of the noise), but there is no draft for this as of now. @martinthomson has lots of ideas for this :)\r\n\r\n",
          "createdAt": "2023-11-07T09:27:45Z",
          "updatedAt": "2023-11-07T09:27:45Z"
        },
        {
          "author": "bmcase",
          "authorAssociation": "NONE",
          "body": "Thanks @cjpatton. It's great to see this draft of DP in PPM! I had just found it and starting reviewing; I'll leave some feedback over on that repo.   On the MPC in DP I've been following the slack conversation and also discussed some with @martinthomson; it's an interesting problem. ",
          "createdAt": "2023-11-07T10:45:16Z",
          "updatedAt": "2023-11-07T10:45:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 we decided to close this issue without changes. Our conclusion was that it should be possible to implement @tgeoghegan's proposal as a report extension.",
          "createdAt": "2023-11-07T13:28:45Z",
          "updatedAt": "2023-11-07T13:28:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hey everyone, this thread has been stewing for a while, and given that we're close to finishing DAP, I want to see if there's consensus to close it. A couple of observations:\r\n\r\nFirst, I believe this functionality can be specified as a DAP extension. In particular, the teport extension could  be used to assign reports to attribute classes for grouping them during aggregation or collection. We may need an extension point for collection in order to express what attributes the collector wants to collect by. Is there interest in specifying this?\r\n\r\nSecond, partitioning reports by client attributes _in the clear_ invariably reduces the anonymity set. (In general there may be fewer clients with a particular attribute than the minimum batch size.) A new VDAF called [Mastic](https://eprint.iacr.org/2024/221) allows grouping aggregates by client attributes _without reducing the anonymity set_.\r\n\r\nThe functionality you get from Mastic is somewhat restricted, in particular it doesn't support all of the \"drill down\" queries discussed on this thread. It also has higher communication cost than you'd have with Prio3 + plaintext attributes.\r\n\r\nAn implementation of Mastic and corresponding I-D are in progress:\r\n1. https://github.com/divviup/libprio-rs/issues/947\r\n2. https://github.com/jimouris/draft-mouris-cfrg-mastic ",
          "createdAt": "2024-06-04T15:55:51Z",
          "updatedAt": "2024-06-04T15:55:51Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The fact that Mastic was able to do something useful with attributes without DAP layer changes proves (or at least suggests) to me that we don't need to do anything here.\r\n\r\nI think the remaining use case we're speculating about is Prio3 with labels, but what if instead of making DAP layer changes for that, we imagine a Prio3WithLabels VDAF, in which (shooting from the hip here) the public share was the encoded labels and the aggregation parameter was the labels the collector is interested in? Basically my intuition is that if Mastic made this work, we should be able to imagine a Prio3 variant that can make it work, and one again not need DAP changes.",
          "createdAt": "2024-06-05T15:49:40Z",
          "updatedAt": "2024-06-05T15:49:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> I think the remaining use case we're speculating about is Prio3 with labels, but what if instead of making DAP layer changes for that, we imagine a Prio3WithLabels VDAF, in which (shooting from the hip here) the public share was the encoded labels and the aggregation parameter was the labels the collector is interested in?\r\n\r\nNice observation, I think that would work just fine.\r\n",
          "createdAt": "2024-06-05T16:33:10Z",
          "updatedAt": "2024-06-05T16:33:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "[I brought this to the list for discussion, and it doesn't seem like there's appetite to modify DAP to support this feature.]( https://mailarchive.ietf.org/arch/msg/ppm/MVyU7ZsdQ3rfVx3dtvZ8nnFVoo4/) If you'd like to consider specifying this functionality as a DAP extension and think there needs to be a protocol change, please feel free to re-open. ",
          "createdAt": "2024-06-10T21:15:44Z",
          "updatedAt": "2024-06-10T21:15:44Z"
        }
      ]
    },
    {
      "number": 490,
      "id": "I_kwDOFEJYQs5tRGts",
      "title": "New Problem Details RFC",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/490",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "RFC 9457 was recently published, obsoleting RFC 7807. We should update our use of problem details to reflect it, and update the document reference.",
      "createdAt": "2023-08-02T13:32:29Z",
      "updatedAt": "2023-10-23T20:17:49Z",
      "closedAt": "2023-10-23T20:17:49Z",
      "comments": []
    },
    {
      "number": 493,
      "id": "I_kwDOFEJYQs5uGnCK",
      "title": "Include aggregation parameter in `AggregateShareAad`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/493",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "When Aggregators encrypt aggregate shares to the Collector, they construct associated data:\r\n\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  BatchSelector batch_selector;\r\n} AggregateShareAad;\r\n```\r\n\r\nThis doesn't include the aggregation parameter, and I don't immediately see why not. For instance, in the Poplar1 setting, you would want the AAD to vary across multiple queries that use different aggregation parameters.\r\n\r\nThis came up in discussion of a different change: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/488#discussion_r1291458636",
      "createdAt": "2023-08-11T18:05:14Z",
      "updatedAt": "2023-08-31T17:01:13Z",
      "closedAt": "2023-08-31T17:01:13Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Sounds like a sensible change to me.",
          "createdAt": "2023-08-12T00:56:48Z",
          "updatedAt": "2023-08-12T00:56:48Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Robustness assumes that Leader and Helper are trustworthy, so this is not needed. Privacy assumes that only one of Leader and Helper needs to be trusted so having the Collector check something wouldn't help. ",
          "createdAt": "2023-08-14T06:38:56Z",
          "updatedAt": "2023-08-14T06:38:56Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, the current text in security considerations acknowledges that. However if we follow that reasoning, then we don't need the batch selector in there, either. If we agree that what goes into the AAD doesn't _really_ make a difference to the protocol's security, then this is basically an esthetic issue, in that it's a little surprising that the aggregate share AAD is built this way and we have to carry around a few extra lines of protocol text to explain it.",
          "createdAt": "2023-08-14T15:00:25Z",
          "updatedAt": "2023-08-14T15:00:25Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I am totally for this change, either way it might catch errors.\r\n\r\nThat being said, this came up in the context of catching the leader forging collect requests: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/488#discussion_r1291458636\r\nSo, maybe that does not belong under security considerations or maybe we should make the definition of robustness more nuanced.",
          "createdAt": "2023-08-14T15:29:41Z",
          "updatedAt": "2023-08-14T15:29:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I vote we just chalk this up to \"defense-in-depth\" and call it a day :)",
          "createdAt": "2023-08-15T01:22:46Z",
          "updatedAt": "2023-08-15T01:22:46Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "+1, sounds good. I think this protects against the Leader varying the parameters passed to the Helper in its aggregate share requests in an undetectable way.",
          "createdAt": "2023-08-17T19:10:40Z",
          "updatedAt": "2023-08-17T19:10:40Z"
        }
      ]
    },
    {
      "number": 497,
      "id": "I_kwDOFEJYQs5vu3Jf",
      "title": "Do we still need `max_batch_query_count`?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/497",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Discussion in #495 leads us to believe that we might want to remove the DAP-level concept of `max_batch_query_count` and replace it with a VDAF-specific check that would take into account the history of queries made against a batch. We don't yet completely understand what the shape of that check is.",
      "createdAt": "2023-08-31T00:29:45Z",
      "updatedAt": "2023-08-31T00:56:12Z",
      "closedAt": "2023-08-31T00:55:36Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "VDAF requires that we do aggregation parameter validation, i.e., before agreeing to run a collection job with aggregation parameter `agg_param`, it must be that `Vdaf.is_valid(agg_param, prev_agg_params) == True`, where `prev_agg_params` is the set of aggregation parameters used previously for collecting the batch: https://cfrg.github.io/draft-irtf-cfrg-vdaf/draft-irtf-cfrg-vdaf.html#section-5.3\r\n\r\nDAP enforces this here: https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.1.4-3.6.1\r\n\r\nNote that this is a vestige from at least draft-00 (it used to be called `max_batch_lifetime`). The VDAF-level check is stricter (and necessary according to {{DRPS23}}), so we can probably just get rid of it.\r\n\r\n",
          "createdAt": "2023-08-31T00:37:47Z",
          "updatedAt": "2023-08-31T00:37:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Oops, this is a dup of https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/436. Sorry about that.",
          "createdAt": "2023-08-31T00:55:36Z",
          "updatedAt": "2023-08-31T00:56:12Z"
        }
      ]
    },
    {
      "number": 499,
      "id": "I_kwDOFEJYQs5wq2MI",
      "title": "Report selection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/499",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The Leader sees client IP addresses and can easily inject and drop reports. This puts the Leader in a powerful position for Sybil attacks. On the surface, it seems like an interesting suggestion to let the Helper select the reports for a certain aggregation instead.\r\n\r\nHere are some counterarguments:\r\n1.. The helper needs to explicitly mix reports for this to have any effect.\r\n2. The leader can still single out reports by dropping and injecting sufficiently many to make the mixing ineffective.\r\n3. An OHTTP server to obfuscate client identities might be more beneficial anyway.\r\n4. Sybil attack protection would limit what the leader can do to single out reports.\r\n",
      "createdAt": "2023-09-11T11:19:36Z",
      "updatedAt": "2023-10-23T20:18:29Z",
      "closedAt": "2023-10-23T20:18:29Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": ">  On the surface, it seems like an interesting suggestion to let the Helper select the reports for a certain aggregation instead.\r\n\r\nWhat's the Helper behavior your envisioning here?\r\n\r\nHave you had a look at https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth? The idea suggested there is that the Helper (and Leader, in fact) would only accept reports from clients that got issued a valid (rate-limited) PAT.",
          "createdAt": "2023-09-11T15:50:52Z",
          "updatedAt": "2023-09-11T15:50:52Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "The leader would submit all reports to the helper and the helper would select which reports go into a specific aggregation. This should make it harder for the leader to pick reports by a specific client and aggregate them with fakes to break privacy.",
          "createdAt": "2023-09-12T06:37:24Z",
          "updatedAt": "2023-09-12T06:37:24Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think a report authentication scheme like https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth plus IP anonymisation like OHTTP would resolve this issue nicely. ",
          "createdAt": "2023-09-29T18:11:59Z",
          "updatedAt": "2023-09-29T18:11:59Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "+1 -- authenticating reports and anonymizing the upload flow seems like it addresses the issue. Closing on that basis.",
          "createdAt": "2023-10-19T20:39:21Z",
          "updatedAt": "2023-10-19T20:39:21Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Actually, let's just add some text describing these options for deployments that need to worry about this particular threat.",
          "createdAt": "2023-10-19T20:40:10Z",
          "updatedAt": "2023-10-19T20:40:10Z"
        }
      ]
    },
    {
      "number": 500,
      "id": "I_kwDOFEJYQs5w-DpK",
      "title": "Making privacy-relevant task parameters visible to clients",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/500",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "ietf118"
      ],
      "body": "The `min_batch_size` task parameter is relevant to the question of how well a given DAP tasks protects the privacy of its measurements. While agreement on the value of `min_batch_size` is out of scope of the protocol, both Aggregators must know its value. In some deployment scenarios, we may have the goal that that reverse engineering a Client provides enough information to assess how the privacy of measurements is protected. It would be easy to distribute `min_batch_size` to the Client as well, but it does not make use of this value, and any results would require trust in the means by which this `min_batch_size` was distributed to the client, on top of the Aggregator non-collusion assumption.\r\n\r\nWe could provide more robust transparency for this parameter in a couple ways: either by asking both aggregators, or by having the Client commit to its view of the parameter and honest Aggregators checking this commitment.\r\n\r\nOn-line verification verification of parameters would require both Aggregators to support some new endpoint, and return a select set of task parameters in response to requests for a particular task. Someone auditing a Client implementation could identify the task ID and aggregator endpoints, query both aggregators, confirm that they match, and then assume that aggregate shares will obey the `min_batch_size` restriction. It would not be necessary for clients to check this endpoint before uploading.\r\n\r\nHaving clients commit to `min_batch_size` is already accomplished by the [`taskprov`](https://datatracker.ietf.org/doc/draft-wang-ppm-dap-taskprov/) draft, because `min_batch_size` is hashed into the derivation of the task ID. If we assume that at least one aggregator is honest and that it only accepts task parameters via the `taskprov` header mechanism, then it must agree with the client on the value of `min_batch_size`, and we know it will follow that when releasing aggregate shares. (if there is one dishonest Aggregator claiming to use `taskprov` and one honest Aggregator that accepts any set of task parameters from the dishonest Aggregator via a non-`taskprov` means, then this falls apart)\r\n\r\nThere are other ways one could achieve the same commitment, without taking on the entirety of `taskprov`. For example, we could distribute `min_batch_size` to the client, and have it include it in some new extension in both report shares. We know this will be distributed to both Aggregators, since extensions are included in the HPKE AAD. Then, both Aggregators would check that the client's view of task parameters matches theirs, and proceed if it does. Alternately, we could consider adding `min_batch_size` to the report share AAD. This would save a few bytes per report, at the expense of requiring that Clients always be provided this parameter.\r\n\r\nIf we consider cases where the Aggregators add noise to aggregate shares for differential privacy, then we would want to include differential privacy parameters along with `min_batch_size` in the above mechanisms as well. As before, once we have confirmed that the Client and both Aggregators have a common view of the DP parameters, then we assume that at least one honest Aggregator will add noise following those parameters to all aggregate shares. (or, if noise is added inside MPC instead of in the clear by each Aggregator, we would have a stronger conclusion)",
      "createdAt": "2023-09-13T21:52:02Z",
      "updatedAt": "2023-11-08T16:18:58Z",
      "closedAt": "2023-11-08T16:18:58Z",
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> For example, we could distribute min_batch_size to the client, and have it include it in some new extension in both report shares. We know this will be distributed to both Aggregators, since extensions are included in the HPKE AAD\r\n\r\n@divergentdave this is the approach used by old taskprov-02, but since the core protocol moved extension to each share, this method is deemed too costly. \r\n\r\nYou are right that differential privacy parameters should also be included, taskprov includes a [DpConfig](https://www.ietf.org/archive/id/draft-wang-ppm-dap-taskprov-04.html#section-3-11) that meant to capture DP related parameters. In fact, taskprov wants to make all the task parameters visible to clients, because all of them together form the privacy guarantee DAP promises. For example, [VdafConfig](https://www.ietf.org/archive/id/draft-wang-ppm-dap-taskprov-04.html#section-3-9) is also important because the Differential Privacy parameters will only make sense if the right VDAF is used. \r\nPerhaps we can propose a \"taskprov-lite\", which uses taskprov's design to pass task parameters from client to aggregator, allowing aggregators to verify the parameters, without enforcing them to provision the task on-demand.",
          "createdAt": "2023-09-29T21:13:02Z",
          "updatedAt": "2023-09-29T21:13:02Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Since the aggregators are supposed to know the parameters already it would be sufficient to include them as AAD in the authentication of the reports.",
          "createdAt": "2023-10-11T06:53:30Z",
          "updatedAt": "2023-10-11T06:53:30Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@wangshan I agree with the goal of splitting taskprov into two problems: parameter transparency and task provisioning. Would it be possible to refactor the taskprov draft such that deployments can choose when they want to transparency vs when they want on-demand provisioning support? If so, I think we can close this issue out and use that draft to track resolution.",
          "createdAt": "2023-10-19T22:24:46Z",
          "updatedAt": "2023-10-19T22:24:46Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The taskprov draft specifies and encoding of the task configuration that is used to derive the task ID. I really like this feature of the extension because it means that successful upload/aggregation/collection implies agreement on the task configuration. I think this would be a cool feature to have in the core DAP protocol.\r\n",
          "createdAt": "2023-10-19T22:37:20Z",
          "updatedAt": "2023-10-19T22:37:20Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Since the aggregators are supposed to know the parameters already it would be sufficient to include them as AAD in the authentication of the reports.\r\n\r\nIt should also be sufficient to derive the task ID from the task config, as taskprov does. This is nice because it doesn't require wire changes.",
          "createdAt": "2023-10-23T23:32:31Z",
          "updatedAt": "2023-10-23T23:32:31Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton True, but it conflicts with the suggestion that task ID should be based on the verification key to ensure that the aggregators agree on a verification key before reports are generated. (This could probably be handled by distributing a hash of the verification key to clients.)",
          "createdAt": "2023-10-25T08:16:58Z",
          "updatedAt": "2023-10-25T08:16:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Well, could you derive the task ID from both the verification key and the task config and accomplish the same thing?",
          "createdAt": "2023-10-25T20:34:56Z",
          "updatedAt": "2023-10-25T20:34:56Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It should be doable, yes. IIRC clients are not allowed to have the verification key, right? I cannot find this in the current version of the draft.\r\n",
          "createdAt": "2023-10-26T14:57:09Z",
          "updatedAt": "2023-10-26T14:57:09Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "That's right: knowning the verification key would allow them to break robustness.",
          "createdAt": "2023-10-26T21:10:50Z",
          "updatedAt": "2023-10-26T21:10:50Z"
        },
        {
          "author": "npdoty",
          "authorAssociation": "NONE",
          "body": "Agreement on parameters being visible to the client (even if it relies on at least one of the parties being honest) would contribute to an important privacy goal, transparency about the privacy parameters of what measurements the client is engaged in.",
          "createdAt": "2023-11-07T12:55:30Z",
          "updatedAt": "2023-11-07T12:55:30Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 we decided to close this without doing anything. @chris-wood suggested that @simon-friedberger's proposal (to add parameters like min_batch_size to AAD encryption) could be implemented by a report extension. @tgeoghegan points out that taskprov is probably more than we need, and in any case agreement/transparency of the task config is not a core requirement.\r\n\r\nIs this satisfactory, @npdoty, or would you like to anything else here?",
          "createdAt": "2023-11-07T13:32:18Z",
          "updatedAt": "2023-11-07T13:32:18Z"
        },
        {
          "author": "npdoty",
          "authorAssociation": "NONE",
          "body": "I understood the conversation to conclude that the goal could be accomplished through multiple means, including taskprov or a different extension to dap. But that currently those extensions do not exist. \r\n\r\nIf you want to close this issue, just let me know where I should open the issue to track whether/where the visibility property is satisfied.",
          "createdAt": "2023-11-07T14:15:22Z",
          "updatedAt": "2023-11-07T14:15:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I understood the conversation to conclude that the goal could be accomplished through multiple means, including taskprov or a different extension to dap. But that currently those extensions do not exist.\r\n\r\nYes, that's right.\r\n\r\n> If you want to close this issue, just let me know where I should open the issue to track whether/where the visibility property is satisfied.\r\n\r\nWe believe this notion of visibility is satisfied by taskprov. It could also be satisfied by a not-yet-defined extension that encodes the privacy-relevant parameters in a report extension. (The Aggregators would check that the parameters sent by the client match the task.)\r\n\r\nYou're right that we didn't settle whether this property should be a requirement for the core DAP protocol. It's definitely worth settling this before closing the issue. I'm not sure we all agreed this should be a requirement, but we could at least strongly suggest it.\r\n\r\nHere is an implementation of [Proposal 3 from \"DAP open issues\" slides](https://datatracker.ietf.org/meeting/118/materials/slides-118-ppm-dap-open-issues), except the MUST is changed to a SHOULD: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/524\r\n\r\nPlease take a look!",
          "createdAt": "2023-11-07T17:29:43Z",
          "updatedAt": "2023-11-07T17:29:43Z"
        }
      ]
    },
    {
      "number": 502,
      "id": "I_kwDOFEJYQs5zDQj_",
      "title": "Align VDAF interface with vdaf-07",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/502",
      "state": "CLOSED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The latest VDAF has simplified its API with verbs: https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/267.\r\nA few places in DAP is still using the old ones.",
      "createdAt": "2023-10-06T13:55:18Z",
      "updatedAt": "2023-10-13T20:04:32Z",
      "closedAt": "2023-10-13T20:04:32Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\ud83d\udc4d please send a PR when you have the chance.",
          "createdAt": "2023-10-06T14:26:47Z",
          "updatedAt": "2023-10-06T14:26:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed by #503.",
          "createdAt": "2023-10-13T20:04:32Z",
          "updatedAt": "2023-10-13T20:04:32Z"
        }
      ]
    },
    {
      "number": 505,
      "id": "I_kwDOFEJYQs50Axx8",
      "title": "Security considerations: privacy impact of per-task HpkeConfigs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/505",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "ietf120"
      ],
      "body": "We may want to make note of the fact that the helper sees IP addresses of clients only during /hpke_config requests. Therefore the helper will see the combination of IP addresses and task IDs if using per-task HpkeConfigs, or IP addresses alone if not. This is a smaller difference (note that the leader sees them together either way) but we may want to note it as a consideration for choosing whether to use per-task HpkeConfigs. Note that OHTTP proxies would mitigate this.\n\nh/t @tgeoghegan",
      "createdAt": "2023-10-17T01:38:29Z",
      "updatedAt": "2024-07-26T23:40:46Z",
      "closedAt": "2024-07-26T22:21:00Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Relatedly: one benefit of 1-byte IDs for HpkeConfigs is that it reduces the \"fingerprinting\" surface by restricting the number of possible distinct IDs.\r\n\r\nI don't recall if we did this mindfully; as I recall we just copy-pasted this from ECH. @chris-wood do you remember how we eneded up shortening the HpkeConfig ID in ECH to one byte? Was this about fingerprinting or something else?",
          "createdAt": "2023-10-17T01:44:13Z",
          "updatedAt": "2023-10-17T01:44:13Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "That was precisely the reason. ",
          "createdAt": "2023-10-17T01:45:06Z",
          "updatedAt": "2023-10-17T01:45:06Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This issue initially was about adding a security consideration, but I think we should do a couple other things in these paragraphs that seem related. There's a TODO lurking in this text:\r\n\r\n> [TODO: Allow Aggregators to return HTTP status code 403 Forbidden in deployments that use authentication to avoid leaking information about which tasks exist.]\r\n\r\nAdditionally, I don't know if this prescription is necessary:\r\n\r\n> Clients retrieve the HPKE configuration from each Aggregator by sending an HTTP GET request to {aggregator}/hpke_config. Clients MAY specify a query parameter task_id whose value is the task ID whose HPKE configuration they want. *If the Aggregator does not recognize the task ID, then it MUST abort with error unrecognizedTask.*\r\n\r\nWe don't need a DAP-layer error. An HTTP client error should suffice (as usual, implementations can put a problem document in the body of the HTTP 4XX if they want). Note that I still think we need the `missingTaskID` langauge that instructs clients to \"upgrade\" their HPKE config request if the aggregator doesn't use global HPKE configs.",
          "createdAt": "2023-11-01T16:46:29Z",
          "updatedAt": "2023-11-01T16:46:29Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I and others have come around to a new proposal here: we should do away with per-task HPKE keys altogether. The primary motivation for this is to reduce the complexity of the protocol: if only global HPKE keys are supported, then we can remove the task ID query parameter from the `/hpke_config` route and remove the description of the fallback logic.\r\n\r\nI have two more arguments in favour of doing this.\r\n\r\n## Oblivious HTTP\r\n\r\nRecently, we've been experimenting with composing DAP with [Oblivious HTTP](https://datatracker.ietf.org/doc/rfc9458/). The intent is to prevent the aggregators from learning which clients are participating in which tasks. If DAP uploads go over OHTTP, then the leader gets to see client-identifying metadata and in some settings, participating in some task can leak information (i.e., the user has opted into some data collection or has been assigned to some cohort).\r\n\r\nBut just doing uploads over OHTTP is insufficient: the helper and leader both get to see the clients and which task they are participating in when they fetch per-task HPKE configs. We could also serve HPKE configs over OHTTP, but this is kind of unsatisfactory: HPKE configs are static content that changes rarely, making it natural to delegate serving them to a content delivery network. Serving them over OHTTP is more expensive and a burden for aggregators. However, if you serve global HPKE keys, then aggregators no longer get to learn which clients are participating in which tasks.\r\n\r\nThis still lets the aggregators learn that some client may participate in DAP tasks at all in the future (it's also possible that clients will fetch HPKE keys but never upload). It'd be better to avoid this, but let's think about OHTTP again. That protocol _also_ uses HPKE to protect messages as they transit through a third party, the OHTTP relay. In order to make sure you're delivering messages to the intended gateway, OHTTP suggests a mechanism for fetching OHTTP HPKE keys from the gateway. So that means that no matter what you do about serving DAP HPKE configs over OHTTP, the OHTTP HPKE key fetches will reveal the same information anyway. The conclusion we at Divvi Up reached is that you might as well serve global DAP HPKE keys without OHTTP, because you don't learn anything additional past what you learned from the OHTTP key fetches.\r\n\r\nWe think that composing DAP with OHTTP is going to be a very common deployment pattern, and so deployments may wind up using exclusively global HPKE keys anyway. Additionally, taskprov mandates global HPKE keys.\r\n\r\n## TLS equivalency\r\n\r\nRecall that the reason we use HPKE at all is to build an authenticated, encrypted channel from the client to the helper. The helper receives the client's report shares from the leader in aggregation job initialization messages. But let's suppose DAP had shaken out differently and we did have clients upload report shares directly to the helper. Then we would simply rely on TLS to provide the authenticated, encrypted channel, and we wouldn't think twice about a single certificate and hence key being used for all tasks. So going from per-task HPKE keys to global ones should equivalently be acceptable.\r\n\r\n## Next steps\r\n\r\nThe DAP editors and several implementers agree that this change is a good idea. If we don't hear any objections, we'll be making that change on Friday, July 5.",
          "createdAt": "2024-06-27T18:57:55Z",
          "updatedAt": "2024-06-27T18:59:48Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Just to nit-pick one thing: \r\n\r\n> @tgeoghegan: Recall that the reason we use HPKE at all is to build an authenticated, encrypted channel from the client to the helper.\r\n\r\nThis \"channel\" is not client-authenticated. To more a bit more precise, we need non-malleability, not authenticity.",
          "createdAt": "2024-07-01T15:13:50Z",
          "updatedAt": "2024-07-01T15:13:50Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Just to nit-pick one thing:\r\n> \r\n> > @tgeoghegan: Recall that the reason we use HPKE at all is to build an authenticated, encrypted channel from the client to the helper.\r\n> \r\n> This \"channel\" is not client-authenticated. To more a bit more precise, we need non-malleability, not authenticity.\r\n\r\nI agree about non-malleability, but I think it is important that the server authenticate to the client, in the sense that the client wants to be sure that only the helper it fetched an HPKE configuration from could decrypt the report share. Perhaps that's already captured by the non-malleability property?",
          "createdAt": "2024-07-01T18:45:03Z",
          "updatedAt": "2024-07-01T18:45:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> > Just to nit-pick one thing:\r\n> > > @tgeoghegan: Recall that the reason we use HPKE at all is to build an authenticated, encrypted channel from the client to the helper.\r\n> > \r\n> > \r\n> > This \"channel\" is not client-authenticated. To more a bit more precise, we need non-malleability, not authenticity.\r\n> \r\n> I agree about non-malleability, but I think it is important that the server authenticate to the client, in the sense that the client wants to be sure that only the helper it fetched an HPKE configuration from could decrypt the report share. Perhaps that's already captured by the non-malleability property?\r\n\r\nYes it's important that the client has the correct public key, but this isn't provided by HPKE on its own. ",
          "createdAt": "2024-07-01T18:53:58Z",
          "updatedAt": "2024-07-01T18:53:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Decision at IETF 120 was to merge @tgeoghegan's PR.",
          "createdAt": "2024-07-26T23:40:45Z",
          "updatedAt": "2024-07-26T23:40:45Z"
        }
      ]
    },
    {
      "number": 514,
      "id": "I_kwDOFEJYQs50r2BE",
      "title": "Widen `PrepareError` to 2 bytes",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/514",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "breaking"
      ],
      "body": "@tgeoghegan @cjpatton the size increase [of `PrepareError` from 1 byte to 2] is to account for random selection of new error types for experimentation. But we can do that separately, if at all, so I'll revert it from this PR.\r\n\r\n_Originally posted by @chris-wood in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/509#issuecomment-1771867183_\r\n            ",
      "createdAt": "2023-10-23T17:24:39Z",
      "updatedAt": "2023-11-06T16:59:53Z",
      "closedAt": "2023-11-06T16:59:53Z",
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "It seems like it wouldn't be too hard to fix this later and we definitely don't need it at the moment. It also wouldn't be a huge increase in cost. @chris-wood do you want to make a case for this?",
          "createdAt": "2023-11-01T16:08:22Z",
          "updatedAt": "2023-11-01T16:08:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'll try to remember to bring this up if time permits tomorrow. ",
          "createdAt": "2023-11-06T16:42:20Z",
          "updatedAt": "2023-11-06T16:42:20Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Honestly, I'd be fine closing this. ",
          "createdAt": "2023-11-06T16:59:47Z",
          "updatedAt": "2023-11-06T16:59:47Z"
        }
      ]
    },
    {
      "number": 519,
      "id": "I_kwDOFEJYQs508CQr",
      "title": "Batch selection as Collector-Leader buisness logic",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/519",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "ietf118"
      ],
      "body": "DAP requires some means of partitioning reports into batches (batch selection) such that the Leader and Helper agree on which reports go into which batch. Many strategies are possible. We formalize these as different \"query types\", of which there are currently two:\r\n\r\n1. \"Time-interval\": Collector specifies a time interval and the Aggregators aggregate all reports with timestamps that fall in that time interval.\r\n2. \"Fixed-size\": The Leader maps reports to batches identified by distinct \"batch IDs\" by any strategy it wishes. (The Collector has no control over partitioning, at least in the core DAP protocol.) \r\n\r\nSupporting multiple query types adds complexity for implementations. For example, for Time-interval, we don't know which batch a report corresponds to until collection time. For Fixed-size, we already know at aggregation time: it corresponds to the batch ID sent by the Leader at the start of the aggregation job. This makes an implementation of a fixed size queries much easier because it we always know where to store output shares and report IDs.\r\n\r\nThe requirement we're trying to satisfy by supporting multiple query types is to give the Collector some degree of control over how reports are partitioned.\r\n\r\n(Aside: There is another benefit, but we haven't been able to articulate it very well. Fixing the query type to the task config gives the Client some assurance about how its report is used: for Time-interval in particular, the Client knows it will always be aggregated with reports generated around the same time. This seems like a nice property to have, but it's hard to say why. It doesn't seem necessary for our core privacy consideration.)\r\n\r\nGiven the implementation complexity, we (Janus and Daphne maintainers) would like to ask if anyone who actually cares about this requirement. Observe that Fixed-size is general enough to support a variety of batching strategies, so long as the Collector and Leader have some out-of-band mechanism for implementing it. In other words, we could view batch selection as \"business logic\" that is not part of the core protocol.\r\n\r\nOf course, another option is to do nothing and emphasize that an implementation is free to ignore query types it doesn't want to implement. In particular, Daphne and Janus could just drop Time-interval support and optimize for Fixed-size.\r\n\r\nWhat do folks think we should do here?\r\n\r\nWe have discussed this in the past. I think this is the first thread on the list about this: https://mailarchive.ietf.org/arch/msg/ppm/NKPIIm5HvZ1p8EkS3tmwj6DBXDs/",
      "createdAt": "2023-10-25T17:24:26Z",
      "updatedAt": "2023-11-08T08:16:44Z",
      "closedAt": "2023-11-08T08:16:44Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "As a maintainer of Janus, I support the removal of the time-interval query type.\r\n\r\nA few points:\r\n* We implemented removal of the time-interval query type as an experimental change to Janus. This reduced code size by about 10%, build times by about 20%, and test runtimes by about 25%.\r\n* Implementing the protocol is simplified in several ways:\r\n  * The Collector no longer needs to know how to collect larger & larger intervals until it finds a time interval with at least `min_batch_size` reports. I suspect that this behavior would be a significant corner case for many realistic deployment scenarios.\r\n  * Similarly, Aggregators no longer needs to implement the ability to collect across multiple contiguous time intervals when requested to do so by a Collector.\r\n  * Janus would be able to remove 100% of its interval-indexing (e.g. looking up entities like aggregation/collection jobs by intersection of some interval associated with the entity with a given timestamp or interval). This likely implies that implementations would no longer require interval-indexing, allowing use of simpler \"scalar\" indexes, which may enable use of a wider variety of datastore technologies without potentially requiring implementation of interval-indexing. In particular, since only \"scalar\" indexes are required, this may ease implementation on key-value datastores. Interval logic is also more error-prone -- corner-case bugs due to confusion between open & closed intervals in particular are very plausible.\r\n  * In general, supporting two similar-but-different protocols in time-interval and fixed-size can lead to confusion where checks intended for one query type are incorrectly applied to the other query type. I found one such bug while surveying Janus to write this response! https://github.com/divviup/janus/issues/2174\r\n* We implemented a business-logic feature called \"time-bucketed fixed-size\", which causes our Leader to generate fixed-size batches such that all reports in each batch are in the same \"time window\", similarly to how the time-interval query type groups all reports in the same \"time window\" into the same batch. This gives confidence that the fixed-size query type can support use-cases currently supported by the time-interval query type. Much more detail below.\r\n  * This approach effectively allows the reports in each time window to be collected incrementally (up to the batch size). This usually requires more collections by the Collector per time window. But this also allows collection of reports for as long as reports are arriving for the time window: one downside of the time-interval query type is that once Collection has occurred for a given time window, any late-arriving reports will effectively be discarded; the fixed-size query type does not share this downside. We are aware of at least one DAP user interested in significantly backdating their reports, and who have expressed interest in keeping the \"long tail\" of reports.\r\n  * This \"incremental\" reception of aggregate values for each time window allows collection to occur more eagerly, which may enable DAP users who wish to perform analysis quickly despite having a long tail of post-dated reports.\r\n  * One downside of time-bucketed fixed-size is that there may be up to `min_batch_size` reports which cannot be added to a batch (i.e. any grouping of reports to batches inside a given time window may leave up to `min_batch_size - 1` reports ungrouped). We've discussed tuning batch size to keep this number of reports acceptably small, or perhaps providing a \"smeared\" batch joining the final few reports in a given time window with reports from another time window (the smearing idea has not yet been implemented).\r\n  * We do not yet have significant production deployment experience with this feature.",
          "createdAt": "2023-10-25T22:41:20Z",
          "updatedAt": "2023-10-25T23:24:36Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I also support the removal of the time-interval query type. But for backward compatibility purpose, can we keep the current query config structure and allow aggregators to ignore time-interval type? Essentially making time-interval optional, perhaps we can spell it out in the spec that it's kept for backward-compatible reason only.\r\n\r\n> we could view batch selection as \"business logic\" that is not part of the core protocol.\r\n\r\nWhat does this mean in practice? \r\n\r\nI think some use case may benefit from a relaxed `max_batch_size`: aggregator output a batch that is >= `min_bathc_size`, but does not abort if batch size exceeds `max_batch_size`.",
          "createdAt": "2023-11-06T16:14:28Z",
          "updatedAt": "2023-11-06T16:17:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I also support the removal of the time-interval query type. But for backward compatibility purpose, can we keep the current query config structure and allow aggregators to ignore time-interval type? Essentially making time-interval optional, perhaps we can spell it out in the spec that it's kept for backward-compatible reason only.\r\n> \r\n> > we could view batch selection as \"business logic\" that is not part of the core protocol.\r\n> \r\n> What does this mean in practice?\r\n\r\nBasically it means that the Collector and Leader would need to figure out between themselves how to partition reports. If the Collector and Leader are in the same org, then this is already the case; it's really only a problem that comes up if the Collector and Leader are in different organizations. (In this case, there may be a benefit to enforcing some partitioning rules within the protocol, but so far no one has really needed this.)\r\n\r\n> \r\n> I think some use case may benefit from a relaxed `max_batch_size`: aggregator output a batch that is >= `min_bathc_size`, but does not abort if batch size exceeds `max_batch_size`.\r\n\r\nYeah we could definitely relax this.\r\n",
          "createdAt": "2023-11-06T16:40:54Z",
          "updatedAt": "2023-11-06T16:40:54Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> I think some use case may benefit from a relaxed `max_batch_size`: aggregator output a batch that is >= `min_bathc_size`, but does not abort if batch size exceeds `max_batch_size`.\r\n\r\nI think this is a good idea, too. `min_batch_size` is a privacy parameter, but making `max_batch_size` optional is acceptable. A fixed-size task without a `max_batch_size` could drop some corner cases in the aggregation-job creation logic.",
          "createdAt": "2023-11-06T22:06:33Z",
          "updatedAt": "2023-11-06T22:06:33Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "At IETF 118 we decided to keep the query the types they are. I've created a PR to close this issue, which clarifies that implementations are free to ignore query types they don't want to implement.",
          "createdAt": "2023-11-07T13:27:19Z",
          "updatedAt": "2023-11-07T13:27:19Z"
        }
      ]
    },
    {
      "number": 520,
      "id": "I_kwDOFEJYQs509Joy",
      "title": "Relax Helper handling of duplicate report IDs to support retries",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/520",
      "state": "CLOSED",
      "author": "kristineguo",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Currently, the spec specifies that if the Helper detects any duplicate report IDs within an aggregation job, it should abort: https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-07#section-4.5.1.2-3\r\n> Next, the Helper checks that the report IDs in AggregationJobInitReq.prepare_inits are all distinct. If two preparation initialization messages have the same report ID, then the Helper MUST abort with error invalidMessage.\r\n\r\nIf the Leader sends an AggregationJobInitReq but loses the response, it should be able to retry the same request (or even with additional new reportIds). Under the current behavior, the Helper would reject the entire request. \r\n\r\nWe should consider dropping this requirement to support AggregationJobInitReq retries. We should only consider reports replayed if they have been previously used for _different_ `part_batch_selector`/`agg_params` (per https://www.ietf.org/archive/id/draft-ietf-ppm-dap-07.html#section-4.5.1.4-3.6.3).\r\n\r\nOtherwise, a previously seen report ID for the _same_ `part_batch_selector`/`agg_params` should be considered a \"duplicate report\" and handled in a way that prevents verifying / contributing twice. For example, the Helper can store reports with the report ID as the primary key. If the Helper has previously seen a report ID, it can use the previously stored state. For new reports, it can compute the prepare responses.\r\n\r\nWithin a job, the first occurrence of a report ID can be accepted. All further occurrences within that job can be rejected as `report_duplicated`.\r\n\r\nWith these changes, the Leader can now retry a failed AggregationJobInitReq without having to lose all previously processed reports.",
      "createdAt": "2023-10-25T20:35:37Z",
      "updatedAt": "2023-11-08T18:22:30Z",
      "closedAt": "2023-11-08T18:22:30Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree completely with you that retryability is crucial. The intent is indeed that multiple requests to `PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` for a given `task-id` and `aggregation-job-id` should all succeed, provided the `AggregationJobInitReq` in the body has not changed. We don't explicitly say so because it is implied by the `PUT` HTTP method.\r\n\r\nAs for 4.5.1.2-3: the intent of that paragraph is that report IDs must be unique in the scope of a single `AggregationJobInitReq` message, so I don't think it rules out retrying `AggregationJobInitReq` messages. This is about rejecting malformed messages where the same report ID appears twice and absolving aggregators of guessing which is the real report share for that report ID.\r\n\r\nIf you'd like to propose some text that makes the scope of this requirement more clear, we'd be happy to review a PR on the spec.",
          "createdAt": "2023-10-25T22:31:13Z",
          "updatedAt": "2023-10-25T22:31:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I see things as @tgeoghegan does here. We definitely want to make sure that, if a request is aborted, it should be safe to retry. In this particular case, the Leader should be able to retry with de-duplicated candidate set (i.e., with one of the reports with the duplicate ID removed), and the Helper shouldn't consider any of the reports replayed.\r\n\r\nAs I understand it, this problem is solvable with clarification to the text. Of course, let us know if we're missing the point here :)\r\n\r\nOne clarification: The intent of anti-replay is that a report is assigned to one and only one batch for a task. Thus we want to prevent replays even for different values of `part_batch_selector` (i.e., the batch ID). Different values of `agg_param` need to be handled according to the [agg param validity rules for the VDAF](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf#section-5.3). These rules govern exactly the type of problem you point out (preventing some report randomness from getting reused insecurely).\r\n\r\n> Within a job, the first occurrence of a report ID can be accepted. All further occurrences within that job can be rejected as `report_duplicated`.\r\n\r\nHere it sounds like you're suggesting a protocol change. Do you mean that the Helper should be smart enough to reject duplicated reports within the same aggregation job, rather than abort the job completely?\r\n\r\n",
          "createdAt": "2023-10-26T00:13:20Z",
          "updatedAt": "2023-10-26T00:13:20Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "Thank you both for clarifying! In that case, the Helper supports idempotent requests in so far as the Leader can retry the exact same request multiple times. In these cases, since we cannot re-verify the same reports again (e.g., such as with Prio3), I'm assuming the expected behavior would be to return the Helper's stored state from the first time it processed the aggregation job.\r\n\r\nFor duplicated reports, this point makes sense to me:\r\n> absolving aggregators of guessing which is the real report share for that report ID\r\n\r\nI'm assuming the Helper will not have counted any of the other reports as processed if the aggregation job init is rejected, so we can retry the request with the duplicate report removed and succeed.",
          "createdAt": "2023-10-26T23:53:14Z",
          "updatedAt": "2023-10-26T23:53:14Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Indeed as long as no actual aggregation steps have been performed (at most until the result has been shared with the other aggregator) the report can still be used and also in a different batch. If a leader is sending multiple reports with the same ID that does seem like a bug in the leader, though. IIUC the leader is supposed to perform the same checks as the helper. This follows from the fact, that ensuring that reports are not used multiple times is necessary for protecting client privacy, so according to the security model both aggregators have to do it.",
          "createdAt": "2023-10-27T07:39:37Z",
          "updatedAt": "2023-10-27T07:40:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@kristineguo,\r\n\r\n> Thank you both for clarifying! In that case, the Helper supports idempotent requests in so far as the Leader can retry the exact same request multiple times. In these cases, since we cannot re-verify the same reports again (e.g., such as with Prio3), I'm assuming the expected behavior would be to return the Helper's stored state from the first time it processed the aggregation job.\r\n\r\nThe Helper has a little leeway in how it implements things, but yeah that's one way it could work :) \r\n\r\n\r\n> For duplicated reports, this point makes sense to me:\r\n> \r\n> > absolving aggregators of guessing which is the real report share for that report ID\r\n> \r\n> I'm assuming the Helper will not have counted any of the other reports as processed if the aggregation job init is rejected, so we can retry the request with the duplicate report removed and succeed.\r\n\r\nCorrect. This can get a little tricky because there are certain failure cases where the Helper may have partially committed state changes when something goes wrong. (Think \"500 Internal Error\".) An implementation needs to be designed to be as resilient as possible in such cases, but the [Two General's Problem](https://en.wikipedia.org/wiki/Two_Generals%27_Problem) implies I think that something can always go wrong in theory. One way we plan to address this in Daphne is to commit to stage changes at the latest possible stage of an aggregation job.\r\n\r\nHowever, this failure case is quite easy to handle fortunately. The Helper will abort the request as soon as it detects the repeated  report ID in the AggregationJobInitReq; this happens quite early in the life of the aggregation job, before there's any state changes.\r\n\r\n@simon-friedberger ,\r\n\r\n> IIUC the leader is supposed to perform the same checks as the helper. This follows from the fact, that ensuring that reports are not used multiple times is necessary for protecting client privacy, so according to the security model both aggregators have to do it.\r\n\r\nYup, that's correct.\r\n",
          "createdAt": "2023-10-27T18:28:44Z",
          "updatedAt": "2023-10-27T18:28:44Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "Sounds good. Thanks both of you for your responses. It doesn't seem like any changes are required to the spec here, so will close out this issue.",
          "createdAt": "2023-11-08T18:22:30Z",
          "updatedAt": "2023-11-08T18:22:30Z"
        }
      ]
    },
    {
      "number": 526,
      "id": "I_kwDOFEJYQs53Fe1l",
      "title": "Potential simplification of current-batch collection request semantics",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/526",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "branlwyd"
      ],
      "labels": [],
      "body": "In the fixed-size query type, `current-batch` collection requests allow the Leader to choose an outstanding batch to associate with the request.\r\n\r\nThe current semantics are that the Leader can associate the same batch to multiple `current-batch` collection requests (DAP-07 4.1.2):\r\n\r\n```\r\nThe Collector may not know which batch ID it is interested in; in this case, it can also issue a query of type current_batch, which allows the Leader to select a recent batch to aggregate. The Leader SHOULD select a batch which has not yet began collection.\r\n```\r\n\r\nThe reason these semantics were chosen was to avoid data loss in the case that a Collector issued a collection request, then crashed before recording the collection job ID. Janus, for example, can associate the same batch to an arbitrary number of `current-batch` collection requests, until at least one of those collection requests is polled for the first time.\r\n\r\n===\r\n\r\nHowever, since the above semantics were chosen, DAP changed such that the Collector now determines the collection job ID itself (as part of the changes for the resource-oriented API). This means that we could similarly avoid data loss if we expect the Collector to durably store the collection job ID _before_ making the collection request for that ID, and lean on either idempotency or appropriate error codes to allow recovery in the face of process failure. This would allow aggregator implementations to associate each batch to exactly one `current-batch` request.\r\n\r\nThe upside of making this change is that Collectors would no longer need to deduplicate `current-batch` requests that happen to be mapped to the same batch. Along with the simplified aggregator semantics, I think this is pretty likely to be an overall complexity win.",
      "createdAt": "2023-11-16T22:50:50Z",
      "updatedAt": "2023-12-15T00:04:43Z",
      "closedAt": "2023-12-15T00:04:43Z",
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Thinking about what this would look in terms of textual change to the spec: I think we would just change \"The Leader SHOULD select a batch which has not yet began collection.\" from a `SHOULD` to a `MUST`.\r\n\r\nWe could optionally choose to provide an implementation advice that the Collector note the collection job ID to durable storage before sending a collection request to avoid data loss.",
          "createdAt": "2023-11-17T17:12:39Z",
          "updatedAt": "2023-11-17T17:12:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "SGTM, please send a PR :)",
          "createdAt": "2023-11-20T13:02:33Z",
          "updatedAt": "2023-11-20T13:02:33Z"
        }
      ]
    },
    {
      "number": 541,
      "id": "I_kwDOFEJYQs57bgcR",
      "title": "Versioning PPM media types",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/541",
      "state": "CLOSED",
      "author": "suman-ganta",
      "authorAssociation": "NONE",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "feature",
        "ietf120"
      ],
      "body": "PPM prescribed media types do not have version component in them. As DAP protocol progressing, the structures are changing in a non-compatible way. In order to consistently decode those structures, we need an indicator on what structure to use to decode them with.\r\n\r\nI propose adding a MIME parameter, `version`, to express the version of the payload.\r\n\r\nEx: To represent Report structure of protocol version DAP-09, use the media type - `application/dap-report;version=09`\r\n\r\nLeader/Helper need to rely on this MIME parameter to consistently parse the payload received, and should also be used to reject unsupported versions.\r\n\r\n",
      "createdAt": "2024-01-08T16:24:21Z",
      "updatedAt": "2024-08-15T21:28:59Z",
      "closedAt": "2024-08-15T21:28:59Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I wouldn't object to this change if it's helpful for your implementation, but as things stand I'm not sure it's strictly necessary. [When we last discussed versioning](https://mailarchive.ietf.org/arch/msg/ppm/svv73HxLvTMdOqk2fLyuWmZRU6Q/), we decided an implementation could do this completely out of band. For example, in [Daphne](https://github.com/cloudflare/daphne/blob/main/daphne_server/src/router/helper.rs#L29) we prefix each endpoint with \"/:version\", where \":version\" is a Daphne-specific representation of the draft number (e.g., \"v09\" for draft09).\r\n\r\nI think we should also be open to arguments like \"this is already HTTP best practice\". I for one do not have a lot of experience developing HTTP applications.",
          "createdAt": "2024-01-08T16:31:27Z",
          "updatedAt": "2024-01-08T16:31:27Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "Agreed on path based ways to distinguish versions as an alternative.\r\n\r\nBut, given that we are defining a dedicated mime type (`application/dap-report`), and not using a generic mime type (`application/octet-stream`), the type should also indicate how to decode it. The encoding is purely based on a specific version and the type information is not part of the stream. So for the reasons of making it self describable as much as possible, I think it is important to express version there.\r\n\r\n* It is not a problem for textual structured payloads such as json (`application/json`) since the structure has delimiters.\r\n* It is not an issue with protobuf (`application/protobuf`) since the message encapsulates the field boundaries.",
          "createdAt": "2024-01-08T17:16:02Z",
          "updatedAt": "2024-01-08T17:16:02Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think if we end up with multiple incompatible versions after this is moved out of the draft state adding version information would be a good idea.\r\n\r\n@mnot a comment from you would be greatly appreciated!",
          "createdAt": "2024-01-09T09:06:40Z",
          "updatedAt": "2024-01-09T09:07:02Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@suman-ganta please prepare a PR when you have a moment.",
          "createdAt": "2024-01-10T16:15:31Z",
          "updatedAt": "2024-01-10T16:15:31Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "My .02 - it's fine to add a version parameter (or something else) for draft development purposes. When it ships, however, there shouldn't be any need for a version; the media type _is_ the version identifier.",
          "createdAt": "2024-01-10T23:38:23Z",
          "updatedAt": "2024-01-10T23:38:23Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "@mnot how do we distinguish objects of same mime type belonging to two different major DAP versions without version parameter?",
          "createdAt": "2024-01-11T00:43:51Z",
          "updatedAt": "2024-01-11T00:43:51Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "If it's a new major version, it needs a new media type.",
          "createdAt": "2024-01-11T00:55:58Z",
          "updatedAt": "2024-01-11T00:55:58Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "> If it's a new major version, it needs a new media type.\r\n\r\nThat makes sense. But the current media types do not have any version embedded in them, So we need one of the following:\r\n\r\nOption-1:\r\nAdd version to the existing mime types. Which means, change current report mime type from `application/dap-report` to `application/dap-report_01`.\r\nSimilar naming will be followed for major/incompatible revisions.\r\n\r\nOption-2:\r\nmaintain a mandatory MIME parameter (version=) that points to a semver of the entity.\r\n",
          "createdAt": "2024-01-11T02:42:51Z",
          "updatedAt": "2024-01-11T02:42:51Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It seems like a safe  bet that this document will be \"DAP v1\" and a future document with be \"DAP v2\". What about something like `application/dap1-report`?",
          "createdAt": "2024-01-12T15:32:55Z",
          "updatedAt": "2024-01-12T15:32:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Otherwise I think I prefer @suman-ganta's option 1.",
          "createdAt": "2024-01-12T15:59:50Z",
          "updatedAt": "2024-01-12T15:59:50Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "Remember that they're opaque strings; the first version could be `application/dap-report` and the second could be `application/dap2-report` or even `application/dap-results`.",
          "createdAt": "2024-01-12T21:26:04Z",
          "updatedAt": "2024-01-12T21:26:04Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'd prefer we keep the first version as `application/dap-report` and the implementation can optionally add `;version=0x`.\r\nIf we have a second version we can have `application/dap2-report`. ",
          "createdAt": "2024-01-15T18:01:21Z",
          "updatedAt": "2024-01-15T18:01:21Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "Why do we need to treat first version differently from the rest?",
          "createdAt": "2024-01-15T19:04:20Z",
          "updatedAt": "2024-01-15T19:04:20Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I don't have a good reason other than existing MIME types seem to follow that, I don't see any common media types have version1 in the name. Besides, we don't know if there will be dap2, or whether it will still be called dap.",
          "createdAt": "2024-01-15T20:51:32Z",
          "updatedAt": "2024-01-15T20:51:32Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "Right. Which is why I was proposing version as a mime parameter.\n\nI'm not sure if there are other cases where tls encoding is used in http layer, but having no version info doesn't help to decode it in a deterministic way.",
          "createdAt": "2024-01-15T23:19:33Z",
          "updatedAt": "2024-01-15T23:19:33Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "I would love to see another non DAP tls encoding mime type, if exists, and the practice followed there. \n\nTo me, this is analogous to expressing decoder parameter for video mime types. Without them, there is no consistent way to decode the stream. ",
          "createdAt": "2024-01-15T23:22:26Z",
          "updatedAt": "2024-01-15T23:22:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not aware of such a thing. One example we might follow is OHTTP, which uses yet another serialization format: https://www.rfc-editor.org/rfc/rfc9458.html#section-9",
          "createdAt": "2024-01-16T17:23:40Z",
          "updatedAt": "2024-01-16T17:23:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "IETF 120: There was opposition to merging #568. @bemasc (as chair) suggested we get some advice from an HTTP expert about whether normative language for media type parameters is needed. I will take this action.",
          "createdAt": "2024-07-26T23:43:24Z",
          "updatedAt": "2024-07-26T23:43:24Z"
        }
      ]
    },
    {
      "number": 546,
      "id": "I_kwDOFEJYQs5-UAtH",
      "title": "projexct",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/546",
      "state": "CLOSED",
      "author": "alexanderramos89",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2024-02-05T18:13:22Z",
      "updatedAt": "2024-02-05T19:13:13Z",
      "closedAt": "2024-02-05T19:13:13Z",
      "comments": []
    },
    {
      "number": 548,
      "id": "I_kwDOFEJYQs5_2Cns",
      "title": "Recommend against min_batch_size = max_batch_size when aggregation parameters are non-trivial",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/548",
      "state": "CLOSED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "divergentdave"
      ],
      "labels": [
        "collecting a batch more than once"
      ],
      "body": "It is possible that a report may pass VDAF preparation with one aggregation parameter, and later fail when prepared with another aggregation parameter. (for example, if a later correction word is corrupted in an otherwise-honestly generated Poplar1 report) We require that the same set of reports in any batch be used as input for aggregation with each aggregation parameter. The batch validation size check enforces that the number of successfully aggregated reports is at least `min_batch_size`. Therefore, if a deployment were to set `min_batch_size` equal to `max_batch_size`, a batch that has already been successfully collected with one aggregation parameter, and contains at least one such report, would be un-collectable with some other aggregation parameters, resulting in a loss of data, and an efficient DoS vector.\r\n\r\nI think we should recommend that `max_batch_size` be strictly greater than `min_batch_size` for VDAFs that make use of the aggregation parameter. The other solution I can think of would be to somehow allow combining previously-collected batches (while still not allowing subdivision of batches) for subsequent collection with different aggregation parameters, but that would be a lot of complexity.",
      "createdAt": "2024-02-20T17:11:31Z",
      "updatedAt": "2024-02-28T23:26:52Z",
      "closedAt": "2024-02-28T23:26:52Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "We should try to generalize this into guidance for any query type. Once collected, a batch cannot change: If X% of reports are going to be rejected, then you need to aim for a batch size that is X% larger than the minimum.",
          "createdAt": "2024-02-21T15:51:19Z",
          "updatedAt": "2024-02-21T15:51:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "On the other hand, this is basically a Sybil attack, which DAP does not address on its own. We could consider relaxing the batch size requirement to allow a batch size smaller than the minimum after the initial collection.",
          "createdAt": "2024-02-21T15:52:59Z",
          "updatedAt": "2024-02-21T15:52:59Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Naive question: if aggregation with one aggregation parameter succeeds with a different set of reports than aggregation with a second aggregation parameter, are we sure we haven't leaked information about the reports in the delta between the two sets?\r\n\r\nI don't think there's any generic VDAF property that guarantees we haven't leaked information, though I might be wrong.\r\n\r\nIf that's the case, perhaps we should guarantee that the same set of reports is successfully aggregated for every aggregation over the batch. However, I believe that some VDAFs would allow a maliciously-crafted report that passes aggregation with one aggregation parameter but fails aggregation with another -- i.e., an attacker could use this as a DoS vector.",
          "createdAt": "2024-02-28T01:01:42Z",
          "updatedAt": "2024-02-28T01:01:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @cjpatton  On the other hand, this is basically a Sybil attack, which DAP does not address on its own. We could consider relaxing the batch size requirement to allow a batch size smaller than the minimum after the initial collection.\r\n\r\nWhoops, meant to update here: Thinking on this more, this is clearly a bad idea. Given our threat model, I think we just have to include more reports in the batch than we think we need and hope that no more than we need end up being invalid. Certainly mechanisms to prevent Sybil attacks would help, but since we've ruled this out for core DAP, @divergentdave's suggestion is the most sound.\r\n\r\n\r\n> @branlwyd  Naive question: if aggregation with one aggregation parameter succeeds with a different set of reports than aggregation with a second aggregation parameter, are we sure we haven't leaked information about the reports in the delta between the two sets?\r\n> \r\n> I don't think there's any generic VDAF property that guarantees we haven't leaked information, though I might be wrong.\r\n> \r\n> If that's the case, perhaps we should guarantee that the same set of reports is successfully aggregated for every aggregation over the batch. However, I believe that some VDAFs would allow a maliciously-crafted report that passes aggregation with one aggregation parameter but fails aggregation with another -- i.e., an attacker could use this as a DoS vector.\r\n\r\nGreat question. To crystalize this a bit: once a report is rejected, it is removed from the batch. That means if it's rejected during the first collection job, then it won't be included in the next collection job. In the case of Poplar1, we do indeed learn some information about the measurement that was rejected, since the prefix counts have changed by 1.\r\n\r\nVADF's answer to this question is: honest Clients are perfectly capable of generating measurements that are deemed valid each time they're aggregated; dishonest Clients forfeit privacy. It would indeed be a problem if a valid measurement could be deemed invalid, exactly for the reason you describe.\r\n\r\n",
          "createdAt": "2024-02-28T01:58:15Z",
          "updatedAt": "2024-02-28T01:58:15Z"
        }
      ]
    },
    {
      "number": 551,
      "id": "I_kwDOFEJYQs6AtJ4t",
      "title": "Prep draft-ietf-ppm-dap-10",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/551",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [],
      "body": "We want to publish draft 10 of DAP for IETF 119. The submission deadline is Monday March 4. We've already taken some breaking changes since draft-ietf-ppm-dap-09 (such as changing a couple HTTP verbs). The VDAF authors do not intend to publish a new draft yet, so we'll stay on draft-irtf-cfrg-vdaf-08.\r\n\r\n- [x] Take https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/550\r\n- [x] Update HPKE domain separation strings (#552)\r\n- [x] Update changelog (#552)",
      "createdAt": "2024-02-28T16:11:04Z",
      "updatedAt": "2024-02-29T21:44:54Z",
      "closedAt": "2024-02-29T21:44:40Z",
      "comments": []
    },
    {
      "number": 553,
      "id": "I_kwDOFEJYQs6GjZWF",
      "title": "Add a ladder diagram illustrating what bits need to be stored",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/553",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "It should also show what bits need to be \"eventually consistent\" versus transactional.",
      "createdAt": "2024-04-22T20:40:20Z",
      "updatedAt": "2024-04-22T20:40:49Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 555,
      "id": "I_kwDOFEJYQs6G17C7",
      "title": "Consider removing `max_batch_size`.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/555",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "branlwyd"
      ],
      "labels": [
        "feature",
        "ietf120"
      ],
      "body": "Once multi-collection of batches has been removed, we should consider whether the `max_batch_size` parameter of the fixed-size query type might be removed.\r\n\r\nSee https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/554/files#r1577003240 for context; in short, `max_batch_size`'s strongest use-case is in the case of a multiply-collected VDAF. It is unclear if there is a strong justification for this parameter in a DAP which supports only singly-collected VDAFs.",
      "createdAt": "2024-04-24T22:22:28Z",
      "updatedAt": "2024-07-26T23:51:36Z",
      "closedAt": "2024-07-26T23:51:36Z",
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "My thoughts here:\r\n1. A maximum batch size is useful for capacity planning. In particular, the manner in which an aggregator stores report IDs may depend on how large of a batch it needs to plan for. In fact, I think we can make the case to add this parameter to all tasks, not just tasks with this query type.\r\n2. If we take this change, then the name \"fixed-size\" no longer really applies, so we should consider renaming it as well.",
          "createdAt": "2024-04-25T20:41:09Z",
          "updatedAt": "2024-04-25T20:41:09Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "cc/ @wangshan who may have thoughts here.",
          "createdAt": "2024-04-25T20:41:41Z",
          "updatedAt": "2024-04-25T20:41:41Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Hmm -- I was thinking of the removal of `max_batch_size` as indicating that batches must be exactly `min_batch_size` in size (i.e. if we took this change, we could also rename `min_batch_size` to just `batch_size`). If we want to keep the existing semantics that a missing `max_batch_size` leads to unbounded batch sizes, IMO we should keep `max_batch_size`.",
          "createdAt": "2024-04-26T16:53:37Z",
          "updatedAt": "2024-04-26T16:53:53Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah, thanks for clarifying! FWIW, taskprov interprets unspecified max_batch_size as unlimited: https://datatracker.ietf.org/doc/html/draft-wang-ppm-dap-taskprov-06#section-3-9",
          "createdAt": "2024-05-01T19:45:13Z",
          "updatedAt": "2024-05-01T19:45:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "One more thought here, after working out some bottlenecks in our DAP service: in fact, the maximum batch size is important for capacity planning. In order to implement replay protection, existing DAP implementations generally do [some form of sharding](https://docs.google.com/document/d/1MkYMFlW0UGSP_-tyQyF4qrrbp84240Kbjx4WSmHBW6g/edit#heading=h.e6kyob2ixvk3) of stored report IDs in order to mitigate the cost of checking an ID already exists. Your sharding scheme, and the number of shards, depends on the maximum batch size you expect.\r\n\r\nBased on this experience, I would actually advocate for the opposite out come: **make the maximum batch size a parameter of every task.** I would probably also rename fixed size to something like \"leader-specified\" that is closer to the semantics of the query type.",
          "createdAt": "2024-06-05T17:03:42Z",
          "updatedAt": "2024-06-05T17:03:42Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think there's a problem with requiring a maximum batch size for time interval. Suppose the collector request a time interval where there are `max_batch_size + min_batch_size` reports available for aggregation. Two problems present themselves:\r\n\r\n1 - Only `max_batch_size` of the reports may be aggregated. How do the leader and helper choose which reports get included in the batch and which don't? What if they disagree? (recall that unlike in the fixed size case, the leader and helper independently decide batch membership in time interval)\r\n2 - Subsequently, what happens to the remaining `min_batch_size` reports? The time interval is now collected, so there's no way for the collector to request an aggregation over them.\r\n\r\nSolving either problem would require more DAP changes than just making `max_batch_size` unconditionally required.\r\n\r\nCan you provide more detail on why batch size matters to your sharding strategy? Specifically, would a bound on aggregation job size rather than batch size solve your problem?",
          "createdAt": "2024-06-10T22:46:25Z",
          "updatedAt": "2024-06-10T22:50:47Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "A solution to (1) would be to make time interval work more like fixed size in the specific respect of making the leader responsible for assigning reports to batches. This would have an additional benefit of letting the helper treat every task as fixed-size.\r\n\r\nSolving (2) seems tricky. I think you'd need to introduce something like the `current_batch`/batch ID semantics of fixed size, but compartmentalized by time interval. i.e., the collector would have to make queries not just for some interval of time, but a tuple of an interval and a batch ID (which could be `current_batch`). So then the collector could query on `([t1, t2), current_batch)` until it gets told there are fewer than `min_batch_size` reports left in `[t1, t2)`, and it could refer to each collection of that time interval by its batch ID.",
          "createdAt": "2024-06-10T22:55:47Z",
          "updatedAt": "2024-06-10T22:55:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Can you provide more detail on why batch size matters to your sharding strategy? Specifically, would a bound on \r\naggregation job size rather than batch size solve your problem?\r\n\r\nI don't see how that this would help, but it's possible we're doing something weird.\r\n\r\nSuppose the task is time interval: we would quantize time into windows, and for each window we have an instance of the underlying storage object. In that object, we store the IDs of all reports with timestamps in that window.\r\n\r\nFor us, the rate of growth is the most important metric: the faster the aggregation rate in a given time window, the more  pressure we put on the underlying storage object. To relieve the pressure, we can further shard it into more objects by a [deterministic map from report ID to shard number](https://github.com/cloudflare/daphne/blob/0f1c7d8a48721f8fdefc861d9d548a0e6b51e3f3/crates/daphne/src/lib.rs#L354-L373).\r\n\r\nBut how many shards should we choose? More shards allows you to cope with higher aggregation rate, but it also consumes more resources, since their are more storage objects in existence simultaneously. Knowing how large a batch might help me figure out how many shards I need to cope with a given aggregation rate.\r\n\r\n\r\n\r\n> Solving (2) seems tricky. I think you'd need to introduce something like the `current_batch`/batch ID semantics of fixed size, but compartmentalized by time interval. i.e., the collector would have to make queries not just for some interval of time, but a tuple of an interval and a batch ID (which could be `current_batch`). So then the collector could query on `([t1, t2), current_batch)` until it gets told there are fewer than `min_batch_size` reports left in `[t1, t2)`, and it could refer to each collection of that time interval by its batch ID.\r\n\r\nMy first thought here was: \"you collected as much data as you need, so throw the rest away\". My guess is that you're not happy with that answer :)\r\n\r\nTurning this around: are there situations where you don't know a priori how much data to expect? If I have 1,000 users, each reporting about every 10 minutes, then I should expect about 6,000 reports in an hour.\r\n\r\n",
          "createdAt": "2024-06-12T00:18:23Z",
          "updatedAt": "2024-06-12T00:19:43Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Turning this around: are there situations where you don't know a priori how much data to expect? If I have 1,000 users, each reporting about every 10 minutes, then I should expect about 6,000 reports in an hour.\r\n\r\nIf you're that confident in your estimation of batch sizes, then why can't you use that for your sharding heuristic and leave `max_batch_size` out of the spec?",
          "createdAt": "2024-06-12T15:37:37Z",
          "updatedAt": "2024-06-12T15:37:37Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "> My thoughts here:\r\n> \r\n>     1. A maximum batch size is useful for capacity planning. In particular, the manner in which an aggregator stores report IDs may depend on how large of a batch it needs to plan for. In fact, I think we can make the case to add this parameter to all tasks, not just tasks with this query type.\r\n> \r\n>     2. If we take this change, then the name \"fixed-size\" no longer really applies, so we should consider renaming it as well.\r\n\r\nWith async jobs as suggested in #557, IMO the usefulness of this for capacity planning largely goes away",
          "createdAt": "2024-06-12T15:56:32Z",
          "updatedAt": "2024-06-12T15:56:32Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Per 2024/6/12 DAP sync: @tgeoghegan is correct, I missed something important: the semantics of time interval means a maximum batch size cannot be specified for it. And in light of @Noah-Kennedy's point, it's not going to be important for capcity planning once we've added async aggregation jobs.",
          "createdAt": "2024-06-12T16:04:00Z",
          "updatedAt": "2024-06-12T16:04:00Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "One more argument against specifying `max_batch_size`:\r\n\r\n`min_batch_size` is a privacy parameter, and must be evaluated by both Leader & Helper. If they do not agree on the minimum batch size, confusion ensues; therefore, it makes sense to specify `min_batch_size`.\r\n\r\nThere is no corresponding argument for `max_batch_size`; more specifically, there is no requirement that both the Leader & Helper both evaluate a maximum batch size. We could remove `max_batch_size` from the specification, and leave any maximum size as an implementation detail for the Leader to enforce.",
          "createdAt": "2024-06-13T03:48:23Z",
          "updatedAt": "2024-06-13T03:48:23Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This is ready for text @branlwyd. Would you mind putting up a PR?\r\n\r\nWhile at it, we should try to think of a replacement for the \"fixed-size\" name.",
          "createdAt": "2024-06-13T14:32:07Z",
          "updatedAt": "2024-06-13T14:32:07Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think the salient feature is that the leader constructs batches, so the name should reflect that. `leader-chosen`? I dunno.",
          "createdAt": "2024-06-13T19:37:50Z",
          "updatedAt": "2024-06-13T19:37:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "That works for me.",
          "createdAt": "2024-06-13T19:43:56Z",
          "updatedAt": "2024-06-13T19:43:56Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Decision at IETF 120 is to merge #563. However there is some follow up discussion about whether `leader_selected` should be supported at all. @martinthomson I'll ask you to open up a PR with a summary of your thoughts.",
          "createdAt": "2024-07-26T23:18:10Z",
          "updatedAt": "2024-07-26T23:18:10Z"
        }
      ]
    },
    {
      "number": 556,
      "id": "I_kwDOFEJYQs6Hiqfq",
      "title": "Discuss distributed systems-related concerns in Operational Considerations section",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/556",
      "state": "OPEN",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "operational considerations"
      ],
      "body": "I think it would be helpful to explicitly list what sort of synchronization guarantees the aggregators need to uphold. Some of these are implicit in the text elsewhere, and they would be important to the architecture of a distributed aggregator. Here's what I have so far:\r\n\r\n- Leader\r\n  - The leader has to perform anti-replay checks between receiving a report and sending it in an aggregation job (i.e. deduplicating by ReportMetadata). This is easily amenable to approaches that only provide eventual consistency.\r\n  - The leader needs some synchronization between aggregate share requests and aggregation job requests to make sure it doesn't aggregate any new reports into a batch that has already been collected. This requirement is significantly different between time interval queries, where the client metadata determines the batch, and fixed size query, where the leader has full control of batches.\r\n  - The leader needs to synchronize between sending aggregation job requests and sending aggregate share requests, to ensure that it never has both an aggregate share request collecting a batch and an aggregation job that affects the same batch outstanding at the same time. Note that with time interval queries, there is a many-to-many mapping between aggregation jobs and batches, while with fixed size queries, each aggregation job impacts only one batch.\r\n- Helper\r\n  - The helper needs to perform duplicate report detection across aggregation job requests.\r\n  - The helper needs strong consistency between aggregation job requests and subsequent aggregate share requests, so that it includes every eligible output share in its aggregate share.",
      "createdAt": "2024-05-01T18:56:18Z",
      "updatedAt": "2024-05-01T22:38:56Z",
      "closedAt": null,
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Right now, all of these require explicit \"transactional\"/\"serializable\" synchronization between the relevant components of the system (except for report uploads, as noted).\r\n\r\nReducing these to something requiring only eventual consistency would be valuable even for an implementation using a monolithic database (e.g. Postgres transactions can still encounter distributed systems-like inconsistencies at transaction isolation levels lower than `SERIALIZABLE`, without implementation effort to ensure the appropriate transactions necessarily encounter a write conflict).",
          "createdAt": "2024-05-01T20:40:54Z",
          "updatedAt": "2024-05-01T20:40:54Z"
        }
      ]
    },
    {
      "number": 557,
      "id": "I_kwDOFEJYQs6HjBl4",
      "title": "Explicit backoff logic for aggregation jobs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/557",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "feature",
        "ietf120"
      ],
      "body": "As mentioned in #556, replay protection implies a relatively stringent operational requirement for the Helper: many aggregation jobs might make concurrent transactions on the same database, which can easily overwhelm the database if not sufficiently provisioned.\r\n\r\nTo mitigate this problem, the Helper can cancel an aggregation job and ask the Leader to try again later. One option is to respond with HTTP status 429 and a \"retry-after\" header indicating when it should be safe to retry.\r\n\r\nI'd like to suggest that we spell this out explicitly in the draft.",
      "createdAt": "2024-05-01T20:01:38Z",
      "updatedAt": "2024-08-08T22:44:45Z",
      "closedAt": null,
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "One thing we touched on today is: what is the \"unit\" of backoff, e.g. what is the domain over which the retry-after header applies to?\r\n\r\n\r\nPlausible ideas include:\r\n* Only that specific aggregation job\r\n* Aggregation jobs sharing a batch with the backoff'ed aggregation job (this might be tricky for time-interval, where an aggregation job can touch an arbitrary number of different batches)\r\n* All aggregation jobs for the same task as the backoff'ed aggregation job\r\n* All aggregation jobs",
          "createdAt": "2024-05-01T20:29:04Z",
          "updatedAt": "2024-05-01T20:29:04Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "For Daphne, it's for jobs sharing a batch \"bucket\":\r\n1. For time-interval tasks, each bucket is a time window\r\n2. For fixed-size queries, there is just one bucket, which is identified by the batch ID.\r\n",
          "createdAt": "2024-05-01T21:14:22Z",
          "updatedAt": "2024-05-01T21:14:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Just wanted to quickly document here an alternative mentioned by @divergentdave on a call: allow aggregation jobs to be asynchronous. In response to an aggregation job initialization request, the helper is allowed to respond with 201 Created without producing a response. The leader would then poll the helper to see if the response is ready, similar to how the collector polls the leader on collection jobs.\r\n\r\nThis gets a little complicated for multi-round VDAFs, but I think we can figure this out.\r\n\r\ncc/ @Noah-Kennedy",
          "createdAt": "2024-06-04T20:00:50Z",
          "updatedAt": "2024-06-04T20:00:50Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "> Just wanted to quickly document here an alternative mentioned by @divergentdave on a call: allow aggregation jobs to be asynchronous. In response to an aggregation job initialization request, the helper is allowed to respond with 201 Created without producing a response. The leader would then poll the helper to see if the response is ready, similar to how the collector polls the leader on collection jobs.\r\n> \r\n> This gets a little complicated for multi-round VDAFs, but I think we can figure this out.\r\n> \r\n> cc/ @Noah-Kennedy\r\n\r\nI quite like this approach, as it gives a lot more flexibility to implementations than the current status quo.\r\n\r\nThis makes scaling up the protocol a lot easier.",
          "createdAt": "2024-06-06T21:03:36Z",
          "updatedAt": "2024-06-06T21:03:36Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This shouldn't take too much protocol text to achieve: we'd need to clarify that it's OK for the helper to respond to `PUT /tasks/{task_id}/aggregation_jobs/{aggregation_job_id}` with 201 Created and then spell out that the leader should then poll `GET /tasks/{task_id}/aggregation_jobs/{aggregation_job_id}` until they get 200 OK and an `AggregationJobResp` (basically the same semantics as for collection jobs).",
          "createdAt": "2024-06-06T21:48:59Z",
          "updatedAt": "2024-06-06T21:48:59Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Tim's comment raises a good question: if we take this, should we allow both asynchronous & synchronous aggregation job behavior, or only asynchronous?\r\n\r\nIMO, it would be nice if we could specify only one of asynchronous/synchronous behavior: requiring implementations to support both modes would be added complexity; specifying both modes but allowing implementations to only implement one mode could lead to interoperability problems between aggregator implementations. But maybe that is too hopeful?\r\n\r\n(In general, I think asynchronous aggregation jobs are a good idea since they decouple expensive computation from synchronous HTTP requests. But they do increase the communication cost of each aggregation job -- every aggregation job will now require at least two network round trips, or more precisely two round trips per aggregation step required by the VDAF. I think this means that implementations would want to tune for fewer, larger aggregation jobs to amortize this cost per-report.)",
          "createdAt": "2024-06-06T23:57:27Z",
          "updatedAt": "2024-06-06T23:58:40Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I do think asynchronous aggregation jobs lead to increased complexity in the \"aggregation job state machine\".\r\n\r\nWith synchronous aggregation jobs, the state machine goes from step 1 -> step 2 ... until the aggregation job is complete.\r\n\r\nWith asynchronous aggregation jobs, an additional \"computing\" step will be added to each step above, so that the state machine goes computing step 1 -> step 1 -> computing step 2 -> step 2 -> ...\r\n\r\nThis is not a fatal flaw, just something to consider when weighing synchronous vs asynchronous behaviors.",
          "createdAt": "2024-06-07T00:05:13Z",
          "updatedAt": "2024-06-07T00:05:13Z"
        },
        {
          "author": "erks",
          "authorAssociation": "NONE",
          "body": "We can make both asynchronous & synchronous aggregations co-exist by doing something like this:\r\n1. Each round/call to Helper will start with the regular PUT (for init) / POST (for continue/collect) calls.\r\n1. If Helper responds with the corresponding 201 (for init) / 200 (for continue/collect), then the call is done and Leader can move on to the next step.\r\n1. If Helper responds with [202 Accepted](https://datatracker.ietf.org/doc/html/rfc7231#autoid-66), Leader starts to poll with a GET version of the call until receiving 200/201.",
          "createdAt": "2024-06-11T23:15:30Z",
          "updatedAt": "2024-06-11T23:15:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "That sounds to me like it would work just fine. It's up to the Helper if they want to implement both or one or the other.",
          "createdAt": "2024-06-11T23:44:43Z",
          "updatedAt": "2024-06-11T23:44:43Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "> Tim's comment raises a good question: if we take this, should we allow both asynchronous & synchronous aggregation job behavior, or only asynchronous?\r\n> \r\n> IMO, it would be nice if we could specify only one of asynchronous/synchronous behavior: requiring implementations to support both modes would be added complexity; specifying both modes but allowing implementations to only implement one mode could lead to interoperability problems between aggregator implementations. But maybe that is too hopeful?\r\n> \r\n> (In general, I think asynchronous aggregation jobs are a good idea since they decouple expensive computation from synchronous HTTP requests. But they do increase the communication cost of each aggregation job -- every aggregation job will now require at least two network round trips, or more precisely two round trips per aggregation step required by the VDAF. I think this means that implementations would want to tune for fewer, larger aggregation jobs to amortize this cost per-report.)\r\n\r\nAs far as I am concerned, the additions to compute are pretty negligable compared with the additional scalability we would be getting.",
          "createdAt": "2024-06-12T15:37:37Z",
          "updatedAt": "2024-06-12T15:37:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Per 2024/6/12 DAP sync: @erks would be alright with making each step asynchronous and not allowing the synchronous option? We just had a call on this, and we expect that most of the time the Helper will always make this asynchronous.",
          "createdAt": "2024-06-12T16:06:48Z",
          "updatedAt": "2024-06-12T16:06:48Z"
        },
        {
          "author": "erks",
          "authorAssociation": "NONE",
          "body": "I think, from the spec perspective, it's probably okay to move towards the asynchronous option exclusively. I'm just worried about the migration of the existing implementations to the new async model, as there could be periods where Leader and Helper have to support both at the same time.",
          "createdAt": "2024-06-13T23:16:30Z",
          "updatedAt": "2024-06-14T06:23:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I think, from the spec perspective, it's probably okay to move towards the asynchronous option exclusively. I'm just worried about the migration of the existing implementations to the new async model, as there could be periods where Leader and Helper have to support both at the same time.\r\n\r\nSupporting multiple drafts at once adds complexity, but it's complexity that many have learned to manage. (I believe at one point, Cloudflare had deployed up to three drafts of TLS 1.3 (RFC8446) simultaneously.) We already have versioning infrastructure in [daphne](https://github.com/cloudflare/daphne) for this very reason.",
          "createdAt": "2024-06-25T14:42:28Z",
          "updatedAt": "2024-06-25T14:42:28Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "IETF 120: There was no objection to merging #564, but @bemasc and @martinthomson both expressed interest in revisiting the design space. On the other hand, @Noah-Kennedy and myself expressed some urgency in making some change here soon: right now we're having to do way too much work (storage consistency and computation) on the hot path of HTTP requests.\r\n\r\nWe have considered other alternatives, including @martinthomson's suggestion of having the Helper tell the Leader later on which reports are replayed and thus need to reviewed. As @branlwyd and others observed previously, this creates the requirement to store individual report shares until the batch is collected. We would rather not impose this requirement.\r\n\r\nTo @bemasc's point at the mic: if there is a different HTTP API pattern we should be following to resolve this, please let us know what it is. The folks working on this probably have far less expertise than you do on building applications on HTTP.\r\n\r\nI think we should give this issue a little more time to see if someone has a better idea. That said, we can't wait forever to fix this problem, so I think we should lean on the deployment of experience of folks who have implemented DAP. If there is no discussion on this topic in the next couple of weeks, I'm going to merge #564 as-is.\r\n\r\nThank you again @inahga for wrangling the PR!",
          "createdAt": "2024-07-26T23:35:38Z",
          "updatedAt": "2024-07-31T16:06:44Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "My suggestion was not limited to a batch.  The leader would send reports to the helper and store them until the helper gives an OK or a REJECT.  In most cases, that batch could be bounded in time or number such that the total work on hand is manageable.\r\n\r\nHowever, I think that our current exploration of consistent hashing and other techniques is more likely to produce a good outcome here.\r\n\r\nI would like to avoid the need for asynchronous processing.  It creates a burden for the leader that is almost exactly the same as what you were concerned about, it just hides it.  So, if we can get the processing cost down -- in particular, to remove any reliance on consistent global state -- I would like to see the asynchronous stuff removed.  It complicates where the protocol should be simplified.",
          "createdAt": "2024-07-31T03:08:26Z",
          "updatedAt": "2024-07-31T03:08:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> However, I think that our current exploration of consistent hashing and other techniques is more likely to produce a good outcome here.\r\n\r\n[Consisting hashing is already used in Daphne](https://mailarchive.ietf.org/arch/msg/ppm/uJDUCZPJ9EYgeVE-90aFx489OPI/) to shard replay protection across multiple actors. This is the main tool we have to reduce the latency of resolving storage transactions across aggregation jobs.\r\n\r\n\r\n> I would like to avoid the need for asynchronous processing. It creates a burden for the leader that is almost exactly the same as what you were concerned about, it just hides it. So, if we can get the processing cost down -- in particular, to remove any reliance on consistent global state -- I would like to see the asynchronous stuff removed. It complicates where the protocol should be simplified.\r\n\r\nWhat consistent global state are you thinking of here?\r\n\r\nBy all means, let's explore ways to mitigate costs, but we need to start turning our intuition about what might be possible into concrete proposals on paper. \r\n\r\n@bemasc's [proposal on the list](https://mailarchive.ietf.org/arch/msg/ppm/rNpe3Ky070_voM5BAk3ICieF6OI/) is a good first step, but there are some concerns it doesn't fully address. I'll reply with details on the list in a moment, but at a high level, there is a certain amount of computation both aggregators have to do ($O(m \\cdot n \\cdot \\log n)$, where $m$ is the batch size and $n$ is the size of measurement); and unlike the leader, it has to carry out that computation on the hot path of HTTP requests.\r\n\r\nGiven the operational requirements currently in the draft, our expectation is that whatever burden exists for coordinating the computation is meant to be born by the leader, not the helper. So if there is an irreducible cost that some aggregator has to pay, it should be payed by the leader.",
          "createdAt": "2024-07-31T16:20:42Z",
          "updatedAt": "2024-07-31T19:26:16Z"
        }
      ]
    },
    {
      "number": 558,
      "id": "I_kwDOFEJYQs6Hjw79",
      "title": "Thought experiment: no more report ID",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/558",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "#556 enumerates requirements in DAP that can be challenging to meet depending on the architecture of an implementation. Among them is the requirement that aggregators guard against duplicate report IDs. The Leader does this during upload handling and the Helper does it during aggregation job handling. In both cases, the requirement can be difficult to meet because it introduces a global lookup in an operation that would otherwise be embarrassingly parallel, in the sense that it has no dependencies on other data.\r\n\r\nIdeally, the Leader's upload handler would involve just examining the incoming message and then inserting a single row into a table (or a single record into a k-v store, or...), but now it has to check against many previously seen report IDs to check for a collision. Similarly, the preparation of an input share into an output share doesn't require synchronization with any other report's preparation, but the Helper must do a potentially expensive check for the report ID's uniqueness. In particular, this means that aggregators must have a consistent view of storage shared with all other replicas of themselves (or at least a shard of storage).\r\n\r\nSo why actually do we have this requirement of no repeated report IDs? Recalling that the DAP report ID is used as the VDAF nonce, [`draft-irtf-cfrg-vdaf`'s {{nonce-requirements}}](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf-08#section-9.2) says:\r\n\r\n```\r\nThe sharding and preparation steps of VDAF execution depend on a nonce\r\nassociated with the Client's report. To ensure privacy of the underlying\r\nmeasurement, the Client MUST generate this nonce using a CSPRNG. This is\r\nrequired in order to leverage security analysis for the privacy definition of\r\n{{DPRS23}}, which assumes the nonce is chosen at random prior to generating the\r\nreport.\r\n```\r\n\r\nNonce collisions are definitely a problem, but ours are 16 bytes and so generating them with a CSPRNG gives us a reasonable guarantee of collision avoidance.\r\n\r\nThis section continues:\r\n\r\n```\r\nOther security considerations may require the nonce to be non-repeating. For\r\nexample, to achieve differential privacy it is necessary to avoid \"over\r\nexposing\" a measurement by including it too many times in a single batch or\r\nacross multiple batches. It is RECOMMENDED that the nonce generated by the\r\nClient be used by the Aggregators for replay protection.\r\n```\r\n\r\nIf the attack is a malicious client including a single measurement in a batch \"too many times\", then I don't think report ID/nonce uniqueness helps: the malicious client could just send the desired measurement multiple times under distinct report IDs and have them incorporated into the batch.\r\n\r\nFurther, the report ID is never used in DAP to uniquely identify a report, except to enable anti-replay checks. It's not possible to GET a report from `/tasks/{task-id}/reports/{report-id}`. Aggregators never exchange a list of report IDs, either: instead, aggregation jobs contain whole report shares inline, and both collection job and aggregate share requests contain selectors that inform how aggregators will construct batches. In the time interval case, the report's timestamp is used, and for fixed size queries, the Leader arbitrarily assigns reports to batches. An aggregator implementation will certainly need some unique report identifier to keep track of which batch reports belong to, but that can be an implementation detail, generated inside either aggregator, and thus wouldn't require anything in the protocol to rule out duplicates. Crucially, we can trust an aggregator to correctly generate its own primary keys.\r\n\r\nThere are a handful of protocol messages where report ID appears, but really what's being transmitted there is the VDAF nonce, so we could just rename those fields.\r\n\r\nI'm coming around to the view that we could remove the notion of unique report IDs from DAP, and thus delete the attendant uniqueness requirement. Unless, that is, there's some other reason that aggregators must guarantee that there are no nonce collisions in a task?",
      "createdAt": "2024-05-01T22:36:03Z",
      "updatedAt": "2024-05-08T16:24:13Z",
      "closedAt": "2024-05-08T16:24:13Z",
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We're not concerned with a malicious client violating its own privacy, so the question is whether a malicious client choosing its nonce allows attacks on anyone else's privacy. The threat model assumes that an attacker can control both an aggregator and a coalition of clients. So the malicious aggregator could observe the nonce of an honest client's report, and then have a malicious client use that nonce in a report. Does this violate the privacy of the honest client?",
          "createdAt": "2024-05-01T22:39:07Z",
          "updatedAt": "2024-05-01T22:39:07Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> If the attack is a malicious client including a single measurement in a batch \"too many times\", then I don't think report ID/nonce uniqueness helps: the malicious client could just send the desired measurement multiple times under distinct report IDs and have them incorporated into the batch.\r\n\r\nTrue, but to be clear: we are concerned about a malicious Aggregator replaying an honest Client's report too many times.\r\n\r\n> We're not concerned with a malicious client violating its own privacy, so the question is whether a malicious client choosing its nonce allows attacks on anyone else's privacy. The threat model assumes that an attacker can control both an aggregator and a coalition of clients. So the malicious aggregator could observe the nonce of an honest client's report, and then have a malicious client use that nonce in a report. Does this violate the privacy of the honest client?\r\n\r\nNo, as long as the honest Aggregator only processes a report with that nonce once. ",
          "createdAt": "2024-05-01T22:46:33Z",
          "updatedAt": "2024-05-01T22:46:33Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Enforcing report ID uniqueness is useful because it is cheaper than enforcing uniqueness of the rest of the contents of a report. We extend the protection against duplicate report IDs to protection against replayed report contents by binding the decryption of input shares to the report ID. One replay attack we need to defend against is when the adversary controls the leader, and it sends report shares from an honest client's report multiple times, either in one batch or across multiple batches. This is what the \"over exposing\" paragraph is referring to, rather than the attacker freshly generating multiple reports with the same measurement.",
          "createdAt": "2024-05-01T22:47:11Z",
          "updatedAt": "2024-05-01T22:47:11Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Removing the report ID does not remove the requirement on the report's uniqueness--e.g. if a malicious Leader could repeatedly include the same report in multiple batches, it would be able to violate the privacy of that report, whether the report has an attached ID or not.\r\n\r\nI think removing the ID would make it more complicated for an aggregator to determine a report's identity. For example, they might derive an identity based on the hash of appropriate parts of the report's content, or even the entire content of the report. I think we'd probably need to spell out how a report's identity is determined in the spec.\r\n\r\nDepending on how the AADs change & how report identity is determined, removing the report ID might also allow a malicious aggregator to forge reports using a \"real\" report's measurement, effectively allowing the privacy of that report to be violated.",
          "createdAt": "2024-05-01T22:47:20Z",
          "updatedAt": "2024-05-01T22:47:20Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "One bit of related trivia, inspired by this conversation: checking for replay across batches is harder for fixed-size tasks than for time-interval tasks. In particular, this implementation detail is expensive for Daphne:\r\n\r\n>  An aggregator implementation will certainly need some unique report identifier to keep track of which batch reports belong to, but that can be an implementation detail, ...\r\n\r\n\r\n",
          "createdAt": "2024-05-01T22:54:52Z",
          "updatedAt": "2024-05-01T22:54:52Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think I am persuaded by the argument that we have to prevent the Leader from replaying an honest Client's report: this is not equivalent to doing a Sybil attack where the same measurement is repeatedly uploaded, because the malicious Leader can do this without knowing the honest Client's measurement, and could learn what that measurement is by constructing a batch containing exclusively the honest Client report. So on that basis alone, I'm convinced we have to keep report IDs and the anti-replay check (though perhaps only in the Helper...)\r\n\r\nBut there's one more thing I want to make sure I understand, and would like to have on the record:\r\n\r\n@cjpatton wrote:\r\n> No, as long as the honest Aggregator only processes a report with that nonce once.\r\n\r\nForgetting over exposure for a second, let's say two different reports containing two different measurements both use the same nonce. Is that a problem? Why?",
          "createdAt": "2024-05-01T22:57:18Z",
          "updatedAt": "2024-05-01T22:57:18Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It's definitely problematic from the standpoint of the proven security of Prio3,: ia.cr/2023/130, Theorem 2 says that privacy only holds up to nonce collisions. So if two clients, honest or not, happen to choose the same nonce, and we process both reports, then the attacker might learn something it's not supposed to.\r\n\r\n[This is a little speculative, and I may not have the details pinned down.] To make this more concrete: in Prio3 we evaluate the proof polynomial at a random point derived from the nonce. In addition, we get to learn some intermediate outputs. Normally those intermediate outputs don't leak anything, if the random point is indeed independent across all executions of the FLP. But in this situation, this is not the case.",
          "createdAt": "2024-05-01T23:11:33Z",
          "updatedAt": "2024-05-01T23:11:33Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "OK, thanks. While you've collectively talked me off the ledge of killing report IDs, I think we could use a touch more text in VDAF and DAP:\r\n\r\n- [VDAF {{nonce-requirements}}](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf-08#section-9.2) only says clients MUST randomly generate the nonce, but clearly that's not enough. We should add a sentence about aggregators verifying that there are no nonce collisions.\r\n- DAP should spell out that besides meeting the requirement from VDAF 9.2, enforcing report ID uniqueness prevents the leader from replaying an honest client's report, to stop dimwits like me from arguing to remove the report ID.\r\n",
          "createdAt": "2024-05-01T23:25:19Z",
          "updatedAt": "2024-05-01T23:25:19Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Finally: I notice that section 4.5.1.4 _currently_ lacks explicit instruction that the helper should reject duplicate report IDs, but #554 addresses this by replacing the call to `Vdaf.is_valid` with\r\n\r\n> Check if the report has been previously aggregated. If so, the input share\r\n> MUST be marked as invalid with the error `report_replayed`.",
          "createdAt": "2024-05-01T23:27:16Z",
          "updatedAt": "2024-05-01T23:27:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n>     * [VDAF {{nonce-requirements}}](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf-08#section-9.2) only says clients MUST randomly generate the nonce, but clearly that's not enough. We should add a sentence about aggregators verifying that there are no nonce collisions.\r\n\r\nThinking about this more, I think this is not strictly necessary. See https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/340#discussion_r1587840456.\r\n",
          "createdAt": "2024-05-02T15:30:36Z",
          "updatedAt": "2024-05-02T15:30:36Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We took https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/340 and #559 for this. Nothing left to be done here.",
          "createdAt": "2024-05-08T16:24:13Z",
          "updatedAt": "2024-05-08T16:24:13Z"
        }
      ]
    },
    {
      "number": 560,
      "id": "I_kwDOFEJYQs6IYsmH",
      "title": "Discuss lifecycle of various objects and how they relate to each other",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/560",
      "state": "OPEN",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "tgeoghegan"
      ],
      "labels": [
        "editorial"
      ],
      "body": "On the mailing list, @wbl observed that it's hard for readers to understand the relationship of reports to aggregation jobs to batches to aggregate shares. We should add a section between the introduction and the the sections describing the sub-interactions which discusses the lifecycle and cardinality of these objects. ",
      "createdAt": "2024-05-09T17:50:22Z",
      "updatedAt": "2024-05-14T21:23:16Z",
      "closedAt": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Mailing list thread in question: https://mailarchive.ietf.org/arch/msg/ppm/E1OAX-HKwknXGevr5EmVjSEdOtE/",
          "createdAt": "2024-05-09T17:51:19Z",
          "updatedAt": "2024-05-09T17:51:19Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "One reason this can be confusing is the notion of \"eager\" aggregation, which is doable for VDAFs like Prio3 that don't have aggregation parameters. Simon noted in #385 that this is confusing, and perhaps we could clear this up by making the notion of eager aggregation more explicit in DAP.\r\n\r\nThere's an awkward tension here: we've avoided explicitly discussing eager aggregation, partial aggregates, etc. because technically those concepts are implementation details of a specific aggregator. For instance it'd be perfectly OK for the helper to do eager aggregation into buckets, but for the leader to retain individual output shares until it's time to construct aggregate shares. Neither needs to know how the other handles this, and for that reason it didn't seem appropriate to discuss the concepts in a protocol document whose principal goal is to specify interfaces between participants, not implementation details. But of course now it seems this implementation detail is leaking into the protocol, in that we take enabling this strategy to be a hard requirement for the protocol.",
          "createdAt": "2024-05-09T18:01:07Z",
          "updatedAt": "2024-05-10T00:09:52Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I think it would be useful to discuss lifecycles, but the DAP spec has historically avoided being too prescriptive about lifetimes/deletion -- IMO we should remain descriptive rather than prescriptive, and if we are prescriptive, say as little as possible. (I suppose the goal is to proscribe the minmimal amount of behavior leading implementations to be interoperable.)",
          "createdAt": "2024-05-14T21:23:15Z",
          "updatedAt": "2024-05-14T21:23:15Z"
        }
      ]
    },
    {
      "number": 561,
      "id": "I_kwDOFEJYQs6JLwBv",
      "title": "Support for batched preparation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/561",
      "state": "OPEN",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "cjpatton"
      ],
      "labels": [
        "feature"
      ],
      "body": "There is a [new paper](https://eprint.iacr.org/2024/666) (to appear at IEEE S&P 2024) that shows how to lift an [FLP](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-vdaf-09#section-7.1) into a proof system that is \"silently verifiable\". For such a proof system, it's possible to check if _some_ measurement in a set of measurements is invalid, exchanging only a constant number of bits (128). In Prio3, we must exchange a number of bits linear in the number of measurements.\r\n\r\nThe bandwidth savings are significant. For Prio3SumVec with bits=1, length=1,000, and chunk_length=32 and 100 reports, an `AggregationJobInitReq` is 122KB. If we modified Prio3 by making the FLP silently verifiable, the same message would be roughly 7KB (back-of-a-napkin calculation). Computational cost remains about the same. (See Figure 7 from the paper for details.)\r\n\r\nThis approach does a have a potential downside: if verification fails, then we learn only that _some_ measurement is invalid, not _which_ measurement is invalid. Thus, in the case that some fraction of the reports in the aggregation job, you either need to throw away the data or exchange more bits in order to find the subset of measurements that are invalid. The communication cost increases as the fraction of malicious Clients increases.\r\n\r\nThere are many ways we might implement this. Perhaps the simplest would be to treat batched preparation as an alternative aggregation flow that falls back to the existing aggregation flow (i.e., per report validation) if verification fails. Concretely, we would specify an alternative aggregation sub-protocol that is the same as the existing flow except:\r\n1. It is only compatible with 1-round, \"silently verifiable\" VDAFs.\r\n2. If verification fails, then it needs to be safe to fallback to per-report validation.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "createdAt": "2024-05-16T23:48:08Z",
      "updatedAt": "2024-08-13T16:03:43Z",
      "closedAt": null,
      "comments": [
        {
          "author": "wbl",
          "authorAssociation": "NONE",
          "body": "Via coding theory you can survive a handful of bad reports and remove them (https://www.nature.com/articles/s41586-020-2885-5 is one reference but there are others)",
          "createdAt": "2024-05-21T18:46:22Z",
          "updatedAt": "2024-05-21T18:46:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "That's a super interesting idea! Care to flesh it out a bit more, even if just for fun? \r\n\r\nThe problem at hand is to minimize the communication cost (bits on the wire, number of round trips) when you expect X% of the reports to invalid. Though I think there will always be a need to fallback to per-report verification.",
          "createdAt": "2024-06-03T14:49:59Z",
          "updatedAt": "2024-06-03T14:49:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "There is a draft PR (#565) with a proposal of how to integrate this into DAP. In the meantime, I've been looking at how much bandwidth savings we would actually get.\r\n\r\nUnfortunately, **[Whisper](https://eprint.iacr.org/2024/666) provides no benefit for DAP _as-is_, even if all clients are honest.** In Whisper, each Client sends some additional information to each Aggregator. The number of bytes included in the report is roughly as much information as is exchanged during per-report preparation Prio3.\r\n\r\nIn more detail, the Client sends to each Aggregator the verifier share that would be computed by its co-Aggregator. This is denoted by $\\mathsf{vtag}_{\\mathsf{NS},i}$ in Figure 4 of the paper. (In the transformation from to the \"silently verifiable\" proof system, the Aggregators arrange to check that Client sent the correct value. This is morally similar to how we check that joint randomness computation in Prio3.) This is the same value that would be transmitted during preparation of Prio3.\r\n\r\nNow, in DAP, the Helper's report share is proxied by the Leader during aggregation. Thus, one verifier share per-report is transmitted either in a traditional aggregation job via Prio3 or a batched aggregation job via Whisper. The only difference is who generates the message.\r\n\r\nWhisper would only make a difference in a \"Split-Upload\" architecture, where each Client sends report shares to the Leader and Helper directly. In this setting, the Clients pay the bandwidth cost, rather than the Leader. It has been observed (#130) that bandwidth is usually cheaper for Clients than it would be for a Leader. Note that [we ruled out Split-Upload for DAP at IETF 113](https://datatracker.ietf.org/meeting/113/materials/slides-113-ppm-ppm-upload-flow).\r\n\r\nAll that said, we may see improvements to Whisper in the future that make it worth it for DAP as-is. And there are other VDAFs that may fair better. For example, [VIDPFs](https://eprint.iacr.org/2023/080) not only allow for batched preparation, but also don't increase Client->Aggregator bandwidth[^1].\r\n\r\n\r\n[^1]: There is a big caveat here: The parameters of VIDPF are subject to change as we learn more about their concrete security.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2024-07-09T18:06:50Z",
          "updatedAt": "2024-07-09T18:16:51Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "This topic was brought up at IETF 120: https://datatracker.ietf.org/meeting/120/materials/slides-120-ppm-batched-vdaf-preparation\r\n\r\nThere wasn't much enthusiasm for this feature in the room. In any case, there is no point in supporting it in DAP unless we're willing to re-consider the Split-Upload architecture.",
          "createdAt": "2024-07-29T17:59:53Z",
          "updatedAt": "2024-07-29T17:59:53Z"
        },
        {
          "author": "mayank0403",
          "authorAssociation": "NONE",
          "body": "Hi, \r\n\r\nI co-authored the [Whisper](https://eprint.iacr.org/2024/666) paper and thanks to everyone for showing interest in the work!\r\n\r\nI think a small change in the construction of silently verifiable proofs would fix the issue mentioned above. The idea is that the public proof doesn't need to be sent to the helper; as long as the leader has it, that's enough. \r\n\r\nReport shares would look like this:\r\nLeader gets: measurement share, prio3 proof share, public proof\r\nHelper gets: measurement seed, prio3 proof seed, one extra seed for Fiat-Shamir\r\n\r\nWith this change, even when the leader relays Helper's report shares, the leader's egress stays very low (512 bits egress per report - includes egress during proof verification).\r\n\r\nPlease refer to the attached document for a formal description. Note that the document's exposition is simplified by considering only Prio3 proofs, but any proofs that we talk about in our paper will work here. \r\n\r\nI am curious to hear your thoughts! \r\n[Whisper_IETF_Draft.pdf](https://github.com/user-attachments/files/16567347/Whisper_IETF_Draft.pdf)\r\n",
          "createdAt": "2024-08-09T22:17:13Z",
          "updatedAt": "2024-08-09T22:17:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi @mayank0403, thanks so much for correcting the record here! Quick question about the batched verification step: a collision hash function is used to compute the tag for each report; how then are the tags batched? My recollection from the paper is that the tags are field elements, and we take a random linear combination of the tags to get the batched tag. I guess the same thing works here if the range of the hash function is the field: is that your suggestion here?",
          "createdAt": "2024-08-12T14:51:12Z",
          "updatedAt": "2024-08-12T14:51:12Z"
        },
        {
          "author": "mayank0403",
          "authorAssociation": "NONE",
          "body": "Hi @cjpatton, yeah, you can take random linear combination to batch them. Moreover, in the draft above, I used a hash function to compute the tags for each report, but you could have used a random linear combination to compute each tag too. \r\n\r\nFrom an implementation standpoint, a hash function whose output is in a field can be more inefficient (e.g. due to rejection sampling). If that is a real concern, then you can just use a collision-resistant hash function on top of the batch of tags to batch verify them. ",
          "createdAt": "2024-08-13T16:03:41Z",
          "updatedAt": "2024-08-13T16:03:41Z"
        }
      ]
    },
    {
      "number": 571,
      "id": "I_kwDOFEJYQs6ROQwV",
      "title": "Clarification request: Does aggregation happen in the AggregationJob?",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/571",
      "state": "OPEN",
      "author": "bemasc",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Section 4.5 says that an Aggregation Job has three phases, the last of which is\r\n\r\n>    *  Completion: Finish the aggregate flow, yielding an output share\r\n      corresponding to each report share in the aggregation job.\r\n\r\nThat makes it sound like Aggregation Jobs don't perform any aggregation.  If that's true, maybe they should be called Preparation Jobs.  If it's false, maybe it can be corrected.",
      "createdAt": "2024-07-29T21:44:37Z",
      "updatedAt": "2024-07-29T22:05:51Z",
      "closedAt": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "It's false: we leave it up to an implementation to decide whether it will store output shares individually or aggregate in a streaming fashion (once an output share is produced, add it to an aggregate share).\r\n\r\nA bit of pertinent history: Early on we wanted to allow the possibility that the aggregation function (i.e., the function computed by the VDAF) may be sensitive to the order of measurements. For example, you might have that $f(m_1, m_2, m_3) \\ne f(m_1, m_3, m_2)$. The VDAF draft doesn't rule this out, but in practice we have not found a need for this. In fact, no open source implementation of DAP that I'm aware of supports it.\r\n\r\nI think we can resolve this by clarifying that most of the time the aggregate share will usually be updated in a streaming fashion. (Thanks @bemasc for suggesting the terminology offline.)",
          "createdAt": "2024-07-29T21:54:44Z",
          "updatedAt": "2024-07-29T21:54:44Z"
        }
      ]
    },
    {
      "number": 572,
      "id": "I_kwDOFEJYQs6R4ItB",
      "title": "Eric's affiliation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/572",
      "state": "OPEN",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "@ekr, do you want to update this?",
      "createdAt": "2024-08-05T01:23:25Z",
      "updatedAt": "2024-08-05T01:23:25Z",
      "closedAt": null,
      "comments": []
    }
  ],
  "pulls": [
    {
      "number": 2,
      "id": "MDExOlB1bGxSZXF1ZXN0NTc4OTQxNzU0",
      "title": "Describe the cryptographic dependencies for the candidate protocol",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/2",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Solves #1.\r\n\r\nThis adds a high level description of the current candidate for our version of Prio. (Note that this is subject to change as the threat \r\nmodel and system requirements evolve.) \r\n\r\nIt then enumerates the cryptographic primitives with which the protocol could be instantiated, including the FF parameters, KEM, and PRG.",
      "createdAt": "2021-02-24T03:29:56Z",
      "updatedAt": "2021-06-17T21:15:37Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ca9cebcfd164db7f0c0963d4fa7458de0e35976e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/primitives",
      "headRefOid": "18a82ef4412443cc378b71e3c2e82ccd4e9b9f31",
      "closedAt": "2021-02-27T03:20:02Z",
      "mergedAt": "2021-02-27T03:20:02Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "2b46fe7cdab7bd6f15fd467c4da4e526d28bec31"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk3Nzg4Njc0",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-24T18:21:19Z",
          "updatedAt": "2021-02-24T19:03:28Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "Will s-way secret sharing be a vital feature of the v3 API? Neither libprio nor libprio-rs have implemented a system with more than two parties.",
              "createdAt": "2021-02-24T18:21:19Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 86,
              "body": "Be explicit about what state is (input-shares vs accumulated shares):\r\n\r\n```suggestion\r\n   without needing to maintain input-shares from the previous step.\r\n```",
              "createdAt": "2021-02-24T18:39:12Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 88,
              "body": "```suggestion\r\n**Minimizing bandwidth overhead.**\r\n```",
              "createdAt": "2021-02-24T18:41:23Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 114,
              "body": "To clarify, does the size of the field affect the performance of the polynomial operations? I would guess that fields that aren't powers of 2 will be less performant than those that are, unless the p in the next section can be chosen carefully to address this.",
              "createdAt": "2021-02-24T18:45:30Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 118,
              "body": "The numbers in this list are all 1. ",
              "createdAt": "2021-02-24T18:46:58Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 152,
              "body": "Is this how libprio-rs handles the exchange of the shared PRG secret now ([via ECIES](https://github.com/abetterinternet/libprio-rs/blob/61efbcc10f4ece6f3403edb7c49de3f9c479a1d7/src/encrypt.rs#L72-L76))?",
              "createdAt": "2021-02-24T18:52:24Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 133,
              "body": "What do the bits refer to?",
              "createdAt": "2021-02-24T18:57:28Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 114,
              "body": "Does arbitrary sized finite fields imply the use of bignums? Will the field sizes be carefully chosen for performance (e.g. recommended EC primes of 256, 384, etc)?",
              "createdAt": "2021-02-24T19:03:05Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MDk0NzYw",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-25T02:02:48Z",
          "updatedAt": "2021-02-25T02:02:48Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "In my view, it's worth adding in this flexibility now in case we end up needing it later. Depending on our security considerations and how address resilience, it may end up being useful to have 3 or more aggregators.",
              "createdAt": "2021-02-25T02:02:48Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MDk1MzU2",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-25T02:04:22Z",
          "updatedAt": "2021-02-25T02:04:23Z",
          "comments": [
            {
              "originalPosition": 86,
              "body": "Changed \"maintain state\" to \"keep around the input share\".",
              "createdAt": "2021-02-25T02:04:22Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MDk5ODM4",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-25T02:16:08Z",
          "updatedAt": "2021-02-25T02:16:09Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I don't think we should try to support arbitrary fields; instead, we should pick a few fields of different sizes and develop optimized versions of each. I suspect that generic bignum libraries would be too slow. (That said, a coworker of mine developed some pretty fast code that works for any prime field of size < 2^126. We'll be open sourcing this very soon.) As far as using off-the-shelf code for ECC fields. My sense is that he design criteria for ECC are quite different from ours. Hence, I'm not sure that this would be suitable. Worth looking into, though.\r\n",
              "createdAt": "2021-02-25T02:16:09Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MTAwMzMz",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-25T02:17:15Z",
          "updatedAt": "2021-02-25T02:17:15Z",
          "comments": [
            {
              "originalPosition": 118,
              "body": "Most Markdown renderers turn these into an ascending sequence, i.e., \"1.\", \"2.\", \"3.\", and so on. This just indicates that I want an enumerated list.",
              "createdAt": "2021-02-25T02:17:15Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MTAxMDI5",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-25T02:19:07Z",
          "updatedAt": "2021-02-25T02:19:07Z",
          "comments": [
            {
              "originalPosition": 152,
              "body": "No, since ECIES is full blown encryption.\r\n\r\nNote that we may end up wanting full-blown encryption, depending on our security considerations. If we do, then HPKE would still provide it.",
              "createdAt": "2021-02-25T02:19:07Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MTAxOTM4",
          "commit": {
            "abbreviatedOid": "43ae90e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-25T02:21:30Z",
          "updatedAt": "2021-02-25T02:21:30Z",
          "comments": [
            {
              "originalPosition": 133,
              "body": "\"bits\" indicates the size of the prime, i.e., the prime is roughly 2^bits. E.g, log2(3221225473) \\approx 31.58. I added a note to clarify.",
              "createdAt": "2021-02-25T02:21:30Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk4MTA5MTAz",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udea2 great start!",
          "createdAt": "2021-02-25T02:29:44Z",
          "updatedAt": "2021-02-25T02:52:06Z",
          "comments": [
            {
              "originalPosition": 51,
              "body": "Should we mark this consensus protocol (for both leader and r selection) as a dependency?",
              "createdAt": "2021-02-25T02:29:44Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 49,
              "body": "Out of curiosity, why use separate functions to denote the extension of K for each type? Is it not the case that `n = p(n) = u(n) = v(n)`?",
              "createdAt": "2021-02-25T02:31:03Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 64,
              "body": "```suggestion\r\nobtain the final result.\r\n\r\n[[OPEN ISSUE: sketch out the b=1 path]]\r\n```",
              "createdAt": "2021-02-25T02:31:48Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 52,
              "body": "```suggestion\r\ndesignate one of the aggregators as the leader.\r\n\r\n[[OPEN ISSUE: specify consensus protocol.]]\r\n\r\nThe protocol proceeds as\r\n```",
              "createdAt": "2021-02-25T02:32:22Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 70,
              "body": "What is the core protocol? (Ideally this doc would be self-contained, so the core should be sketched here or somewhere nearby.)",
              "createdAt": "2021-02-25T02:33:11Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 79,
              "body": "```suggestion\r\n   encrypts each (input, proof) share under the public key of the share's\r\n```",
              "createdAt": "2021-02-25T02:33:49Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 86,
              "body": "```suggestion\r\n   without needing to cache the input share from the previous step.\r\n```",
              "createdAt": "2021-02-25T02:34:39Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 117,
              "body": "Can we mark as an open issue to parameterize this (as HCG provided)?",
              "createdAt": "2021-02-25T02:36:25Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 135,
              "body": "How did we generate these primes? ",
              "createdAt": "2021-02-25T02:40:40Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 147,
              "body": "```suggestion\r\npublic key encryption and cryptographically secure pseudorandom number generation (CSPRNG). The combination of\r\n```",
              "createdAt": "2021-02-25T02:42:39Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 147,
              "body": "(common form in IETF standards)",
              "createdAt": "2021-02-25T02:43:13Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 154,
              "body": "```suggestion\r\nthese primitives that we use here allows us to make an additional\r\nsimplification. However, we assume users and leaders communicate over a secure,\r\nauthenticated channel, such as TLS. As a result, we only need to encrypt\r\nCSPRNG seeds, which requires only a key-encapsulation mechanism (KEM) \r\nrather than full-blown encryption.\r\n```",
              "createdAt": "2021-02-25T02:47:42Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 171,
              "body": "```suggestion\r\nvariety of languages.\r\n\r\n[[OPEN ISSUE: specify how HPKE can implement Encaps and Decaps above.]]\r\n```",
              "createdAt": "2021-02-25T02:48:57Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 183,
              "body": "```suggestion\r\nK, then map each chunk of l bytes to an element of K in the natural way.\r\n\r\n[[OPEN ISSUE: determine if more than k*l byes are needed to deal with bias for each element.]]\r\n```",
              "createdAt": "2021-02-25T02:51:50Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzQ5OTQz",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T16:41:16Z",
          "updatedAt": "2021-02-26T16:41:16Z",
          "comments": [
            {
              "originalPosition": 51,
              "body": "This is something we'll have to figure out once we have picked a threat model. I'll mark it as an open issue for now. ",
              "createdAt": "2021-02-26T16:41:16Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzU3MDg0",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T16:49:31Z",
          "updatedAt": "2021-02-26T16:49:31Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "Actually \"K^n\" is meant to denote the set of vectors over K of length n. :grimacing: I added this notation to the \"Notation\" paragraph above.\r\n\r\nTo your question: in general, the length of the proof, verification message, and joint randomness is a function of the length of length of the input. For example, for an input length of n, the corresponding proof might be p(n) = a*n + b for some constants a, b.",
              "createdAt": "2021-02-26T16:49:31Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzU5Mzgw",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T16:52:08Z",
          "updatedAt": "2021-02-26T16:52:08Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "The 'core protocol\" is the input-validation protocol described in the previous section.",
              "createdAt": "2021-02-26T16:52:08Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzYwMDY3",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T16:52:56Z",
          "updatedAt": "2021-02-26T16:52:57Z",
          "comments": [
            {
              "originalPosition": 79,
              "body": "Done, here and elsewhere.",
              "createdAt": "2021-02-26T16:52:56Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzczNTc3",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T17:08:51Z",
          "updatedAt": "2021-02-26T17:08:51Z",
          "comments": [
            {
              "originalPosition": 117,
              "body": "Done.",
              "createdAt": "2021-02-26T17:08:51Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzkyNTYx",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T17:32:05Z",
          "updatedAt": "2021-02-26T17:32:05Z",
          "comments": [
            {
              "originalPosition": 135,
              "body": "Added an explanation below.",
              "createdAt": "2021-02-26T17:32:05Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5NzkzOTgw",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T17:33:51Z",
          "updatedAt": "2021-02-26T17:33:51Z",
          "comments": [
            {
              "originalPosition": 135,
              "body": "(Also updated one of the primes.)",
              "createdAt": "2021-02-26T17:33:51Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5Nzk4NTY1",
          "commit": {
            "abbreviatedOid": "4c1024f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T17:39:39Z",
          "updatedAt": "2021-02-26T17:39:40Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "Great point! Done.",
              "createdAt": "2021-02-26T17:39:40Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5ODI2MzIw",
          "commit": {
            "abbreviatedOid": "e6082ce"
          },
          "author": "aaomidi",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T18:16:25Z",
          "updatedAt": "2021-02-26T18:16:26Z",
          "comments": [
            {
              "originalPosition": 92,
              "body": "How does the public key of the other recipients get advertised? ",
              "createdAt": "2021-02-26T18:16:25Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5ODMyMzQ1",
          "commit": {
            "abbreviatedOid": "e6082ce"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T18:24:33Z",
          "updatedAt": "2021-02-26T18:24:34Z",
          "comments": [
            {
              "originalPosition": 92,
              "body": "That's TBD. I was thinking the user would simply make an HTTP request to each aggregator, but there might be a better way.\r\n\r\nAdded a TODO below.",
              "createdAt": "2021-02-26T18:24:34Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDMzNTAz",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-26T23:51:41Z",
          "updatedAt": "2021-02-27T00:24:38Z",
          "comments": [
            {
              "originalPosition": 3,
              "body": "```suggestion\r\n## Terminology\r\n```\r\n\r\nGlossaries are super important in a design doc like this, so I suggest promoting this to a main section of the doc",
              "createdAt": "2021-02-26T23:51:41Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 11,
              "body": "We should make a distinction between the _client_ and the _user_ of the client (e.g., a web browser and the human using that web browser) since they are distinct actors.",
              "createdAt": "2021-02-26T23:53:53Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 31,
              "body": "nit: would be nice to have a link to some reference material on \"standard\" linear secret sharing for folks like me who aren't as up to date on the literature.",
              "createdAt": "2021-02-26T23:57:29Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 44,
              "body": "I have concerns about the leader performing the verification on behalf of all the other aggregations. However the onus is on me to develop a plausible attack the leader can execute if it can lie to aggregators about share validity, so this is fine for now.",
              "createdAt": "2021-02-26T23:59:55Z",
              "updatedAt": "2021-02-27T02:45:07Z"
            },
            {
              "originalPosition": 111,
              "body": "I'm a bit confused by `in` vs. `x` in this paragraph. Is `in` meant to be the input before being split into the vector `{x}`? If so, should line 116 read \"Let in be an element of...\"?",
              "createdAt": "2021-02-27T00:04:46Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            },
            {
              "originalPosition": 183,
              "body": "Users (/clients) communicate with each aggregator, not just the leader, right?",
              "createdAt": "2021-02-27T00:07:09Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            },
            {
              "originalPosition": 192,
              "body": "Should this be `Decaps(sk, c)`?",
              "createdAt": "2021-02-27T00:10:18Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            },
            {
              "originalPosition": 28,
              "body": "I think it's worth clarifying that `{x:i}` is itself an element of `K^n` (right?)",
              "createdAt": "2021-02-27T00:14:12Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            },
            {
              "originalPosition": 65,
              "body": "I think this should say something like \"Once all of the inputs have been validated and a sufficient number of input shares have been aggregated, the aggregators send their aggregate shares to the leader...\", to make it clear that aggregators aren't disclosing individual input shares to the leader.",
              "createdAt": "2021-02-27T00:17:32Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            },
            {
              "originalPosition": 152,
              "body": "For what it's worth, I think ECIES as used in Prio v2 is overkill for v3 for two reasons:\r\n\r\n1. Prio v2 needs to tunnel a confidential+authenticated channel from mobile devices to aggregators through Apple or Google servers, so we couldn't rely on TLS. In v3, if clients send shares directly to aggregators, we don't have that problem.\r\n2. In v2, clients will generate an ephemeral ECDSA key and do ECDH with the aggregator's long term public key to make a fresh symmetric for every single datum moving through the system. We were constrained there by the fact that the mobile OS vendors had already built and shipped their clients and were reluctant to make changes to them, but I think we could reuse symmetric keys for >1 datum.",
              "createdAt": "2021-02-27T00:24:38Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDQ0ODI4",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Thanks for getting the ball rolling!",
          "createdAt": "2021-02-27T00:27:29Z",
          "updatedAt": "2021-02-27T00:27:29Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDUwNjMy",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T00:52:14Z",
          "updatedAt": "2021-02-27T00:52:14Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "Here I mean \"client\". Will fix.",
              "createdAt": "2021-02-27T00:52:14Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDUzMDUy",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:01:46Z",
          "updatedAt": "2021-02-27T01:01:47Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Done.",
              "createdAt": "2021-02-27T01:01:47Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU0NDE0",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:07:13Z",
          "updatedAt": "2021-02-27T01:07:13Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "```suggestion\r\n1. k := Decaps(sk, c) denotes decapsulation of symmetric key k under the\r\n```",
              "createdAt": "2021-02-27T01:07:13Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU0NTg1",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:08:08Z",
          "updatedAt": "2021-02-27T01:08:08Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "I think, in this proposed design, clients communicate only with the leader, who mediates all things. @cjpatton?",
              "createdAt": "2021-02-27T01:08:08Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU0ODY0",
          "commit": {
            "abbreviatedOid": "f326c44"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:09:35Z",
          "updatedAt": "2021-02-27T01:09:36Z",
          "comments": [
            {
              "originalPosition": 111,
              "body": "```suggestion\r\nLet x be an element of K^n for some n. Suppose we split x into {x} by choosing\r\n{x:1}, ..., {x:s-1} at random and letting {x:s} = x - ({x:1} + ... + {x:s-1}).\r\n```",
              "createdAt": "2021-02-27T01:09:35Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU1MTU3",
          "commit": {
            "abbreviatedOid": "25787f2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:11:22Z",
          "updatedAt": "2021-02-27T01:11:22Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "The threat model for Prio --- as it's described in the original paper and [BBC+19] --- considers **either** a malicious client (attacking soundness) **or** a malicious subset of aggregators (attacking privacy). In particular, soundness isn't guaranteed if any one of the aggregators is malicious; in theory it may be possible for a malicious client and aggregator to collude and break soundness.\r\n\r\nThere are techniques described in [BBC+19] that address the stronger threat model in which a malicious client may collude with a malicious aggregator. I think it's worth exploring how practical these techniques are. Meanwhile, thinking about attacks is a great idea.",
              "createdAt": "2021-02-27T01:11:22Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU1MTcz",
          "commit": {
            "abbreviatedOid": "25787f2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:11:30Z",
          "updatedAt": "2021-02-27T01:11:30Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "`{x:i}` is a share of `x`, so it's just an element of `K`. ",
              "createdAt": "2021-02-27T01:11:30Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU1MjQx",
          "commit": {
            "abbreviatedOid": "25787f2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:11:49Z",
          "updatedAt": "2021-02-27T01:11:49Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "```suggestion\r\n{x:s}, where {x:i} is the share held by the i-th party and an element of K. We write {x} as\r\n```",
              "createdAt": "2021-02-27T01:11:49Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU1NTcy",
          "commit": {
            "abbreviatedOid": "25787f2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:13:32Z",
          "updatedAt": "2021-02-27T01:13:33Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "Maybe just flag this as an open issue?\r\n```suggestion\r\n   whether the input is deemed valid. This algorithm is run by the leader.\r\n   \r\n[[OPEN ISSUE: what can go wrong when the leader is responsible for verifying everything on behalf of all aggregators?]]\r\n```",
              "createdAt": "2021-02-27T01:13:32Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDYwMTY1",
          "commit": {
            "abbreviatedOid": "25787f2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:35:18Z",
          "updatedAt": "2021-02-27T01:35:19Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "```suggestion\r\n1. Each aggregator i runs {vf:i} := Query({x:i}, {pf:i}, r) and sends {vf:i} to\r\n```",
              "createdAt": "2021-02-27T01:35:19Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY2MTM2",
          "commit": {
            "abbreviatedOid": "a7c9df8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:26:24Z",
          "updatedAt": "2021-02-27T02:26:24Z",
          "comments": [
            {
              "originalPosition": 111,
              "body": "Oops, typo: s/in/x/.",
              "createdAt": "2021-02-27T02:26:24Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY2MzUx",
          "commit": {
            "abbreviatedOid": "a7c9df8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:28:06Z",
          "updatedAt": "2021-02-27T02:28:06Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "I envision the client sending its encrypted shares to the leader. However, it does need to speak to the aggregators prior to the input-validation protocol to get their public keys.",
              "createdAt": "2021-02-27T02:28:06Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY2Mzk5",
          "commit": {
            "abbreviatedOid": "a7c9df8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:28:33Z",
          "updatedAt": "2021-02-27T02:28:34Z",
          "comments": [
            {
              "originalPosition": 192,
              "body": "Yup, good catch!",
              "createdAt": "2021-02-27T02:28:34Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY2NTEx",
          "commit": {
            "abbreviatedOid": "a7c9df8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:29:36Z",
          "updatedAt": "2021-02-27T02:29:36Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "Yes, done.",
              "createdAt": "2021-02-27T02:29:36Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY2NjE4",
          "commit": {
            "abbreviatedOid": "a7c9df8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:30:41Z",
          "updatedAt": "2021-02-27T02:30:41Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "Changed to \"Once a sufficient number of shares have been ...\"",
              "createdAt": "2021-02-27T02:30:41Z",
              "updatedAt": "2021-02-27T02:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY4MTc5",
          "commit": {
            "abbreviatedOid": "18a82ef"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:45:21Z",
          "updatedAt": "2021-02-27T02:45:21Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "Added this to \"Thread model\".",
              "createdAt": "2021-02-27T02:45:21Z",
              "updatedAt": "2021-02-27T02:45:21Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY5NzM2",
          "commit": {
            "abbreviatedOid": "18a82ef"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T02:54:09Z",
          "updatedAt": "2021-02-27T02:54:09Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "Does it? Why can't the client get these keys from the leader?",
              "createdAt": "2021-02-27T02:54:09Z",
              "updatedAt": "2021-02-27T02:54:09Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDcwNzE2",
          "commit": {
            "abbreviatedOid": "98eb939"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T03:02:16Z",
          "updatedAt": "2021-02-27T03:02:16Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": " The leader is incentivized to break privacy... it could just hand the client fake public keys for which it knows the secret keys and decrypt all of the shares.",
              "createdAt": "2021-02-27T03:02:16Z",
              "updatedAt": "2021-02-27T03:02:16Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDcyNTI0",
          "commit": {
            "abbreviatedOid": "18a82ef"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-02-27T03:18:02Z",
          "updatedAt": "2021-02-27T03:18:02Z",
          "comments": []
        }
      ]
    },
    {
      "number": 3,
      "id": "MDExOlB1bGxSZXF1ZXN0NTc5NTU0MzI2",
      "title": "Start system overview and requirements.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/3",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Imperfect, incomplete, maybe even inaccurate! Starting to get some stuff down. ",
      "createdAt": "2021-02-24T19:36:30Z",
      "updatedAt": "2021-12-30T02:09:36Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2b46fe7cdab7bd6f15fd467c4da4e526d28bec31",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/start-overview",
      "headRefOid": "397417aabbea73fa910ccb3a5b47fd0446c7c1a6",
      "closedAt": "2021-03-02T18:28:23Z",
      "mergedAt": "2021-03-02T18:28:23Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "fee74dacf41e8e333c5fbef4d860dfdfbc435c50"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk5ODAyOTc0",
          "commit": {
            "abbreviatedOid": "d63afcb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Good start. The main thing is that this needs to be aligned with the candidate protocol in #2. Let's merge that PR, then rebase this.",
          "createdAt": "2021-02-26T17:45:20Z",
          "updatedAt": "2021-02-26T17:58:35Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "```suggestion\r\ninput. An aggregation function F is one that computes an output y = F(x[1],x[2],...) for inputs\r\nx[i]. In general, Prio supports any aggregation function whose inputs can be encoded in a \r\n```",
              "createdAt": "2021-02-26T17:45:20Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 14,
              "body": "[[OPEN ISSUE: It's possible to estimate quantiles such as the median. How practical is this?]]",
              "createdAt": "2021-02-26T17:46:40Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 16,
              "body": "```suggestion\r\n- Data structures, like Bloom filters, counting Bloom filters, and count-min sketches, that approximately represent (multi-)sets of strings.\r\n```",
              "createdAt": "2021-02-26T17:49:13Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 23,
              "body": "```suggestion\r\nor aggregators, run a protocol that validates each input x[1], x[2], ... and computes the final output y. The final collector  \r\n```\r\n\r\n(\"invoke multi-party computation\" sounds like we're doing general purpose MPC.)",
              "createdAt": "2021-02-26T17:52:41Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 48,
              "body": "Pretty! Though I think we need to align this with the candidate protocol #2. In particular, I would remove the wires going from the client to the top and bottom aggregators, and I would rename the middle aggregator to \"Leader'.",
              "createdAt": "2021-02-26T17:53:55Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 56,
              "body": "Needs alignment with the candidate protocol of #2.",
              "createdAt": "2021-02-26T17:55:06Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 66,
              "body": "```suggestion\r\nclients. In doing so, the adversary can provide malicious (yet truthful) inputs to the aggregation \r\n```\r\nAlso, what does \"yet truthful\" mean here?",
              "createdAt": "2021-02-26T17:56:15Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 100,
              "body": "What does \"f^\" mean?",
              "createdAt": "2021-02-26T17:57:50Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDU3MjU1",
          "commit": {
            "abbreviatedOid": "6c84653"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-02-27T01:18:55Z",
          "updatedAt": "2021-02-27T01:18:55Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "```suggestion\r\n- Simple statistics, including sum, mean, min, max, variance, and standard deviation; [[OPEN ISSUE: It's possible to estimate quantiles such as the median. How practical is this?]]\r\n\r\n\r\n```",
              "createdAt": "2021-02-27T01:18:55Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAwMDY3MzQw",
          "commit": {
            "abbreviatedOid": "7f81949"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-02-27T02:37:28Z",
          "updatedAt": "2021-02-27T02:37:28Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxMjUxODkz",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-01T23:10:49Z",
          "updatedAt": "2021-03-02T15:36:42Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "\"a particular way\" is tantalizingly vague. Is this referring to affine-aggregatable encodings as discussed in the '17 paper?",
              "createdAt": "2021-03-01T23:10:50Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 58,
              "body": "I don't think clients would be aware of data batching--they would just continuously upload data as they generate it right? It's only the aggregators that need to be aware of batches so they can withhold aggregation parts from the leader or collector until the batch size threshold is met.",
              "createdAt": "2021-03-01T23:40:22Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 60,
              "body": "This design where clients only talk to the leader requires that we establish some kind of authenticated+encrypted tunnel through the leader. It might be simpler to have clients send shares directly to aggregators, because then we could use TLS to protect that channel. I think @cjpatton has argued elsewhere that there needs to be some kind of secure channel between clients and each aggregator no matter what, so that clients can discover each aggregator's encryption keys (i.e., we can't let the leader advertise parameters on behalf of the other aggregators). ",
              "createdAt": "2021-03-01T23:42:54Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 100,
              "body": "I'm guessing this is in reference to f-privacy and f^-privacy as discussed in section 2/page 3 of the 2017 paper. I certainly need to learn more about the information theory around privacy and what it means to quantify information leakage.",
              "createdAt": "2021-03-02T00:15:04Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            },
            {
              "originalPosition": 88,
              "body": "The 2017 paper's section 2 discusses _anonymity_ alongside privacy and robustness. However I'm not sure how to draw a clear distinction between anonymity and privacy in this context so maybe it's not worth discussing in this doc.",
              "createdAt": "2021-03-02T15:33:38Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxOTk0ODMz",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T16:09:02Z",
          "updatedAt": "2021-03-02T16:09:02Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Indeed -- and the mechanism is described later on in this doc, so I figured best to keep it high level here.",
              "createdAt": "2021-03-02T16:09:02Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxOTk1NTI1",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T16:09:38Z",
          "updatedAt": "2021-03-02T16:09:38Z",
          "comments": [
            {
              "originalPosition": 58,
              "body": "That's a good point! I'll move this batch cutoff to the leader step.",
              "createdAt": "2021-03-02T16:09:38Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxOTk3MjYx",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T16:11:07Z",
          "updatedAt": "2021-03-02T16:11:08Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "Indeed. I suggest we leave this as-is for now.",
              "createdAt": "2021-03-02T16:11:08Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMDAxODky",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T16:15:19Z",
          "updatedAt": "2021-03-02T16:15:19Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "That's one possibility, sure, but I don't think that's necessary. The leader could package up signed keys that it collects from aggregators and send them to the client. Clients could also fetch them directly from aggregators (if they knew what aggregators to fetch from). From a client perspective, the former seems simpler, since it would allow us to punt on aggregator discovery.",
              "createdAt": "2021-03-02T16:15:19Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMDc5NDQ5",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:27:55Z",
          "updatedAt": "2021-03-02T17:27:55Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "The leader could vend signed keys on behalf of aggregators, but the client still needs to discover a trusted public key for each aggregator to then verify the signed keys. I think you have to bootstrap trust between the client and each participating aggregator independently from the leader, or the leader can impersonate aggregators and defeat the system's privacy guarantees.",
              "createdAt": "2021-03-02T17:27:55Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTAzMzk2",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:53:40Z",
          "updatedAt": "2021-03-02T17:53:40Z",
          "comments": [
            {
              "originalPosition": 100,
              "body": "To resolve this, please add a reference to the paper.",
              "createdAt": "2021-03-02T17:53:40Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTA0NDMz",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:54:54Z",
          "updatedAt": "2021-03-02T17:54:55Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "IMO it's better to avoid creating a new PKI and just have aggregators distribute their own encryption keys. I'm open to being wrong, however :)",
              "createdAt": "2021-03-02T17:54:55Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTA1NDc3",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:56:04Z",
          "updatedAt": "2021-03-02T17:56:04Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "I'm not suggesting a new PKI for this.",
              "createdAt": "2021-03-02T17:56:04Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTA1ODA2",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:56:24Z",
          "updatedAt": "2021-03-02T17:56:24Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "I would resolve this by adding a refereence to the paper. ",
              "createdAt": "2021-03-02T17:56:24Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTA1OTY3",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:56:34Z",
          "updatedAt": "2021-03-02T17:56:34Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": ">  but the client still needs to discover a trusted public key for each aggregator to then verify the signed keys.\r\n\r\nYes, it does, and I'm suggesting this can be done by asking the leader to provide the signed keys. ",
              "createdAt": "2021-03-02T17:56:34Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTA2NjQw",
          "commit": {
            "abbreviatedOid": "733132f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T17:57:18Z",
          "updatedAt": "2021-03-02T17:57:18Z",
          "comments": [
            {
              "originalPosition": 100,
              "body": "Can you please leave what you're looking for as a suggestion? (There is a reference -- \"HCG's paper\" -- no?)",
              "createdAt": "2021-03-02T17:57:18Z",
              "updatedAt": "2021-03-02T18:23:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMTMyMzQ5",
          "commit": {
            "abbreviatedOid": "397417a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-02T18:26:12Z",
          "updatedAt": "2021-03-02T18:26:12Z",
          "comments": []
        }
      ]
    },
    {
      "number": 6,
      "id": "MDExOlB1bGxSZXF1ZXN0NTgyNTc1NDA1",
      "title": "Add an initial section on system constraints",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/6",
      "state": "MERGED",
      "author": "acmiyaguchi",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This PR adds a draft of system constraints. I had difficulty finding example documents to lend structure here, so apologies if the style or ideas are off the rails.\r\n\r\nI've added three constraints that should be relevant to future discussions: data resolution, timeliness, and integrity. I'm likely missing others. Some of the text involves services or applications that implement/use Prio -- I'm not sure if it's appropriate for this document. ",
      "createdAt": "2021-03-02T00:52:49Z",
      "updatedAt": "2021-03-03T17:57:07Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2b46fe7cdab7bd6f15fd467c4da4e526d28bec31",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "system-constraints",
      "headRefOid": "235f5a9563155602eb98f3790e9adb20d40a20c8",
      "closedAt": "2021-03-03T17:55:11Z",
      "mergedAt": "2021-03-03T17:55:11Z",
      "mergedBy": "acmiyaguchi",
      "mergeCommit": {
        "oid": "83c4ad39a101c17dcfc2421353996297aa503506"
      },
      "comments": [
        {
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "body": "> Nice work! Beyond @cjpatton's comments, my only blocking point here is on the notion of a \"deadline\", and what that means in a distributed system. I've suggested relaxing this to be soft.\r\n\r\nIn certain use cases (e.g., click measurement), most of the data might need to be passed along within a certain timeframe (say, 1 hour). That's not quite a deadline, but a constraint that we'll want to keep in mind.",
          "createdAt": "2021-03-02T21:43:17Z",
          "updatedAt": "2021-03-02T21:43:17Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> That's not quite a deadline, but a constraint that we'll want to keep in mind.\r\n\r\nYeah, that's a fine thing to note. I was just trying to stress that any tightly synchronized version of a deadline (\"all nodes stop collecting the batch at time X\") would be quite hard to implement.",
          "createdAt": "2021-03-02T22:04:26Z",
          "updatedAt": "2021-03-02T22:04:35Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxMzY3MTgx",
          "commit": {
            "abbreviatedOid": "34efd33"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "This looks good, minor comments only. ",
          "createdAt": "2021-03-02T03:07:43Z",
          "updatedAt": "2021-03-02T03:27:39Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "Instead of \"proportional to the verification circuit's size\" I would suggest \"related to the circuit's complexity\". The size of the proof, and the time complexity of generating and verifying it, can be reduced significantly if the circuit has a lot of redundancy. ",
              "createdAt": "2021-03-02T03:07:43Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 30,
              "body": "```suggestion\r\ncomputational budget. Meeting these deadlines will require efficient\r\n```",
              "createdAt": "2021-03-02T03:08:22Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 32,
              "body": "What's meant by \"binary serialization\"? ",
              "createdAt": "2021-03-02T03:08:42Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 45,
              "body": "```suggestion\r\ninaccuracies in the aggregates. An example data integrity constraint is that\r\n```\r\nDefinitely an important constraint :)",
              "createdAt": "2021-03-02T03:10:11Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 52,
              "body": "Invalid shares are always a problem if they go undetected. It sounds like this paragraph is more about DoS mitigation? I.e., regardless of their validity, the aggregators need to be able to detect if they're being flooded by inputs?",
              "createdAt": "2021-03-02T03:17:37Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 58,
              "body": "This is good stuff. I feel like we're definitely gonna expand this paragraph as we implement this :)",
              "createdAt": "2021-03-02T03:18:51Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 46,
              "body": "```suggestion\r\nevery share must be processed exactly once by all aggregators. Data integrity\r\n```",
              "createdAt": "2021-03-02T03:27:16Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAxODg3MDUy",
          "commit": {
            "abbreviatedOid": "34efd33"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Nice work! Beyond @cjpatton's comments, my only blocking point here is on the notion of a \"deadline\", and what that means in a distributed system. I've suggested relaxing this to be soft. (Other comments and suggestions around additional content can be addressed in a followup PR.)",
          "createdAt": "2021-03-02T14:38:27Z",
          "updatedAt": "2021-03-02T14:52:07Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "What is the throughput of the system? How many inputs/second can be processed?\r\n\r\nTaking a step back: Should we have a section where we discuss this and other key performance indicators that we're trying to be mindful of? (Time between batch start and batch end, where the aggregate is available, comes to mind, for example.)",
              "createdAt": "2021-03-02T14:38:27Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nrelated to the verification circuit's complexity and the rate of verifications\r\n```",
              "createdAt": "2021-03-02T14:38:51Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 17,
              "body": "```suggestion\r\nApplications that utilize proofs with a large number of multiplication gates or a high\r\n```",
              "createdAt": "2021-03-02T14:39:09Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 8,
              "body": "```suggestion\r\n### Data resolution limitations\r\n```",
              "createdAt": "2021-03-02T14:39:41Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 25,
              "body": "```suggestion\r\nA soft real-time system should produce a response within a deadline in order to be useful. This\r\n```\r\n\r\nAny sort of time synchronization or deadline enforcement here seems challenging, given the nature of the distributed system. I'd suggest relaxing this text a bit to say that deadlines are soft, and missing a deadline just means that the output has lesser value.",
              "createdAt": "2021-03-02T14:47:02Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 23,
              "body": "```suggestion\r\n### Aggregation utility and soft batch deadlines\r\n```",
              "createdAt": "2021-03-02T14:47:27Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 28,
              "body": "```suggestion\r\nAn example of a soft real-time constraint is the expectation that input data can be\r\n```",
              "createdAt": "2021-03-02T14:48:07Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 32,
              "body": "```suggestion\r\nrequests or utilize more efficient serialization to improve throughput.\r\n```",
              "createdAt": "2021-03-02T14:48:46Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 38,
              "body": "```suggestion\r\nto produce a response within a soft deadline.\r\n```",
              "createdAt": "2021-03-02T14:49:15Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 57,
              "body": "```suggestion\r\nrequests and controls the schedule for signaling the end of the aggregation\r\n```",
              "createdAt": "2021-03-02T14:50:39Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMjc2Nzg1",
          "commit": {
            "abbreviatedOid": "34efd33"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T21:12:19Z",
          "updatedAt": "2021-03-02T22:00:03Z",
          "comments": [
            {
              "originalPosition": 32,
              "body": "The example I had in mind was using something like Avro over JSON+base64 in cases where string parsing takes up more time than it should. ",
              "createdAt": "2021-03-02T21:12:19Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 52,
              "body": "I mean for this paragraph to distinguish between normal and unusual. In my day-to-day work, I've trawled through garbage coming through public ingestion endpoint (a lot of fuzzers and malformed data). I think some error is tolerable and normal. A large number of invalid inputs could be attributed to misconfigured application, and not necessarily an explicit denial of service. I did have DoS in mind too; I'll be more specific. ",
              "createdAt": "2021-03-02T21:20:09Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 15,
              "body": "I'm interested in learning more about the optimizations at some point after overcoming my admittedly weak understanding of arithmetic circuits. ",
              "createdAt": "2021-03-02T21:30:08Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            },
            {
              "originalPosition": 13,
              "body": "I think inputs/second is a reasonable definition of throughput, which is dependent on input parameters and the resources allocated for work. I guess \"rate of work\" would do here too. \r\n\r\nUsing a maybe not so great physics analogy to explain in a different frame:\r\n* verifying input is work\r\n* throughput is power (rate of work)\r\n* compute-time is energy (capacity for work)\r\n* the amount of data that can be processed is constrained by throughput and available compute-time\r\n\r\n> Should we have a section where we discuss this and other key performance indicators that we're trying to be mindful of?\r\n\r\nIt's probably a good idea. Maybe something to consider in the requirements or a separate performance section. I imagine an implementation would also want to monitor some of these things (e.g time to completion, error counts).  This section could also be expanded for discussion.\r\n",
              "createdAt": "2021-03-02T21:59:51Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMzI2NTE1",
          "commit": {
            "abbreviatedOid": "73184c6"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-02T22:21:22Z",
          "updatedAt": "2021-03-02T22:21:22Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "> It's probably a good idea. Maybe something to consider in the requirements or a separate performance section. I imagine an implementation would also want to monitor some of these things (e.g time to completion, error counts). This section could also be expanded for discussion.\r\n\r\nBoth seem fine. Maybe just mark an OPEN ISSUE here for us to resolve later?",
              "createdAt": "2021-03-02T22:21:22Z",
              "updatedAt": "2021-03-03T00:13:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAyMzI2NzQ3",
          "commit": {
            "abbreviatedOid": "73184c6"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-02T22:21:44Z",
          "updatedAt": "2021-03-02T22:21:44Z",
          "comments": []
        }
      ]
    },
    {
      "number": 11,
      "id": "MDExOlB1bGxSZXF1ZXN0NTg0MjM4OTg4",
      "title": "add link to Crypto 19 paper",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/11",
      "state": "MERGED",
      "author": "stpeter",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-03-03T20:20:42Z",
      "updatedAt": "2021-12-30T02:09:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "83c4ad39a101c17dcfc2421353996297aa503506",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "crypto19",
      "headRefOid": "91e42f265d682f38b8a0c96dcb4975662078a596",
      "closedAt": "2021-03-04T03:34:46Z",
      "mergedAt": "2021-03-04T03:34:46Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "a9efe7d10647ac9fa2a93d5930f932155ef96052"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAzNjc0ODQ0",
          "commit": {
            "abbreviatedOid": "91e42f2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-04T03:33:59Z",
          "updatedAt": "2021-03-04T03:33:59Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAzNjc1MTAz",
          "commit": {
            "abbreviatedOid": "91e42f2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-04T03:34:43Z",
          "updatedAt": "2021-03-04T03:34:43Z",
          "comments": []
        }
      ]
    },
    {
      "number": 12,
      "id": "MDExOlB1bGxSZXF1ZXN0NTg0NDYzODM1",
      "title": "Add \"collector\" to terminology and fix some typos",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/12",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Solves #10.\r\n\r\nThe collector is the endpoint that receives the final aggregate and defines the \"parameters of the protocol\". I'm being intentionally vague about the latter. I envision the parameters including the following:\r\n1. Who are the aggregators, and who is the leader;\r\n2. What type of data is being collected;\r\n3. How many inputs are aggregated in a given window (or, how long is a data collection window);\r\n4. and cryptographic parameters.",
      "createdAt": "2021-03-04T03:49:24Z",
      "updatedAt": "2021-06-17T21:15:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a9efe7d10647ac9fa2a93d5930f932155ef96052",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/collector",
      "headRefOid": "afb81f9b00c119d56c33b3bf530c0067ff6c4637",
      "closedAt": "2021-03-04T03:52:36Z",
      "mergedAt": "2021-03-04T03:52:36Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "5dd000153dc8c7136c271b14499b43a64316fbaf"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjAzNjgxODQ2",
          "commit": {
            "abbreviatedOid": "afb81f9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-04T03:52:33Z",
          "updatedAt": "2021-03-04T03:52:33Z",
          "comments": []
        }
      ]
    },
    {
      "number": 13,
      "id": "MDExOlB1bGxSZXF1ZXN0NTg1MDM1NTg0",
      "title": "Update field parameters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/13",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "@ekr and others have discussed using an 80 bit field in some application. This PR adds one. It also removes the smallest field (unlikely to ever be used) and renumbers the table.",
      "createdAt": "2021-03-04T18:50:04Z",
      "updatedAt": "2021-06-17T21:15:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5dd000153dc8c7136c271b14499b43a64316fbaf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/fields",
      "headRefOid": "c20b7839e8182811cd13d910217139ae08e9ae8e",
      "closedAt": "2021-03-19T15:56:46Z",
      "mergedAt": "2021-03-19T15:56:46Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f63c8fd340e1befa5db524602eb4dd7f802613f9"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "@henrycg would love to get your input here.",
          "createdAt": "2021-03-04T18:57:10Z",
          "updatedAt": "2021-03-04T18:57:10Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "As far as I can tell, this accurately subsets the previous fields. I have no opinion on the substance.",
          "createdAt": "2021-03-04T18:57:57Z",
          "updatedAt": "2021-03-04T18:57:57Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Oh, I see the 72 bit field. No idea if that one is right :)",
          "createdAt": "2021-03-04T18:58:58Z",
          "updatedAt": "2021-03-04T18:58:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@ekr, #3 is the roughly 80-bit field. It's actually just under 80 bits; that way we can represent field elements using 10 bytes.\r\n@henrycg, we would especially like your input on our criteria for choosing the parameters. These criteria are listed above.",
          "createdAt": "2021-03-04T19:30:03Z",
          "updatedAt": "2021-03-04T19:30:19Z"
        },
        {
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "body": "> - It might be a good idea to explain somewhere that soundness error (probability that the servers accept a false proof) is `2n/(p-n)` when carrying out all arithmetic modulo `p` and with input size `n`. As written, the \"bits\" column in the parameters table might give the impression that using modulus 1 gives ~31 bits of security, when it could actually give only ~15 bits of security when used with inputs of size 2^16.\r\n\r\nAdded the bound to the \"Field size\" criterion, and changed \"bits\" to \"size\". That column now says how many bits are needed to represent field elements.\r\n\r\n> - Nit: In this table and in the preceding discussion on [Finite field arithmetic](https://github.com/abetterinternet/prio-documents/blob/main/design-document.md#finite-field-arithmetic), it might make sense to use a symbol other than `n`, since `n` is used to mean something else in the earlier section on [Cryptographic Components](https://github.com/abetterinternet/prio-documents/blob/main/design-document.md#cryptographic-components).\r\n\r\nGood call! Changed `n` to ~`s`.~ `b`.",
          "createdAt": "2021-03-04T23:40:18Z",
          "updatedAt": "2021-03-05T16:26:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> Good call! Changed `n` to `s`.\r\n\r\nOof, this is supposed to be `b` instead of `s`. Edited above.\r\n",
          "createdAt": "2021-03-05T16:25:40Z",
          "updatedAt": "2021-03-05T16:25:40Z"
        },
        {
          "author": "tlepoint",
          "authorAssociation": "NONE",
          "body": "Out of curiosity, why do you need to hardcode prime numbers rather than making the whole system more flexible and accept any prime that would respect the constraints?",
          "createdAt": "2021-03-08T21:41:01Z",
          "updatedAt": "2021-03-08T21:41:01Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Out of curiosity, why do you need to hardcode prime numbers rather than making the whole system more flexible and accept any prime that would respect the constraints?\r\n\r\nIt seems sensible to settle on a handful of primes and develop optimized implementations of the arithmetic for each field. Initially, however, we plan to have generic arithmetic: see https://github.com/abetterinternet/libprio-rs/issues/10.",
          "createdAt": "2021-03-18T21:24:57Z",
          "updatedAt": "2021-03-18T21:24:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squahsed.",
          "createdAt": "2021-03-19T15:56:36Z",
          "updatedAt": "2021-03-19T15:56:36Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE1OTQ5ODYy",
          "commit": {
            "abbreviatedOid": "129d0ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I'm well out of my depth on the math here but one typo that this PR didn't even introduce aside, this LGTM.",
          "createdAt": "2021-03-18T23:21:14Z",
          "updatedAt": "2021-03-18T23:32:12Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "```suggestion\r\n   needs to be in order to effectively detect malicious clients. Typically the\r\n```",
              "createdAt": "2021-03-18T23:21:14Z",
              "updatedAt": "2021-03-19T15:56:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE2NTQ3OTMz",
          "commit": {
            "abbreviatedOid": "00bba6d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-19T15:50:31Z",
          "updatedAt": "2021-03-19T15:52:07Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Why \"typically\"? Is this not an upper bound?",
              "createdAt": "2021-03-19T15:50:31Z",
              "updatedAt": "2021-03-19T15:56:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE2NTUyMDc2",
          "commit": {
            "abbreviatedOid": "00bba6d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-19T15:54:46Z",
          "updatedAt": "2021-03-19T15:54:46Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "It's going to depend on the circuit.",
              "createdAt": "2021-03-19T15:54:46Z",
              "updatedAt": "2021-03-19T15:56:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE2NTUzMzY3",
          "commit": {
            "abbreviatedOid": "00bba6d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-19T15:56:07Z",
          "updatedAt": "2021-03-19T15:56:07Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Will it be higher? Lower?",
              "createdAt": "2021-03-19T15:56:07Z",
              "updatedAt": "2021-03-19T15:56:32Z"
            }
          ]
        }
      ]
    },
    {
      "number": 16,
      "id": "MDExOlB1bGxSZXF1ZXN0NTg3MzE5MTIw",
      "title": "design document: threat model",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/16",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Initial threat model, enumerating the system's actors and their\r\ncapabilities, as well as current or hypothetical mitigations.",
      "createdAt": "2021-03-09T02:42:35Z",
      "updatedAt": "2021-12-30T00:53:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5dd000153dc8c7136c271b14499b43a64316fbaf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/threat-model",
      "headRefOid": "d4fb63bc5773f9c9e8b8e55ff0ea03500408f290",
      "closedAt": "2021-03-29T20:59:21Z",
      "mergedAt": "2021-03-29T20:59:21Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "1fbcac5a3b4e779716a6687adccc2671ee49467f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjA5MDEwNDY2",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "This is a great start! I think my only blocking comment is around the network adversary.",
          "createdAt": "2021-03-10T18:01:17Z",
          "updatedAt": "2021-03-16T13:43:38Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "Is this an attack on Prio, or the client shooting itself in the foot? It seems the like the latter, so maybe we just say something like, \"Clients can leak their data and compromise their own privacy. This does not affect the privacy of others in the system.\"",
              "createdAt": "2021-03-10T18:01:17Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 77,
              "body": "Indeed -- I'd mark it as out of scope.",
              "createdAt": "2021-03-10T18:01:53Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 93,
              "body": "I think this is trying to say that if a client reveals identifying information to users, such as via client auth, then aggregators have this capability. Is that right?",
              "createdAt": "2021-03-10T18:03:48Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 104,
              "body": "Clarifying question: Is this suggesting or recommending that aggregators should somehow secret-share data before storing it? If so, I might mark this as out of scope. (Perhaps by saying that state persistence for each aggregator is assumed secure? Clearly if aggregators make their data public then things go wrong.)",
              "createdAt": "2021-03-10T18:14:08Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 94,
              "body": "```suggestion\r\n   system by revealing that a particular client contributed data to the system.\r\n   Moreover, aggregators may choose to selectively drop or omit data from certain \r\n   clients.\r\n```",
              "createdAt": "2021-03-16T01:34:35Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 105,
              "body": "I might omit this one, since it seems *somewhat* orthogonal to protocol or deployment mitigations.",
              "createdAt": "2021-03-16T01:36:46Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 152,
              "body": "```suggestion\r\n1. Aggregators should refuse shared parameters that are trivially insecure\r\n```",
              "createdAt": "2021-03-16T01:39:35Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 35,
              "body": "What about the network in general? That is, the attacker that is not a client, aggregator, or collector? You could imagine, for example, a network attacker forcing clients to use certain aggregators by making others appear to be unavailable. \ud83e\udd37 ",
              "createdAt": "2021-03-16T01:41:49Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNzA1MDYw",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T20:42:18Z",
          "updatedAt": "2021-03-16T20:42:18Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "I'm making a distinction between the user and the client here (I think my use of \"individual\" is confusing). I'm imagining a scenario where an application vendor might adopt Prio to elicit trust from users, but ship client software that covertly leaks user data over some non-Prio channel. \r\n\r\nI'm not sure we can do anything about it, unless the model is extended to include a trusted OS that a client runs in, which could attest to various properties of the client (e.g., measurements of reproducible builds, or attest to a log of the client's network activity so that external observers can verify that there are no unexpected connections to `secret-cleartext-metrics.appvendor.com`).\r\n\r\nI'm going to reword this to explicitly separate the case of an individual user/client disclosing their own data (which as you say does not compromise anyone else's privacy) and the case of a client used by many users leaking data (which is out of scope in the current model).",
              "createdAt": "2021-03-16T20:42:18Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNzE4OTk5",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T20:59:45Z",
          "updatedAt": "2021-03-16T20:59:45Z",
          "comments": [
            {
              "originalPosition": 105,
              "body": "I agree that this one isn't specific to Prio but rather would apply to any distributed computing system, so I'll strike it.",
              "createdAt": "2021-03-16T20:59:45Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNzIwMzU4",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T21:01:29Z",
          "updatedAt": "2021-03-16T21:01:30Z",
          "comments": [
            {
              "originalPosition": 104,
              "body": "The point of Prio, as I see it, is that you need to subvert all of the aggregators in order to defeat privacy. But if all the aggregators are using the same cloud storage vendor, then the attacker no longer needs to subvert _n_ different aggregators -- they just need to subvert a single cloud vendor. So users are back to hoping that a single entity will not have any breaches, will not get any national security letters and will consistently comply with its own privacy policy, which I think undermines Prio.\r\n\r\nReading this back, I don't like the sentence \"Prio deployments should ensure that aggregators' security do not have a common point of failure.\". I'll try to find a better way to express this.\r\n",
              "createdAt": "2021-03-16T21:01:29Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNzM2NjQw",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T21:23:21Z",
          "updatedAt": "2021-03-16T21:23:21Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "I'm not so sure, because at least one mitigation could be applied within the Prio protocol: requiring client identities. If clients had to authenticate to aggregators (or to a trusted authenticator who would then relay anonymized data shares to aggregators), then the system could detect clients sending excessive contributions and reject them.",
              "createdAt": "2021-03-16T21:23:21Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNzM4MTg0",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T21:25:37Z",
          "updatedAt": "2021-03-16T21:25:37Z",
          "comments": [
            {
              "originalPosition": 93,
              "body": "I would phrase it as \"if a client reveals identifying information to aggregators, such as via client auth, then aggregators have this capability\".",
              "createdAt": "2021-03-16T21:25:37Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNzQzMDQ0",
          "commit": {
            "abbreviatedOid": "511977a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T21:32:28Z",
          "updatedAt": "2021-03-16T21:32:29Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "I think you're right that there should be a section for an attacker on the network. However I find it hard to reason about a threat like the one in your example because we haven't yet sketched out how aggregator discovery works. But certainly we can already call out the necessity of confidential and mutually authenticated links between participating servers.",
              "createdAt": "2021-03-16T21:32:28Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE1OTA5Mzc5",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "This is super useful, thank you for getting this started. My only high-level comment is that this analysis doesn't completely settle the question of whether we care if a malicious server colludes with a malicious client in order to corrupt the final output. As I mention below, this threat model is only briefly considered in [BBC+19] (and not at all in the original Prio paper, [GB17]); and because a malicious aggregator could always submit a bogus aggregation share, this model doesn't seem (to me anyway) relevant for our deployment.\r\n\r\nI think this PR should explicitly remove this threat model from scope. We should assume that aggregators do their very best to validate and accumulate data shares correctly. That said, we should make it a goal to detect misbehaving aggregators. However, the target there needs to be detecting bogus aggregation shares.",
          "createdAt": "2021-03-18T22:01:15Z",
          "updatedAt": "2021-03-24T01:53:33Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "^ I've been the terms \"input\" and \"input share\" instead of \"data\" and \"data share\" respectively. I prefer \"input\" because the term \"data\" might also be used to describe the output aggregate.",
              "createdAt": "2021-03-18T22:01:16Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 11,
              "body": "^ Elsewhere in this doc, the term \"invalid input\" refers to an input whose shares have been deemed invalid by the aggregators in their run of the input-validation protocol. I would use the term \"malformed\" to describe un-encodable input.",
              "createdAt": "2021-03-24T00:43:56Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 3,
              "body": "I suggest keeping this list of terms in alphabetical order.",
              "createdAt": "2021-03-24T00:45:45Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 17,
              "body": "The problem with using \"aggregation\" is that the same term applies to the task of aggregating inputs. I think a better term would be \"output\" or \"aggregate\".",
              "createdAt": "2021-03-24T00:46:53Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 27,
              "body": "To my thinking, \"anonymity\" is implied by \"privacy\". I suggest removing this.",
              "createdAt": "2021-03-24T00:47:50Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 60,
              "body": "```suggestion\r\n       clients do besides uploading data shares. Accordingly, such attacks\r\n```",
              "createdAt": "2021-03-24T00:51:10Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 71,
              "body": "```suggestion\r\n1. The input-validation protocol executed by the aggregators prevents either individual\r\n```",
              "createdAt": "2021-03-24T00:54:10Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 98,
              "body": "```suggestion\r\n   to emit aggregation shares.\r\n```",
              "createdAt": "2021-03-24T00:55:41Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 104,
              "body": "```suggestion\r\n   is preserved as long as at least one aggregator is does not reveal its data shares.\r\n```",
              "createdAt": "2021-03-24T00:57:11Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 113,
              "body": "^ I agree, but the design here is tricky. I would add a [[TODO]] here that reminds us to flesh this out.",
              "createdAt": "2021-03-24T00:58:03Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 119,
              "body": "^ Does this defeat the bogus aggregation share attack? I'm not sure anything does. There may be other, more subtle attacks it detects, but given how easy it is for an aggregator to submit a bogus aggregation share, I don't think it matters much. (See comment in \"Future work and possible extensions\".)\r\n\r\nHowever, detecting bad aggregators is a really good idea. We'll need to flesh this out later.",
              "createdAt": "2021-03-24T01:08:08Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 129,
              "body": "This capability is only marginally stronger than the bogus aggregation share attack, IMO.",
              "createdAt": "2021-03-24T01:09:41Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 127,
              "body": "Another capability to call out: The leader can try to shrink the anonymity set by requesting aggregation shares early (cf. https://github.com/abetterinternet/prio-documents/issues/15).",
              "createdAt": "2021-03-24T01:12:08Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 137,
              "body": "I think what's meant by this is that the aggregators could exchange verification shares so that each aggregator can run the decision algorithm locally. Is that right?\r\n\r\nI agree that this would help mitigate the leader's additional capabilities. However, doing so would require a secure channel between each pair of aggregators. I think this cost is too high.",
              "createdAt": "2021-03-24T01:17:06Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 147,
              "body": "In [BBC+19] (and hence libprio-rs) the concept of \"polynomial identity test\" is generalized to distributed evaluation of a PCP over joint randomness. Hence, I would use the term \"joint randomness\" instead of \"polynomial identity test values\".\r\n\r\nTo your point: We haven't yet decided how the joint randomness is chosen. It could very well be the collector, but this is TBD. (See \"Consensus protocol\" below.) Also, I would add \"designation the leader role\" to this list.\r\nBut ",
              "createdAt": "2021-03-24T01:21:57Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 149,
              "body": "```suggestion\r\n   shares submitted by aggregators.\r\n```",
              "createdAt": "2021-03-24T01:22:14Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 154,
              "body": "The parameters need to be authenticated and enforced by each aggregator, but I think it's up to the collector to choose how large the anonymity set should be.",
              "createdAt": "2021-03-24T01:23:12Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 180,
              "body": "The size of the input is a function of the data type, not the data itself. In particular, all clients will submit the same amount of data for a given aggregation job.",
              "createdAt": "2021-03-24T01:26:25Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 194,
              "body": "Aggregation isn't well-defined if input lengths vary, so I think the first part of this mitigation is irrelevant.",
              "createdAt": "2021-03-24T01:27:52Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 213,
              "body": "```suggestion\r\nfalse data in order to spoil aggregations. Deployments could require client\r\nauthentication. For example,\r\n```",
              "createdAt": "2021-03-24T01:29:05Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 219,
              "body": "Another authentication mechanism: The leader might just require the user to login beforehand.\r\n\r\nI'm curious how whitebox crypto might be used here?",
              "createdAt": "2021-03-24T01:30:33Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 226,
              "body": "```suggestion\r\nsome other channel which compromises privacy. If we introduce the\r\n```\r\n\r\nAs I mentioned above, privacy implies anonymity as the terms are defined above. (You may disagree?)",
              "createdAt": "2021-03-24T01:32:15Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 203,
              "body": "An attacker capability explicitly considered in [BBC+19], but not in [GB17], is that a coalition of verifiers may attempt to convince the honest verifiers that an invalid data is valid. This doesn't matter much in our setting, since the bogus aggregation share attack descrbed above is equally effective and much easier to pull off. It would matter, however, if the aggregator were divided into two entities: one which validates input shares (the verifier); and another that accumulates them and emits the aggregation share. I think it's worth noting this somewhere, probably here.",
              "createdAt": "2021-03-24T01:36:43Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNDM2OTgx",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T18:10:31Z",
          "updatedAt": "2021-03-25T18:10:31Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "That's a good idea. I keep finding myself writing \"original data\" or \"unshared data\" to disambiguate, so I'll adopt your \"input\" instead.",
              "createdAt": "2021-03-25T18:10:31Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNDY4ODgy",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T18:45:59Z",
          "updatedAt": "2021-03-25T18:45:59Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "My phrasing here was based on a misreading of the 2017 paper. I'll rewrite this to make it plain that invalid data is simply data for which the validity predicate does not hold.",
              "createdAt": "2021-03-25T18:45:59Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNDcyNTI0",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T18:50:10Z",
          "updatedAt": "2021-03-25T18:50:11Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "The [2017 paper](https://crypto.stanford.edu/prio/paper.pdf) calls out privacy and anonymity as distinct security properties (section 2). Anonymity is the adversary not knowing which client submitted which input, privacy is the adversary not knowing what the inputs are. The reason I added anonymity to this list is because I think that in the setting where aggregators talk directly to clients, privacy is preserved but client anonymity is weakened since the aggregator can identify clients e.g. by IP. Further we can sketch out a mitigation -- an anonymizing proxy -- so I think it's worth discussing the anonymity property.",
              "createdAt": "2021-03-25T18:50:11Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNDczNjQ3",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T18:51:31Z",
          "updatedAt": "2021-03-25T18:51:31Z",
          "comments": [
            {
              "originalPosition": 129,
              "body": "Plus, HCG clarified for us yesterday that any aggregator, leader or not, can collude with malicious clients to achieve the same thing. I'll rewrite this.",
              "createdAt": "2021-03-25T18:51:31Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNTU5MjM5",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T20:40:03Z",
          "updatedAt": "2021-03-25T20:40:03Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "Good point! I chose \"output\" for symmetry with your other good suggestion of \"input\".",
              "createdAt": "2021-03-25T20:40:03Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNTg0Nzg3",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T21:14:15Z",
          "updatedAt": "2021-03-25T21:14:15Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "I think this isn't something we'd tackle in an initial deployment of Prio anyway, so I wrote up #22 on this topic, and will move this mitigation into future work and possible extensions.",
              "createdAt": "2021-03-25T21:14:15Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNTkwMjA5",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T21:22:04Z",
          "updatedAt": "2021-03-25T21:22:04Z",
          "comments": [
            {
              "originalPosition": 219,
              "body": "The notion with whitebox is that you could use it to distribute a shared secret to all \"legitimate\" instances of your client, and so having a message HMACed with a whiteboxed key gives you some kind of assurance on the provenance of the message. But thinking about it further, I've never been a big fan of whitebox, and mentioning it here isn't particularly helpful to explaining Prio, so I'll just strike it.",
              "createdAt": "2021-03-25T21:22:04Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNTk1MDM5",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T21:29:12Z",
          "updatedAt": "2021-03-25T21:29:13Z",
          "comments": [
            {
              "originalPosition": 203,
              "body": "I think the leader in our design is the verifier you describe, no? What HCG argued to us on 3/24 was that a verifier colluding with a malicious client doesn't have any capability that isn't already available to any aggregator colluding with a malicious client. I think what you're sketching out is a further variation where the leader is _only_ a verifier and not an aggregator?",
              "createdAt": "2021-03-25T21:29:12Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNTk4OTM4",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T21:35:08Z",
          "updatedAt": "2021-03-25T21:35:09Z",
          "comments": [
            {
              "originalPosition": 180,
              "body": "For any individual aggregation, yes, but imagine if a client supports dozens or hundreds of individual aggregations, and users can opt out of any of them. If the client only transmits input shares for the aggregations the client has opted into, then the volume of traffic observable by the adversary would change. Hence I argue that a client should always transmit the same amount of data, regardless of which aggregations a user has consented to.",
              "createdAt": "2021-03-25T21:35:09Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNjAwOTk3",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T21:38:23Z",
          "updatedAt": "2021-03-25T21:38:23Z",
          "comments": [
            {
              "originalPosition": 154,
              "body": "I agree, but the intent here is that aggregators should implement basic checks independent of the configuration of any aggregation to prevent abuse.",
              "createdAt": "2021-03-25T21:38:23Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIxNjAxOTQ3",
          "commit": {
            "abbreviatedOid": "702f11c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-25T21:39:55Z",
          "updatedAt": "2021-03-25T21:39:55Z",
          "comments": [
            {
              "originalPosition": 137,
              "body": "HCG convinced us (or at least me!) that it's OK to have a designated proof verifier because the verifier is no more powerful than any aggregator that colludes with a malicious client, so I'm going to remove this item.",
              "createdAt": "2021-03-25T21:39:55Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIyNDg2ODI3",
          "commit": {
            "abbreviatedOid": "f090834"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM modulo editorial things.",
          "createdAt": "2021-03-26T19:48:11Z",
          "updatedAt": "2021-03-26T21:07:52Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "```suggestion\r\n   not truthful. For example, if the data being gathered is whether or not users have\r\n```",
              "createdAt": "2021-03-26T19:48:11Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 31,
              "body": "```suggestion\r\n1. Privacy. The aggregators and collector learn only the output of F computed over all client inputs, \r\n```",
              "createdAt": "2021-03-26T19:49:50Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 34,
              "body": "```suggestion\r\n1. Robustness. As long as the aggregators execute the input-validation protocol correctly, a malicious client can skew the output of F only by reporting false \r\n```",
              "createdAt": "2021-03-26T19:50:24Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 138,
              "body": "```suggestion\r\n1. Shrinking the anonymity set. The leader instructs aggregators to construct\r\n```",
              "createdAt": "2021-03-26T20:58:14Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 180,
              "body": "Works for me.",
              "createdAt": "2021-03-26T21:00:15Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            },
            {
              "originalPosition": 203,
              "body": "> I think the leader in our design is the verifier you describe, no? \r\n\r\nNo. Here I'm imagining if any aggregator were split into two services: one which validates input shares and another that accumulates them.\r\n\r\nIn any case, the point here is moot I think. We will assume that the aggregators execute the input-validation correctly (this is setting I of [BBC+19].)\r\n",
              "createdAt": "2021-03-26T21:03:49Z",
              "updatedAt": "2021-03-26T21:58:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIzNjQ0Nzgy",
          "commit": {
            "abbreviatedOid": "d4fb63b"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-29T20:58:52Z",
          "updatedAt": "2021-03-29T20:58:52Z",
          "comments": []
        }
      ]
    },
    {
      "number": 17,
      "id": "MDExOlB1bGxSZXF1ZXN0NTg4NTUwNjk1",
      "title": "Add batch size constraint and aggregator count constraint to the design document",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/17",
      "state": "CLOSED",
      "author": "aaomidi",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Introduces two new sections under `System constraints`",
      "createdAt": "2021-03-09T20:03:35Z",
      "updatedAt": "2021-12-30T02:09:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5dd000153dc8c7136c271b14499b43a64316fbaf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "amir/constrants/batch_aggregator",
      "headRefOid": "4ad6e66911b5a600027152024d101f62479a55ea",
      "closedAt": "2021-07-01T01:51:57Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe the intention of this change is now subsumed by #59. Can we close this?",
          "createdAt": "2021-07-01T01:48:52Z",
          "updatedAt": "2021-07-01T01:48:52Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzNTg5NjQx",
          "commit": {
            "abbreviatedOid": "4ad6e66"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-16T18:26:00Z",
          "updatedAt": "2021-03-16T18:30:42Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "This paragraph conflates two separate instances of content size. It's reasonable for the leader to validate incoming shares based on their length, which is some function of aggregation parameters (e.g., it should be cheap to filter data for a 10-bit encoding that looks like it's 1000-bits). These would get rejected during the verification process anyways. At the same time, the leader should also be aware that aggregators may only accept up to a certain number of messages at a time. \r\n\r\nThe burden should be on the leader to ensure that the clients are behaving with respect to batch constraint since the client (a single device/user) has limited awareness about the batch it's part of.",
              "createdAt": "2021-03-16T18:26:00Z",
              "updatedAt": "2021-03-16T18:30:42Z"
            },
            {
              "originalPosition": 22,
              "body": "Might be useful to include an approach of how to resolve one of the issues that arises in this context (e.g., ensuring redundancy of some kind). ",
              "createdAt": "2021-03-16T18:28:33Z",
              "updatedAt": "2021-03-16T18:30:42Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE5MjQyMzg3",
          "commit": {
            "abbreviatedOid": "4ad6e66"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I don't think the aggregators should be able to enforce minimum batch sizes. IMO this should be a parameter chosen by the collector.",
          "createdAt": "2021-03-24T02:00:54Z",
          "updatedAt": "2021-03-24T02:07:31Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "```suggestion\r\nThe collector may opt into using\r\n```\r\n(Not sure if this is what you mean ... see \"Terminology\" above.)",
              "createdAt": "2021-03-24T02:00:54Z",
              "updatedAt": "2021-03-24T02:07:31Z"
            }
          ]
        }
      ]
    },
    {
      "number": 23,
      "id": "MDExOlB1bGxSZXF1ZXN0NjAxODc0MzY1",
      "title": "EKR pass 1.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/23",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "- Try to keep things concrete. We always send things to the leader\r\n  so just say so.\r\n\r\n- Move the \"System Constraints\" section to the end and rename it.\r\n  This will remove forward refs when the rest of the doc is\r\n  written.\r\n\r\nOther editorial.",
      "createdAt": "2021-03-26T20:03:56Z",
      "updatedAt": "2021-03-29T21:02:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f63c8fd340e1befa5db524602eb4dd7f802613f9",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "ekr_comments",
      "headRefOid": "4fc2641189ae683255e191b07618624db64ee408",
      "closedAt": "2021-03-29T21:02:12Z",
      "mergedAt": "2021-03-29T21:02:12Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "1457f9ba189dfd70a18393c6b6af6459e3ab78fd"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIyNTYxMjA1",
          "commit": {
            "abbreviatedOid": "4fc2641"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-26T22:04:13Z",
          "updatedAt": "2021-03-26T22:09:13Z",
          "comments": [
            {
              "originalPosition": 134,
              "body": "The most recent consensus is that aggregators will decrypt proof+input shares received from clients, forward the proof shares to the leader (while keeping input shares secret), and the leader is responsible for validating the proof and instructing aggregators to include an input in an aggregation.\r\n\r\nA further nit: the proof proves that the input is _valid_ (#16 adds a definition of invalid input to the glossary). Prio can't say anything useful about the correctness or truthfulness of the input.",
              "createdAt": "2021-03-26T22:04:14Z",
              "updatedAt": "2021-03-26T22:09:13Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIzNjQ0Mjk0",
          "commit": {
            "abbreviatedOid": "4fc2641"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udea2 ",
          "createdAt": "2021-03-29T20:58:13Z",
          "updatedAt": "2021-03-29T20:58:13Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjIzNjQ2Nzg0",
          "commit": {
            "abbreviatedOid": "4fc2641"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-29T21:01:29Z",
          "updatedAt": "2021-03-29T21:01:29Z",
          "comments": []
        }
      ]
    },
    {
      "number": 25,
      "id": "MDExOlB1bGxSZXF1ZXN0NjA0NTQxNDI0",
      "title": "Beef up the introduction and add an outline for the document structure",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/25",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Solves #24. ",
      "createdAt": "2021-03-30T22:20:48Z",
      "updatedAt": "2021-06-17T21:15:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "67c276d22919506d2a949a35a4937eedcbef2408",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton-pass-1",
      "headRefOid": "724e42ed9f77346d5bb8f19ea0f48082168acb99",
      "closedAt": "2021-03-30T23:03:41Z",
      "mergedAt": "2021-03-30T23:03:41Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "bd54726bf0a5af09d58548a06a9df31faefebbd1"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0NzYzNTY3",
          "commit": {
            "abbreviatedOid": "fdc6a35"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This is a huge improvement. Once you land this, I have some thoughts on how to bridge to the crypto section.",
          "createdAt": "2021-03-30T22:47:49Z",
          "updatedAt": "2021-03-30T22:52:40Z",
          "comments": [
            {
              "originalPosition": 93,
              "body": "```suggestion\r\nx[1,1] + x[1,2] modulo some value p.  For convenience, we will omit the \"mod p\" in the rest of this section. It then uploads x[1,1] to one server x[1,2] to the other. The\r\n```",
              "createdAt": "2021-03-30T22:47:49Z",
              "updatedAt": "2021-03-30T23:03:11Z"
            },
            {
              "originalPosition": 91,
              "body": "This seems like an example where we actually have to invoke modular arithmetic, because obviously if x[,] are just integers then we know that x[1] < x[1,1] and x[1] < x[1,2].\r\n\r\n",
              "createdAt": "2021-03-30T22:49:22Z",
              "updatedAt": "2021-03-30T23:03:11Z"
            },
            {
              "originalPosition": 103,
              "body": "```suggestion\r\nbecause addition is commutative. i.e., \r\n```",
              "createdAt": "2021-03-30T22:51:22Z",
              "updatedAt": "2021-03-30T23:03:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0NzY5Mjg1",
          "commit": {
            "abbreviatedOid": "fdc6a35"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-30T23:00:27Z",
          "updatedAt": "2021-03-30T23:00:27Z",
          "comments": [
            {
              "originalPosition": 91,
              "body": "I took your suggestion above, which should make it clear.",
              "createdAt": "2021-03-30T23:00:27Z",
              "updatedAt": "2021-03-30T23:03:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0NzY5NzA4",
          "commit": {
            "abbreviatedOid": "fdc6a35"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-30T23:01:24Z",
          "updatedAt": "2021-03-30T23:01:24Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "Done.",
              "createdAt": "2021-03-30T23:01:24Z",
              "updatedAt": "2021-03-30T23:03:11Z"
            }
          ]
        }
      ]
    },
    {
      "number": 26,
      "id": "MDExOlB1bGxSZXF1ZXN0NjA0Njc3NDkz",
      "title": "Try to make the on-ramp to the crypto easier and clean up a bit of no\u2026",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/26",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "\u2026tation to make it more friendly to comsec types",
      "createdAt": "2021-03-31T00:15:12Z",
      "updatedAt": "2021-04-07T20:46:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "bd54726bf0a5af09d58548a06a9df31faefebbd1",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "crypto_bridge",
      "headRefOid": "0c45dd2a02bc854ea385232a4e55032a734ef409",
      "closedAt": "2021-04-07T20:46:33Z",
      "mergedAt": "2021-04-07T20:46:33Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "89cc97e283500263c82024d784cdfcd78f5926c0"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0ODA1MTQ5",
          "commit": {
            "abbreviatedOid": "ec93aa3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-03-31T00:34:29Z",
          "updatedAt": "2021-03-31T00:43:46Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "s/break/split/",
              "createdAt": "2021-03-31T00:34:29Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            },
            {
              "originalPosition": 46,
              "body": "s/Where/Above,/",
              "createdAt": "2021-03-31T00:35:01Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            },
            {
              "originalPosition": 25,
              "body": "I understand the intention of this paragraph, however I think it's a bit confusing as it is. For instance, you say that a client wishes to report a \"set of inputs\", whereas above we think of each clients sending exactly one input, and we refer to the inputs of the aggregation function collectively as the \"set of inputs\"\r\n\r\nWDYTA:\r\n> Each run of the Prio protocol is parameterized by a finite field, which we will call K, and an integer n. Each client encodes its input as a length-n vector of element of K. The length of the vector depends on the type of data being collected. A single field element may be sufficient for some applications, whereas more sophisticated measurements will require larger encodings.",
              "createdAt": "2021-03-31T00:43:32Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0ODEzNDA4",
          "commit": {
            "abbreviatedOid": "ec93aa3"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-31T00:59:47Z",
          "updatedAt": "2021-03-31T00:59:47Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "Perhaps this is just me being confused. When you and I talked the other day, what I thought I heard was that if I wanted to report, say, \"number of urls\" and \"number of clicks\", I would turn that into a single 2-length vector of integers rather then sending two entirely different 1-length vectors and proofs.' Is that not correct?\r\n\r\n",
              "createdAt": "2021-03-31T00:59:47Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0ODIzNzA4",
          "commit": {
            "abbreviatedOid": "ec93aa3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-03-31T01:30:10Z",
          "updatedAt": "2021-03-31T01:30:11Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "That's correct. My point is that, in the language of this doc, the \"input\" in this case is the pair (number_of_urls, number_of_clicks). I'm trying to be precise about the language. In particular, I want to be clear that \"set of inputs\" refers to the collection of user inputs. Each input is a datum of some type, e.g., a pair of integers, a single integer, a widget, what have you.\r\n",
              "createdAt": "2021-03-31T01:30:10Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjI0ODI0NjA3",
          "commit": {
            "abbreviatedOid": "ec93aa3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-03-31T01:32:42Z",
          "updatedAt": "2021-03-31T01:32:53Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "```suggestion\r\nIn order to share x between s servers, we split it up into s shares\r\n```",
              "createdAt": "2021-03-31T01:32:42Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            },
            {
              "originalPosition": 46,
              "body": "```suggestion\r\nAbove, p(n), u(n), and v(n) are functions that we specify later.\r\n```",
              "createdAt": "2021-03-31T01:32:50Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjMwNTY1Nzk5",
          "commit": {
            "abbreviatedOid": "ec93aa3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-07T20:45:07Z",
          "updatedAt": "2021-04-07T20:45:08Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "Taking my suggestion.",
              "createdAt": "2021-04-07T20:45:07Z",
              "updatedAt": "2021-04-07T20:46:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjMwNTY3ODk0",
          "commit": {
            "abbreviatedOid": "0c45dd2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-04-07T20:46:16Z",
          "updatedAt": "2021-04-07T20:46:16Z",
          "comments": []
        }
      ]
    },
    {
      "number": 28,
      "id": "MDExOlB1bGxSZXF1ZXN0NjE0ODEzMTMw",
      "title": "More background",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/28",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-04-13T23:04:33Z",
      "updatedAt": "2021-04-14T17:08:14Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "89cc97e283500263c82024d784cdfcd78f5926c0",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "more_background",
      "headRefOid": "d381c284c332121d51e55a217f5a7d7c5516c485",
      "closedAt": "2021-04-14T17:08:14Z",
      "mergedAt": "2021-04-14T17:08:14Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "d868f8cae34f07b1acf902832f4aa627f56925cd"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM1ODAxNTA1",
          "commit": {
            "abbreviatedOid": "e968c27"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": ":boat: ",
          "createdAt": "2021-04-14T16:05:03Z",
          "updatedAt": "2021-04-14T16:05:38Z",
          "comments": [
            {
              "originalPosition": 132,
              "body": "```suggestion\r\n{{CITE}} describes how to encode measurements for each statistic.\r\n```",
              "createdAt": "2021-04-14T16:05:03Z",
              "updatedAt": "2021-04-14T16:32:16Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM1ODA0MDM3",
          "commit": {
            "abbreviatedOid": "e968c27"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-14T16:07:29Z",
          "updatedAt": "2021-04-14T16:07:29Z",
          "comments": [
            {
              "originalPosition": 132,
              "body": "I think this is actually going to be a cross-reference to some to-be-written section of this doc.",
              "createdAt": "2021-04-14T16:07:29Z",
              "updatedAt": "2021-04-14T16:32:16Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM1ODI2MjEz",
          "commit": {
            "abbreviatedOid": "e968c27"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-14T16:29:51Z",
          "updatedAt": "2021-04-14T16:29:52Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "```suggestion\r\nmeasurements of all clients. In this case, the protocol input is a single measurement consisting of \r\na single positive integer; no additional encoding is done. Given this input, the first client splits its\r\nmeasurement x[1] with additive secret-sharing into a pair of integers x[1,1] and x[1,2] for which \r\nx[1] = x[1,1] + x[1,2] modulo a prime p. (For convenience, we will omit the\r\n```",
              "createdAt": "2021-04-14T16:29:51Z",
              "updatedAt": "2021-04-14T16:32:16Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM1ODUzNjg1",
          "commit": {
            "abbreviatedOid": "d381c28"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-14T16:58:52Z",
          "updatedAt": "2021-04-14T16:58:53Z",
          "comments": [
            {
              "originalPosition": 132,
              "body": "Indeed. \"{{CITE}}\" the way we've been writing this.",
              "createdAt": "2021-04-14T16:58:53Z",
              "updatedAt": "2021-04-14T16:58:53Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM1ODYwNzE1",
          "commit": {
            "abbreviatedOid": "d381c28"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-14T17:06:39Z",
          "updatedAt": "2021-04-14T17:06:39Z",
          "comments": [
            {
              "originalPosition": 132,
              "body": "That's not going to be ideal if/when we want to turn this into an IETF draft, b/c it will cause the formatter to choke.",
              "createdAt": "2021-04-14T17:06:39Z",
              "updatedAt": "2021-04-14T17:06:39Z"
            }
          ]
        }
      ]
    },
    {
      "number": 29,
      "id": "MDExOlB1bGxSZXF1ZXN0NjE2MTc5MTUx",
      "title": "Add a disclaimer",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/29",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-04-15T16:09:53Z",
      "updatedAt": "2021-06-17T21:15:31Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d868f8cae34f07b1acf902832f4aa627f56925cd",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/wip",
      "headRefOid": "fe93b6cf74f50244e1fe09e409789c9d5879f5aa",
      "closedAt": "2021-04-15T16:15:56Z",
      "mergedAt": "2021-04-15T16:15:56Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "934b48a1119acfb264b213180437794b8241fdc4"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM2ODc2NDAw",
          "commit": {
            "abbreviatedOid": "fe93b6c"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2021-04-15T16:14:33Z",
          "updatedAt": "2021-04-15T16:14:33Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM2ODc3NzA1",
          "commit": {
            "abbreviatedOid": "fe93b6c"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-04-15T16:15:53Z",
          "updatedAt": "2021-04-15T16:15:53Z",
          "comments": []
        }
      ]
    },
    {
      "number": 31,
      "id": "MDExOlB1bGxSZXF1ZXN0NjIwNzExMzY3",
      "title": "Specify the ingestion flow",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/31",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Specifies the ingestion flow, including parameter discovery, uploading of shares, and validating of the shares. The same flow applies to both Prio and heavy hitters (\"Hits\"). The protocol-specific messages are left unspecified. (We will specify them in future PRs.)\r\n\r\nNote that I decided to go with TLS syntax for this PR. This was done only because I'm familiar with this style and it was easy for me to write quickly. I'm happy to completely re-do message encoding, but let's do so in a future PR.\r\n\r\nIssues addressed by this change:\r\n* Closes #4: In this design, the leader specifies any number of helpers. For each helper, the client initiates a run of the protocol with the leader and the helper. This allows the a helper to drop out without impacting data processing later on. However, the leader cannot drop out. (This seems OK, since this is the same up-time requirement as usual.)\r\n* Addresses #8. In this design, the joint randomness needed for a run of the protocol is picked by the leader and sent to the helpers in the `PAVerifyStartReq` message.\r\n* Closes #9. In this design, the leader tells the client the URL of each helper. The client gets each helper's public key by making an HTTP request.\r\n* Addresses #18. This PR specifies a high-level flow that should fit the input-validation protocol for heavy hitters.\r\n* Closes #21. Any protocol-specific parameters are carried by the `PAClientParam` message.\r\n* Addresses #22. In this design, multiple protocol runs are used to add resilience to aggregator drop-out. We still don't know whether we can use threshold secret sharing (i.e., Shamir) for the same purpose. (In any case, this design disallows it.) We should think about whether this works before closing that issue.",
      "createdAt": "2021-04-22T02:32:50Z",
      "updatedAt": "2021-04-27T15:38:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "934b48a1119acfb264b213180437794b8241fdc4",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/ingestion",
      "headRefOid": "0e672cbdc5cdb3e7e29373bb13c4590ec12f72c8",
      "closedAt": "2021-04-27T15:33:08Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Be advised: I changed the first client->leader request to a POST. That way the leader can \"challenge\" the client (e.g., give it a random nonce), which may be needed for some protocols",
          "createdAt": "2021-04-23T14:42:19Z",
          "updatedAt": "2021-04-23T14:42:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood has some major revisions (restructuring the doc, mostly) so I'll be closing this PR and opening a new one. I'm also making the following changes:\r\n- Remove \"resend_shares\" flag\r\n- Add an \"upload ID\" to PAClientParam.",
          "createdAt": "2021-04-27T15:33:08Z",
          "updatedAt": "2021-04-27T15:33:08Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNTkwODEz",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I got about half-way through this and left some comments, but I think it probably needs a fair amount of rework before it's ready for detailed review. In particular, it's pretty informal about the HTTP mechanics. There's a lot of prior art in how to describe this. I would suggest imitating ACME (RFC 8555) which will also help with the client auth if you don't want to do TLS client auth.",
          "createdAt": "2021-04-22T19:26:44Z",
          "updatedAt": "2021-04-22T19:36:42Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "This just uses TLS, right? If so, we usually just assume the WebPKI and wave at 6125.",
              "createdAt": "2021-04-22T19:26:45Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 59,
              "body": "Can you reorder these so we don't have a forward reference?",
              "createdAt": "2021-04-22T19:27:16Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 57,
              "body": "This is kind of a confused thing in TLS, but based on 8446, I believe you want:\r\n\r\n```\r\nopaque<0..255> URl;\r\n```\r\n\r\nWith that said, you also want 0..2^16-1;",
              "createdAt": "2021-04-22T19:29:11Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 55,
              "body": "You need to explain these structs.",
              "createdAt": "2021-04-22T19:30:03Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 45,
              "body": "You usually need a length field here so that you can extent his to new protos",
              "createdAt": "2021-04-22T19:30:19Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 74,
              "body": "A GET? A POST? [ACME](https://www.rfc-editor.org/rfc/rfc8555.html) does a pretty good job of describing how to talk to this kind of HTTP services server.\r\n\r\n",
              "createdAt": "2021-04-22T19:32:29Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 87,
              "body": "This seems redundant.",
              "createdAt": "2021-04-22T19:32:46Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 85,
              "body": "With an OK or...? What's the content type.",
              "createdAt": "2021-04-22T19:33:18Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 107,
              "body": "How does this interact with the related structs in ECH?",
              "createdAt": "2021-04-22T19:35:05Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNjAwODAz",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Good start! Lots of ways we can do this, and I'm happy to just iterate going forward.",
          "createdAt": "2021-04-22T19:39:47Z",
          "updatedAt": "2021-04-22T20:00:12Z",
          "comments": [
            {
              "originalPosition": 110,
              "body": "This is unused -- did you mean to include in the key config?",
              "createdAt": "2021-04-22T19:39:48Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 162,
              "body": "```suggestion\r\n} PAUpload;\r\n```",
              "createdAt": "2021-04-22T19:42:19Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 165,
              "body": "Should the config_id be in this struct? The leader doesn't need to parse it upon receipt of the PAUpload message, right?",
              "createdAt": "2021-04-22T19:43:33Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 146,
              "body": "How? With what AAD? ",
              "createdAt": "2021-04-22T19:43:48Z",
              "updatedAt": "2021-04-23T21:13:14Z"
            },
            {
              "originalPosition": 131,
              "body": "Why one message for each helper? Can then not be sent in a single message? (Also, isn't there just one helper, per the list above^?)",
              "createdAt": "2021-04-22T19:44:19Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 162,
              "body": "Maybe we can rearrange this like so? The leader then just copies the contents from the end of this message and sends them to the right helper along with the `PATask`.\r\n\r\n```\r\nstruct {\r\n   PATask task;\r\n   PALeaderShare leader_share;\r\n   Url helper_url;\r\n   struct {\r\n      uint8 helper_key_config_id;\r\n      opaque enc<0..2^16-1>;\r\n      opaque payload<0..2^24-1>;\r\n   } PAHelperUpload;\r\n} PALeaderUpload;",
              "createdAt": "2021-04-22T19:46:53Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 178,
              "body": "Should it wait to send \"200 OK\" until the helper confirms that the upload is well formed? (That is, it can decrypt the share?)",
              "createdAt": "2021-04-22T19:48:01Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 185,
              "body": "Why would we not relay this signal to clients? It could mean that their configuration information is out of date, or something else. Would that be important for monitoring purposes?",
              "createdAt": "2021-04-22T19:49:02Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 194,
              "body": "We should probably say that the leader buckets up uploads based on matching task IDs and helper config IDs above. (It's listed below.)",
              "createdAt": "2021-04-22T19:50:10Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 212,
              "body": "How can `PAVerifyStartReq.shares` carry protocol-specific information if it only includes `enc` and `payload`? Is `payload` the share encryption and then other goop? (Reading on, that does seem to be the case. I'm not sure how we'd  clarify that now.)",
              "createdAt": "2021-04-22T19:51:58Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 217,
              "body": "What happens if this is not the case? Would it be better to just have a list of (share, payload) tuple in `PAVerifyStartReq`?",
              "createdAt": "2021-04-22T19:53:10Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 262,
              "body": "It also requires the leader to maintain a mapping between shares and messages it sent to the helper. That sort of state seems error prone to manage. Hmm... could this message indicate specifically which shares should be re-sent, if any?\r\n\r\nAlso, I wonder if we should even make this an option. State management is hard enough as-is, so perhaps requiring just one entity to do it is better than possibly allowing others to do it?",
              "createdAt": "2021-04-22T19:55:56Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNjg5NTMy",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T21:40:04Z",
          "updatedAt": "2021-04-22T21:40:04Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "Yup. Changed to \"The client and leader can establish a leader-authenticated TLS channel.\"",
              "createdAt": "2021-04-22T21:40:04Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNjkwMDU0",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T21:40:55Z",
          "updatedAt": "2021-04-22T21:40:56Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "What's a \"forward reference\"?",
              "createdAt": "2021-04-22T21:40:56Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzA0Mzgz",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T21:53:13Z",
          "updatedAt": "2021-04-22T21:53:14Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "It is, removed.",
              "createdAt": "2021-04-22T21:53:13Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzA1ODEw",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T21:54:31Z",
          "updatedAt": "2021-04-22T21:54:31Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "It's not the same struct, so they're not related. We (Cloudflare) wouldn't use our ECHConfig for this purpose, if that's what you're asking. ",
              "createdAt": "2021-04-22T21:54:31Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzMzNDI5",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:14:12Z",
          "updatedAt": "2021-04-22T22:14:12Z",
          "comments": [
            {
              "originalPosition": 74,
              "body": "I think it should be a GET here. Specified this here and elsewhere.",
              "createdAt": "2021-04-22T22:14:12Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzM0MzA4",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:14:39Z",
          "updatedAt": "2021-04-22T22:14:40Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "200. Specified here and elsewhere.",
              "createdAt": "2021-04-22T22:14:39Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzM1Nzgy",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:15:39Z",
          "updatedAt": "2021-04-22T22:15:39Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "Didn't seem necessary here, but I don't mind adding it.",
              "createdAt": "2021-04-22T22:15:39Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzM5ODMz",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:22:26Z",
          "updatedAt": "2021-04-22T22:22:27Z",
          "comments": [
            {
              "originalPosition": 110,
              "body": "Good catch, added AEAD code point to key config",
              "createdAt": "2021-04-22T22:22:26Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzQ0NDY5",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:31:59Z",
          "updatedAt": "2021-04-22T22:32:00Z",
          "comments": [
            {
              "originalPosition": 165,
              "body": "Ah, good call.",
              "createdAt": "2021-04-22T22:31:59Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzQ1MjQ2",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:33:36Z",
          "updatedAt": "2021-04-22T22:33:37Z",
          "comments": [
            {
              "originalPosition": 165,
              "body": "Actually it's better if the config id is not in the `PAHelperShare`. This struct is reused in the `PAVerifyStartReq` message, and there a single config id is used for several helper shares.",
              "createdAt": "2021-04-22T22:33:36Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzQ2Mjgw",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:35:50Z",
          "updatedAt": "2021-04-22T22:35:51Z",
          "comments": [
            {
              "originalPosition": 146,
              "body": "As we discussed today, some PA protocols (like Prio) might derive an HPKE context, export a PRG seed, and use that seed for secret sharing. Others (like HH) might secret share first, then encrypt.",
              "createdAt": "2021-04-22T22:35:51Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzQ3MDc5",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:37:16Z",
          "updatedAt": "2021-04-22T22:37:17Z",
          "comments": [
            {
              "originalPosition": 131,
              "body": "I suppose the PAUploads could be sent in the same POST request. This seemed easier to implement. Besides, the leader will never share state between the individual runs.",
              "createdAt": "2021-04-22T22:37:16Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzQ5NTQ0",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:42:35Z",
          "updatedAt": "2021-04-22T22:42:35Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": "Hmm, that's an interesting suggestion. I'm leaning towards \"no\" because the leader is permitted to batch a bunch of verify-start requests. This might cause latency that the client would notice. Besides, there's no reason to wait, since the client doesn't care if its shares were deemed valid.",
              "createdAt": "2021-04-22T22:42:35Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzU0MTMw",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T22:52:49Z",
          "updatedAt": "2021-04-22T22:52:50Z",
          "comments": [
            {
              "originalPosition": 162,
              "body": "~Done, but I called the inner struct `PAHelperShare`.~ Eh, actually I want to keep it as-is. For this 'cut-and-paste' trick to work, you need to stick the key config id in the helper share. But in the verify start request message, there are a bunch of helper shares that all correspond to the same key config id.",
              "createdAt": "2021-04-22T22:52:49Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzY4Mzk3",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T23:21:30Z",
          "updatedAt": "2021-04-22T23:21:30Z",
          "comments": [
            {
              "originalPosition": 194,
              "body": "Done.",
              "createdAt": "2021-04-22T23:21:30Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzY5OTE5",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T23:25:38Z",
          "updatedAt": "2021-04-22T23:25:39Z",
          "comments": [
            {
              "originalPosition": 185,
              "body": "Done.",
              "createdAt": "2021-04-22T23:25:38Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNjY0MDU2",
          "commit": {
            "abbreviatedOid": "620c88f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-22T21:01:57Z",
          "updatedAt": "2021-04-22T23:26:47Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "The client also needs a root certificate that can authenticate all the helpers, so that the HPKE parameters can be fetched.",
              "createdAt": "2021-04-22T21:01:57Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 65,
              "body": "```suggestion\r\n[[NOTE: `PAParameters.task.id` is a 64-bit integer because it may be useful for\r\n```",
              "createdAt": "2021-04-22T21:07:18Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 74,
              "body": "Presumably a single leader could be participating in many different deployments using different parameters, so this request needs a task ID (as in, `PAParam.task.id`) parameter (which could be an HTTP header or a query or path fragment in the URI) to specify which deployment is being asked about.",
              "createdAt": "2021-04-22T21:10:27Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 117,
              "body": "```suggestion\r\nset up a base-mode HPKE context to use to derive symmetric keys with which to protect the shares sent to the helper\r\nvia the leader.\r\n```",
              "createdAt": "2021-04-22T21:14:09Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 122,
              "body": "Does this specifically refer to when the client does an HTTP GET to obtain HPKE parameters from the helper?",
              "createdAt": "2021-04-22T21:16:53Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 131,
              "body": "+1 to this. It also helps with idempotency if clients can upload all the input shares in a single HTTP request as opposed to having to track retries across _n_ requests to _n_ helpers.",
              "createdAt": "2021-04-22T21:21:11Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 33,
              "body": "There should be a definition of \"helper\" in the glossary.",
              "createdAt": "2021-04-22T21:51:42Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 37,
              "body": "What is the scope of an HPKE key pair? Does the helper use a single HPKE key for all deployments it participates in, or one per `PATask`?\r\n\r\nIf a helper has a single HPKE key pair across all `PATask`s (which makes sense to me if we think of the HPKE pub as analogous to a TLS server's certificate), then I think we should specify that `PAParam.task.id` gets used as a label in the HPKE KDF to ensure keys aren't re-used across `PATask`s.",
              "createdAt": "2021-04-22T22:00:16Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 97,
              "body": "Similarly to `[leader]/parameters` this needs a parameter for task ID.",
              "createdAt": "2021-04-22T22:02:02Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 49,
              "body": "```suggestion\r\n} PAServerParam;\r\n```\r\nfor symmetry with `PAClientParam`.",
              "createdAt": "2021-04-22T22:09:41Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 105,
              "body": "What value should be used when not encrypting helper shares (e.g. the Prio case where HPKE is used only to derive a PRNG seed shared between client and helper)? [`0xFFFF`/Export Only?](https://www.ietf.org/archive/id/draft-irtf-cfrg-hpke-08.html#section-7.3)",
              "createdAt": "2021-04-22T22:20:25Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 103,
              "body": "Should this be \"Used by client to decide if it recognizes this config\"? Does this identifier refer to some list of well known HPKEKeyConfigs?",
              "createdAt": "2021-04-22T22:23:42Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 124,
              "body": "```suggestion\r\n* the key config specifies a KEM, KDF or AEAD algorithm the client doesn't recognize.\r\n```",
              "createdAt": "2021-04-22T22:24:09Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 103,
              "body": "Oh I get it now -- the client is meant to echo this value back into `PAUpload` so that helper can later reject shares that use algos helper doesn't recognize. I think this comment could be made more clear.",
              "createdAt": "2021-04-22T22:26:03Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 167,
              "body": "IIUC, in the Prio case, there won't be an encrypted share because the helper's shares are recomputed from a PRNG seed derived from the HPKE context. So what goes in this message field in that case? Empty string?",
              "createdAt": "2021-04-22T22:28:32Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 178,
              "body": "I think the answer is no, because we expect the leader to be getting _lots_ of input submissions from clients (e.g., millions, billions of installations of Firefox or Chrome periodically uploading telemetry). If helpers are on the hot path of input submission, then helpers are subject to the same availability and performance requirements as leaders, which I think negates one of the main advantages of the leader design.",
              "createdAt": "2021-04-22T22:32:45Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 198,
              "body": "Farther down, we state\r\n>Note that a well-formed message contains as many payloads as shares\r\n\r\nSo should both `shares` and `payloads` be `0..2^24-1` (or `0..2^16-1`)?",
              "createdAt": "2021-04-22T22:34:36Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 40,
              "body": "We should have a reference to https://tools.ietf.org/html/rfc8446#section-3 somewhere in here",
              "createdAt": "2021-04-22T22:39:54Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 137,
              "body": "include a reference to https://www.ietf.org/archive/id/draft-irtf-cfrg-hpke-08.html#name-encryption-to-a-public-key, where I assume `SetupBaseS` comes from",
              "createdAt": "2021-04-22T22:45:08Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 193,
              "body": "Is the entire `PATask` structure needed here or just the unique identifier so that helpers can look it up in the list of tasks they know about?",
              "createdAt": "2021-04-22T22:57:10Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 198,
              "body": "The use of \"payload\" here is confusing. Aren't the shares the payload? I'm also not clear on why there is a unique `{Prio|Hits}VerifyStartReq` for each `PAHelperShare`. Would `PrioVerifyStartReq` contain the random value used for the polynomial identity test?",
              "createdAt": "2021-04-22T23:03:57Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 210,
              "body": "\"share\" vs. \"payload\" is confusing in these lines.",
              "createdAt": "2021-04-22T23:05:21Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 224,
              "body": "Looks where? I think the idea is that each helper maintains a list of `PAParam`s it is aware of and participating in?",
              "createdAt": "2021-04-22T23:06:18Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 44,
              "body": "Is there a 1:1 relationship between `PAParam` and `PATask`? I think yes, since participants need to be able to use a task ID to look up a `PAParam` value.",
              "createdAt": "2021-04-22T23:08:16Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 262,
              "body": "One way or the other, I don't think `resend_shares` is an attribute of an individual `PAVerifyStartResp`. A helper server either has storage or not and so would presumably always send the same value of `resend_shares`. Should this be part of the `PATask` or `PAParam` that the leader and all helpers have to agree upon before the protocol starts?",
              "createdAt": "2021-04-22T23:11:07Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 302,
              "body": "```suggestion\r\n`PAVerifyFinishReq.key_config_id`. If not found, then it aborts and alerts the\r\n```",
              "createdAt": "2021-04-22T23:13:57Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 272,
              "body": "IIUC, the leader will run the FLPCP `decide` algorithm for the input using the `PAVerifyStartResp` messages it gathers from helpers, and the verify finish step means informing the helper(s) of the results of `decide`?",
              "createdAt": "2021-04-22T23:20:56Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 278,
              "body": "If this is empty then I think you still need a list of share unique identifiers so that helper knows how to resolve the list of `{Prio|Hits}VerifyFinishReq` messages it gets against the shares it has stored.",
              "createdAt": "2021-04-22T23:21:59Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzk3OTYz",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:20:01Z",
          "updatedAt": "2021-04-23T00:20:01Z",
          "comments": [
            {
              "originalPosition": 212,
              "body": "Yes that's the case. `PAVerifyStartREq.shares` should be `PAVerifyStartReq.payloads` here.",
              "createdAt": "2021-04-23T00:20:01Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyNzk4OTkx",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:23:03Z",
          "updatedAt": "2021-04-23T00:23:03Z",
          "comments": [
            {
              "originalPosition": 217,
              "body": "That seemed tricky to do in TLS syntax, particularly because we need to select the type of `payload` on `task.proto`. I'm open to suggestions for how to spell this differently. In the meantime, I added a TODO below explaining that this would be better as a sequence of (share, payload) tuples.",
              "createdAt": "2021-04-23T00:23:03Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODAwODAw",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:28:18Z",
          "updatedAt": "2021-04-23T00:28:19Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "To clarify why this bit exists: the helper shares may be so big that the leader and helper agree to use state rather than have to retransmit them. If set, then the leader is supposed to retransmit *all* of the shares.\r\n\r\n> It also requires the leader to maintain a mapping between shares and messages it sent to the helper. That sort of state seems error prone to manage. Hmm... could this message indicate specifically which shares should be re-sent, if any?\r\n\r\nThe intention is that the leader needs to re-send *all* of the shares if the bit is true, or *none* if the bit is false. I don't think we should bother with supporting anything in the middle.\r\n\r\n\r\n> Also, I wonder if we should even make this an option. State management is hard enough as-is, so perhaps requiring just one entity to do it is better than possibly allowing others to do it?\r\n\r\nShares are likely to be fairly large, especially for heavy hitters. I think this is going to need to be an option.\r\n\r\n\r\n",
              "createdAt": "2021-04-23T00:28:18Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODAxMTUx",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:29:16Z",
          "updatedAt": "2021-04-23T00:29:17Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "> One way or the other, I don't think `resend_shares` is an attribute of an individual `PAVerifyStartResp`. A helper server either has storage or not and so would presumably always send the same value of `resend_shares`. Should this be part of the `PATask` or `PAParam` that the leader and all helpers have to agree upon before the protocol starts?\r\n\r\nThat is an option, though the bit is more deployment-specific rather than protocol-specific, which is why I put it in the high level message.",
              "createdAt": "2021-04-23T00:29:16Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODAxNDY5",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:30:05Z",
          "updatedAt": "2021-04-23T00:30:06Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "That's what the next line says. (See updated text.)",
              "createdAt": "2021-04-23T00:30:06Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODAyMzIx",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:32:40Z",
          "updatedAt": "2021-04-23T00:32:40Z",
          "comments": [
            {
              "originalPosition": 74,
              "body": "The task id is presumably encoded by the string `[leader]`.",
              "createdAt": "2021-04-23T00:32:40Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODA0Nzc4",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:34:23Z",
          "updatedAt": "2021-04-23T00:34:24Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "This refers to the TLS channel between client and helper.",
              "createdAt": "2021-04-23T00:34:24Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODEwNDQ1",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:37:59Z",
          "updatedAt": "2021-04-23T00:37:59Z",
          "comments": [
            {
              "originalPosition": 37,
              "body": "> What is the scope of an HPKE key pair? Does the helper use a single HPKE key for all deployments it participates in, or one per `PATask`?\r\n\r\nThat's up to the deployment I think.\r\n\r\n> If a helper has a single HPKE key pair across all `PATask`s (which makes sense to me if we think of the HPKE pub as analogous to a TLS server's certificate), then I think we should specify that `PAParam.task.id` gets used as a label in the HPKE KDF to ensure keys aren't re-used across `PATask`s.\r\n\r\nIt does! The context is computed as `SetupBaseR(share.enc, sk, PAParam.task)`\r\n\r\n",
              "createdAt": "2021-04-23T00:37:59Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODExNTk0",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:38:45Z",
          "updatedAt": "2021-04-23T00:38:46Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "That's premature. We may end up re-doing the encoding for #30.",
              "createdAt": "2021-04-23T00:38:45Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODEyNjgz",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:39:27Z",
          "updatedAt": "2021-04-23T00:39:28Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "~Not necessarily. In any case, `PATask` is supposed to encode what's common between `PAParam` and `PAClientParam`.~",
              "createdAt": "2021-04-23T00:39:27Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODE0NzE4",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:41:02Z",
          "updatedAt": "2021-04-23T00:41:03Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "In fact, the symmetric name would be `PAAggregatorParam`. I have a weak preference for `PAParam` because it's less wordy.",
              "createdAt": "2021-04-23T00:41:02Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODE5NTc5",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:44:23Z",
          "updatedAt": "2021-04-23T00:44:24Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "I removed the comment and added some clarification in the paragraph below.",
              "createdAt": "2021-04-23T00:44:23Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODIwNDQ3",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:44:58Z",
          "updatedAt": "2021-04-23T00:44:59Z",
          "comments": [
            {
              "originalPosition": 105,
              "body": "Yup.",
              "createdAt": "2021-04-23T00:44:58Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODI2MTA1",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:48:44Z",
          "updatedAt": "2021-04-23T00:48:44Z",
          "comments": [
            {
              "originalPosition": 167,
              "body": "For Prio, `PAHelperShare.payload` will be the empty string.",
              "createdAt": "2021-04-23T00:48:44Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODI3NjY1",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:49:46Z",
          "updatedAt": "2021-04-23T00:49:46Z",
          "comments": [
            {
              "originalPosition": 193,
              "body": "`task` also specifies the version (of this document) and the PA protocol (\"prio\" or \"hits\"). This info is needed in order to parse the rest of the message.",
              "createdAt": "2021-04-23T00:49:46Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODI5NTc4",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:51:05Z",
          "updatedAt": "2021-04-23T00:51:05Z",
          "comments": [
            {
              "originalPosition": 198,
              "body": "Shares might be big, but I don't think the corresponding protocol message will be big. Hence U24 for the former and U16 for the latter. For consistency, we could do U24 for both. I'm agnostic. (This may also change when we pin down a solution for #30.)",
              "createdAt": "2021-04-23T00:51:05Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODMyOTkx",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:53:29Z",
          "updatedAt": "2021-04-23T00:53:30Z",
          "comments": [
            {
              "originalPosition": 198,
              "body": "Right, `PrioVerifyStartReq` and `HItsVerifyStartReq` carries the protocol-specific stuff, e.g., the random value input to `query` in the context of Prio.",
              "createdAt": "2021-04-23T00:53:29Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODM2ODcz",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:56:07Z",
          "updatedAt": "2021-04-23T00:56:08Z",
          "comments": [
            {
              "originalPosition": 210,
              "body": "\"share\" is the thing that is encrypted and contains the input and proof share; \"payload\" is whatever else is needed by the helper in the next step. The reason these are separate is because the \"share\" is copied verbatim from the client's upload request, and the \"payload\" is generated by the leader.",
              "createdAt": "2021-04-23T00:56:07Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODM3MDg0",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:56:48Z",
          "updatedAt": "2021-04-23T00:56:49Z",
          "comments": [
            {
              "originalPosition": 224,
              "body": "Yup, that's the idea.",
              "createdAt": "2021-04-23T00:56:49Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODM3MjU4",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:57:22Z",
          "updatedAt": "2021-04-23T00:57:23Z",
          "comments": [
            {
              "originalPosition": 272,
              "body": "Yes.",
              "createdAt": "2021-04-23T00:57:23Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODM4MDQ4",
          "commit": {
            "abbreviatedOid": "0780ef2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T00:59:54Z",
          "updatedAt": "2021-04-23T00:59:54Z",
          "comments": [
            {
              "originalPosition": 278,
              "body": "That seems like a lot of overhead that's not really necessary. It should be clear that the first element of `payloads` corresponds to the first share, the second element of `payloads` to the second share, and so on. This should be fairly straightforward if the helper is already maintaining state.",
              "createdAt": "2021-04-23T00:59:54Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODM5ODIx",
          "commit": {
            "abbreviatedOid": "f6b7589"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Thanks, all, for your comments! I've tried to address them all... several remain unresolved.\r\n\r\nOne area of confusion and which we don't quite agree is the `resend_shares` flag in the verify start response. This allows the leader and helper to avoid retransmitting the shares, as long as the helper is able to maintain state between requests. This is helpful if the shares are really big. I don't think we have a use case with really big shares yet, so I'd be fine with just dropping this flag and having the leader always retransmit the shares. However, I think we want to keep this option in our back pocket.\r\n\r\nAlso, @ekr, at this stage I'm not trying to be overly formal with the spec. My goal is to describe the protocol in enough detail that we all basically know what's going on and we can start working on an implementation. There are also some open questions, which preclude a more formal description. That said, I did make an effort to be more clear about what's a POST and what's a GET, as well as the response status for each HTTP request.",
          "createdAt": "2021-04-23T01:05:10Z",
          "updatedAt": "2021-04-23T01:21:10Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQyODQ0NzEx",
          "commit": {
            "abbreviatedOid": "f6b7589"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T01:19:56Z",
          "updatedAt": "2021-04-23T01:19:56Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "OH wait, I misunderstood. Yes, there's 1:1 map.",
              "createdAt": "2021-04-23T01:19:56Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQzNjMxOTUz",
          "commit": {
            "abbreviatedOid": "5d2a6ad"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "With the understanding that we still need to specify lots of details about message encoding and HTTP semantics, this looks good to me. Thanks Chris!",
          "createdAt": "2021-04-23T20:11:40Z",
          "updatedAt": "2021-04-23T20:21:44Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "```suggestion\r\n1. Helper: An aggregator that is not a leader.\r\n```",
              "createdAt": "2021-04-23T20:11:40Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 198,
              "body": "Oh, I think I got confused by the notation. I thought `PAHelperShare shares<0..2^24-1>` meant \"between 0 and 2^24-1 `PAHelperShare` structures, but it means `0..2^24-1` bytes.",
              "createdAt": "2021-04-23T20:17:12Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            },
            {
              "originalPosition": 224,
              "body": "I think it'd help to have language explaining that all participants are configured with a list of known `PAParam`s before the start of the discovery or ingestion protocols.",
              "createdAt": "2021-04-23T20:19:06Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQzNjY3MjE0",
          "commit": {
            "abbreviatedOid": "5d2a6ad"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T21:12:02Z",
          "updatedAt": "2021-04-23T21:12:57Z",
          "comments": [
            {
              "originalPosition": 224,
              "body": "Done. (See the changes to the beginning of {{proto-ingestion}}.",
              "createdAt": "2021-04-23T21:12:03Z",
              "updatedAt": "2021-04-23T21:13:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQzNzA1ODcz",
          "commit": {
            "abbreviatedOid": "0e672cb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T22:44:05Z",
          "updatedAt": "2021-04-23T22:44:06Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "Quick follow-up: I'm fine with removing the `resend_shares` bit for simplicity and just require the leader to always resend. We can always add it later if it's needed.",
              "createdAt": "2021-04-23T22:44:06Z",
              "updatedAt": "2021-04-23T22:44:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQzNzA2MTY1",
          "commit": {
            "abbreviatedOid": "0e672cb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-23T22:45:00Z",
          "updatedAt": "2021-04-23T22:45:00Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "I think we should err on the side of making the protocol simpler, and hence remove the option. My intuition is that shares should be small for Prio (since there aren't actually shares but rather just a PRNG seed from which shares are derived).",
              "createdAt": "2021-04-23T22:45:00Z",
              "updatedAt": "2021-04-23T22:45:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ2MDI0MTI5",
          "commit": {
            "abbreviatedOid": "0e672cb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-27T15:38:00Z",
          "updatedAt": "2021-04-27T15:38:01Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "I plan to remove the bit.",
              "createdAt": "2021-04-27T15:38:00Z",
              "updatedAt": "2021-04-27T15:38:01Z"
            }
          ]
        }
      ]
    },
    {
      "number": 33,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI0NzM4NTg3",
      "title": "Use mdbook to render doc",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/33",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This allows us to use `mdbook` to render the design doc into HTML, as suggested by @acmiyaguchi. To run, do\r\n```\r\ncargo install mdbook\r\nmdbook server\r\n```\r\nThis starts an HTTP server that updates the page each time the source is edited. This makes editing the document much a more pleasant experience, I think.",
      "createdAt": "2021-04-27T23:50:15Z",
      "updatedAt": "2021-06-17T21:15:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "934b48a1119acfb264b213180437794b8241fdc4",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/mkbook",
      "headRefOid": "3f1594a2f2d395c05fc9e6f89642751d1de589ea",
      "closedAt": "2021-04-29T17:29:43Z",
      "mergedAt": "2021-04-29T17:29:43Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "c195673a79cf4665e4df11bf3c5768f0b5644ad9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ2NDg2Njk5",
          "commit": {
            "abbreviatedOid": "9674f31"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I suggest adding a README with the instructions to build/serve the docs. Otherwise it's a good starting point \ud83d\udc4d.",
          "createdAt": "2021-04-27T23:57:29Z",
          "updatedAt": "2021-04-27T23:57:29Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ2NDk1OTQ0",
          "commit": {
            "abbreviatedOid": "8c9d67e"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-04-28T00:22:26Z",
          "updatedAt": "2021-04-28T00:22:26Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ2NDk2MjM2",
          "commit": {
            "abbreviatedOid": "8c9d67e"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-28T00:23:10Z",
          "updatedAt": "2021-04-28T00:23:14Z",
          "comments": [
            {
              "originalPosition": 1,
              "body": "The document is title REAMDE instead of README",
              "createdAt": "2021-04-28T00:23:11Z",
              "updatedAt": "2021-04-29T17:27:54Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ2NTA0NDkw",
          "commit": {
            "abbreviatedOid": "8c9d67e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-28T00:38:10Z",
          "updatedAt": "2021-04-28T00:38:10Z",
          "comments": [
            {
              "originalPosition": 1,
              "body": "Ugh, sorry.",
              "createdAt": "2021-04-28T00:38:10Z",
              "updatedAt": "2021-04-29T17:27:54Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ4NDY3OTEx",
          "commit": {
            "abbreviatedOid": "b55a223"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-04-29T17:23:59Z",
          "updatedAt": "2021-04-29T17:24:12Z",
          "comments": [
            {
              "originalPosition": 2,
              "body": "Not that I don't appreciate your contributions but could we change this to \"The authors\" or \"The contributors\" or remove it altogether if it's optional?",
              "createdAt": "2021-04-29T17:23:59Z",
              "updatedAt": "2021-04-29T17:27:54Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ4NDY4ODcy",
          "commit": {
            "abbreviatedOid": "b55a223"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-29T17:25:05Z",
          "updatedAt": "2021-04-29T17:25:06Z",
          "comments": [
            {
              "originalPosition": 2,
              "body": "Oops, that was added auto-magically\r\n",
              "createdAt": "2021-04-29T17:25:06Z",
              "updatedAt": "2021-04-29T17:27:54Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ4NDcxMzI3",
          "commit": {
            "abbreviatedOid": "3f1594a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-29T17:27:56Z",
          "updatedAt": "2021-04-29T17:27:57Z",
          "comments": [
            {
              "originalPosition": 2,
              "body": "Done.",
              "createdAt": "2021-04-29T17:27:56Z",
              "updatedAt": "2021-04-29T17:27:57Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ4NDcxODE2",
          "commit": {
            "abbreviatedOid": "3f1594a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-04-29T17:28:31Z",
          "updatedAt": "2021-04-29T17:28:31Z",
          "comments": []
        }
      ]
    },
    {
      "number": 34,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI3OTY4NzQy",
      "title": "Doc rework and specification of upload and verify flows",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/34",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Specifies the upload and verify phases of a PA protocol. In the upload phase, the client uploads a report; in the verify phase, the aggregators verify that their shares correspond to a valid input. These flows apply to both Prio and heavy hitters (\"Hits\"), though the protocol-specific messages in these flows are left unspecified. (We will specify them in future PRs.)\r\n\r\nI've also broken the various sections of the document into separate files so that `mdbook` can create a table of contents.\r\n\r\n**Non-goal for this PR:** This PR is not meant to provide a definitive, formalized \"standard document\". It's meant only to get us one step closer to the final shape of the protocol.\r\n\r\nIssues addressed by this change:\r\n\r\n* Closes #4: In this design, the leader specifies any number of helpers. For each helper, the client initiates a run of the protocol with the leader and the helper. This allows the a helper to drop out without impacting data processing later on. However, the leader cannot drop out. (This seems OK, since this is the same up-time requirement as usual.)\r\n*Addresses #8. In this design, the joint randomness needed for a run of the protocol is picked by the leader and sent to the helpers in the PAVerifyStartReq message.\r\n* Closes #9. In this design, the leader tells the client the URL of each helper. The client gets each helper's public key by making an HTTP request.\r\n* Addresses #18. This PR specifies a high-level flow that should fit the input-validation protocol for heavy hitters.\r\n* Closes #21. Any protocol-specific parameters are carried by the PAClientParam message.\r\n* Addresses #22. In this design, multiple protocol runs are used to add resilience to aggregator drop-out. We still don't know whether we can use threshold secret sharing (i.e., Shamir) for the same purpose. (In any case, this design disallows it.) We should think about whether this works before closing that issue.\r\n",
      "createdAt": "2021-04-30T19:11:51Z",
      "updatedAt": "2021-04-30T21:16:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c195673a79cf4665e4df11bf3c5768f0b5644ad9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/ingestion",
      "headRefOid": "478fdf435ec66df2f23189a301d6b775446e3189",
      "closedAt": "2021-04-30T20:02:49Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NTcxNTE1",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T19:49:55Z",
          "updatedAt": "2021-04-30T20:56:52Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Who assigns this identifier. Are 65k identifiers enough?",
              "createdAt": "2021-04-30T19:49:56Z",
              "updatedAt": "2021-04-30T20:56:52Z"
            },
            {
              "originalPosition": 43,
              "body": "UUIDs may be useful, albeit somewhat large? Also looks like `PAoTask.id` should be `PATask.id`.",
              "createdAt": "2021-04-30T19:52:21Z",
              "updatedAt": "2021-04-30T20:56:52Z"
            },
            {
              "originalPosition": 51,
              "body": "I think the `key_config` request should be broken out into it's own sub-section. ",
              "createdAt": "2021-04-30T20:44:26Z",
              "updatedAt": "2021-04-30T20:56:52Z"
            },
            {
              "originalPosition": 95,
              "body": "Should this be an array of helper shares, if there's more than a single helper?",
              "createdAt": "2021-04-30T20:45:09Z",
              "updatedAt": "2021-04-30T20:56:52Z"
            },
            {
              "originalPosition": 48,
              "body": "Suppose we're dealing with an anonymizing proxy: Is it possible for the parameters to be known ahead of time (e.g. hpke parameters shipped directly into client code) and to omit the first 2 stages of `[leader]/upload_start` and `[helper(s)]/key_config`?",
              "createdAt": "2021-04-30T20:49:52Z",
              "updatedAt": "2021-04-30T20:56:52Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjE3NjE3",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:01:51Z",
          "updatedAt": "2021-04-30T21:01:51Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "- Unspecified at this time, though I imagine it'll be the collector that assigns this.\r\n- I'm not sure if 65k is enough (see OPEN ISSUE below).",
              "createdAt": "2021-04-30T21:01:51Z",
              "updatedAt": "2021-04-30T21:01:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjE4NjUz",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:03:41Z",
          "updatedAt": "2021-04-30T21:03:42Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "+1 to UUID.",
              "createdAt": "2021-04-30T21:03:41Z",
              "updatedAt": "2021-04-30T21:03:42Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjE5MDMw",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:04:16Z",
          "updatedAt": "2021-04-30T21:04:16Z",
          "comments": [
            {
              "originalPosition": 51,
              "body": "Ack, will do.",
              "createdAt": "2021-04-30T21:04:16Z",
              "updatedAt": "2021-04-30T21:04:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjE5MzQ5",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:04:49Z",
          "updatedAt": "2021-04-30T21:04:50Z",
          "comments": [
            {
              "originalPosition": 95,
              "body": "Actually, there will only be one helper for this message.",
              "createdAt": "2021-04-30T21:04:49Z",
              "updatedAt": "2021-04-30T21:04:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjIwMDg3",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:06:14Z",
          "updatedAt": "2021-04-30T21:06:15Z",
          "comments": [
            {
              "originalPosition": 95,
              "body": "Oh I see, the helper url is set in the message. :+1:",
              "createdAt": "2021-04-30T21:06:15Z",
              "updatedAt": "2021-04-30T21:06:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjIxNzM0",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:09:13Z",
          "updatedAt": "2021-04-30T21:09:13Z",
          "comments": [
            {
              "originalPosition": 48,
              "body": "Yeah, that should be possible for most PA protocols. What's the added value of the anonymizing proxy? Unlinking any metadata, like UA, from the client IP?",
              "createdAt": "2021-04-30T21:09:13Z",
              "updatedAt": "2021-04-30T21:09:13Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ5NjI1NDM5",
          "commit": {
            "abbreviatedOid": "478fdf4"
          },
          "author": "acmiyaguchi",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-04-30T21:16:39Z",
          "updatedAt": "2021-04-30T21:16:39Z",
          "comments": [
            {
              "originalPosition": 48,
              "body": "Yeah, unlinking metadata like ip addresses, timestamps, and frequency of requests.\r\n\r\nI was thinking about ingestion services (e.g. the prio-server architecture) and whether it was possible to support that use-case with the http api proposed here. ",
              "createdAt": "2021-04-30T21:16:39Z",
              "updatedAt": "2021-04-30T21:16:39Z"
            }
          ]
        }
      ]
    },
    {
      "number": 35,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI5MjkzMTYw",
      "title": "Port to IETF I-D format",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/35",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "cc @ekr, @cjpatton ",
      "createdAt": "2021-05-03T18:49:45Z",
      "updatedAt": "2021-12-30T02:09:40Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c195673a79cf4665e4df11bf3c5768f0b5644ad9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/reformat",
      "headRefOid": "85d1284d7815b5d2ed30848984d64ebd1a479e98",
      "closedAt": "2021-05-03T18:54:19Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "(Wow, this was based on a way old version of main!)",
          "createdAt": "2021-05-03T18:54:30Z",
          "updatedAt": "2021-05-03T18:54:30Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 36,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI5MzAxMzI2",
      "title": "Port document and toolchain to IETF I-D format.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/36",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "cc @ekr, @cjpatton ",
      "createdAt": "2021-05-03T19:04:40Z",
      "updatedAt": "2021-12-30T02:09:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c195673a79cf4665e4df11bf3c5768f0b5644ad9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/i-d-format",
      "headRefOid": "d62a94af619e7f3605b2928cee45a9b30a595e56",
      "closedAt": "2021-05-03T20:25:53Z",
      "mergedAt": "2021-05-03T20:25:53Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "d40d3893c1112d00cc640c131a075022f90357a3"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "FYI, I'll fix the build after this is merged to main. (It might be due to me creating this on a branch that's not main, though I'm not entirely sure.)",
          "createdAt": "2021-05-03T19:07:47Z",
          "updatedAt": "2021-05-03T19:07:47Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNjcxNTcw",
          "commit": {
            "abbreviatedOid": "d62a94a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This dumps some text related to choosing parameters for Prio. I need to rework this text anyway, so I'm happy to add it back later.",
          "createdAt": "2021-05-03T19:53:42Z",
          "updatedAt": "2021-05-03T19:53:42Z",
          "comments": []
        }
      ]
    },
    {
      "number": 37,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI5MzU1NzYy",
      "title": "Create .nojekyll",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/37",
      "state": "CLOSED",
      "author": "aaomidi",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "If we're not using jekyll, we should opt out of the github pages stuff for it.",
      "createdAt": "2021-05-03T20:47:08Z",
      "updatedAt": "2021-12-30T02:09:53Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "gh-pages",
      "baseRefOid": "d38d2f153a85ff73a80441e1b9cbefaed97d0edb",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "Add-no-jekyll-file-to-gh-pages",
      "headRefOid": "ae5636d4eb9cf52fbbed7ecebf6ca625de4a0399",
      "closedAt": "2021-05-03T20:47:35Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [],
      "reviews": []
    },
    {
      "number": 38,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI5MzYyMjE3",
      "title": "Rework overview and add skeleton for other stuff",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/38",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-05-03T20:59:02Z",
      "updatedAt": "2021-06-17T21:15:23Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c27740e1e9ba4acd85fae807444d1c3278c3baf2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/upload-verify",
      "headRefOid": "468fff76c34853e555bf94856663a4822f4c8976",
      "closedAt": "2021-05-08T22:38:15Z",
      "mergedAt": "2021-05-08T22:38:15Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "3b0e4468c79a00c5c20d1228248c8688bfac776a"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "That seems fine.\n\n\n\nOn Mon, May 3, 2021 at 4:00 PM Christopher Patton ***@***.***>\nwrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In draft-pda-core.md\n> <https://github.com/abetterinternet/prio-documents/pull/38#discussion_r625418805>\n> :\n>\n> > -particular, a malicious client can corrupt the computation by submitting random\n> -integers instead of a proper secret sharing of a valid input.\n> -\n> -To solve this problem, Prio introduces a light-weight zero-knowledge proof\n> -system designed to operate on secret shared data. In addition to its input\n> -share, each client sends to each aggregator a share of a \"proof\" of the input's\n> -validity. The aggregators use these proof shares in a protocol designed to\n> -establish the input's validity, without leaking the input itself. We describe\n> -this input-validation protocol in detail in [[TODO:citeme]].\n> -\n> -## Assembling Reports\n> +**Hits.**\n> +A common PA task that can't be solved efficiently with Prio is the\n> +`t`-*heavy-hitters* problem {{BBCp21}}. In this setting, each user is in\n> +possession of a single `n`-bit string, and the goal is to compute the compute\n> +the set of strings that occur at least `t` times.\n>\n> How about: One reason Prio isn't applicable to the use case is that the\n> proof that is generated would be huge.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/pull/38#discussion_r625418805>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIJ5IQHZLSDXNCN5YB3TL4TKBANCNFSM44BP4QKA>\n> .\n>\n",
          "createdAt": "2021-05-03T23:07:00Z",
          "updatedAt": "2021-05-03T23:07:00Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzIwMjM0",
          "commit": {
            "abbreviatedOid": "e756234"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:01:07Z",
          "updatedAt": "2021-05-03T21:10:14Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "This is {{BBCp19}} in the document, referenced by DOI, so maybe we can drop this (or the other) so there aren't duplicates?",
              "createdAt": "2021-05-03T21:01:08Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 107,
              "body": "Small = 2?",
              "createdAt": "2021-05-03T21:03:16Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 117,
              "body": "```suggestion\r\nThe main cryptographic tool used for achieving this privacy goal is\r\n```",
              "createdAt": "2021-05-03T21:04:11Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 207,
              "body": "```suggestion\r\npossession of a single `n`-bit string, and the goal is to compute the compute the set\r\n```",
              "createdAt": "2021-05-03T21:05:38Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 229,
              "body": "Can we move these to security considerations? I don't think they're important here.",
              "createdAt": "2021-05-03T21:09:17Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 225,
              "body": "```suggestion\r\nprotocol that allows the aggregators to verify that\r\n```\r\n\r\nIn general, I think we ought to try and keep these technical terms in the security considerations where possible.",
              "createdAt": "2021-05-03T21:09:55Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzI5MjU4",
          "commit": {
            "abbreviatedOid": "e756234"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:13:59Z",
          "updatedAt": "2021-05-03T21:13:59Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "Small = O(1). Remember that the plan is to execute the protocol among multiple sets of two servers.",
              "createdAt": "2021-05-03T21:13:59Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzMyNzgy",
          "commit": {
            "abbreviatedOid": "e756234"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:19:20Z",
          "updatedAt": "2021-05-03T21:19:21Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "I disagree.",
              "createdAt": "2021-05-03T21:19:20Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzQwNzQ3",
          "commit": {
            "abbreviatedOid": "e756234"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:32:07Z",
          "updatedAt": "2021-05-03T21:32:07Z",
          "comments": [
            {
              "originalPosition": 225,
              "body": "Again, I disagree. \"ZKP\" is no less technical of a term than \"encryption\", which we use freely in this section. It's just that \"ZKP\" is less common.",
              "createdAt": "2021-05-03T21:32:07Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzQ2MDI1",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:41:00Z",
          "updatedAt": "2021-05-03T21:41:56Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "FWIW, I agree with Wood.  The point here is that there is a proof system that demonstrates validity. The details should go below.\r\n\r\n\r\n\r\n\r\n\r\n",
              "createdAt": "2021-05-03T21:41:00Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 225,
              "body": "I would split the difference here and say \"includes a zero-knowledge proof\". That is somewhat less formal than \"zero-knowledge proof system\" and I think clearer in this context.",
              "createdAt": "2021-05-03T21:41:48Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzUwMzU3",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:48:26Z",
          "updatedAt": "2021-05-03T21:48:26Z",
          "comments": [
            {
              "originalPosition": 225,
              "body": "Or, for that matter, \"proof\"",
              "createdAt": "2021-05-03T21:48:26Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzUxMjgw",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:50:10Z",
          "updatedAt": "2021-05-03T21:50:10Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "Yes, I'm aware. My point was that the core protocol runs with s=2 servers.",
              "createdAt": "2021-05-03T21:50:10Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzUxNTI2",
          "commit": {
            "abbreviatedOid": "e756234"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T21:50:36Z",
          "updatedAt": "2021-05-03T21:50:47Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "Lightweight,",
              "createdAt": "2021-05-03T21:50:36Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzUyMjM0",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-05-03T21:51:48Z",
          "updatedAt": "2021-05-03T21:57:35Z",
          "comments": [
            {
              "originalPosition": 39,
              "body": "```suggestion\r\nthemselves. This is made possible by distributing the computation among the servers in\r\n```",
              "createdAt": "2021-05-03T21:51:49Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 54,
              "body": "I would rework this and the previous graf. It's not conventional in specifications to provide this kind of \"in this section, we do X, then in the next section we do Y\". Rather,  we just try to make the specification evolve clearly.",
              "createdAt": "2021-05-03T21:53:23Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 106,
              "body": "```suggestion\r\nsecret sharing*. Rather than send its input in the clear, each client splits\r\n```",
              "createdAt": "2021-05-03T21:53:48Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 109,
              "body": "```suggestion\r\n- It's impossible to deduce the measurement without knowing *all* of the\r\n```",
              "createdAt": "2021-05-03T21:54:02Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 111,
              "body": "```suggestion\r\n- It allows the aggregators to compute the final output by first adding\r\n```\r\n\r\nBulleted lists do not need verbal indices.",
              "createdAt": "2021-05-03T21:54:32Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 158,
              "body": "I think we could do without the \"prime field\" bit here. It's not necessary to explain what Prio is for.",
              "createdAt": "2021-05-03T21:55:43Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 196,
              "body": "```With Prio, it would be necessary to encode 2^n values, where all but one is zero. This is clearly inefficient.```",
              "createdAt": "2021-05-03T21:56:35Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            },
            {
              "originalPosition": 200,
              "body": "I would not actually talk about DPFs here. These sections should treat the protocols as black boxes.",
              "createdAt": "2021-05-03T21:57:01Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzU5MDcx",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:03:57Z",
          "updatedAt": "2021-05-03T22:03:57Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "Ah, gotcha. I still think \"small set of servers\" is more accurate than \"two servers\" here, since this section is meant to describe the general of case of one leader and multiple helpers.",
              "createdAt": "2021-05-03T22:03:57Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzY1NTg0",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:16:43Z",
          "updatedAt": "2021-05-03T22:16:44Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "Good catch! Thanks.",
              "createdAt": "2021-05-03T22:16:43Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzY2OTMz",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:19:35Z",
          "updatedAt": "2021-05-03T22:19:35Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "Is that true? I believe there's more to it than this. To avoid misleading the reader I would just say less here and let them take it on faith that Prio isn't usable for heavy hitters.",
              "createdAt": "2021-05-03T22:19:35Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzY3NTYx",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:20:44Z",
          "updatedAt": "2021-05-03T22:20:44Z",
          "comments": [
            {
              "originalPosition": 200,
              "body": "+1 to not mentioning DPFs.",
              "createdAt": "2021-05-03T22:20:44Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzY3OTg0",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:21:37Z",
          "updatedAt": "2021-05-03T22:21:38Z",
          "comments": [
            {
              "originalPosition": 158,
              "body": "What would you put instead?",
              "createdAt": "2021-05-03T22:21:37Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzY4Mjg3",
          "commit": {
            "abbreviatedOid": "828c850"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:22:13Z",
          "updatedAt": "2021-05-03T22:22:13Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "Ack, I'll remove the paragraphs for now.",
              "createdAt": "2021-05-03T22:22:13Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzY5OTQ4",
          "commit": {
            "abbreviatedOid": "f7bdcb3"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:25:29Z",
          "updatedAt": "2021-05-03T22:25:29Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "Why?",
              "createdAt": "2021-05-03T22:25:29Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzcwMTA4",
          "commit": {
            "abbreviatedOid": "f7bdcb3"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:25:48Z",
          "updatedAt": "2021-05-03T22:25:48Z",
          "comments": [
            {
              "originalPosition": 158,
              "body": "I would remove the entire sentence.",
              "createdAt": "2021-05-03T22:25:48Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzcwNjU3",
          "commit": {
            "abbreviatedOid": "f7bdcb3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:27:00Z",
          "updatedAt": "2021-05-03T22:27:00Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "I would push back as follows: several weeks ago I had to explain to @ekr that input validation fails to detect an invalid input with non-zero probability, but that this probability could be bounded. In general, this is a property of ZKP systems: the ZKP system is *sound* if verification correctly detects invalid inputs except with small probability. I don't think this is overly technical. Moreover, it's necessary to explain to the reader that input validation may fail. In my view, we might as well just say what the cryptographic technique is. \"Zero-knowlege proof\" is just another thing we use, like \"encryption\" or \"secret sharing\". Why not just say it?",
              "createdAt": "2021-05-03T22:27:00Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzczNDcx",
          "commit": {
            "abbreviatedOid": "f7bdcb3"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:32:53Z",
          "updatedAt": "2021-05-03T22:32:53Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "Nobody is saying that you don't say it. Rather, we're saying that you should say it *later*, because it's a detail and is not needed to understand the big picture.\r\n\r\nThe more detail you have in this section the harder it is for the reader to get the overall idea.\r\n\r\n\r\n\r\n\r\n",
              "createdAt": "2021-05-03T22:32:53Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzczNzg1",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:33:36Z",
          "updatedAt": "2021-05-03T22:33:36Z",
          "comments": [
            {
              "originalPosition": 225,
              "body": "I reworked it to avoid using the word \"system\".",
              "createdAt": "2021-05-03T22:33:36Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzczOTEy",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:33:51Z",
          "updatedAt": "2021-05-03T22:33:51Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "As with the other text, I think the purpose of this section is to give people the right intuition.\r\n\r\nWhat's misleading about what I said?",
              "createdAt": "2021-05-03T22:33:51Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzc5MjMw",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:46:03Z",
          "updatedAt": "2021-05-03T22:46:03Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "In general, I think there's a danger here of contorting the language so much in order to avoid using \"advanced\" crypto terminology that we confuse the reader.",
              "createdAt": "2021-05-03T22:46:03Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzgxMjMy",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:50:43Z",
          "updatedAt": "2021-05-03T22:50:44Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "There's two things: \r\n1. Instead of \"encode 2^n values\" it would be more accurate to say \"generate a proof of length `O(some_function_of(n))`\". where `some_function_of(.)` depends on the proof strategy. I suppose we could just say \"generate a huge proof\".\r\n2. The \"collect\" step doesn't work the same way. It's not a simple matter of adding up shares; the collector has to interact with the aggregators over several rounds.",
              "createdAt": "2021-05-03T22:50:43Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzgxNDYz",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:51:16Z",
          "updatedAt": "2021-05-03T22:51:16Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "That may be true, but I think Ekr's point is that we want to try and decrease cognitive load on the reader (implementer). They should exactly what they need to know to implement the protocol. Everything else can be described elsewhere!",
              "createdAt": "2021-05-03T22:51:16Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzgyMTYz",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:52:48Z",
          "updatedAt": "2021-05-03T22:52:48Z",
          "comments": [
            {
              "originalPosition": 158,
              "body": "That works.",
              "createdAt": "2021-05-03T22:52:48Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzgyNzIy",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:53:59Z",
          "updatedAt": "2021-05-03T22:54:00Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "I think the intuition (\"should I use Prio or HH?\") is good to add here. (@ekr's suggestion seems to clarify the \"this can't be solved efficiently claim\" with a simple example, so I'd support adding it)",
              "createdAt": "2021-05-03T22:53:59Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzg0ODA4",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T22:58:40Z",
          "updatedAt": "2021-05-03T22:58:41Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "I think implementers are going to have to understand the term \"ZKP\" to the same depth they understand \"encrypt\" or \"secret share\". I'm defining \"ZKP\" here because w'er also defining \"secret sharing\" here. I don't think it makes sense to define one but not the other. They are both equally important at this stage.\r\n\r\nIn any case, this is a purely editorial point. I would suggest one of two paths forward: (1) we punt on this conversation and leave an \"OPEN ISSUE\" for cleaning up the presentation. (We'll have to do this at some point anyway.) (2) remove the text completely and leave a \"TODO\" for describing input validation.",
              "createdAt": "2021-05-03T22:58:40Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzg1NjUw",
          "commit": {
            "abbreviatedOid": "df23def"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T23:00:33Z",
          "updatedAt": "2021-05-03T23:00:34Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "How about: `One reason Prio isn't applicable to the use case is that the proof that is generated would be huge.`",
              "createdAt": "2021-05-03T23:00:33Z",
              "updatedAt": "2021-05-08T22:37:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzkyMjgy",
          "commit": {
            "abbreviatedOid": "83e2a45"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T23:16:36Z",
          "updatedAt": "2021-05-03T23:16:36Z",
          "comments": [
            {
              "originalPosition": 196,
              "body": "Done.",
              "createdAt": "2021-05-03T23:16:36Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzk0MDY1",
          "commit": {
            "abbreviatedOid": "83e2a45"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T23:21:27Z",
          "updatedAt": "2021-05-03T23:21:27Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "Or (3) does someone want to propose alternate text? The things I think are important to convey are (a) that the verification protocol does not leak the secret inputs to the aggregators and (b) the verification protocol may fail to detect invalid inputs with small probability.",
              "createdAt": "2021-05-03T23:21:27Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwNzk3NjU5",
          "commit": {
            "abbreviatedOid": "83e2a45"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T23:30:58Z",
          "updatedAt": "2021-05-03T23:30:59Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "@cjpatton I think we're talking past each other: I agree that this is useful text, I just don't think it belongs *here*.\r\n\r\nThe generic structure of this kind of specification is:\r\n\r\n- Problem statement\r\n- Overview of the solution that just is enough to give people the idea\r\n- Detailed protocol specification\r\n- Security considerations and the like\r\n\r\nThe key idea with point (2) is to be high level. That means avoiding detail which isn't absolutely necessary. In particular, why do we need to tell users at this point that it might fail to detect invalid inputs? They don't need to know that to understand the protocol specification. See https://tools.ietf.org/rfcmarkup?doc=4101 for more on this.",
              "createdAt": "2021-05-03T23:30:58Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwODA3NzM0",
          "commit": {
            "abbreviatedOid": "83e2a45"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-03T23:59:17Z",
          "updatedAt": "2021-05-03T23:59:17Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "> The key idea with point (2) is to be high level. That means avoiding detail which isn't absolutely necessary. In particular, why do we need to tell users at this point that it might fail to detect invalid inputs?\r\n\r\nI guess I don't quite understand what \"absolutely necessary\" means. It's important to know that the input validation protocol can be attacked, since mitigating such attacks is one of the core design criteria for this class of protocols. IMO we have to mention it up front.\r\n\r\nTowards finding common ground, I'm wondering if it's strictly about using the term \"ZKP\"? Wood suggested that I might just be trying to use a fancy crypto term here. I assure you that I'm not. I'm just trying to provide the context one needs to understand the system. Using the term itself is useful because it connects what we're doing to a fairly well-known body of work. I don't want to give people the impression that the crypto herein is one-off.\r\n\r\n> They don't need to know that to understand the protocol specification. See https://tools.ietf.org/rfcmarkup?doc=4101 for more on this.\r\n\r\nSo, up until this point, I haven't been thinking of this document as a formal standard. Based on the feedback I'm getting from you and Wood, it seems like I might need to recalibrate. I thought the goal of this change was merely to get us all on the same page about overall shape of the protocol, not (yet!) to provide a reference for others to implement. It's probably worth working this out in the next design call.\r\n\r\nBy the way, I would love to participate in the task of shaping this into a formal standard. I just didn't think that's what we were doing yet.",
              "createdAt": "2021-05-03T23:59:17Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwODEwNTIw",
          "commit": {
            "abbreviatedOid": "83e2a45"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-04T00:07:25Z",
          "updatedAt": "2021-05-04T00:07:26Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "I don't care much one way or the other about the term \"zero-knowledge proof\". I think you could say \"proof\" just as well. What I am saying is that your list of properties is unnecessary here. You can just say:\r\n\r\n\"To solve this problem, in each PA protocol, the client generates a\r\n*zero-knowledge proof (ZKP)* of its input's validity, which the aggregators use\r\nto verify that their shares correspond to as valid input.\"\r\n\r\nAnd stop.  Or maybe say \"This proof can be verified without learning the input\".\r\n\r\nYou then move the properties to the Security Considerations. If you must, you can put a pointer to that here. Anyone who knows enough about ZKPs to care about the details can check there, and anyone who doesn't will read the text I have proposed and get substantively the right impression.\r\n\r\n\r\n> Using the term itself is useful because it connects what we're doing to a fairly well-known body of work. I don't want to give people the impression that the crypto herein is one-off.\r\n\r\nThat can be done later in the specification\r\n\r\n> So, up until this point, I haven't been thinking of this document as a formal standard. Based on the feedback I'm getting from you and Wood, it seems like I might need to recalibrate. I thought the goal of this change was merely to get us all on the same page about overall shape of the protocol, not (yet!) to provide a reference for others to implement. It's probably worth working this out in the next design call.\r\n\r\nWell, I don't understand the point of this text at all if it's not targeted at an eventual specification. But yes, I do think this should be an attempt to write down pieces of a specification to use as input into the standards process.\r\n\r\n\r\n\r\n\r\n",
              "createdAt": "2021-05-04T00:07:25Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUwODE1ODkz",
          "commit": {
            "abbreviatedOid": "248a0d6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-04T00:24:21Z",
          "updatedAt": "2021-05-04T00:24:22Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "How's this:\r\n```\r\nTo solve this problem, in each PA protocol, the client generates a\r\nzero-knowledge proof of its input's validity that the aggregators use\r\nto verify that their shares correspond to as valid input. The verification\r\nprocedure is designed to ensure that the aggregators learn nothing about the\r\ninput beyond its validity.\r\n```\r\nTo me it's useful to use the term \"zero-knowledge\" because it signals what's under the hood. If you want I can remove it.\r\n\r\n> Well, I don't understand the point of this text at all if it's not targeted at an eventual specification. But yes, I do think this should be an attempt to write down pieces of a specification to use as input into the standards process.\r\n\r\nOf course that's what we're doing :) I meant more that I don't see why we need to settle on the presentation right now. I'd prefer to focus on the protocol itself and less on how it's presented. I figure that we will need to rework this doc quite a lot before we consider trying to get buy in from the wider community.",
              "createdAt": "2021-05-04T00:24:22Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUxNTk4ODE1",
          "commit": {
            "abbreviatedOid": "0c90bb5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-04T19:39:20Z",
          "updatedAt": "2021-05-04T19:39:21Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "Sounds good!",
              "createdAt": "2021-05-04T19:39:21Z",
              "updatedAt": "2021-05-08T22:37:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUzOTI4MTUw",
          "commit": {
            "abbreviatedOid": "2f8a6c9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-05-06T21:56:01Z",
          "updatedAt": "2021-05-06T21:56:01Z",
          "comments": []
        }
      ]
    },
    {
      "number": 40,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI5NDU0OTAw",
      "title": "Fixup gh-pages",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/40",
      "state": "MERGED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "No idea if this is going to fix your problems here, but it might.  Something went pretty badly wrong somewhere along the line and this might help fix it.\r\n\r\nThe fallback is to completely remove the branch and start over.  There's a script that you can use for that, but it's pretty scary.",
      "createdAt": "2021-05-04T01:04:04Z",
      "updatedAt": "2021-05-04T02:53:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "gh-pages",
      "baseRefOid": "d38d2f153a85ff73a80441e1b9cbefaed97d0edb",
      "headRepository": "martinthomson/ppm-dap",
      "headRefName": "gh-pages",
      "headRefOid": "7929b895297cb1632640368a34aebd3d74afd167",
      "closedAt": "2021-05-04T02:53:12Z",
      "mergedAt": "2021-05-04T02:53:12Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "f617ecc40d5af7e9ac1353a1f594737f748c652b"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, I tried blowing away gh-pages and trying from scratch. Maybe your fix will stick?",
          "createdAt": "2021-05-04T02:53:09Z",
          "updatedAt": "2021-05-04T02:53:09Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 42,
      "id": "MDExOlB1bGxSZXF1ZXN0NjI5NDU1ODQx",
      "title": "Remove circle config",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/42",
      "state": "MERGED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "You don't need it.",
      "createdAt": "2021-05-04T01:07:47Z",
      "updatedAt": "2021-05-04T02:52:20Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "dd0d113fd6daf2d0fa8d9c9c661bcad2c6dafee8",
      "headRepository": "martinthomson/ppm-dap",
      "headRefName": "no-circle",
      "headRefOid": "2387049ae46af503b29ac4aafe0cfa01185ee15a",
      "closedAt": "2021-05-04T02:52:20Z",
      "mergedAt": "2021-05-04T02:52:20Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "c27740e1e9ba4acd85fae807444d1c3278c3baf2"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 43,
      "id": "MDExOlB1bGxSZXF1ZXN0NjMwNjg1NDg1",
      "title": "Specify the upload and verify phases of the PA protocol",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/43",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #38.\r\n\r\n@chris-wood, @ekr ",
      "createdAt": "2021-05-05T15:19:56Z",
      "updatedAt": "2021-06-17T21:15:23Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c9b7f30edf33f31979f514a756a8eeeb5fded787",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/upload-verify-2",
      "headRefOid": "e962ba20ae6cd49cce87f491022fd33c861a8ef6",
      "closedAt": "2021-06-03T21:18:33Z",
      "mergedAt": "2021-06-03T21:18:33Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "f928255838517e6254d48d2d3e686da2d8346fa5"
      },
      "comments": [
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I have a high level concern with the architecture here, following up from our meeting on Wednesday related to heavy hitters.\r\n\r\n**TL;DR evaluating a hierarchy at only a subset of levels (which are specified at query time) is not optimal for performance if the verification step does not have access to those specified levels.**\r\n\r\nFor heavy hitters, we are considering a model where records form a binary tree, where aggregates can be reported at different levels of the tree. There are a few possible ways this could go:\r\n1. For every record, verify and evaluate the entire binary tree (i.e. bit-by-bit evaluation)\r\n2. For every record, verify and evaluate only a subset of the tree (at the limit, this can look more like Prio which only evaluates the last level of the tree)\r\n  2a. The subset of the tree to evaluate is configured within a particular record at record-creation time\r\n  2b. The subset of the tree to evaluate is configured dynamically at _collection / aggregation_ time\r\n\r\nFor our use-case (discussed in https://github.com/abetterinternet/prio-documents/issues/18#issuecomment-801248636), we are interested in (2b), for a few reasons:\r\n- (1) is potentially inefficient from both a performance and accuracy POV in that it requires more rounds, and additionally when used with differential privacy requires splitting a DP budget across all levels. Some prefixes might just not be interesting to the caller so we shouldn't waste privacy budget / compute on them.\r\n- (2a) is problematic in two dimensions.\r\n  - Embedding configuration in records may compromise user privacy if the configurations can be set by an adversary colluding with one of the aggregators (e.g. you can leak log2(n choose k) bits of information if you allow any subset of k prefixes for an n-bit domain).\r\n  - Levels must be pre-specified at record-creation time, which is non-ideal if new information comes up at collection time which informs how aggregation should work (e.g. realizing you have fewer records than expected so it is better to focus on querying only up to a given prefix).\r\n\r\nHowever, I think (2b) runs into a few issues with this architecture that separates verification from collection, in that without knowing the specific levels to evaluate, we run the risk of spending unnecessary compute verifying levels that will never end up being used. The protocol is more efficient (less computation, fewer rounds) if we can verify multiple levels at once (i.e. the levels the collector cares about).\r\n\r\nPossible solutions:\r\n1. Add a step where the collector and leader communicate prior to verification (but keep verification and collection separate)\r\n2. More tightly couple \"verify\" and \"collect\" stages, so that verification can be done based on communication with the collector.\r\n\r\nIn our setting, the \"leader\" and \"collector\" roles are somewhat merged which aligns with option (1), but it seems like a good idea to make the general architecture robust to this use-case.\r\n\r\ncc @schoppmp who is working on our C++ implementation of IDPFs.",
          "createdAt": "2021-05-06T18:33:58Z",
          "updatedAt": "2021-05-07T16:01:54Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@csharrison can you please file an issue with your comment? It doesn't seem to apply to this PR, which punts on the aggregate protocol. ",
          "createdAt": "2021-05-06T23:49:06Z",
          "updatedAt": "2021-05-06T23:49:06Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Ah yes sorry, I included it here because I thought it may impact design for upload / verify. Let me delete and re-upload as an issue.",
          "createdAt": "2021-05-07T15:58:27Z",
          "updatedAt": "2021-05-07T15:58:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "TODO(@cjpatton) Allow an aribtrary number of verify requests",
          "createdAt": "2021-05-12T17:41:26Z",
          "updatedAt": "2021-05-12T17:41:26Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUzOTYxMDc0",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-06T23:03:19Z",
          "updatedAt": "2021-05-06T23:46:51Z",
          "comments": [
            {
              "originalPosition": 68,
              "body": "This probably needs to be larger, say, `uint8 id[32]`?",
              "createdAt": "2021-05-06T23:03:19Z",
              "updatedAt": "2021-05-11T16:35:44Z"
            },
            {
              "originalPosition": 57,
              "body": "This seems out of place -- was it supposed to be move to some later point in the document?",
              "createdAt": "2021-05-06T23:03:40Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 111,
              "body": "Can we lift the config from [OHTTP/ECH](https://unicorn-wg.github.io/oblivious-http/draft-thomson-http-oblivious.html#name-key-configuration-encoding)?",
              "createdAt": "2021-05-06T23:05:18Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 135,
              "body": "Should we break out preconditions for each sub-protocol (upload, verify, aggregate), and then list them in their relevant section(s)?",
              "createdAt": "2021-05-06T23:06:28Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 150,
              "body": "I envisioned this first request -- parameter fetching -- to be part of the configuration. Is there a scenario where clients would know the PATask but not the corresponding PAParam? ",
              "createdAt": "2021-05-06T23:09:46Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 159,
              "body": "Yeah, it does. \ud83d\udc4d ",
              "createdAt": "2021-05-06T23:10:11Z",
              "updatedAt": "2021-05-11T16:35:44Z"
            },
            {
              "originalPosition": 157,
              "body": "\ud83d\udc4d agree with this, though it seems deployment specific? I might include this as an option for some deployments, maybe in an appendix.",
              "createdAt": "2021-05-06T23:10:23Z",
              "updatedAt": "2021-05-11T16:35:44Z"
            },
            {
              "originalPosition": 176,
              "body": "The `id` seems to tie the upload start and finish together. A couple questions:\r\n1. Can we drop it if we turn this into a single HTTP request?\r\n2. if we keep both requests, why do they need to be tied together?",
              "createdAt": "2021-05-06T23:11:17Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 188,
              "body": "```suggestion\r\n  Url helper_urls<1..2^16-1>;\r\n```\r\n\r\nAlso, has `Url` been defined yet?",
              "createdAt": "2021-05-06T23:12:52Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 215,
              "body": "Why is this alert important? What is the leader to do with this information?",
              "createdAt": "2021-05-06T23:15:49Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 253,
              "body": "```suggestion\r\n  opaque enc<1..2^16-1>;     // Encapsulated HPKE context\r\n```",
              "createdAt": "2021-05-06T23:17:16Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 254,
              "body": "Why is this allowed to be empty? ",
              "createdAt": "2021-05-06T23:17:25Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 265,
              "body": "What does the client do if one helper returns a 200 and another returns an error?",
              "createdAt": "2021-05-06T23:28:58Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 274,
              "body": "```suggestion\r\nthe *verify finish request*. The contents of each request depend on the\r\n```",
              "createdAt": "2021-05-06T23:30:04Z",
              "updatedAt": "2021-05-11T16:35:44Z"
            },
            {
              "originalPosition": 283,
              "body": "Is the set here the batch in total, or the set of individual reports?",
              "createdAt": "2021-05-06T23:31:33Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 312,
              "body": "We've not defined joint randomness, so I might drop this example text entirely. The preceding text seems clear enough as-is.",
              "createdAt": "2021-05-06T23:35:50Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 330,
              "body": "What does the leader do for a batch in this situation?",
              "createdAt": "2021-05-06T23:36:30Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 389,
              "body": "Is there missing text here? What does the helper actually do with the FinishReq message?",
              "createdAt": "2021-05-06T23:37:55Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 410,
              "body": "To clarify, this means the helper knows the valid bit after sending `PAVerifyFinishResp`, right?",
              "createdAt": "2021-05-06T23:39:23Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 424,
              "body": "Is this what we want to do if the helper fails to process one share of a batch?",
              "createdAt": "2021-05-06T23:40:09Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 438,
              "body": "```suggestion\r\nrequest, the response status is 400. When sent in a request to an\r\n```",
              "createdAt": "2021-05-06T23:42:33Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            },
            {
              "originalPosition": 276,
              "body": "```suggestion\r\nreport submitted during the upload protocol, allowing the helper to run the verification protocol statelessly.\r\n```",
              "createdAt": "2021-05-06T23:46:27Z",
              "updatedAt": "2021-05-11T16:35:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU1OTI0MTg0",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-10T18:03:32Z",
          "updatedAt": "2021-05-10T18:03:32Z",
          "comments": [
            {
              "originalPosition": 488,
              "body": "This should also have `PAParam`, right? How else does the client get that?",
              "createdAt": "2021-05-10T18:03:32Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODYzNTE3",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:19:51Z",
          "updatedAt": "2021-05-11T15:19:52Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "No, it was meant to go here.",
              "createdAt": "2021-05-11T15:19:51Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODY3Njg2",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:23:27Z",
          "updatedAt": "2021-05-11T15:23:27Z",
          "comments": [
            {
              "originalPosition": 111,
              "body": "The delta would be adding support for multiple HPKE ciphersuites. Although I see the value of aligning the structures across specs, I'd also like to push back on adding agility for now. I've added a TODO for resolving this later.",
              "createdAt": "2021-05-11T15:23:27Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODc0NzE1",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:26:12Z",
          "updatedAt": "2021-05-11T15:26:13Z",
          "comments": [
            {
              "originalPosition": 135,
              "body": "I agree that would be clearer. However, addressing #44 will require us to change this three-phase structure in some way. I've added a TODO to address this later.",
              "createdAt": "2021-05-11T15:26:12Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODgyMjA0",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:29:02Z",
          "updatedAt": "2021-05-11T15:29:03Z",
          "comments": [
            {
              "originalPosition": 150,
              "body": "Yes I think so. For aggregating mean/var of a sequence of integers, there are two parameters that might vary between uploads: the of the sequence and the length of each integer in bits.",
              "createdAt": "2021-05-11T15:29:03Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODg0OTQ2",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:30:48Z",
          "updatedAt": "2021-05-11T15:30:48Z",
          "comments": [
            {
              "originalPosition": 159,
              "body": "Thanks for confirming. Moved this from an OPEN ISSUE to a NOTE. (We can massage it into text later.)",
              "createdAt": "2021-05-11T15:30:48Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODk3NjAx",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:41:49Z",
          "updatedAt": "2021-05-11T15:41:49Z",
          "comments": [
            {
              "originalPosition": 176,
              "body": ">  The id seems to tie the upload start and finish together. A couple questions:\r\n>\r\n>     1. Can we drop it if we turn this into a single HTTP request?\r\n\r\nFor some Prio proof systems, it's useful for the leader to send the client a \"challenge\" that it'll use to generate the proof. One way to use the id is as a way of generating the challenge in a way that's unique for each upload.\r\n\r\n>     2. if we keep both requests, why do they need to be tied together?\r\n\r\nRetransmitting the id allows the two HTTP requests to be handled without carrying state between them.\r\n\r\n",
              "createdAt": "2021-05-11T15:41:49Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2ODk5OTM5",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:43:53Z",
          "updatedAt": "2021-05-11T15:43:54Z",
          "comments": [
            {
              "originalPosition": 188,
              "body": "It hasn't! Added a definition.\r\n\r\nI'm not taking the suggestion because I don't want to get nit-picky about size ranges yet. This is something that's likely to change a lot.",
              "createdAt": "2021-05-11T15:43:54Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2OTAwODU1",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:44:44Z",
          "updatedAt": "2021-05-11T15:44:44Z",
          "comments": [
            {
              "originalPosition": 215,
              "body": "If lots of clients abort this way, then it's a signal that something is amiss. For example, it may be that none of the helpers it is advertising are online.",
              "createdAt": "2021-05-11T15:44:44Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2OTA0MDg3",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:47:40Z",
          "updatedAt": "2021-05-11T15:47:41Z",
          "comments": [
            {
              "originalPosition": 265,
              "body": "Hm, not sure what you mean. The upload finish request is made to the leader, not the helper.",
              "createdAt": "2021-05-11T15:47:41Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU2OTIwODE2",
          "commit": {
            "abbreviatedOid": "e2b3273"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-11T15:58:50Z",
          "updatedAt": "2021-05-11T15:58:50Z",
          "comments": [
            {
              "originalPosition": 330,
              "body": "Ah, good point. This might happen, say, if the helper advertises the wrong key config during the upload phase. The shares are useless if the helper can't decrypt them, so I don't think the leader has a choice other than to abort. This falls into the bucket of situations that re-running the protocol with multiple helpers is meant to address.",
              "createdAt": "2021-05-11T15:58:50Z",
              "updatedAt": "2021-05-11T16:36:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjI3NzUy",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:13:21Z",
          "updatedAt": "2021-05-12T18:13:21Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "Hmm, okay. It just feels out of place. We can always move later.",
              "createdAt": "2021-05-12T18:13:21Z",
              "updatedAt": "2021-05-12T18:13:21Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjI3ODUz",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:13:28Z",
          "updatedAt": "2021-05-12T18:13:29Z",
          "comments": [
            {
              "originalPosition": 135,
              "body": "\ud83d\udc4d ",
              "createdAt": "2021-05-12T18:13:28Z",
              "updatedAt": "2021-05-12T18:13:29Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjI3OTk0",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:13:39Z",
          "updatedAt": "2021-05-12T18:13:39Z",
          "comments": [
            {
              "originalPosition": 150,
              "body": "\ud83d\udc4d ",
              "createdAt": "2021-05-12T18:13:39Z",
              "updatedAt": "2021-05-12T18:13:39Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjI4MzA3",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:13:59Z",
          "updatedAt": "2021-05-12T18:13:59Z",
          "comments": [
            {
              "originalPosition": 188,
              "body": "Well, can the helper_urls be empty? If not, then we should take the suggestion. ",
              "createdAt": "2021-05-12T18:13:59Z",
              "updatedAt": "2021-05-12T18:13:59Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjMwMDkw",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:15:53Z",
          "updatedAt": "2021-05-12T18:15:53Z",
          "comments": [
            {
              "originalPosition": 215,
              "body": "Can't the leader determine if helpers are \"online\"? This seems to reveal information that's specific to clients. Imagine, for example, that clients are prohibited from talking to helpers but not the leader. Is it OK that leaders learn that about a client? I'm not sure, so I'd be inclined to remove this unless we have a concrete use case.",
              "createdAt": "2021-05-12T18:15:53Z",
              "updatedAt": "2021-05-12T18:15:53Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjMzNDU2",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:17:11Z",
          "updatedAt": "2021-05-12T18:17:11Z",
          "comments": [
            {
              "originalPosition": 265,
              "body": "The protocol (as specified here) allows n>1 helpers to be used, right? (We may just only ever use one in practice.) I'm asking what we do if there are n>1 helpers and they all don't return 200. Does that clarify?",
              "createdAt": "2021-05-12T18:17:11Z",
              "updatedAt": "2021-05-12T18:17:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjU4MjM0Nzg2",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-12T18:17:40Z",
          "updatedAt": "2021-05-12T18:17:41Z",
          "comments": [
            {
              "originalPosition": 330,
              "body": "Yeah, we might want to note this (or flag a TODO to spell it out later).",
              "createdAt": "2021-05-12T18:17:40Z",
              "updatedAt": "2021-05-12T18:17:41Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjYwMTM5Njk1",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-14T19:25:40Z",
          "updatedAt": "2021-05-14T19:25:40Z",
          "comments": [
            {
              "originalPosition": 188,
              "body": "Well, a Url is itself a u16-prefixed string, so I guess the minimum is actually 2. But that begs the question of whether the only Url can be empty or not. If not, then it needs be 3. But is a length-1 URL a valid URL?\r\n\r\nAgain, I don't think it's worth being picky about this right now.",
              "createdAt": "2021-05-14T19:25:40Z",
              "updatedAt": "2021-05-14T19:26:34Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjYwMTQzNjQ5",
          "commit": {
            "abbreviatedOid": "1cc6235"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Post-prototype clean-up.",
          "createdAt": "2021-05-14T19:31:46Z",
          "updatedAt": "2021-05-14T22:22:32Z",
          "comments": [
            {
              "originalPosition": 215,
              "body": "I think it's OK, but I think we should cut off the discussion here and deal with it later. I'll leave your comment as an open issue.",
              "createdAt": "2021-05-14T19:31:46Z",
              "updatedAt": "2021-05-14T22:22:32Z"
            },
            {
              "originalPosition": 265,
              "body": "These requests are sent to the leader, not each helper. There's nothing for the client to do except abort. If, how, and when it retries is an open question. I suspect the answer will be deployment-specific.",
              "createdAt": "2021-05-14T19:34:56Z",
              "updatedAt": "2021-05-14T22:22:32Z"
            },
            {
              "originalPosition": 283,
              "body": "The verify phase runs several instances of the input-validation protocol in parallel. The \"set of valid client inputs\" corresponds to the set of inputs validated in the current flow.",
              "createdAt": "2021-05-14T19:37:25Z",
              "updatedAt": "2021-05-14T22:22:32Z"
            },
            {
              "originalPosition": 389,
              "body": "Described below.",
              "createdAt": "2021-05-14T19:41:24Z",
              "updatedAt": "2021-05-14T22:22:32Z"
            },
            {
              "originalPosition": 410,
              "body": "Right.",
              "createdAt": "2021-05-14T19:41:56Z",
              "updatedAt": "2021-05-14T22:22:32Z"
            },
            {
              "originalPosition": 424,
              "body": "It depends on how the helper fails, right? If it fails to send VerifyFinishResp, then the leader can't determine the inputs' valididty and must abort. If the helper sends VerifyFinishResp, but fails to store its input shares, the leader won't know that the batch has been corrupted. Either failure scenario can be addressed by running with multiple helpers.",
              "createdAt": "2021-05-14T19:45:10Z",
              "updatedAt": "2021-05-14T22:22:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY2OTkyOTY3",
          "commit": {
            "abbreviatedOid": "5f9c72f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-24T17:22:56Z",
          "updatedAt": "2021-05-24T18:15:39Z",
          "comments": [
            {
              "originalPosition": 53,
              "body": "```suggestion\r\nEach round of the protocol corresponds to an HTTP request and response. Use of\r\nHTTPS is REQUIRED for each transaction. The server MUST authenticate to the client\r\nfor each connection.\r\n```",
              "createdAt": "2021-05-24T17:22:56Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 89,
              "body": "```suggestion\r\n```\r\nI think we can comfortably drop this.",
              "createdAt": "2021-05-24T17:24:36Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 94,
              "body": "```suggestion\r\nid* in the remainder. This value is decided by the collector and configured for clients and \r\naggregators out-of-band. Clients and aggregators MUST ignore new `PATask` values if the\r\nid conflicts with an existing `PATask`.\r\n```",
              "createdAt": "2021-05-24T17:25:18Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 96,
              "body": "Do we want to _negotiate_ it, or rather allow it to be configured some way? I think we ought to just drop this text.",
              "createdAt": "2021-05-24T17:25:58Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 118,
              "body": "```suggestion\r\nThe `batch_size` field encodes the *batch size*, the minimum number of input shares\r\n```",
              "createdAt": "2021-05-24T17:27:51Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 125,
              "body": "```suggestion\r\nThis protocol uses HPKE for public-key encryption {{!I-D.irtf-cfrg-hpke}}.  Each\r\n```",
              "createdAt": "2021-05-24T17:28:10Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 146,
              "body": "```suggestion\r\nWe call this the helper's *key configuration*. The key configuration is used to\r\n```",
              "createdAt": "2021-05-24T17:28:38Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 147,
              "body": "```suggestion\r\nset up a base-mode HPKE context (see {{I-D.irtf-cfrg-hpke}}, Section 5.1.1) to use to derive symmetric keys for protecting\r\n```",
              "createdAt": "2021-05-24T17:29:21Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 154,
              "body": "What is \"the protocol\" here? Maybe we can say \"before clients can start uploading data\" instead?",
              "createdAt": "2021-05-24T17:31:08Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 159,
              "body": "```suggestion\r\n1. The client and leader can establish a leader-authenticated HTTPS connection.\r\n```",
              "createdAt": "2021-05-24T17:31:31Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 161,
              "body": "```suggestion\r\n1. The leader and each helper can establish a leader-authenticated HTTPS connection.\r\n```",
              "createdAt": "2021-05-24T17:31:44Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 166,
              "body": "Yeah, some of these preconditions seem to apply to the upload protocol, whereas some apply only to the verify/collect protocols. It's probably worth splitting them out.",
              "createdAt": "2021-05-24T17:32:59Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 238,
              "body": "```suggestion\r\n* the client failed to connect to the helper helper over HTTPS;\r\n```",
              "createdAt": "2021-05-24T17:38:35Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 240,
              "body": "```suggestion\r\n* the GET request to the helper URL failed, i.e., returned a non-200 status code, or contained an invalid `HpkeConfig` message; or\r\n```",
              "createdAt": "2021-05-24T17:39:10Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 305,
              "body": "```suggestion\r\n  opaque enc<1..2^16-1>;\r\n```\r\n`helper_enc` was undefined",
              "createdAt": "2021-05-24T17:43:03Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 314,
              "body": "```suggestion\r\nField `enc` encodes the helper's encapsulated key computed as above. The remainder of the\r\n```",
              "createdAt": "2021-05-24T17:43:32Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 265,
              "body": "This has been overcome by events now that the leader responds with 200 upon upload complete. ",
              "createdAt": "2021-05-24T17:45:37Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 333,
              "body": "```suggestion\r\n{{pa-error-common-aborts}}. The leader does not attempt to validate any information for the \r\nhelper, e.g., whether or not PAHelperShare is valid. Such validation occurs during the subsequent \r\nverify protocol.\r\n```",
              "createdAt": "2021-05-24T17:48:01Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 343,
              "body": "```suggestion\r\nparticular, the protocol is comprised of a sequence of HTTPS transactions between\r\nthe leader as client and helper as server. At the end of this phase, the leader and helper \r\nwill have\r\n```",
              "createdAt": "2021-05-24T17:48:45Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 208,
              "body": "```suggestion\r\ngenerated. The second, `id` is the client's *upload id*, which MUST be chosen\r\nfrom a cryptographically secure pseudorandom number generator (CSPRNG);\r\nsee {{?RFC4086}} for guidance on generating such random values.\r\n```\r\n\r\nThough I do prefer the text in RFC8446 Appendix C.1, so maybe just pull some stuff from there? ",
              "createdAt": "2021-05-24T17:55:35Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 201,
              "body": "Can we drop this field? It's not currently used during upload, and if it were, would require leaders to be stateful. (I see it's used during verify, but why can't the leader choose a per-report ID honestly, rather than each client in that case?) ",
              "createdAt": "2021-05-24T17:57:12Z",
              "updatedAt": "2021-05-25T16:07:13Z"
            },
            {
              "originalPosition": 382,
              "body": "```suggestion\r\nhelper's encapsulated HPKE context sent in the report. The remainder of the\r\n```",
              "createdAt": "2021-05-24T17:57:48Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 396,
              "body": "```suggestion\r\nThe response is structured as a sequence of *sub-responses*, where the i-th\r\n```",
              "createdAt": "2021-05-24T17:58:50Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 348,
              "body": "```suggestion\r\nThe leader begins with a sequence of reports that are all associated\r\n```\r\n\r\nSlightly reworded so as to not require servers to do some sort of batching if they don't have any reports to batch.\r\n",
              "createdAt": "2021-05-24T18:00:09Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 414,
              "body": "```suggestion\r\nFor each sub-request `PAVerifyReq`, the helper computes the corresponding\r\n```",
              "createdAt": "2021-05-24T18:00:32Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 424,
              "body": "Sorry, I was zeroing in on the \"tear down the HTTPS connection\" piece. I don't think that's really required. All that seems required is for the protocol (be it upload, verify, etc) to be aborted, which might manifest in tearing down HTTPS, or it might not. For upload, aborting is likely akin to tearing down the connection, but it might not be. (Clients could, e.g., keep open persistent connections to the leader if they're uploading many things.)",
              "createdAt": "2021-05-24T18:04:01Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 60,
              "body": "Let's use [\"application/octet-stream\"](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types).",
              "createdAt": "2021-05-24T18:06:04Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            },
            {
              "originalPosition": 334,
              "body": "```suggestion\r\n[[OPEN ISSUE: consider client->leader upload replays, and what leaders should do to mitigate against them. Prohibit early data? Strike register? Something else?]]\r\n```",
              "createdAt": "2021-05-24T18:08:55Z",
              "updatedAt": "2021-05-24T18:15:39Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY3MDMzNjc5",
          "commit": {
            "abbreviatedOid": "5f9c72f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-24T18:19:00Z",
          "updatedAt": "2021-05-24T18:19:00Z",
          "comments": [
            {
              "originalPosition": 176,
              "body": "I don't see why that's valuable or desired, since it just pushes state onto the server. (Left a separate comment for that, so will resolve this.)",
              "createdAt": "2021-05-24T18:19:00Z",
              "updatedAt": "2021-05-24T18:19:00Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY3MDMzOTQ1",
          "commit": {
            "abbreviatedOid": "5f9c72f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-24T18:19:24Z",
          "updatedAt": "2021-05-24T18:19:24Z",
          "comments": [
            {
              "originalPosition": 188,
              "body": "It's not really being picky. It's just saying that the URLs must be non-empty. ",
              "createdAt": "2021-05-24T18:19:24Z",
              "updatedAt": "2021-05-24T18:19:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY3MDM0MzY5",
          "commit": {
            "abbreviatedOid": "5f9c72f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-24T18:20:04Z",
          "updatedAt": "2021-05-24T18:20:05Z",
          "comments": [
            {
              "originalPosition": 368,
              "body": "```suggestion\r\n  opaque enc<1..2^16-1>;\r\n```",
              "createdAt": "2021-05-24T18:20:04Z",
              "updatedAt": "2021-05-24T18:20:05Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY3OTg5MjIy",
          "commit": {
            "abbreviatedOid": "5f9c72f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-25T15:36:22Z",
          "updatedAt": "2021-05-25T15:36:22Z",
          "comments": [
            {
              "originalPosition": 208,
              "body": "I'm fine with saying whatever here. (I don't actually think we need language about this yet.) I'll take the suggestion and we can overwrite it later if we choose.",
              "createdAt": "2021-05-25T15:36:22Z",
              "updatedAt": "2021-05-25T15:36:22Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY4MDM2ODA2",
          "commit": {
            "abbreviatedOid": "5f9c72f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-05-25T16:07:15Z",
          "updatedAt": "2021-05-25T16:49:00Z",
          "comments": [
            {
              "originalPosition": 166,
              "body": "Let's do so, but in a follow-up PR.",
              "createdAt": "2021-05-25T16:07:15Z",
              "updatedAt": "2021-05-25T16:49:00Z"
            },
            {
              "originalPosition": 201,
              "body": "(Getting rid of upload id, as it's motivation is tied to a particular instantiation of Prio.)",
              "createdAt": "2021-05-25T16:27:35Z",
              "updatedAt": "2021-05-25T16:49:00Z"
            },
            {
              "originalPosition": 424,
              "body": "Good point! \"Aborting\" just means forgetting any state associated with the protocol ... it needn't result in dropping the HTTPS state altogether.\r\n\r\nI guess we don't need to be explicit about what \"abort\" means right now. I just deleted this paragraph.",
              "createdAt": "2021-05-25T16:45:23Z",
              "updatedAt": "2021-05-25T16:49:00Z"
            },
            {
              "originalPosition": 154,
              "body": "That's right. Added.",
              "createdAt": "2021-05-25T16:46:41Z",
              "updatedAt": "2021-05-25T16:49:00Z"
            },
            {
              "originalPosition": 96,
              "body": "s/negotiated/configured",
              "createdAt": "2021-05-25T16:47:25Z",
              "updatedAt": "2021-05-25T16:49:00Z"
            },
            {
              "originalPosition": 60,
              "body": "Done.",
              "createdAt": "2021-05-25T16:48:06Z",
              "updatedAt": "2021-05-25T16:49:00Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzI2NDQ1",
          "commit": {
            "abbreviatedOid": "2d93f6a"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-06-02T22:16:26Z",
          "updatedAt": "2021-06-02T22:39:34Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "```suggestion\r\n```\r\n\r\nA reasonable point, but we are accumulating a lot of TODOs.",
              "createdAt": "2021-06-02T22:16:26Z",
              "updatedAt": "2021-06-02T22:39:34Z"
            },
            {
              "originalPosition": 90,
              "body": "Do you mean like draft-blah-blah-01?",
              "createdAt": "2021-06-02T22:16:50Z",
              "updatedAt": "2021-06-02T22:39:34Z"
            },
            {
              "originalPosition": 97,
              "body": "I'm not quite following what's going on here. Why not just give each task version its own UID? Or alternatively have version somewhere else.",
              "createdAt": "2021-06-02T22:18:35Z",
              "updatedAt": "2021-06-02T22:39:34Z"
            },
            {
              "originalPosition": 218,
              "body": "I don't quite understand the underlying assumptions here. If the client wants to upload some data, it's not going to be confused about whether the task involves heavy hitters or prio. What happens if it thinks it's doing prio and the leader responds with heavy hitterS?",
              "createdAt": "2021-06-02T22:35:57Z",
              "updatedAt": "2021-06-02T22:39:34Z"
            },
            {
              "originalPosition": 229,
              "body": "This seems like it has some obvious security problems. What stops the leader from just telling the client it runs all the helpers?",
              "createdAt": "2021-06-02T22:36:44Z",
              "updatedAt": "2021-06-02T22:39:34Z"
            },
            {
              "originalPosition": 291,
              "body": "Why are we replicating the leader_share for each helper?",
              "createdAt": "2021-06-02T22:38:43Z",
              "updatedAt": "2021-06-02T22:39:34Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzM4NzM4",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:42:04Z",
          "updatedAt": "2021-06-02T22:42:04Z",
          "comments": [
            {
              "originalPosition": 90,
              "body": "Yeah, I assume so, similar to what we did for ECH.",
              "createdAt": "2021-06-02T22:42:04Z",
              "updatedAt": "2021-06-02T22:42:04Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzM5NzIy",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:44:24Z",
          "updatedAt": "2021-06-02T22:44:24Z",
          "comments": [
            {
              "originalPosition": 97,
              "body": "The version here is \"version of the protocol.\" But maybe I'm misunderstanding you?",
              "createdAt": "2021-06-02T22:44:24Z",
              "updatedAt": "2021-06-02T22:44:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQwMjMw",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:45:30Z",
          "updatedAt": "2021-06-02T22:45:30Z",
          "comments": [
            {
              "originalPosition": 218,
              "body": "This is to accommodate different parameters for the actual upload that are protocol specific. For example, if Hits or some future protocol requires per-upload data, that'd be in the HitsUploadStartResp.",
              "createdAt": "2021-06-02T22:45:30Z",
              "updatedAt": "2021-06-02T22:45:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQxMTk3",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:47:38Z",
          "updatedAt": "2021-06-02T22:47:38Z",
          "comments": [
            {
              "originalPosition": 218,
              "body": "Sure, but why is PAParam here? Almost nothing there can change.",
              "createdAt": "2021-06-02T22:47:38Z",
              "updatedAt": "2021-06-02T22:47:38Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQxNDky",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:48:07Z",
          "updatedAt": "2021-06-02T22:48:08Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "Presumably clients will just check against their set of helpers or some local policy?",
              "createdAt": "2021-06-02T22:48:08Z",
              "updatedAt": "2021-06-02T22:48:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQxNjYw",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:48:31Z",
          "updatedAt": "2021-06-02T22:48:32Z",
          "comments": [
            {
              "originalPosition": 291,
              "body": "There's only one helper here, so no duplication.",
              "createdAt": "2021-06-02T22:48:32Z",
              "updatedAt": "2021-06-02T22:48:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQyOTk4",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:51:33Z",
          "updatedAt": "2021-06-02T22:51:33Z",
          "comments": [
            {
              "originalPosition": 291,
              "body": "\"Otherwise, for each supported helper the client issues a POST request to `[leader]/upload_finish` with a payload\r\nconstructed as described below.",
              "createdAt": "2021-06-02T22:51:33Z",
              "updatedAt": "2021-06-02T22:51:33Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQzNzI3",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:53:12Z",
          "updatedAt": "2021-06-02T22:53:13Z",
          "comments": [
            {
              "originalPosition": 218,
              "body": "I guess it depends on how things are configured? The assumption now is that clients start with a Task and then learn the Params for that task during Upload, but if we assume that Params are also distributed out-of-band, then we can drop it here.",
              "createdAt": "2021-06-02T22:53:12Z",
              "updatedAt": "2021-06-02T22:53:13Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQ0MTcx",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:54:06Z",
          "updatedAt": "2021-06-02T22:54:06Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "But then why do I need the server to tell me.\r\n\r\nI think there's a philosophical issue here: We should avoid having the leader tell the client anything that could be inconsistent with things it already needs to know. If we are trying to detect mismatch, we should instead have a config structure where the server sends the hash (or it's included in an AAD or something). That way we detect mismatch but don't rely on the client checking each parameter. Otherwise, we get into situations where the client might inappropriately trust the leader.\r\n\r\n\r\n",
              "createdAt": "2021-06-02T22:54:06Z",
              "updatedAt": "2021-06-02T22:54:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQ1ODk4",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T22:58:08Z",
          "updatedAt": "2021-06-02T22:58:08Z",
          "comments": [
            {
              "originalPosition": 291,
              "body": "Oh I see. Will flag as an open issue.",
              "createdAt": "2021-06-02T22:58:08Z",
              "updatedAt": "2021-06-02T22:58:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzQ5OTQx",
          "commit": {
            "abbreviatedOid": "8dc85bc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:07:41Z",
          "updatedAt": "2021-06-02T23:07:41Z",
          "comments": [
            {
              "originalPosition": 229,
              "body": "That's a good point -- if the baseline assumption is that the leader is not trusted to not collude, and clients can't reasonably check for this, then this needs to change. (Let's do that in the next PR.)",
              "createdAt": "2021-06-02T23:07:41Z",
              "updatedAt": "2021-06-02T23:07:41Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzA2NDUx",
          "commit": {
            "abbreviatedOid": "2d93f6a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-06-02T21:41:11Z",
          "updatedAt": "2021-06-02T23:17:21Z",
          "comments": [
            {
              "originalPosition": 155,
              "body": "```suggestion\r\ndata:\r\n\r\n```\r\nto make the list render properly in HTML",
              "createdAt": "2021-06-02T21:41:11Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 191,
              "body": "The use of TLS is an optional implementation detail of the transport between two servers, so I think this document should avoid making assumptions about it (maybe I'm doing Prio over IPsec for some reason).",
              "createdAt": "2021-06-02T21:50:04Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 202,
              "body": "IIUC the leader doesn't actually _do_ anything in response to the `upload_start` request except serve up some static configuration parameters. Thus I think that HTTP `GET` is a better fit here. Besides communicating API semantics to the client (`GET` is idempotent and should have no side effects on the server), you also get sensible caching behavior \"for free\" from the HTTP spec.\r\n\r\nThe complication is that [`GET` requests don't have bodies](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET), but I think we can encode the `PATask` as either path fragments in the URI (i.e., `[leader]/upload_start/[task.version]/[task.id]`) or as query parameters (i.e., `[leader]/upload_start?version=[task.version];id=[task.id]`), which has the additional benefit of making it possible for leaders to implement the `upload_start` endpoint by putting a bunch of static files in a CDN.",
              "createdAt": "2021-06-02T22:03:44Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 230,
              "body": "Given that an individual helper can be participating in multiple PDA deployments, I would have expected a `PATask` to be a parameter to this request. Does a helper server use the same HPKE config for all clients, or is the idea that `[helper]` somehow incorporates the `PATask`?",
              "createdAt": "2021-06-02T22:08:33Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 232,
              "body": "```suggestion\r\nupload shares to. It ignores a helper if:\r\n\r\n```\r\nTo make the list render correctly in HTML",
              "createdAt": "2021-06-02T22:08:59Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 231,
              "body": "How is key rotation handled? Since this is a `GET`, I think we could use a `Cache-Control` header in the HTTP response to indicate when the current key config expires, but maybe an explicit expiration date in the `HpkeConfig` message is more appropriate.",
              "createdAt": "2021-06-02T22:20:48Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 262,
              "body": "A `PATask` unambiguously identifies a `PAParam` so it seems like the task version+ID should suffice.",
              "createdAt": "2021-06-02T22:24:05Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 272,
              "body": "This wording is awkward: the HPKE context is used to encrypt the helper share, but it's not used for splitting the input and proof, right?",
              "createdAt": "2021-06-02T22:25:26Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 230,
              "body": "This suggests that the client must consider *all* the helpers described in the leader's `PAUploadStartResp` and only ignore a helper if it doesn't meet one of the criteria listed below. Is that true or is the client permitted to select an arbitrary subset of available helpers?",
              "createdAt": "2021-06-02T22:30:49Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 242,
              "body": "Given that the `PAUploadFinishReq` message includes both the helper and leader share, I find this kind of confusing. I think the idea here is that the client will select `k` out of the `n` helpers in the `PAUploadStartResp` obtained from the leader, and then construct a `PAUploadFinishReq`/report for each. That is, `k` independent instances of the PDA protocol will be run for aggregator pairs `(leader, helper[k])`, right?\r\n\r\n1. Does this mean we only support 2-way secret sharing? I think that's OK, but we should make that constraint of the system more obvious.\r\n1. Since all the reports are going to the same leader server, could they be combined into a single message, and hence a single request with a single state to be managed by the client and leader?\r\n1. Is the client expected to secret share the input differently for each `PAUploadFinishReq` it constructs? If not, then `PAUploadFinishReq.leader_share` will be identical in each message, and since the leader share will be much larger than the helper share (at least in Prio where the helper's share is a PRNG seed), we should work out a way to absolve the client (which might be on an expensive and slow cellular network) of having to upload the leader share `k` times.",
              "createdAt": "2021-06-02T22:47:54Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 327,
              "body": "```suggestion\r\nstatus 200 and an empty body. Malformed requests are handled as described in\r\n```",
              "createdAt": "2021-06-02T22:49:34Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 404,
              "body": "```suggestion\r\nPAParam.proto`. If not, it aborts and alerts the leader with \"incorrect protocol\r\n```\r\nHowever I think the `PAProto` field of `PAVerifyResp` and `PAVerifyReq` should be removed, precisely because you can get the protocol by getting the `PAParam` identified by the `PATask`. Having `PAProto` in the `PAVerify*` messages adds no information and only introduces an opportunity for implementations to get this wrong.",
              "createdAt": "2021-06-02T22:58:59Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            },
            {
              "originalPosition": 193,
              "body": "The client obviously has to perform the `upload_finish` part of the protocol each time they want to submit a report, but it doesn't make sense to require them to run `upload_start` each time they submit metrics. We should specify client caching behavior for helper URL lists obtained from leaders and key configs obtained from helpers (see also my comment just below about using `HTTP GET` for cacheable, idempotent requests).",
              "createdAt": "2021-06-02T23:11:12Z",
              "updatedAt": "2021-06-02T23:17:21Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU0Njg4",
          "commit": {
            "abbreviatedOid": "2efcfd7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:19:34Z",
          "updatedAt": "2021-06-02T23:19:34Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "Yeah, we're unlikely to make this change, so we can probably just delete the note.",
              "createdAt": "2021-06-02T23:19:34Z",
              "updatedAt": "2021-06-02T23:19:34Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU0ODc3",
          "commit": {
            "abbreviatedOid": "2efcfd7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:20:01Z",
          "updatedAt": "2021-06-02T23:20:01Z",
          "comments": [
            {
              "originalPosition": 189,
              "body": "```suggestion\r\nelement.)]\r\n```",
              "createdAt": "2021-06-02T23:20:01Z",
              "updatedAt": "2021-06-02T23:20:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU1MzUy",
          "commit": {
            "abbreviatedOid": "2efcfd7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:21:12Z",
          "updatedAt": "2021-06-02T23:21:12Z",
          "comments": [
            {
              "originalPosition": 202,
              "body": "> IIUC the leader doesn't actually do anything in response to the upload_start request except serve up some static configuration parameters.\r\n\r\nIn some cases, the server may supply per-upload data to be used in the finish message, so I think we ought to keep this to accommodate those protocols.",
              "createdAt": "2021-06-02T23:21:12Z",
              "updatedAt": "2021-06-02T23:21:12Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU1NjU1",
          "commit": {
            "abbreviatedOid": "2efcfd7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:22:00Z",
          "updatedAt": "2021-06-02T23:22:00Z",
          "comments": [
            {
              "originalPosition": 230,
              "body": "@ekr pointed out that client's should probably dictate the helpers (on behalf of the data collector), so this is likely to change. ",
              "createdAt": "2021-06-02T23:22:00Z",
              "updatedAt": "2021-06-02T23:22:00Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU2MjQz",
          "commit": {
            "abbreviatedOid": "2efcfd7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:23:34Z",
          "updatedAt": "2021-06-02T23:23:34Z",
          "comments": [
            {
              "originalPosition": 231,
              "body": "I think a Cache-Control header seems appropriate, but that does seem like an implementation detail. If the key has expired then the client's share will fail to be aggregated. (Though we would need to make sure this happens during Verify.)",
              "createdAt": "2021-06-02T23:23:34Z",
              "updatedAt": "2021-06-02T23:23:35Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU3MTQz",
          "commit": {
            "abbreviatedOid": "94b4669"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:25:54Z",
          "updatedAt": "2021-06-02T23:25:55Z",
          "comments": [
            {
              "originalPosition": 242,
              "body": "> Does this mean we only support 2-way secret sharing? I think that's OK, but we should make that constraint of the system more obvious.\r\n\r\nIndeed -- this is limited to the two-server case. \r\n\r\n> Since all the reports are going to the same leader server, could they be combined into a single message, and hence a single request with a single state to be managed by the client and leader?\r\n\r\nI don't see why not, though this seems like an optimization more than a functional change. \r\n\r\n> Is the client expected to secret share the input differently for each PAUploadFinishReq it constructs? If not, then PAUploadFinishReq.leader_share will be identical in each message, and since the leader share will be much larger than the helper share (at least in Prio where the helper's share is a PRNG seed), we should work out a way to absolve the client (which might be on an expensive and slow cellular network) of having to upload the leader share k times. \r\n\r\nYes, I think that's expected. (That is, I assume the proof is fresh for each upload.)",
              "createdAt": "2021-06-02T23:25:55Z",
              "updatedAt": "2021-06-02T23:25:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU3MzU3",
          "commit": {
            "abbreviatedOid": "94b4669"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:26:25Z",
          "updatedAt": "2021-06-02T23:26:25Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "Yeah, though we might want to fold in the entire PATask here anyway.",
              "createdAt": "2021-06-02T23:26:25Z",
              "updatedAt": "2021-06-02T23:26:25Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc0NzU3NDcx",
          "commit": {
            "abbreviatedOid": "94b4669"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-02T23:26:44Z",
          "updatedAt": "2021-06-02T23:26:44Z",
          "comments": [
            {
              "originalPosition": 272,
              "body": "That's right. Are you looking for a specific text change?",
              "createdAt": "2021-06-02T23:26:44Z",
              "updatedAt": "2021-06-02T23:26:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc1NTM2NDA1",
          "commit": {
            "abbreviatedOid": "e962ba2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-03T17:15:14Z",
          "updatedAt": "2021-06-03T17:15:14Z",
          "comments": [
            {
              "originalPosition": 202,
              "body": "I suspect we can make this a GET without precluding those protocols, but we can pursue that question after merging this change: #48 ",
              "createdAt": "2021-06-03T17:15:14Z",
              "updatedAt": "2021-06-03T17:15:14Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc1NTQyODA2",
          "commit": {
            "abbreviatedOid": "e962ba2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-03T17:18:54Z",
          "updatedAt": "2021-06-03T17:18:54Z",
          "comments": [
            {
              "originalPosition": 231,
              "body": "Filed #49 to consider key rotation and how it fits into the protocol.",
              "createdAt": "2021-06-03T17:18:54Z",
              "updatedAt": "2021-06-03T17:18:54Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc1NTQ0MzY2",
          "commit": {
            "abbreviatedOid": "e962ba2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-03T17:20:52Z",
          "updatedAt": "2021-06-03T17:20:53Z",
          "comments": [
            {
              "originalPosition": 272,
              "body": "How about:\r\n```suggestion\r\nthen generates a validity proof for its input, splits the input and proof into\r\na *leader share* and a *helper share*, using `context` to protect the latter.\r\nNote that the details of each of these processing steps --- encode, prove,\r\nsplit, and encrypt --- are specific to the PA protocol.\r\n```",
              "createdAt": "2021-06-03T17:20:52Z",
              "updatedAt": "2021-06-03T17:20:53Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc1NTQ4MjEz",
          "commit": {
            "abbreviatedOid": "e962ba2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-03T17:25:28Z",
          "updatedAt": "2021-06-03T17:25:28Z",
          "comments": [
            {
              "originalPosition": 242,
              "body": "I think this is fine, so long as each of the `k` protocol runs the client initiates for a given report are totally independent from one another.",
              "createdAt": "2021-06-03T17:25:28Z",
              "updatedAt": "2021-06-03T17:25:28Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njc1NTU3MjAy",
          "commit": {
            "abbreviatedOid": "e962ba2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I wrote up my outstanding questions and comments in distinct issues in the name of unblocking progress here (#48, #49, #50). Otherwise this looks good to me.",
          "createdAt": "2021-06-03T17:36:31Z",
          "updatedAt": "2021-06-03T17:36:31Z",
          "comments": []
        }
      ]
    },
    {
      "number": 46,
      "id": "MDExOlB1bGxSZXF1ZXN0NjQ5NTc5Njc4",
      "title": "remove reference to core@ietf.org",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/46",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Removes a paragraph which declares that Prio work is being discussed on\r\nany IETF mailing list, which is not (currently) the case.\r\n\r\nResolves #41",
      "createdAt": "2021-05-21T01:03:26Z",
      "updatedAt": "2021-12-30T00:53:20Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3b0e4468c79a00c5c20d1228248c8688bfac776a",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/remove-ietf-core",
      "headRefOid": "a29bef442d369c79771d226e18c3d732c1bf72fc",
      "closedAt": "2021-05-21T16:16:48Z",
      "mergedAt": "2021-05-21T16:16:47Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "c9b7f30edf33f31979f514a756a8eeeb5fded787"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjY1NjI4MjY3",
          "commit": {
            "abbreviatedOid": "a29bef4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-05-21T16:02:24Z",
          "updatedAt": "2021-05-21T16:02:24Z",
          "comments": []
        }
      ]
    },
    {
      "number": 47,
      "id": "MDExOlB1bGxSZXF1ZXN0NjYwMzA2ODAz",
      "title": "remove reference to core@ietf.org from .note.xml",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/47",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-06-02T18:36:38Z",
      "updatedAt": "2021-12-30T00:53:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c9b7f30edf33f31979f514a756a8eeeb5fded787",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/remove-ietf-core-again",
      "headRefOid": "b47eedacfe5d09c2ab57e9cc676db1799c88a310",
      "closedAt": "2021-06-12T00:34:30Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing as overcome by events!",
          "createdAt": "2021-06-12T00:34:30Z",
          "updatedAt": "2021-06-12T00:34:30Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 52,
      "id": "MDExOlB1bGxSZXF1ZXN0NjY2MTkxMzYx",
      "title": "Add a TODO for resolving aggregate consistency",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/52",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Addresses #4 by adding a TODO. With #43 this issue has been solved in part: To ensure consistency in case of helper failure, the leader specifies a set of helpers, and the client runs the protocol with the leader and each helper. Each cohort produces an output, and the collector will need a way of deciding which output is \"correct\". Let's wait to answer this question until we close #51.",
      "createdAt": "2021-06-09T18:34:34Z",
      "updatedAt": "2021-06-17T21:15:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d2bba0b8daf22de0cc7a226a8375d4714be6d166",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/cross-aggregator-consistency",
      "headRefOid": "624fc01eee6c33cff2df42cb3161652fe8b7bb30",
      "closedAt": "2021-06-17T19:55:24Z",
      "mergedAt": "2021-06-17T19:55:24Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "ddc9e0904f1ada13935d88d69fe6575f06633b58"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njg1NTIyODk1",
          "commit": {
            "abbreviatedOid": "624fc01"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM but I suggest modifying the PR description so that GitHub doesn't close #51 when this gets merged.",
          "createdAt": "2021-06-16T18:30:58Z",
          "updatedAt": "2021-06-16T18:30:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 54,
      "id": "MDExOlB1bGxSZXF1ZXN0NjY2MjM3MzU1",
      "title": "Define \"Report\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/54",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #53.",
      "createdAt": "2021-06-09T19:13:44Z",
      "updatedAt": "2021-06-17T21:15:21Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d2bba0b8daf22de0cc7a226a8375d4714be6d166",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/term-report",
      "headRefOid": "4f6bfd3d2481ec5d060e20648ad17d4eb01e9644",
      "closedAt": "2021-06-09T19:59:34Z",
      "mergedAt": "2021-06-09T19:59:34Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "9b4920ea8fef41e5b4706178fd1d0e0d00281bdf"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjgwMDkzOTY1",
          "commit": {
            "abbreviatedOid": "4f6bfd3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks for putting the list back in alphabetical order, too!",
          "createdAt": "2021-06-09T19:59:23Z",
          "updatedAt": "2021-06-09T19:59:23Z",
          "comments": []
        }
      ]
    },
    {
      "number": 55,
      "id": "MDExOlB1bGxSZXF1ZXN0NjY3NTQxNjgw",
      "title": "Do rejection sampling when generating pseudorandom field elements",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/55",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #14.",
      "createdAt": "2021-06-11T00:11:34Z",
      "updatedAt": "2021-06-17T21:15:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9b4920ea8fef41e5b4706178fd1d0e0d00281bdf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/prng-bias",
      "headRefOid": "ee0c253a8b071528acaef6cd8fc4a90ae995bb51",
      "closedAt": "2021-06-17T19:55:10Z",
      "mergedAt": "2021-06-17T19:55:10Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "fc27f20e076637b28310c5fc084927d7be31b7e5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njg1NTIwMDQx",
          "commit": {
            "abbreviatedOid": "ee0c253"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-06-16T18:27:48Z",
          "updatedAt": "2021-06-16T18:27:48Z",
          "comments": []
        }
      ]
    },
    {
      "number": 56,
      "id": "MDExOlB1bGxSZXF1ZXN0NjY4NjQ0MjIw",
      "title": "Use le-automaton token for editor copy update",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/56",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We set `GITHUB_TOKEN` to be read-only at the `abetterinternet` org\r\nlevel, but this breaks the \"Update GitHub Pages\" job in the \"Update\r\nEditor's Copy\" workflow. We now instead use a [personal account\r\ntoken](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token)\r\ngenerated from the `le-automaton` GitHub account which is scoped to\r\nallow actions on private repos.\r\n\r\nAdditionally, this deletes the `archive` workflow which I don't think we\r\nneed: GitHub itself is a perfectly good durable store of issues and PRs,\r\nand updating GH pages on push to main should suffice.",
      "createdAt": "2021-06-11T23:34:42Z",
      "updatedAt": "2021-12-30T00:53:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9b4920ea8fef41e5b4706178fd1d0e0d00281bdf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/fix-editor-copy",
      "headRefOid": "e6c23efbb66580c58bba55a0eef53e1c6f2cc8fe",
      "closedAt": "2021-06-12T00:30:31Z",
      "mergedAt": "2021-06-12T00:30:31Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "54dc32050ade4a56494887821a0077e655259caf"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjgyMjc4Nzc3",
          "commit": {
            "abbreviatedOid": "e6c23ef"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-06-12T00:30:28Z",
          "updatedAt": "2021-06-12T00:30:28Z",
          "comments": []
        }
      ]
    },
    {
      "number": 59,
      "id": "MDExOlB1bGxSZXF1ZXN0Njc0NzAzNDU0",
      "title": "Specify data collection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/59",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #44 by having the collector pass parameters to the aggregators for each round of verification and aggregation.\r\n\r\nReframes the execution of a PA protocol in terms of clients *pushing* input shares to the aggregators and the collector *pulling* output shares from the aggregators. This is meant to satisfy the requirements discussed in #51.",
      "createdAt": "2021-06-21T16:24:34Z",
      "updatedAt": "2021-12-30T02:09:57Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "aa4874af0ab968d514eed3859464d1a1984fd904",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/collect-overview",
      "headRefOid": "b0cc2706fbbf0f77068078408a47809b731a2286",
      "closedAt": "2021-07-07T21:27:20Z",
      "mergedAt": "2021-07-07T21:27:20Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "2ddbf5ed540db63bcc6edbb469777aa70ae4fb58"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> ### [leader]/collect calls block on multiple [helper]/aggregate calls\r\n> \r\n> The diagram in 3.4. illustrates that each collect request can cause the leader to make _L_ aggregate calls. The PACollectReq contains `Url helper`, so that can't be an aggregate call to each of the _L_ helpers enumerated in the `PAParams.helper_urls`.\r\n> \r\n>     * Why the _L_ calls then? Is the idea to e.g. allow a leader computing output over 10,000 inputs to feed the inputs to helpers 1,000 at a time to limit how big individual leader->helper HTTP requests can be?\r\n\r\nEach aggregate request corresponds to a round of the input validation protocol. For Prio, `L=1`; for Hits, `L = 2` I think. In general, I'd expect `L` to be small and constant.\r\n\r\n>     * What happens if an insufficient number of inputs (i.e., fewer than `PAParam.batch_size`) have been gathered when the leader receives the collect request? I assume the collector gets an error like \"not yet; try again later\".\r\n\r\nYes.\r\n\r\n> In this draft, the collector's `[leader]/collect` request will block until the _L_ `[helper]/aggregate` requests return. This seems like a problem to me: that would require the leader and collector to keep a connection open for what could be a long time while all the helpers finish aggregating. Additionally, it'd be hard for leaders to maintain any kind of response time SLOs if they are beholden to the response times of all the helpers (though of course the overall system's response times and throughput will be at the mercy of any defective server regardless). If we have to define a means for the leader to tell the collector to back off and try again later anyway, then I think we could use that to avoid blocking for a long time.\r\n\r\nThis is indeed a problem in general. For Hits the latency is unavoidable; for Prio we can avoid it because the aggregate requests can actually be made *as reports are uploade*. (Note that this is one of the requirements outline in #51. @acmiyaguchi and @ekr mention in the comments some ways we might deal with this. )\r\n\r\n\r\n> Collectors would be configured to hit `[leader]/collect` at some fixed interval (say every 12 hours) (I suspect this is already implicitly the case). If an insufficient number of inputs have been received since the last time a `PACollectResp` was delivered, the collector gets `EAGAIN` and tries again after the fixed interval elapses. If the leader decides it has enough inputs to generate output shares, it synchronously returns a collect request ID to the collector, and then asynchronously makes all the necessary `[helper]/aggregate` calls. Subsequently, the collector can poll on `[leader]/collect/<collect request id>` at a more aggressive interval. If it polls on the collect request ID before the output shares are ready, it gets `EAGAIN`. Eventually, leader completes its transactions with the helper(s) and the next time collector polls `[leader]/collect/<collect request id>`, it provides a `PACollectResp` populated with the output shares.\r\n\r\nThat sounds right to me.\r\n\r\n> ### What state does the leader or collector have to maintain, and how does `state.Finished()` work?\r\n> \r\n> Obviously it's protocol specific (prio vs. hits) but some concrete examples would help me understand.\r\n\r\nThe collector's state encodes whatever information it needs from previous request to figure out what the next request will be. I envision this being pretty application specific... I'm not sure it has intrinsic value to either Prio or Hits.\r\n\r\n> In the Prio case, it's finished when the collector gets a `PACollectResp` that contains the leader and helper output shares, and then it can reassemble them into the output.\r\n> \r\n>     * Is the leader then responsible for keeping track of which reports/inputs were included in the most recent `PACollectResp` delivered to the collector, to avoid double-counting reports in the next `PACollectResp`? If so, how does the leader track this state -- does a date suffice (i.e. inputs that arrived after the recorded date should be included in the next `PACollectResp`)?\r\n\r\nThis question reflects a requirement that we worked out on the design call: that right now the \"batch\" of reports that are used to respond to a collect request is not well-defined. As a first stab at this, we will timestamp reports and have the collect request specify a time range that defines the batch.\r\n\r\n> In the Hits case, then IIUC the successive collect requests sent by the collector represent traversing to the next level of a tree, so only the collector can know whether it feels it's gone deep enough.\r\n> \r\n>     * How does the leader know what set of inputs to include in the tree that the collector's query/collect request is applied to? Is it all the inputs gathered in the last _n_ hours, where _n_ is a Hits-specific portion of the PAParams? If so, is the leader responsible for keeping track of whether a given query has been applied to inputs from a given time window? Or is the query applied to all inputs gathered so far in the PATask? How does that affect data retention?\r\n\r\nSee above.\r\n\r\n\r\n\r\n",
          "createdAt": "2021-06-23T20:33:57Z",
          "updatedAt": "2021-06-23T20:33:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased.",
          "createdAt": "2021-06-23T20:47:42Z",
          "updatedAt": "2021-06-23T20:47:42Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njg4OTQ0NjA1",
          "commit": {
            "abbreviatedOid": "68b178b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Thanks for all this great work! Besides the various inline comments, I have some higher-level comments/questions, which I think we should discuss in person during tomorrow (Weds 6/23)'s design call.\r\n\r\n### [leader]/collect calls block on multiple [helper]/aggregate calls\r\n\r\nThe diagram in 3.4. illustrates that each collect request can cause the leader to make _L_ aggregate calls. The PACollectReq contains `Url helper`, so that can't be an aggregate call to each of the _L_ helpers enumerated in the `PAParams.helper_urls`.\r\n\r\n* Why the _L_ calls then? Is the idea to e.g. allow a leader computing output over 10,000 inputs to feed the inputs to helpers 1,000 at a time to limit how big individual leader->helper HTTP requests can be?\r\n* What happens if an insufficient number of inputs (i.e., fewer than `PAParam.batch_size`) have been gathered when the leader receives the collect request? I assume the collector gets an error like \"not yet; try again later\".\r\n\r\nIn this draft, the collector's `[leader]/collect` request will block until the _L_ `[helper]/aggregate` requests return. This seems like a problem to me: that would require the leader and collector to keep a connection open for what could be a long time while all the helpers finish aggregating. Additionally, it'd be hard for leaders to maintain any kind of response time SLOs if they are beholden to the response times of all the helpers (though of course the overall system's response times and throughput will be at the mercy of any defective server regardless). If we have to define a means for the leader to tell the collector to back off and try again later anyway, then I think we could use that to avoid blocking for a long time.\r\n\r\nCollectors would be configured to hit `[leader]/collect` at some fixed interval (say every 12 hours) (I suspect this is already implicitly the case). If an insufficient number of inputs have been received since the last time a `PACollectResp` was delivered, the collector gets `EAGAIN` and tries again after the fixed interval elapses. If the leader decides it has enough inputs to generate output shares, it synchronously returns a collect request ID to the collector, and then asynchronously makes all the necessary `[helper]/aggregate` calls. Subsequently, the collector can poll on `[leader]/collect/<collect request id>` at a more aggressive interval. If it polls on the collect request ID before the output shares are ready, it gets `EAGAIN`. Eventually, leader completes its transactions with the helper(s) and the next time collector polls `[leader]/collect/<collect request id>`, it provides a `PACollectResp` populated with the output shares.\r\n\r\n### What state does the leader or collector have to maintain, and how does `state.Finished()` work?\r\n\r\nObviously it's protocol specific (prio vs. hits) but some concrete examples would help me understand.\r\n\r\nIn the Prio case, it's finished when the collector gets a `PACollectResp` that contains the leader and helper output shares, and then it can reassemble them into the output.\r\n\r\n* Is the leader then responsible for keeping track of which reports/inputs were included in the most recent `PACollectResp` delivered to the collector, to avoid double-counting reports in the next `PACollectResp`? If so, how does the leader track this state -- does a date suffice (i.e. inputs that arrived after the recorded date should be included in the next `PACollectResp`)?\r\n\r\nIn the Hits case, then IIUC the successive collect requests sent by the collector represent traversing to the next level of a tree, so only the collector can know whether it feels it's gone deep enough.\r\n\r\n* How does the leader know what set of inputs to include in the tree that the collector's query/collect request is applied to? Is it all the inputs gathered in the last _n_ hours, where _n_ is a Hits-specific portion of the PAParams? If so, is the leader responsible for keeping track of whether a given query has been applied to inputs from a given time window? Or is the query applied to all inputs gathered so far in the PATask? How does that affect data retention?",
          "createdAt": "2021-06-21T23:37:52Z",
          "updatedAt": "2021-06-23T00:31:24Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "```suggestion\r\n    |                    |\r\n    |                    |\r\n    | 1.              1. | 2.\r\n```\r\nto indicate that leader->helper communication happens during both upload and collect",
              "createdAt": "2021-06-21T23:37:53Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 88,
              "body": "```suggestion\r\n[TODO: Say that the helper is stateless. To make this possible, it encrypts\r\n```",
              "createdAt": "2021-06-21T23:39:00Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 175,
              "body": "```suggestion\r\nURLs. The `collector_config` is the HPKE configuration of the collector\r\n```",
              "createdAt": "2021-06-21T23:54:17Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 239,
              "body": "Surely an individual client could be participating in multiple tasks? As written this implies that clients have a 1:1 relationship with PATasks.",
              "createdAt": "2021-06-21T23:59:14Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 268,
              "body": "I think this diagram should include the `key_config` request made by the client against each helper, which IIUC must occur before upload finish.",
              "createdAt": "2021-06-22T02:35:24Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 326,
              "body": "```suggestion\r\n[OPEN ISSUE: Should the request URL encode the PA task? This would be necessary if we make `upload_start` an idempotent GET per issue#48.]\r\n```",
              "createdAt": "2021-06-22T02:37:41Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 399,
              "body": "We could clarify here that the leader share is sent in plaintext but over a channel that is assumed to be confidential w.r.t. observers on the network (i.e., TLS). Do we enumerate the requirements for the transport anywhere?",
              "createdAt": "2021-06-22T02:41:11Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 514,
              "body": "```suggestion\r\nzero knowledge that the proof is valid. The exact procedure for doing so\r\n```\r\nSince \"validity\" is what we discuss elsewhere in the document. Mind you this was wrong before your PR.",
              "createdAt": "2021-06-22T02:49:13Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 477,
              "body": "Given a `PATask`, the collector should be able to look up the corresponding `PAParam` and figure out the `PAProto`, no? I bring this up because I think collector implementations will need to check `PACollectResp.proto` against `PAParam.proto` no matter what, so including this member in `PACollectResp` only introduces the possibility of malformed messages and doesn't carry any additional information.",
              "createdAt": "2021-06-22T22:09:40Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 463,
              "body": "I think the `PAProto` will be implied by the `PAParam` uniquely identified by the `PATask`.",
              "createdAt": "2021-06-22T22:11:22Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 462,
              "body": "A `PAParam` can contain _n_ `helper_urls`, but a client may only select _k_ < _n_ of them to send reports to. So how does the collector know which `helper_urls` to use? I'm guessing that collectors are meant to send a `PACollectReq` for all _n_ helpers, but if _k_ << _n_ this seems wasteful. Are we assuming that _k_ is exactly or close to _n_?\r\n\r\nSupposing that some helper did not participate, what does the `PACollectResp` look like? Does it return an error (which should be described in 3.x. Common abort conditions) or does it provide a `PACollectResp` with empty `leader_share` and `encrypted_helper_share`?\r\n\r\nWhat about the case where `PACollectReq.helper` does not occur in `PAParam.helper_urls`? Does that get rejected by leader as an invalid message, or treated the same as a valid helper that did not participate?",
              "createdAt": "2021-06-22T22:14:33Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 586,
              "body": "```suggestion\r\nassociated with `PAAggregateReq.helper_hpke_config_id`. If not found, then it aborts and\r\n```",
              "createdAt": "2021-06-22T22:45:54Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 541,
              "body": "`uint8` seems kinda small for this, if a helper is meant to participate in multiple PA instantiations. Or is the idea that key configs are identified by `task+hpke_config_id`?",
              "createdAt": "2021-06-22T22:47:23Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 670,
              "body": "```suggestion\r\n    case prio: PrioOutputShare;\r\n```",
              "createdAt": "2021-06-22T23:02:14Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 479,
              "body": "If we made it the leader's responsibility to reassemble output shares into the final aggregate, then we wouldn't need to go to the trouble of establishing an authenticated+confidential helper<->collector channel. Am I right in guessing that protocols like hits make it impossible for the leader to reassemble outputs on the collector's behalf?",
              "createdAt": "2021-06-22T23:15:57Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 495,
              "body": "We should make sure to version all these API endpoints, but that's probably premature at this stage: #61",
              "createdAt": "2021-06-22T23:27:53Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 647,
              "body": "```suggestion\r\nOnce the aggregators have verified at least as many reports as\r\nrequired for the PA task, the leader issues an *output share request* to the\r\n```\r\nI think this is how this sentence should read?",
              "createdAt": "2021-06-22T23:32:43Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 684,
              "body": "```suggestion\r\nconfiguration and `output_share` is its serialized output share.\r\n```",
              "createdAt": "2021-06-22T23:39:32Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            },
            {
              "originalPosition": 158,
              "body": "If the collector HPKE config is here, then it can't be rotated without distributing a new `PAParam` to all participating servers. Could we have `[collector]/key_config` for discovering collector public keys dynamically?",
              "createdAt": "2021-06-22T23:41:57Z",
              "updatedAt": "2021-06-23T00:31:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkwODc5NTky",
          "commit": {
            "abbreviatedOid": "68b178b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-23T16:15:12Z",
          "updatedAt": "2021-06-23T16:49:45Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "The leader doesn't interact with the helper during upload, only during collect.",
              "createdAt": "2021-06-23T16:15:12Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 158,
              "body": "Yeah, some spelling of this is a good idea I think. I've left this as an OPEN ISSUE for now, since it's somewhat orthogonal. See below.",
              "createdAt": "2021-06-23T16:17:02Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 239,
              "body": "Yes indeed. I don't this criterion is very clear. What we mean is \"each client has selected a PA task for which it will upload a report\". Fixed.",
              "createdAt": "2021-06-23T16:23:33Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 268,
              "body": "Good call. Done.",
              "createdAt": "2021-06-23T16:25:07Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 399,
              "body": "Noted.\r\n\r\nAll we say about transport is that it should be a \"server-authenticated secure channel\". (I don't think we should be more specific than this.)",
              "createdAt": "2021-06-23T16:28:06Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 462,
              "body": "> A `PAParam` can contain _n_ `helper_urls`, but a client may only select _k_ < _n_ of them to send reports to. So how does the collector know which `helper_urls` to use? I'm guessing that collectors are meant to send a `PACollectReq` for all _n_ helpers, but if _k_ << _n_ this seems wasteful. Are we assuming that _k_ is exactly or close to _n_?\r\n\r\nThe collector knows `helper_urls` because it's now specified by `PAParam`. (Note that this is change is new to this PR.) Nominally, _k_ would be close to _n_ (and _n_ would be be around 2 or 3).\r\n\r\n> Supposing that some helper did not participate, what does the `PACollectResp` look like? Does it return an error (which should be described in 3.x. Common abort conditions) or does it provide a `PACollectResp` with empty `leader_share` and `encrypted_helper_share`?\r\n\r\nGood question. (This is subsumed by the OPEN ISSUE below; I added your particular question as an example.) I think the right answer is that the leader would \"abort\" the request as described in {{pa-error}}. An important thing to note about this case is that it's (partially) recoverable, since the collector can try again with another helper.\r\n\r\n> What about the case where `PACollectReq.helper` does not occur in `PAParam.helper_urls`? Does that get rejected by leader as an invalid message, or treated the same as a valid helper that did not participate?\r\n\r\nThis would be another \"abort\" I think.\r\n",
              "createdAt": "2021-06-23T16:36:12Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 463,
              "body": "But there is no `PAParam` in this struct. I guess you could argue that the leader knows the `PAParam` since it has the task id. However, I tend to think that protocol messages should always be deserializaeable without extra context.",
              "createdAt": "2021-06-23T16:38:23Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 477,
              "body": "I see what you mean. As I mentioned above, I think protocol messages should always be deserializeable without needing context. Another way of phrasing this is that protocol messages should be \"self describing\". Maybe this property isn't needed here.",
              "createdAt": "2021-06-23T16:41:37Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 479,
              "body": "The purpose of encryption here is to ensure that the aggregators don't learn the final output. (This is one of the requirements enumerated in #51.) This is a useful property to have if the collector \"outsources\" aggregation to a third-party leader. (See @ekr's comment on #51.)",
              "createdAt": "2021-06-23T16:44:54Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            },
            {
              "originalPosition": 541,
              "body": "The config_id is meant to uniquely identify the HPKE config. We're using the same type for the config_id in ODoH and ECH, and it hasn't been a problem there. Usually the server will only have a small number of keys at any one time, e.g., 3 or 4.",
              "createdAt": "2021-06-23T16:46:24Z",
              "updatedAt": "2021-06-23T16:49:45Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkxMTQ5ODUw",
          "commit": {
            "abbreviatedOid": "d820f60"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-23T21:18:14Z",
          "updatedAt": "2021-06-23T21:18:14Z",
          "comments": [
            {
              "originalPosition": 479,
              "body": "It also occurs to me that encrypting the output share is important to prevent the leader from doing Sybil attacks. ",
              "createdAt": "2021-06-23T21:18:14Z",
              "updatedAt": "2021-06-23T21:18:14Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkxMjEwODE5",
          "commit": {
            "abbreviatedOid": "60de8d3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-06-23T23:01:42Z",
          "updatedAt": "2021-06-23T23:49:22Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "Since we're getting into the -arity of things: does a report contain shares of a _single_ input or could several inputs be encoded into a single report?",
              "createdAt": "2021-06-23T23:01:42Z",
              "updatedAt": "2021-06-23T23:49:22Z"
            },
            {
              "originalPosition": 45,
              "body": "This would allow the leader to impersonate the collector when it relays the HPKE config to the helper -- but as you say, it's an open issue so we don't have to solve it right now.",
              "createdAt": "2021-06-23T23:11:02Z",
              "updatedAt": "2021-06-23T23:49:22Z"
            },
            {
              "originalPosition": 135,
              "body": "I like making the collector specify the batch window, as I think it absolves us of specifying some tricky edge cases w.r.t. insufficient # of reports in the window (we can just say that it's up to the collector to try again with wider batch windows until they find one that works).",
              "createdAt": "2021-06-23T23:24:33Z",
              "updatedAt": "2021-06-23T23:49:22Z"
            },
            {
              "originalPosition": 541,
              "body": "I had assumed that a helper wouldn't re-use HPKE configs across PATasks, but thinking about it further there's no reason to assume that.",
              "createdAt": "2021-06-23T23:47:55Z",
              "updatedAt": "2021-06-23T23:49:22Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjkxMjMwNTM1",
          "commit": {
            "abbreviatedOid": "60de8d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-23T23:52:58Z",
          "updatedAt": "2021-06-23T23:55:41Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "I think one input per report. As has been said, it might be that the \"input\" is actually comprised of multiple measurements. The \"input\" is meant to capture the thing that's validated in a single run of the input validation protocol.",
              "createdAt": "2021-06-23T23:52:58Z",
              "updatedAt": "2021-06-23T23:55:41Z"
            },
            {
              "originalPosition": 45,
              "body": "Actually, at the moment the helper is \"pre-configured\" with the PAParam (and thus the collectors HPKE config). But yeah, this something we have to work out.",
              "createdAt": "2021-06-23T23:54:30Z",
              "updatedAt": "2021-06-23T23:55:41Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk2NzE5Njg2",
          "commit": {
            "abbreviatedOid": "3e73742"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "First pass complete -- looking great! ",
          "createdAt": "2021-07-01T00:38:24Z",
          "updatedAt": "2021-07-01T00:56:56Z",
          "comments": [
            {
              "originalPosition": 100,
              "body": "```suggestion\r\n2. **Collect:** The collector makes one or more requests to the leader in order to obtain the\r\n```",
              "createdAt": "2021-07-01T00:38:24Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 112,
              "body": "Can we move this to an issue and discuss separately? What does \"we run the protocol in parallel with multiple helpers\" actually mean? (Who is \"we\"?)",
              "createdAt": "2021-07-01T00:39:43Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 485,
              "body": "```suggestion\r\n[TODO: Decide if and how the collector's request is authenticated.]\r\n```",
              "createdAt": "2021-07-01T00:45:01Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 135,
              "body": "Or maybe the collector can try again with a smaller window if it wants a different slice?",
              "createdAt": "2021-07-01T00:46:55Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 462,
              "body": "It seems a lot of complexity right now is coming from the fact that the collector allows for multiple helpers to be used by the client, and the client to make a selection. Can we just remove that option, and have the collector force which helpers to use? ",
              "createdAt": "2021-07-01T00:48:38Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 201,
              "body": "Are we concerned about skew between collectors and aggregators?",
              "createdAt": "2021-07-01T00:49:36Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 612,
              "body": "This is generated by the leader, presumably? Is it set when the report is received from the client, or when aggregation is actually done during collection? Could the client set the timestamp in its encrypted share for the helper?",
              "createdAt": "2021-07-01T00:51:40Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 713,
              "body": "```suggestion\r\ncarries is up to the helper implementation.\r\n```",
              "createdAt": "2021-07-01T00:53:36Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 733,
              "body": "Should we require that the helper check that the OutputShareReq is not sent before the batch limit is hit? (That is, should the helper track how many shares aggregated so far and error if the number is less than the size?)",
              "createdAt": "2021-07-01T00:55:07Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            },
            {
              "originalPosition": 729,
              "body": "Hmm, where does the batch window come into play here? Shouldn't it be the case that aggregate outputs are only produced for a given batch window once the size is hit? Do the collector's request parameters need to fold into this OutputShareReq?",
              "createdAt": "2021-07-01T00:55:51Z",
              "updatedAt": "2021-07-01T00:56:56Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3MzQyMDk3",
          "commit": {
            "abbreviatedOid": "3e73742"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T14:50:47Z",
          "updatedAt": "2021-07-01T16:48:56Z",
          "comments": [
            {
              "originalPosition": 201,
              "body": "Clock skew? Yes, their clocks will need to be roughly synchronized, at least within `batch_window` seconds of each other.",
              "createdAt": "2021-07-01T14:50:47Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 485,
              "body": "It will need to be authenticated, so it's not a question of \"if\".",
              "createdAt": "2021-07-01T14:51:25Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 135,
              "body": "I think it should be able to do so, as long a (1) `batch_start` and `batch_end` are multiples of `batch_window` and (2) `batch_end - batch_start >= batch_window`. (The aggregators enforce this.) Furthermore, it shouldn't be able to change `batch_size` or `batch_window` adaptively, as this could lead to privacy violations.",
              "createdAt": "2021-07-01T14:55:54Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 612,
              "body": "Actually, the time stamp is generated by the client when it uploads the report. See \"PAUploadFinishReq\".",
              "createdAt": "2021-07-01T15:11:50Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 729,
              "body": "Ah, good catch. I added the `batch_start` and `batch_end` parameters so that the helper can enforce the `batch_window` and `batch_size` parameters.",
              "createdAt": "2021-07-01T16:35:40Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 733,
              "body": "Yes indeed. I updated the text to make this check explicit.",
              "createdAt": "2021-07-01T16:42:13Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 112,
              "body": "We decided previously that multiple helpers would be used for resiliency. This has already been baked into the spec. This TODO is merely about motivating and describing this design choice. There are two issues that are impacted by this: #4 and #22.",
              "createdAt": "2021-07-01T16:47:09Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            },
            {
              "originalPosition": 462,
              "body": "This discussion seems regressive given that we've already decided that multiple helpers would be used to add resiliency. If we want to discuss changing course, then let's do so in an separate issue.",
              "createdAt": "2021-07-01T16:48:42Z",
              "updatedAt": "2021-07-01T16:48:56Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3NTM5NjQx",
          "commit": {
            "abbreviatedOid": "6089524"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T18:22:08Z",
          "updatedAt": "2021-07-01T18:22:08Z",
          "comments": [
            {
              "originalPosition": 485,
              "body": "Why does a collector's request to get the aggregate output need to be authenticated? That seems like a deployment specific property. ",
              "createdAt": "2021-07-01T18:22:08Z",
              "updatedAt": "2021-07-01T18:22:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3NTM5ODUx",
          "commit": {
            "abbreviatedOid": "6089524"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T18:22:27Z",
          "updatedAt": "2021-07-01T18:22:27Z",
          "comments": [
            {
              "originalPosition": 112,
              "body": "I'll raise as a separate issue as I no longer thing this is necessary to bake into the protocol this way.",
              "createdAt": "2021-07-01T18:22:27Z",
              "updatedAt": "2021-07-01T18:22:27Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3NTQwMTk1",
          "commit": {
            "abbreviatedOid": "6089524"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T18:22:55Z",
          "updatedAt": "2021-07-01T18:22:55Z",
          "comments": [
            {
              "originalPosition": 201,
              "body": "We should then (a) note that as a requirement and (b) note what happens if that does not happen.",
              "createdAt": "2021-07-01T18:22:55Z",
              "updatedAt": "2021-07-01T18:22:56Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3NTQwNzM1",
          "commit": {
            "abbreviatedOid": "6089524"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T18:23:42Z",
          "updatedAt": "2021-07-01T18:23:43Z",
          "comments": [
            {
              "originalPosition": 612,
              "body": "What stops the leader from altering the timestamp, then? Should it be folded into the helper AAD, perhaps?",
              "createdAt": "2021-07-01T18:23:42Z",
              "updatedAt": "2021-07-01T18:23:43Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3NzE4MDQw",
          "commit": {
            "abbreviatedOid": "6089524"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T23:18:48Z",
          "updatedAt": "2021-07-01T23:46:53Z",
          "comments": [
            {
              "originalPosition": 201,
              "body": "More precisely, I think all that's required for privacy is that each aggregator needs to have a clock that's roughly in sync with true time (within batch_window seconds). Added this to \"Pre-conditions\".",
              "createdAt": "2021-07-01T23:18:48Z",
              "updatedAt": "2021-07-01T23:46:53Z"
            },
            {
              "originalPosition": 485,
              "body": "I don't think this is going to end up being deployment specific, but I'll concede the point for now.",
              "createdAt": "2021-07-01T23:21:02Z",
              "updatedAt": "2021-07-01T23:46:53Z"
            },
            {
              "originalPosition": 112,
              "body": "Done -> https://github.com/abetterinternet/prio-documents/issues/68",
              "createdAt": "2021-07-01T23:40:03Z",
              "updatedAt": "2021-07-01T23:46:53Z"
            },
            {
              "originalPosition": 462,
              "body": "Filed: https://github.com/abetterinternet/prio-documents/issues/68",
              "createdAt": "2021-07-01T23:43:32Z",
              "updatedAt": "2021-07-01T23:46:53Z"
            },
            {
              "originalPosition": 612,
              "body": "Yup, that's the idea. There's a TODO for this in \"Upload Finish Request\".",
              "createdAt": "2021-07-01T23:44:54Z",
              "updatedAt": "2021-07-01T23:46:53Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk3NzI4ODMy",
          "commit": {
            "abbreviatedOid": "8605ff6"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-01T23:50:34Z",
          "updatedAt": "2021-07-02T00:26:15Z",
          "comments": [
            {
              "originalPosition": 148,
              "body": "The convention here is ```opaque```",
              "createdAt": "2021-07-01T23:50:34Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 522,
              "body": "Do we understand what the contents of this might be? Are they really going to be per-request? In particular, why do we need ```proto```. Each task should just have one proto\r\n",
              "createdAt": "2021-07-02T00:02:11Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 504,
              "body": "I'd prefer we don't describe this in this object oriented fashion. It's incredibly hard to read. Just talk about the protocol messages, not member functions.",
              "createdAt": "2021-07-02T00:02:50Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 564,
              "body": "As noted in in issue #68 this just seems like extra complexity. The helpers for any given task should be fixed.",
              "createdAt": "2021-07-02T00:05:35Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 608,
              "body": "What is the hpke_config_id used for? And why is it only one octet long?",
              "createdAt": "2021-07-02T00:13:05Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 617,
              "body": "Again, why is ```proto``` being carried here? You can't change it on the fly.",
              "createdAt": "2021-07-02T00:22:42Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 657,
              "body": "Why are you assuming that each report uses the same key_id. Suppose you had a situation in which you had a batch of a day's reports and the helper changed keys halfway through? ",
              "createdAt": "2021-07-02T00:23:49Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 694,
              "body": "This could be avoided by just not having these fields.",
              "createdAt": "2021-07-02T00:24:15Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            },
            {
              "originalPosition": 771,
              "body": "I don't see how this works and why is it just one byte?",
              "createdAt": "2021-07-02T00:26:09Z",
              "updatedAt": "2021-07-02T00:26:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAwMjI0NjE0",
          "commit": {
            "abbreviatedOid": "8605ff6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-06T17:44:11Z",
          "updatedAt": "2021-07-06T17:44:11Z",
          "comments": [
            {
              "originalPosition": 522,
              "body": "- PrioCollectReq would minimally have some identifier for the validity circuit and, potentially, the field.\r\n- HitsCollectReq might might specify one or two fields.",
              "createdAt": "2021-07-06T17:44:11Z",
              "updatedAt": "2021-07-06T17:44:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAwMjI2NzAy",
          "commit": {
            "abbreviatedOid": "8605ff6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-06T17:46:38Z",
          "updatedAt": "2021-07-06T17:56:01Z",
          "comments": [
            {
              "originalPosition": 504,
              "body": "This was @chris-wood's idea. I think it's nice, but not strictly necessary. I added a TODO to decide whether to change this.",
              "createdAt": "2021-07-06T17:46:39Z",
              "updatedAt": "2021-07-06T17:56:01Z"
            },
            {
              "originalPosition": 617,
              "body": "The structure of the rest of this message depends on the PA protocol. Maybe there's a better way to express this?",
              "createdAt": "2021-07-06T17:47:47Z",
              "updatedAt": "2021-07-06T17:56:01Z"
            },
            {
              "originalPosition": 657,
              "body": "In that case, the leader would need to split the reports into two requests.\r\n\r\n> Why are you assuming that each report uses the same key_id.\r\n\r\nIt seemed to me like it would be simpler if there's just one HPKE key used per request. \r\n\r\n> Suppose you had a situation in which you had a batch of a day's reports and the helper changed keys halfway through?\r\n\r\nThe leader would split the reports into two separate requests, one for the old key and another for the new.\r\n\r\n",
              "createdAt": "2021-07-06T17:53:45Z",
              "updatedAt": "2021-07-06T17:56:01Z"
            },
            {
              "originalPosition": 771,
              "body": "This is basically the same thing we do in ECH: there is a 1-byte config identifier that's used to inform the decrypting party which HPKE key it needs to decrypt. It's a byte only because other HPKE applications have only a byte for the config id.",
              "createdAt": "2021-07-06T17:55:54Z",
              "updatedAt": "2021-07-06T17:56:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAwMjU4MzU4",
          "commit": {
            "abbreviatedOid": "3e50ed7"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-06T18:26:07Z",
          "updatedAt": "2021-07-06T18:28:36Z",
          "comments": [
            {
              "originalPosition": 504,
              "body": "OK, I think it's a regression.",
              "createdAt": "2021-07-06T18:26:07Z",
              "updatedAt": "2021-07-06T18:28:36Z"
            },
            {
              "originalPosition": 522,
              "body": "I don't see why any of this needs to be per-request. They should just be fixed per-task.",
              "createdAt": "2021-07-06T18:26:38Z",
              "updatedAt": "2021-07-06T18:28:36Z"
            },
            {
              "originalPosition": 617,
              "body": "You don't need to carry that in the message. You just do like ```select (proto_in_use)``` or something and then note in the text that it's pointing to something external.",
              "createdAt": "2021-07-06T18:27:33Z",
              "updatedAt": "2021-07-06T18:28:36Z"
            },
            {
              "originalPosition": 657,
              "body": "But that might end up with a tail that could never be aggregated because it was smaller than the minimal batch size.",
              "createdAt": "2021-07-06T18:28:15Z",
              "updatedAt": "2021-07-06T18:28:36Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAxNDQ4OTc2",
          "commit": {
            "abbreviatedOid": "3e50ed7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-07T21:08:21Z",
          "updatedAt": "2021-07-07T21:16:35Z",
          "comments": [
            {
              "originalPosition": 522,
              "body": "For Hits this is also going to convey the set of candidate prefixes. It's therefore not fixed per request.",
              "createdAt": "2021-07-07T21:08:21Z",
              "updatedAt": "2021-07-07T21:16:35Z"
            },
            {
              "originalPosition": 564,
              "body": "Ack, I'm sending a PR today to rip this out.",
              "createdAt": "2021-07-07T21:08:43Z",
              "updatedAt": "2021-07-07T21:16:35Z"
            },
            {
              "originalPosition": 617,
              "body": "Added a TODO to address this later.",
              "createdAt": "2021-07-07T21:11:07Z",
              "updatedAt": "2021-07-07T21:16:35Z"
            },
            {
              "originalPosition": 657,
              "body": "Not true, since the number of \"sub-requests\" isn't necessarily the same as the batch size. The leader makes several aggregate requests and not just one.\r\n\r\nIn any case, I'm good with relaxing this constraint. Added a TODO.",
              "createdAt": "2021-07-07T21:13:00Z",
              "updatedAt": "2021-07-07T21:16:35Z"
            },
            {
              "originalPosition": 694,
              "body": "Ack, added a TODO.",
              "createdAt": "2021-07-07T21:14:32Z",
              "updatedAt": "2021-07-07T21:16:35Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAxNDU2MjM3",
          "commit": {
            "abbreviatedOid": "b0cc270"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-07T21:19:04Z",
          "updatedAt": "2021-07-07T21:19:05Z",
          "comments": [
            {
              "originalPosition": 522,
              "body": "Well, we should definitely factor out the things that are per-task and things that are per-collector-request.",
              "createdAt": "2021-07-07T21:19:05Z",
              "updatedAt": "2021-07-07T21:19:05Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAxNDYwOTU0",
          "commit": {
            "abbreviatedOid": "b0cc270"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-07T21:26:05Z",
          "updatedAt": "2021-07-07T21:26:05Z",
          "comments": [
            {
              "originalPosition": 608,
              "body": "[Oops, I thought I commented on this, but somehow the comment didn't make into the queue.] We're following the precedent set by ECH here: the config id is just a byte. It's used by the decrypting party to figure out which key to use.",
              "createdAt": "2021-07-07T21:26:05Z",
              "updatedAt": "2021-07-07T21:26:05Z"
            }
          ]
        }
      ]
    },
    {
      "number": 60,
      "id": "MDExOlB1bGxSZXF1ZXN0Njc0NzExNDg4",
      "title": "Clean up Prio section and update field selection criteria",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/60",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "- Removes the old Prio text, which by now is out-of-date.\r\n- Addresses #32 by changing the criteria.\r\n",
      "createdAt": "2021-06-21T16:37:07Z",
      "updatedAt": "2021-06-23T20:46:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ddc9e0904f1ada13935d88d69fe6575f06633b58",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/prio-cleanup",
      "headRefOid": "afb156e233228da420168e6203c4ac3a0d4fbd3e",
      "closedAt": "2021-06-23T20:46:02Z",
      "mergedAt": "2021-06-23T20:46:01Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "aa4874af0ab968d514eed3859464d1a1984fd904"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njg4ODgwMzQ3",
          "commit": {
            "abbreviatedOid": "1ee985e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-06-21T21:35:10Z",
          "updatedAt": "2021-06-21T21:38:16Z",
          "comments": [
            {
              "originalPosition": 145,
              "body": "```suggestion\r\n   roots of unity for any 1 <= a <= b. Note that b imposes an upper bound on the\r\n```",
              "createdAt": "2021-06-21T21:35:10Z",
              "updatedAt": "2021-06-21T21:38:16Z"
            },
            {
              "originalPosition": 170,
              "body": "The idea here is that we use HPKE and thus don't need to specify key encapsulation beyond a reference to https://datatracker.ietf.org/doc/draft-irtf-cfrg-hpke/?",
              "createdAt": "2021-06-21T21:38:00Z",
              "updatedAt": "2021-06-21T21:38:16Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njg4ODg5NDgw",
          "commit": {
            "abbreviatedOid": "afb156e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-06-21T21:50:32Z",
          "updatedAt": "2021-06-21T21:50:32Z",
          "comments": [
            {
              "originalPosition": 170,
              "body": "No, we'll need to specify this.",
              "createdAt": "2021-06-21T21:50:32Z",
              "updatedAt": "2021-06-21T21:50:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njg5ODY1MDMz",
          "commit": {
            "abbreviatedOid": "afb156e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-06-22T18:25:19Z",
          "updatedAt": "2021-06-22T18:25:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 63,
      "id": "MDExOlB1bGxSZXF1ZXN0Njc2NjQ3MTYy",
      "title": "Make upload start request a GET",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/63",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "In #48 we did away with the requirement for interaction between the client and leader. This PR resolves the issue by making the upload start request an idempotent GET.",
      "createdAt": "2021-06-23T21:54:33Z",
      "updatedAt": "2021-07-07T21:37:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2ddbf5ed540db63bcc6edbb469777aa70ae4fb58",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/upload-start-get",
      "headRefOid": "cb1e5618125b891d808d7e45e13096ce0a7c2edc",
      "closedAt": "2021-07-07T21:31:23Z",
      "mergedAt": "2021-07-07T21:31:23Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "29f83237ad416a493a6f51a9f9bca5c40dc98c38"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Based on this week's design call (2021/7/7), \"upload start\" is likely going away altogether.",
          "createdAt": "2021-07-07T21:31:18Z",
          "updatedAt": "2021-07-07T21:31:18Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk2Njg1MDE0",
          "commit": {
            "abbreviatedOid": "8b9eebe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-06-30T23:04:27Z",
          "updatedAt": "2021-06-30T23:05:21Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "Our experience with caching OCSP for Let's Encrypt has been that caching POST responses is tricky because CDNs don't always support this as well as caching GETs. However we can resolve that question in #61.",
              "createdAt": "2021-06-30T23:04:27Z",
              "updatedAt": "2021-06-30T23:05:21Z"
            }
          ]
        }
      ]
    },
    {
      "number": 70,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg1NTM4ODYz",
      "title": "Don't have the PA params specify multiple helpers",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/70",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Partially addresses #68. (We also need to decide if/how to allow a particular PA protocol to use multiple helpers for privacy.)\r\n\r\nRight now, multiple helpers are used for redundancy in case one of the\r\nhelpers dropped out. This removes this mechanism from the core protocol.",
      "createdAt": "2021-07-07T21:59:26Z",
      "updatedAt": "2021-07-12T15:05:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "e2a6657e0e3328e14eb38a2789f9b91045f5cb4b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/undo-multiple-helpers",
      "headRefOid": "4b14883bd990377ed9c6ca5e73837db7fdf85d5a",
      "closedAt": "2021-07-12T14:47:09Z",
      "mergedAt": "2021-07-12T14:47:09Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e25240af57a9056b358f4a4f35dfc3c082ed4741"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed and rebased.",
          "createdAt": "2021-07-08T22:13:12Z",
          "updatedAt": "2021-07-08T22:13:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> LGTM. Multiple helpers \"for privacy\" seems like it's inherently a per-protocol thing (Prio supports it, Hits does not). If and when we decide to address that, should we move this to the protocol-specific slot in PAParam?\r\n\r\nThat's a good idea,",
          "createdAt": "2021-07-09T16:04:41Z",
          "updatedAt": "2021-07-09T16:04:41Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> LGTM. Multiple helpers \"for privacy\" seems like it's inherently a per-protocol thing (Prio supports it, Hits does not). If and when we decide to address that, should we move this to the protocol-specific slot in PAParam?\r\n\r\nHmm... Do we need to decide. Per our discussion on Wednesday, if the clients learn this out of band then we won't need this field at all because clients will get it separately and helpers don't need to know what the other helpers are, right?\r\n\r\n\r\n\r\n",
          "createdAt": "2021-07-09T17:08:00Z",
          "updatedAt": "2021-07-09T17:08:00Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Hmm... Do we need to decide. Per our discussion on Wednesday, if the clients learn this out of band then we won't need this field at all because clients will get it separately and helpers don't need to know what the other helpers are, right?\r\n\r\nMy takeaway is that the parameters must contain all input needed to run the protocol, including the helpers. And all of this is configured out of band. You could punt everything except for the cryptographic goop out of the parameters structure, forcing clients to know the leader and helper and get their public keys out of band as well, which would be fine. It's just different. I think both work just fine, but I lean towards having params specify everything. It just seems simpler to me.\r\n\r\nThat said... making public key discovery part of the client upload protocol here _is_ more complex, I think. So, \ud83e\udd37. ",
          "createdAt": "2021-07-09T17:24:11Z",
          "updatedAt": "2021-07-09T17:26:34Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2021-07-12T14:43:32Z",
          "updatedAt": "2021-07-12T14:43:32Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAzMTA5NzYx",
          "commit": {
            "abbreviatedOid": "0d656ee"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM. Multiple helpers \"for privacy\" seems like it's inherently a per-protocol thing (Prio supports it, Hits does not). If and when we decide to address that, should we move this to the protocol-specific slot in PAParam?",
          "createdAt": "2021-07-09T14:29:22Z",
          "updatedAt": "2021-07-09T14:30:59Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "```suggestion\r\nmessage. The client aborts if any of the following happen:\r\n```",
              "createdAt": "2021-07-09T14:29:22Z",
              "updatedAt": "2021-07-09T14:30:59Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAzMjAzMTEw",
          "commit": {
            "abbreviatedOid": "27ff023"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-09T16:12:00Z",
          "updatedAt": "2021-07-09T16:12:00Z",
          "comments": []
        }
      ]
    },
    {
      "number": 71,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg1NTQwNDAz",
      "title": "Editorial",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/71",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Updating authors' comments",
      "createdAt": "2021-07-07T22:02:42Z",
      "updatedAt": "2021-07-12T15:05:15Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "fa4707ebad34820f3cd2aeebfbbd9b6edf1ef02f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "editorial",
      "headRefOid": "43121c2c0ce8c4dbb334322b0f247f7cece34f1a",
      "closedAt": "2021-07-08T22:06:53Z",
      "mergedAt": "2021-07-08T22:06:53Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e2a6657e0e3328e14eb38a2789f9b91045f5cb4b"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2021-07-08T22:06:42Z",
          "updatedAt": "2021-07-08T22:06:42Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAxNDg3MDUw",
          "commit": {
            "abbreviatedOid": "90ac29d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-07T22:09:08Z",
          "updatedAt": "2021-07-07T22:10:04Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "```suggestion\r\nserver-authenticated secure channel over which the request is made.)\r\n```",
              "createdAt": "2021-07-07T22:09:09Z",
              "updatedAt": "2021-07-07T22:10:04Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTIyNjU4",
          "commit": {
            "abbreviatedOid": "30ca601"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-08T21:40:18Z",
          "updatedAt": "2021-07-08T21:40:18Z",
          "comments": []
        }
      ]
    },
    {
      "number": 73,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg1NTc1OTI3",
      "title": "Encrypt the leader's share as well",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/73",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #70 (review that first).\r\nCloses #69.",
      "createdAt": "2021-07-07T23:31:02Z",
      "updatedAt": "2021-07-12T15:05:20Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "e25240af57a9056b358f4a4f35dfc3c082ed4741",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/encrypt-leader-share",
      "headRefOid": "8661a987fa275363e047772817b848832098c952",
      "closedAt": "2021-07-12T15:04:33Z",
      "mergedAt": "2021-07-12T15:04:33Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e0d1e7c1985adf398b9c58dd977c9ffa1b3aa1da"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed and rebased.",
          "createdAt": "2021-07-09T00:37:59Z",
          "updatedAt": "2021-07-09T00:37:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased.",
          "createdAt": "2021-07-12T15:01:06Z",
          "updatedAt": "2021-07-12T15:01:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2021-07-12T15:03:34Z",
          "updatedAt": "2021-07-12T15:03:34Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAxNTQ5ODg4",
          "commit": {
            "abbreviatedOid": "d021dfe"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-07-08T00:40:35Z",
          "updatedAt": "2021-07-08T00:44:56Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "Hmm.... So how does this work if the client can't connect to the leader.",
              "createdAt": "2021-07-08T00:40:35Z",
              "updatedAt": "2021-07-08T00:44:56Z"
            },
            {
              "originalPosition": 87,
              "body": "This seems like a good opportunity to just be:\r\n\r\n```\r\nPAEncryptedInputShare shares<...>;\r\n```\r\n\r\nThat is like the least we can do to lay the groundwork for >1 helper.",
              "createdAt": "2021-07-08T00:41:32Z",
              "updatedAt": "2021-07-08T00:44:56Z"
            },
            {
              "originalPosition": 175,
              "body": "This is another opportunity to avoid duplication, but just talking about aggregators.",
              "createdAt": "2021-07-08T00:43:24Z",
              "updatedAt": "2021-07-08T00:44:56Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyMzQ2NDU1",
          "commit": {
            "abbreviatedOid": "d021dfe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-08T17:50:08Z",
          "updatedAt": "2021-07-08T20:49:25Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "It doesn't :) The leader needs to be online for the client to be able to upload a report. (This was already the case before this PR.)",
              "createdAt": "2021-07-08T17:50:08Z",
              "updatedAt": "2021-07-08T20:49:25Z"
            },
            {
              "originalPosition": 87,
              "body": "Done.",
              "createdAt": "2021-07-08T20:48:02Z",
              "updatedAt": "2021-07-08T20:49:25Z"
            },
            {
              "originalPosition": 175,
              "body": "Done.",
              "createdAt": "2021-07-08T20:49:05Z",
              "updatedAt": "2021-07-08T20:49:25Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNDkxMTM0",
          "commit": {
            "abbreviatedOid": "b1b4f7f"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-08T20:53:28Z",
          "updatedAt": "2021-07-08T20:53:50Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "Yeah, you can't do this in the TLS syntax. I would just say this stuff verbally\r\n\"Of the appropriate share type for the protocol and role\" down on line 593 below.",
              "createdAt": "2021-07-08T20:53:28Z",
              "updatedAt": "2021-07-08T20:53:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTAzNzY2",
          "commit": {
            "abbreviatedOid": "b1b4f7f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-08T21:10:53Z",
          "updatedAt": "2021-07-08T21:11:33Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "Removed the PAInputShare structure altogether and clarified above that the the format of the input share depends on the protocol and aggreagtor role.",
              "createdAt": "2021-07-08T21:10:53Z",
              "updatedAt": "2021-07-08T21:11:33Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTA1MTM5",
          "commit": {
            "abbreviatedOid": "e729056"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-08T21:12:58Z",
          "updatedAt": "2021-07-08T22:02:12Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "Right. I think I'm wondering if this feedback channel is actually helpful.",
              "createdAt": "2021-07-08T21:12:59Z",
              "updatedAt": "2021-07-08T22:02:12Z"
            },
            {
              "originalPosition": 117,
              "body": "nit: isn't this 7..2^16-1:\r\n\r\n1 for config id\r\n2 each for the lengths of enc and payload\r\n1 each for the minimum size of length and payload.",
              "createdAt": "2021-07-08T22:00:57Z",
              "updatedAt": "2021-07-08T22:02:12Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTM5OTk1",
          "commit": {
            "abbreviatedOid": "252caf1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-08T22:10:33Z",
          "updatedAt": "2021-07-08T22:10:34Z",
          "comments": [
            {
              "originalPosition": 117,
              "body": "Yeah, well, we'll need to fix up the bounds elsewhere as well. I'll add a TODO to handle this later, once the protocol gets closer to settled.",
              "createdAt": "2021-07-08T22:10:33Z",
              "updatedAt": "2021-07-08T22:10:34Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTQwMTgx",
          "commit": {
            "abbreviatedOid": "252caf1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-08T22:10:54Z",
          "updatedAt": "2021-07-08T22:10:54Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "What do you mean by \"feedback channel\"?",
              "createdAt": "2021-07-08T22:10:54Z",
              "updatedAt": "2021-07-08T22:10:54Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTQ5MDUx",
          "commit": {
            "abbreviatedOid": "252caf1"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-08T22:28:58Z",
          "updatedAt": "2021-07-08T22:28:59Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "I assume he means the alert from client to leader. What is the leader actually supposed to do with that information?",
              "createdAt": "2021-07-08T22:28:58Z",
              "updatedAt": "2021-07-08T22:28:59Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAyNTkyNzY4",
          "commit": {
            "abbreviatedOid": "805aa1b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-09T00:23:34Z",
          "updatedAt": "2021-07-09T00:23:34Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "Ah yes, good call. I removed the alert.",
              "createdAt": "2021-07-09T00:23:34Z",
              "updatedAt": "2021-07-09T00:23:34Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAzMjA2NDEw",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-09T16:16:12Z",
          "updatedAt": "2021-07-09T16:16:12Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAzMzkzOTA1",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM with #75 spun out!",
          "createdAt": "2021-07-09T21:08:41Z",
          "updatedAt": "2021-07-09T21:14:55Z",
          "comments": [
            {
              "originalPosition": 120,
              "body": "```suggestion\r\n  PATask task;\r\n```\r\n(There is no more UploadStartReq, right?)",
              "createdAt": "2021-07-09T21:08:41Z",
              "updatedAt": "2021-07-09T21:14:55Z"
            },
            {
              "originalPosition": 191,
              "body": "I filed #75 to follow up on this, but: what good is authenticating the timestamp if the client can just lie about it?",
              "createdAt": "2021-07-09T21:13:45Z",
              "updatedAt": "2021-07-09T21:14:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MTgwNTg3",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:27:42Z",
          "updatedAt": "2021-07-12T14:27:42Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "Authenticating the timestamp is for the client's sake. It's meant to ensure its report is only used for a single batch.",
              "createdAt": "2021-07-12T14:27:42Z",
              "updatedAt": "2021-07-12T14:27:43Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MTgxMDE1",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:28:03Z",
          "updatedAt": "2021-07-12T14:28:03Z",
          "comments": [
            {
              "originalPosition": 120,
              "body": "There is, as of this PR.",
              "createdAt": "2021-07-12T14:28:03Z",
              "updatedAt": "2021-07-12T14:28:04Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MTg0NDY3",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:30:51Z",
          "updatedAt": "2021-07-12T14:30:52Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "Hmm, I don't really see how it helps the client at all. Can you elaborate?",
              "createdAt": "2021-07-12T14:30:51Z",
              "updatedAt": "2021-07-12T14:30:52Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MTk1NDY5",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:40:17Z",
          "updatedAt": "2021-07-12T14:40:17Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "The idea was discussed a few weeks ago, but hasn't been fully fleshed out. Basically the PA parameters specify a batch window [batch_start, batch_end). To check that a report should be added to a batch, an aggregator decrypts its input share. Decryption will fail if the reported timestamp doesn't match the timestamp generated by the client. This ensures that an honest aggregator only includes an input share in a batch if the timestamp is in the batch window.",
              "createdAt": "2021-07-12T14:40:17Z",
              "updatedAt": "2021-07-12T14:40:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MjAwNTA1",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:44:30Z",
          "updatedAt": "2021-07-12T14:44:30Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "I get that, but how does this benefit the _client_? (We should just take this to #75.)",
              "createdAt": "2021-07-12T14:44:30Z",
              "updatedAt": "2021-07-12T14:44:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MjAxMzAw",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:45:11Z",
          "updatedAt": "2021-07-12T14:45:11Z",
          "comments": [
            {
              "originalPosition": 120,
              "body": "I may be missing something then. I don't see PAUploadStartReq defined in this PR's diff?",
              "createdAt": "2021-07-12T14:45:11Z",
              "updatedAt": "2021-07-12T14:45:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MjE3NzEz",
          "commit": {
            "abbreviatedOid": "b603894"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T14:58:55Z",
          "updatedAt": "2021-07-12T14:58:55Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "Done.",
              "createdAt": "2021-07-12T14:58:55Z",
              "updatedAt": "2021-07-12T14:58:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MjIyNTU1",
          "commit": {
            "abbreviatedOid": "109e97a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T15:02:55Z",
          "updatedAt": "2021-07-12T15:02:55Z",
          "comments": [
            {
              "originalPosition": 120,
              "body": "Ohhh right! The message got removed, but the \"upload start\" flow still exists. Good call :)",
              "createdAt": "2021-07-12T15:02:55Z",
              "updatedAt": "2021-07-12T15:02:55Z"
            }
          ]
        }
      ]
    },
    {
      "number": 74,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg1NTgxODI2",
      "title": "Add a length field to PAParam",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/74",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #66.\r\n\r\nThis ensures that this structure can be parsed once we add more PAProto variants.",
      "createdAt": "2021-07-07T23:49:39Z",
      "updatedAt": "2021-07-12T15:05:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "29f83237ad416a493a6f51a9f9bca5c40dc98c38",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/pa-param-length",
      "headRefOid": "98c4338798bf6414b07ea59114b8aa161cc2983f",
      "closedAt": "2021-07-08T12:15:42Z",
      "mergedAt": "2021-07-08T12:15:42Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "fa4707ebad34820f3cd2aeebfbbd9b6edf1ef02f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzAxNTUxNDg3",
          "commit": {
            "abbreviatedOid": "98c4338"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2021-07-08T00:45:28Z",
          "updatedAt": "2021-07-08T00:45:28Z",
          "comments": []
        }
      ]
    },
    {
      "number": 76,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg3OTUwMTA2",
      "title": "Derive task id and remove doc version",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/76",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #67 by removing the `version` field from each of the protocol messages.\r\nPartially addresses #72 by establishing that the task id is derived from the PA parameters for a task. We still need to specify this procedure.",
      "createdAt": "2021-07-12T15:33:38Z",
      "updatedAt": "2021-07-13T18:54:04Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "e0d1e7c1985adf398b9c58dd977c9ffa1b3aa1da",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/derive-task-id",
      "headRefOid": "0460f73dc27872038aa952821d272b83f26b69af",
      "closedAt": "2021-07-13T18:54:04Z",
      "mergedAt": "2021-07-13T18:54:04Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "adf34141a918e5019d327f67f3ba50bc52ed2457"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MjYwMTA0",
          "commit": {
            "abbreviatedOid": "0460f73"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-12T15:35:42Z",
          "updatedAt": "2021-07-12T15:36:47Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "Can we follow up on this in a new issue? What's the motivation to remove this?",
              "createdAt": "2021-07-12T15:35:42Z",
              "updatedAt": "2021-07-12T15:36:47Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MjY3MjUx",
          "commit": {
            "abbreviatedOid": "0460f73"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-12T15:42:29Z",
          "updatedAt": "2021-07-12T15:42:29Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "It was brought up here: https://github.com/abetterinternet/prio-documents/issues/65",
              "createdAt": "2021-07-12T15:42:29Z",
              "updatedAt": "2021-07-12T15:42:29Z"
            }
          ]
        }
      ]
    },
    {
      "number": 77,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg3OTc0Nzk1",
      "title": "Refactor upload request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/77",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #76 (review that first).\r\nPartially addresses #65.\r\n\r\n* Assume the client knows PAParam ahead of time, per issue#65.\r\n* Add leader URL to PAParam so that client can request its key config.\r\n* Get rid of upload start request.",
      "createdAt": "2021-07-12T16:06:56Z",
      "updatedAt": "2021-08-04T17:43:54Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton/derive-task-id",
      "baseRefOid": "0460f73dc27872038aa952821d272b83f26b69af",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/upload-refactor",
      "headRefOid": "d80c7156c65fdcfc34bab0a21dfe63511295500a",
      "closedAt": "2021-07-13T18:54:37Z",
      "mergedAt": "2021-07-13T18:54:36Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "3585f951e7d7d3847f092b9a8842f5cb764de0dd"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MzA0OTAw",
          "commit": {
            "abbreviatedOid": "d80c715"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-12T16:18:42Z",
          "updatedAt": "2021-07-12T16:18:42Z",
          "comments": []
        }
      ]
    },
    {
      "number": 78,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg4MDE3Nzgy",
      "title": "Allow multiple reports in the same upload request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/78",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #77 (review that first).\r\nCloses #64.\r\n\r\ncc/ @csharrison",
      "createdAt": "2021-07-12T17:09:37Z",
      "updatedAt": "2021-12-30T02:10:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton/upload-refactor",
      "baseRefOid": "d80c7156c65fdcfc34bab0a21dfe63511295500a",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/batched-upload",
      "headRefOid": "78b0e191584195e96bd23d1e4a1697b6f2ceb686",
      "closedAt": "2021-07-14T17:59:58Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I actually don't understand what the benefit of this is. I see why you would want to do multiple submissions in a batch, but modern networking protocols such as H2 and H3 can do multiple concurrent HTTP requests in parallel, so why not just do one report at a time?",
          "createdAt": "2021-07-12T17:22:42Z",
          "updatedAt": "2021-07-12T17:22:42Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I actually don't understand what the benefit of this is.\r\n\r\nI see it as purely an optimization to avoid sending the task ID over and over for large batches. ",
          "createdAt": "2021-07-12T17:25:13Z",
          "updatedAt": "2021-07-12T17:25:13Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> I see it as purely an optimization to avoid sending the task ID over and over for large batches.\r\n\r\nHmm... The task ID is in general going to be quite small compared to almost any report, right? I mean, just the crypto overhead is going to be order 48 bytes.",
          "createdAt": "2021-07-12T17:31:27Z",
          "updatedAt": "2021-07-12T17:31:27Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Hmm... The task ID is in general going to be quite small compared to almost any report, right? I mean, just the crypto overhead is going to be order 48 bytes.\r\n\r\nIt is -- I didn't say it was the most _useful_ optimization ;)",
          "createdAt": "2021-07-12T17:32:14Z",
          "updatedAt": "2021-07-12T17:32:14Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "cc @hostirosti FYI. I like that this matches the mechanism we use to implement batch uploads from leader -> helper. Whatever future optimizations we implement for that could be useful here too (e.g. if we move away from HTTP POST and pass some file handle thing).",
          "createdAt": "2021-07-12T17:52:55Z",
          "updatedAt": "2021-07-12T17:52:55Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't understand why moving away from POST will make things better. The vast amount of HTTP overhead is just moving the actual data around.\r\n\r\nIn any case, this seems like a case of adding protocol complexity for very little additional value, so I think we should not merge it.",
          "createdAt": "2021-07-12T21:58:59Z",
          "updatedAt": "2021-07-12T21:58:59Z"
        },
        {
          "author": "hostirosti",
          "authorAssociation": "NONE",
          "body": "We should also look at this from the server side, yes with HTTP/2 you can have non-blocking parallel connections yet the server side needs to be able to handle the amount of connections as well, every connection carries overhead for establishing the connection (TCP, TLS handshake, ...). So reducing the number of connections from the client side will have a benefit for cost and operation on the server side, after all we talk 10^5s to millions of clients here. I'd definitely recommend reusing the connection and if that means batching, use batching. Since H2 supports streaming and if streaming individual reports is easier this should not add much additional overhead (other than redundant info we'd need for each report instead of once per batch) on the connection side I assume.",
          "createdAt": "2021-07-13T19:53:50Z",
          "updatedAt": "2021-07-13T19:56:52Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@hostirosti I think @ekr's point is that you can do batching _within_ the protocol, as is proposed in this PR, or outside the protocol using independent HTTP streams. In both cases, the number of client<>server connections remains the same, right?",
          "createdAt": "2021-07-13T21:36:50Z",
          "updatedAt": "2021-07-13T21:36:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "In the 2021/7/14 call we decided to \"park\" this change for now. We all agree that the issue needs to be addressed, i.e., that an entity (client or collector) should be able to upload multiple reports. We haven't decided on whether these should be in the same HTTP request.",
          "createdAt": "2021-07-14T17:59:45Z",
          "updatedAt": "2021-07-14T17:59:45Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0MzU3ODU0",
          "commit": {
            "abbreviatedOid": "78b0e19"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-12T17:16:02Z",
          "updatedAt": "2021-07-12T17:16:02Z",
          "comments": []
        }
      ]
    },
    {
      "number": 79,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg4MDQxNjM0",
      "title": "Clean up aggregate sub-request handling",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/79",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "* Update the aggregate request text to reflect recent protocol changes.\r\n* Relax the requirement that all sub-requests use the same key config.\r\n* Do https://github.com/abetterinternet/prio-documents/pull/77, which was previously merged into the wrong branch.",
      "createdAt": "2021-07-12T17:40:10Z",
      "updatedAt": "2021-08-04T17:43:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a9fb326f52273e6aafd1cb148d3dfbaeb607b7c6",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/aggregate-request-relaxation",
      "headRefOid": "ad015cc6e9d60363c8ef69d23f645ef9482a3b7d",
      "closedAt": "2021-07-14T18:24:59Z",
      "mergedAt": "2021-07-14T18:24:59Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "3f47963fcc220ac3c844fbad89638e22c57dd3fd"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan, @ekr: please have a look when you can.",
          "createdAt": "2021-07-13T19:00:03Z",
          "updatedAt": "2021-07-13T19:00:03Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA0NDE0NTM4",
          "commit": {
            "abbreviatedOid": "a2c64ff"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-12T18:22:17Z",
          "updatedAt": "2021-07-12T18:25:05Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "\u2764\ufe0f ",
              "createdAt": "2021-07-12T18:22:17Z",
              "updatedAt": "2021-07-12T18:25:05Z"
            }
          ]
        }
      ]
    },
    {
      "number": 80,
      "id": "MDExOlB1bGxSZXF1ZXN0Njg5MzIzNzUx",
      "title": "Add timestamp rationale.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/80",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Partially addresses #75 via clarification. Intra-batch replay attacks need more text.",
      "createdAt": "2021-07-13T18:56:51Z",
      "updatedAt": "2021-12-30T02:10:05Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "adf34141a918e5019d327f67f3ba50bc52ed2457",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/timestamp-rationale",
      "headRefOid": "8e4c429a5c7c8eaf5708d254f9cc7054f14fc1fb",
      "closedAt": "2021-07-14T17:52:01Z",
      "mergedAt": "2021-07-14T17:52:01Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "a9fb326f52273e6aafd1cb148d3dfbaeb607b7c6"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton I was going to file a separate issue to discuss it. Stay tuned!",
          "createdAt": "2021-07-13T19:13:45Z",
          "updatedAt": "2021-07-13T19:13:45Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA1NTU0MTg0",
          "commit": {
            "abbreviatedOid": "8e4c429"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Might be worth adding an OPEN ISSUE to note the inter-batch replay attack we talked about yesterday.",
          "createdAt": "2021-07-13T19:11:58Z",
          "updatedAt": "2021-07-13T19:11:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 83,
      "id": "MDExOlB1bGxSZXF1ZXN0NjkwMjAwMzA4",
      "title": "s/PA/PDA/g",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/83",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-07-14T20:22:34Z",
      "updatedAt": "2021-08-04T17:43:50Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3f47963fcc220ac3c844fbad89638e22c57dd3fd",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/editorial",
      "headRefOid": "641088ede02e3b80a3f5f2afa3114f0c4f244bd4",
      "closedAt": "2021-07-14T20:49:14Z",
      "mergedAt": "2021-07-14T20:49:14Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f00398767e2ce563b88cf3a6e603bb2f8a17732f"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2021-07-14T20:48:37Z",
          "updatedAt": "2021-07-14T20:48:37Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA2Njk0MTk1",
          "commit": {
            "abbreviatedOid": "494b2e5"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM modulo one nit",
          "createdAt": "2021-07-14T20:32:30Z",
          "updatedAt": "2021-07-14T20:36:47Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "```suggestion\r\n*private data aggregation (PDA) protocol* is to compute `y = F(x_1, ..., x_n)` for\r\n```",
              "createdAt": "2021-07-14T20:32:31Z",
              "updatedAt": "2021-07-14T20:36:47Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA2NzA1Mzkw",
          "commit": {
            "abbreviatedOid": "494b2e5"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-14T20:46:47Z",
          "updatedAt": "2021-07-14T20:46:47Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Good catch",
              "createdAt": "2021-07-14T20:46:47Z",
              "updatedAt": "2021-07-14T20:46:47Z"
            }
          ]
        }
      ]
    },
    {
      "number": 84,
      "id": "MDExOlB1bGxSZXF1ZXN0NjkwMjQ1NTI0",
      "title": "Tidy and expand security considerations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/84",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Cleans up the \"Security Considerations\" section which had some excessive nesting and adds content to address #15 and #58",
      "createdAt": "2021-07-14T21:45:45Z",
      "updatedAt": "2021-12-30T00:53:25Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "989cea565fbeaf0dcc7f5f9ac7f7cf0f3446a246",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/security-considerations",
      "headRefOid": "6a05ef00e84c1537fa8e1cbdbd966313c6d4bd6a",
      "closedAt": "2021-07-28T21:24:37Z",
      "mergedAt": "2021-07-28T21:24:37Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "dd4aeffbfbb9245fff64f1eccff16d35e7cdf07e"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA3NzY3NTE1",
          "commit": {
            "abbreviatedOid": "6cd5910"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-07-15T19:58:29Z",
          "updatedAt": "2021-07-15T20:00:17Z",
          "comments": [
            {
              "originalPosition": 194,
              "body": "Depending on how authentication is implemented, it might end up deanonmyzing clients that go through an an anomyzing proxy service like OHTTP.  For instance, suppose clients just sign their reports, and the leader is supposed to look up their public key to check the signature. Of course, anonymous credentials are possible, e.g., one of the myriad DAA schemes implemented by TPMs these days.\r\n\r\nIn any case, I think we want to decouple these things. OHTTP may be useful *independently* of client authentication. ",
              "createdAt": "2021-07-15T19:58:30Z",
              "updatedAt": "2021-07-15T20:00:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE3NDcyNDA3",
          "commit": {
            "abbreviatedOid": "9c04a9e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I suggest we remove the client auth stuff atogether and punt to the PR we discussed today, then merge this bad boy.",
          "createdAt": "2021-07-28T21:15:03Z",
          "updatedAt": "2021-07-28T21:16:05Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "On today's call (2021/07/28) we decided to address issue#89 by sticking in an opaque blob that deployment can use for attestation purposes. I think we should just remove this section and replace it with:\r\n```\r\n## Client Attestation\r\n[TODO: Solve issue#89]\r\n```\r\nThen we can fully address it in a follow-on PR.",
              "createdAt": "2021-07-28T21:15:04Z",
              "updatedAt": "2021-07-28T21:15:48Z"
            }
          ]
        }
      ]
    },
    {
      "number": 87,
      "id": "MDExOlB1bGxSZXF1ZXN0NjkwOTE2NTUz",
      "title": "clarify PATaskID derivation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/87",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Resolves #72",
      "createdAt": "2021-07-15T17:38:09Z",
      "updatedAt": "2021-12-30T00:53:23Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f00398767e2ce563b88cf3a6e603bb2f8a17732f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/pda-params-uniqueness",
      "headRefOid": "98c2954cbc5befcc17eae3d1cb750cda86612228",
      "closedAt": "2021-07-20T17:54:50Z",
      "mergedAt": "2021-07-20T17:54:50Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "a55f71ac65e531a7a4e7c10ab83307be08032586"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Approved, with the following suggestion: Let's leave a [TODO: ...] comment under the definition of `nonce` for clarifying how it should be chosen.\r\n\r\nI added a clarification that the nonce is random. IMO the question of *who* randomly generates the nonce goes with the question of how participants agree upon PDAParams, which is to say it's outside the protocol.",
          "createdAt": "2021-07-15T21:22:48Z",
          "updatedAt": "2021-07-15T21:22:48Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA3NzcxMDgx",
          "commit": {
            "abbreviatedOid": "2e7348c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Approved, with the following suggestion: Let's leave a [TODO: ...] comment under the definition of `nonce` for clarifying how it should be chosen.",
          "createdAt": "2021-07-15T20:02:51Z",
          "updatedAt": "2021-07-15T20:02:51Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzEwMDY3NjA0",
          "commit": {
            "abbreviatedOid": "549388e"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-19T23:41:55Z",
          "updatedAt": "2021-07-19T23:46:11Z",
          "comments": [
            {
              "originalPosition": 34,
              "body": "```suggestion\r\nThe task ID is of a `PDAParam` instance `param` is derived using the following procedure:\r\n\r\n~~~\r\ntask_id = SHA-256(param)\r\n~~~\r\n```\r\n\r\nWhere SHA-256 is as specified in {{FIPS180-4}}.",
              "createdAt": "2021-07-19T23:41:55Z",
              "updatedAt": "2021-07-19T23:46:11Z"
            },
            {
              "originalPosition": 12,
              "body": "```suggestion\r\n* `nonce`: A unique sequence of bytes used to ensure that two otherwise\r\n```",
              "createdAt": "2021-07-19T23:43:28Z",
              "updatedAt": "2021-07-19T23:46:11Z"
            },
            {
              "originalPosition": 13,
              "body": "```suggestion\r\n  identical `PDAParam` instances will have distinct `PDATaskID`s. It is RECOMMENDED\r\n  that this be set to a random 16-byte string derived from a cryptographically secure \r\n  pseudorandom number generator.\r\n```",
              "createdAt": "2021-07-19T23:45:57Z",
              "updatedAt": "2021-07-19T23:46:11Z"
            }
          ]
        }
      ]
    },
    {
      "number": 88,
      "id": "MDExOlB1bGxSZXF1ZXN0NjkwOTI1Nzcw",
      "title": "introduce definitions for time units",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/88",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Introduces `Time` and `Duration` definitions, principally to make it\r\nmore obvious that `batch_window` is a number of seconds.",
      "createdAt": "2021-07-15T17:52:55Z",
      "updatedAt": "2021-12-30T00:53:24Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a55f71ac65e531a7a4e7c10ab83307be08032586",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/time-units",
      "headRefOid": "a6ed069a6e4b283bc94a5fff4e1eb38e467b2d41",
      "closedAt": "2021-07-20T18:07:42Z",
      "mergedAt": "2021-07-20T18:07:42Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "989cea565fbeaf0dcc7f5f9ac7f7cf0f3446a246"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzA3NzcxODIw",
          "commit": {
            "abbreviatedOid": "e3c0bdf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Good idea!",
          "createdAt": "2021-07-15T20:03:44Z",
          "updatedAt": "2021-07-15T20:03:44Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzEwMDY5Njc1",
          "commit": {
            "abbreviatedOid": "e3c0bdf"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Yeah, this is a nice improvement. I'd keep Time and Duration colocated so we can more clearly define in terms of the other.",
          "createdAt": "2021-07-19T23:47:11Z",
          "updatedAt": "2021-07-19T23:47:34Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nTime uint64; /* seconds elapsed since start of UNIX epoch */\r\nDuration uint64; /* Number of seconds elapsed between two Time instants */\r\n```",
              "createdAt": "2021-07-19T23:47:11Z",
              "updatedAt": "2021-07-19T23:47:34Z"
            }
          ]
        }
      ]
    },
    {
      "number": 90,
      "id": "MDExOlB1bGxSZXF1ZXN0Njk3MzY0ODI4",
      "title": "Add additional anti-replay protection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/90",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #81.",
      "createdAt": "2021-07-26T21:26:19Z",
      "updatedAt": "2021-08-04T17:43:49Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "dd4aeffbfbb9245fff64f1eccff16d35e7cdf07e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/intra-batch",
      "headRefOid": "56a949c5a3a8d697ce1b2bd219bc8a061b404196",
      "closedAt": "2021-07-29T20:55:06Z",
      "mergedAt": "2021-07-29T20:55:06Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "51efe71f37bb2d09acef59dde80326acb2e22b8f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE3MDg1ODc5",
          "commit": {
            "abbreviatedOid": "65495b1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I have a few questions but nothing blocking.",
          "createdAt": "2021-07-28T14:34:15Z",
          "updatedAt": "2021-07-28T14:50:57Z",
          "comments": [
            {
              "originalPosition": 173,
              "body": "```suggestion\r\nreport than intended by the PDA protocol. For example, this may violate\r\ndifferential privacy. To mitigate this issue, the core specification imposes an\r\n```",
              "createdAt": "2021-07-28T14:34:15Z",
              "updatedAt": "2021-07-28T14:50:57Z"
            },
            {
              "originalPosition": 188,
              "body": "With 32 bit jitter, by the birthday paradox, we'd expect jitter collisions if a single task generates more than 2.1 billion reports in a second across all clients. That's definitely a lot, but why not make the jitter 64 bits so that it's astronomically unlikely?",
              "createdAt": "2021-07-28T14:44:09Z",
              "updatedAt": "2021-07-28T14:50:57Z"
            },
            {
              "originalPosition": 45,
              "body": "Do we have to define \"||\" as concatenation somewhere?",
              "createdAt": "2021-07-28T14:45:42Z",
              "updatedAt": "2021-07-28T14:50:57Z"
            },
            {
              "originalPosition": 101,
              "body": "If the leader is responsible for reordering reports, then it should allow a grace period after an aggregation window starts for late reports to arrive from clients. Unlike Prio v2, the two aggregators don't need to agree on the grace period, since the leader drives aggregations (at the request of a collector, but since those are going to have to be asynchronous anyway I think it's OK for a leader to acknowledge a collect request but not service it until the leader is satisfied the aggregation window's grace period has elapsed). So I think we don't need to put grace period into PDAParams or specify much about what leaders should do, but perhaps we should have a SHOULD statement urging leader implementations to implement a grace period proportional to the aggregation window or other deployment specific details (e.g. special knowledge of the schedule on which clients upload reports)?",
              "createdAt": "2021-07-28T14:49:57Z",
              "updatedAt": "2021-07-28T14:50:57Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4MzY0MzE3",
          "commit": {
            "abbreviatedOid": "65495b1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T17:36:57Z",
          "updatedAt": "2021-07-29T17:58:06Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "I don't think so.",
              "createdAt": "2021-07-29T17:36:57Z",
              "updatedAt": "2021-07-29T17:58:07Z"
            },
            {
              "originalPosition": 188,
              "body": "Actually, by the birthday paradox, the probability of a collision among just 2^16 reports is about 1.  For 64 bits, this same is true for 2^32 reports. If we want this probability to be negligible (i.e., about as unlikely as cracking AES-GCM, say) we would need 128 bits.\r\n\r\nPersonally, I'd be happy with 64 bits.",
              "createdAt": "2021-07-29T17:41:14Z",
              "updatedAt": "2021-07-29T17:58:07Z"
            },
            {
              "originalPosition": 101,
              "body": "Good idea, done.",
              "createdAt": "2021-07-29T17:57:55Z",
              "updatedAt": "2021-07-29T17:58:07Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4MzgzMjYw",
          "commit": {
            "abbreviatedOid": "161225f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T17:58:24Z",
          "updatedAt": "2021-07-29T17:58:24Z",
          "comments": [
            {
              "originalPosition": 188,
              "body": "Done.",
              "createdAt": "2021-07-29T17:58:24Z",
              "updatedAt": "2021-07-29T17:58:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NDQ4Nzcz",
          "commit": {
            "abbreviatedOid": "161225f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-29T19:16:14Z",
          "updatedAt": "2021-07-29T19:16:36Z",
          "comments": [
            {
              "originalPosition": 69,
              "body": "Should we say that leaders should drop reports whose timestamps fall outside of the batch window? (Happy for that to be an open issue, if desired.)",
              "createdAt": "2021-07-29T19:16:15Z",
              "updatedAt": "2021-07-29T19:16:36Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTI0Nzc3",
          "commit": {
            "abbreviatedOid": "161225f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:53:56Z",
          "updatedAt": "2021-07-29T20:53:56Z",
          "comments": [
            {
              "originalPosition": 69,
              "body": "I think this would be a bit confusing here. Ideally this is clear from {{pa-aggregate}}.",
              "createdAt": "2021-07-29T20:53:56Z",
              "updatedAt": "2021-07-29T20:53:56Z"
            }
          ]
        }
      ]
    },
    {
      "number": 91,
      "id": "MDExOlB1bGxSZXF1ZXN0Njk5MDYyODk3",
      "title": "Add differential privacy as a threat mitigation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/91",
      "state": "MERGED",
      "author": "csharrison",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Differential privacy can protect against attacks where a portion of the input data is known a priori, or when the size of a batch is small. This PR adds DP as an optional mitigation for PDA deployments and lists where it helps mitigate various threats.\r\n\r\nAdditionally:\r\n- Removes a false statement that revealing user input does not compromise other users' privacy. This is not necessarily the case (e.g. imagine if the batch size is 2 or if many users reveal their input).\r\n- Removes a statement that clients revealing their inputs is outside the threat model (is this still relevant?)",
      "createdAt": "2021-07-28T22:49:41Z",
      "updatedAt": "2021-08-02T16:47:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "dd4aeffbfbb9245fff64f1eccff16d35e7cdf07e",
      "headRepository": "csharrison/prio-documents",
      "headRefName": "diff-priv",
      "headRefOid": "fc636b12595727337cb83c28065306ae78b899d6",
      "closedAt": "2021-08-02T16:47:42Z",
      "mergedAt": "2021-08-02T16:47:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "2590598856adae8dd386d8e6762c42de89f83161"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4MzU1Mzg4",
          "commit": {
            "abbreviatedOid": "5ffc351"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Excellent! Requested changes are editorial.",
          "createdAt": "2021-07-29T17:26:53Z",
          "updatedAt": "2021-07-29T17:34:33Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "I would work in the term \"Sybil attack\" here. While we're at it, let's add a reference for Sybil attacks (i.e., `{{sybil}}`). I think the canonical reference is https://link.springer.com/chapter/10.1007/3-540-45748-8_24.",
              "createdAt": "2021-07-29T17:26:53Z",
              "updatedAt": "2021-07-29T17:34:33Z"
            },
            {
              "originalPosition": 28,
              "body": "Would you mind adding a reference for `{{dp}}` and verify the document builds by running `make`? See the top of the document for guidance. If you don't have time, then please at least provide a link to an article you think would be a suitable reference. The most useful reference would be something that describes the distinction between client-side and server-side noise addition.",
              "createdAt": "2021-07-29T17:31:10Z",
              "updatedAt": "2021-07-29T17:34:33Z"
            },
            {
              "originalPosition": 65,
              "body": "EDITED: Given that the likely trajectory of this document is an IETF WG draft, it would be better to replace this markdown with a reference to a paper about DP. The most useful reference would be something that describes the distinction between client-side and server-side noise addition.",
              "createdAt": "2021-07-29T17:32:27Z",
              "updatedAt": "2021-07-29T18:00:03Z"
            },
            {
              "originalPosition": 66,
              "body": "```suggestion\r\nA simple approach would require the aggregators to add two-sided\r\n```\r\nDepending on what we do for #68 there may be more than one helper.",
              "createdAt": "2021-07-29T17:33:29Z",
              "updatedAt": "2021-07-29T17:34:33Z"
            },
            {
              "originalPosition": 69,
              "body": "```suggestion\r\neven if all but one of the aggregators is malicious. Differential privacy is a strong\r\n```",
              "createdAt": "2021-07-29T17:33:53Z",
              "updatedAt": "2021-07-29T17:34:33Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4MzY2OTg2",
          "commit": {
            "abbreviatedOid": "5ffc351"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I have one question about whether we need more explicit protocol support for DP but if so, that can land in a subsequent PR.",
          "createdAt": "2021-07-29T17:40:01Z",
          "updatedAt": "2021-07-29T17:40:42Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "Besides this informal recommendation, do we need explicit protocol support for differential privacy so that collectors can de-noise outputs? We can leave it up to aggregators to decide how they're going to implement DP but I wonder if `PDAOutputShare` should have a field for the epsilon value that was used by the aggregator. Forgive me if I'm talking nonsense about DP, I am speaking in the terms that we used in Prio v2.",
              "createdAt": "2021-07-29T17:40:01Z",
              "updatedAt": "2021-07-29T17:40:42Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4Mzg0MDU5",
          "commit": {
            "abbreviatedOid": "5ffc351"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T17:59:15Z",
          "updatedAt": "2021-07-29T17:59:15Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "Oops, I missed that {{dp}} refers to a section and not a paper. Disregard.",
              "createdAt": "2021-07-29T17:59:15Z",
              "updatedAt": "2021-07-29T17:59:15Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4Mzg0ODMy",
          "commit": {
            "abbreviatedOid": "5ffc351"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T18:00:06Z",
          "updatedAt": "2021-07-29T18:00:21Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "Edited this comment.",
              "createdAt": "2021-07-29T18:00:21Z",
              "updatedAt": "2021-07-29T18:00:21Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NjA4NDcy",
          "commit": {
            "abbreviatedOid": "20fd3cb"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T23:26:35Z",
          "updatedAt": "2021-07-29T23:53:47Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "Done",
              "createdAt": "2021-07-29T23:26:35Z",
              "updatedAt": "2021-07-29T23:53:47Z"
            },
            {
              "originalPosition": 62,
              "body": "Hm. I am nervous about being to prescriptive here. In the simplest protocol design nothing is needed since the epsilon is hardcoded into the specific protocol instantiation and won't change.\r\n\r\nIn practice, some specific instantiations may want to reveal even more information about how noise was applied, e.g.\r\n- The distribution noise is sampled from (Laplace, Gaussian, etc)\r\n- Parameters of the noise distribution\r\n- Any kind of threshold used (for example, if you are using approximate DP)\r\n\r\nI think we should make this as opaque to the protocol as possible vs. prescribing some single \"epsilon\" field which might be too constraining. What do you think?",
              "createdAt": "2021-07-29T23:41:07Z",
              "updatedAt": "2021-07-29T23:53:47Z"
            },
            {
              "originalPosition": 65,
              "body": "Done, I added a general DP reference in place of the wikipedia link. It discusses central, local, and multi-party DP.",
              "createdAt": "2021-07-29T23:42:20Z",
              "updatedAt": "2021-07-29T23:53:47Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE5MzQ5NzE5",
          "commit": {
            "abbreviatedOid": "1cb3234"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-30T18:09:05Z",
          "updatedAt": "2021-07-30T18:09:05Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "However, I still think this current PR is land-able given that a basic instantiation can hardcode everything without requiring any communication.",
              "createdAt": "2021-07-30T18:09:05Z",
              "updatedAt": "2021-07-30T18:09:05Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE5MzczNzQz",
          "commit": {
            "abbreviatedOid": "1cb3234"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-30T18:44:43Z",
          "updatedAt": "2021-07-30T18:44:52Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "The way to go here, I think, is to document the open question by adding an `[OPEN ISSUE: blah blah blah]`.",
              "createdAt": "2021-07-30T18:44:43Z",
              "updatedAt": "2021-07-30T18:44:52Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE5MzkyODQ1",
          "commit": {
            "abbreviatedOid": "fc636b1"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-30T19:13:14Z",
          "updatedAt": "2021-07-30T19:13:14Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "Done",
              "createdAt": "2021-07-30T19:13:14Z",
              "updatedAt": "2021-07-30T19:13:14Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE5Mzk3NzE3",
          "commit": {
            "abbreviatedOid": "fc636b1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-30T19:20:22Z",
          "updatedAt": "2021-07-30T19:20:22Z",
          "comments": [
            {
              "originalPosition": 62,
              "body": "I put a note about this in #19, which I think is an appropriate issue to track this discussion.",
              "createdAt": "2021-07-30T19:20:22Z",
              "updatedAt": "2021-07-30T19:20:22Z"
            }
          ]
        }
      ]
    },
    {
      "number": 93,
      "id": "MDExOlB1bGxSZXF1ZXN0Njk5NzcyNjgz",
      "title": "Document minimal operational capabilities.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/93",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "I also deleted a bunch of the (outdated) system design text. I'm sure this can be simplified, but I think this captures the basic assumptions (at least in my mental model).",
      "createdAt": "2021-07-29T19:04:06Z",
      "updatedAt": "2021-12-30T02:10:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "da975086d315cfd9e41d8ee7243a19df1fd6c736",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/operational-assumptions",
      "headRefOid": "ff840dc95a07a75a4677728e8c8feb6baa0407fc",
      "closedAt": "2021-08-11T02:25:25Z",
      "mergedAt": "2021-08-11T02:25:25Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "67ac55bf0cd06ad755dc0096f01bfebbbf27717d"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> At this point, isn't it our intent to spell out the anti-replay mechanism? These changes suggest some mechanism is required, but isn't more prescriptive than that.\r\n\r\nIndeed, the text doesn't require any specific mitigation, and I don't see why it should. Aggregators could do the thing described in the appendix, or they could keep a huge list of all reports seen thus far. There could even be some simpler variant in the future \ud83e\udd37 I don't think we lose anything by eliding the implementation details here.",
          "createdAt": "2021-08-10T15:08:34Z",
          "updatedAt": "2021-08-10T15:08:34Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTA0NjMw",
          "commit": {
            "abbreviatedOid": "c802c12"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-07-29T20:27:01Z",
          "updatedAt": "2021-07-29T20:44:31Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "```suggestion\r\nPDA protocols have inherent constraints derived from the tradeoff between privacy\r\n```",
              "createdAt": "2021-07-29T20:27:01Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 67,
              "body": "```suggestion\r\nare (1) the PDAParam structure configured out of band and (2) a measurement. Clients\r\nare not expected to store any state across any upload\r\n```",
              "createdAt": "2021-07-29T20:28:22Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 70,
              "body": "Also worth mentioning here: The aggregators validate inputs before consuming them.",
              "createdAt": "2021-07-29T20:29:36Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 78,
              "body": "```suggestion\r\nor computation limitations or constraints, but only a modestly provisioned helper, i.e., one that\r\n```",
              "createdAt": "2021-07-29T20:30:26Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 93,
              "body": "Clarify here that some of the offloaded state can be used for anti-replay, but not all.",
              "createdAt": "2021-07-29T20:31:38Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 98,
              "body": "This will be resolved (at least partially) by #90.",
              "createdAt": "2021-07-29T20:32:13Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 111,
              "body": "This is also a requirement for the helper.",
              "createdAt": "2021-07-29T20:33:50Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 120,
              "body": "They also need to keep around an HPKE secret key for the lifetime of the PDAParam.",
              "createdAt": "2021-07-29T20:34:25Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 168,
              "body": "```suggestion\r\nPDA deployments should ensure that aggregators do not have common dependencies\r\n```",
              "createdAt": "2021-07-29T20:34:48Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 186,
              "body": "```suggestion\r\ntolerable; the input-verification procedure of the PDA protocol\r\n```\r\nThe term \"AFE\" is specific to Prio.",
              "createdAt": "2021-07-29T20:36:28Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 194,
              "body": "I realize this is copy-pasted, but I'm questioning the value of this paragraph at this point. Is it saying anything that's new and not obvious?",
              "createdAt": "2021-07-29T20:40:23Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            },
            {
              "originalPosition": 182,
              "body": "> Data integrity constraints may be at odds with the threat model if meeting the constraints requires replaying data.\r\n\r\nI realize this statement was copy-pated, but I think we should remove it. The origin of this sentence is the observation that Hits requires multiple collect rounds over the same data. This fact isn't at odds with the need for anti-replay, since mutliple collect requests aren't \"replaying\" reports in an illegal way.",
              "createdAt": "2021-07-29T20:44:16Z",
              "updatedAt": "2021-07-29T20:44:31Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIwMDM3",
          "commit": {
            "abbreviatedOid": "9c60e50"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:47:39Z",
          "updatedAt": "2021-07-29T20:47:39Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "That's just par for the course -- I don't think it changes any of the capacity or operational requirements?",
              "createdAt": "2021-07-29T20:47:39Z",
              "updatedAt": "2021-07-29T20:47:39Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIwMjkz",
          "commit": {
            "abbreviatedOid": "563cb41"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:48:02Z",
          "updatedAt": "2021-07-29T20:48:03Z",
          "comments": [
            {
              "originalPosition": 98,
              "body": "Yep, noted it as such.",
              "createdAt": "2021-07-29T20:48:03Z",
              "updatedAt": "2021-07-29T20:48:03Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIwODMz",
          "commit": {
            "abbreviatedOid": "2d3b361"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:48:44Z",
          "updatedAt": "2021-07-29T20:48:45Z",
          "comments": [
            {
              "originalPosition": 186,
              "body": "This is moved text, so let's change that separately.",
              "createdAt": "2021-07-29T20:48:45Z",
              "updatedAt": "2021-07-29T20:48:45Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTEyMDY2",
          "commit": {
            "abbreviatedOid": "c802c12"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:36:51Z",
          "updatedAt": "2021-07-29T20:48:49Z",
          "comments": [
            {
              "originalPosition": 66,
              "body": "Corresponding to what?",
              "createdAt": "2021-07-29T20:36:51Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            },
            {
              "originalPosition": 68,
              "body": "```suggestion\r\nflows, nor are they required to implement any sort of report upload retry mechanism.\r\n```\r\nIt was unclear what \"it\" referred to, and I think retry implies a failure.",
              "createdAt": "2021-07-29T20:37:29Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            },
            {
              "originalPosition": 100,
              "body": "```suggestion\r\nBeyond the minimal capabilities required of helpers, leaders are generally required to:\r\n```",
              "createdAt": "2021-07-29T20:39:29Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            },
            {
              "originalPosition": 180,
              "body": "Distributed systems people will hiss at you for saying \"exactly once\". Should this be \"at most once\"?",
              "createdAt": "2021-07-29T20:41:27Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            },
            {
              "originalPosition": 186,
              "body": "\"AFE\" turns up in a handful of places in the doc now. Perhaps it merits an entry in the `Terminology` section at the top, especially since it's not obvious that \"AFE\" expands to \"affine-aggregatable encodings\".",
              "createdAt": "2021-07-29T20:43:00Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            },
            {
              "originalPosition": 110,
              "body": "I find this a bit confusing. Is the intent here that for a report, the corresponding aggregator *output* shares must either both be included or both be omitted?",
              "createdAt": "2021-07-29T20:44:12Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            },
            {
              "originalPosition": 36,
              "body": "It seems like this would require clients to maintain a list of uploaded reports to avoid double-uploads, which contradicts the later statement:\r\n>Clients are not expected to store any state across any upload flows\r\n\r\nBesides which, I think leaders will need to implement detection of duplicate report uploads no matter what, in which case they can return an error to clients in the case of a double upload, which downgrades this to a SHOULD NOT.",
              "createdAt": "2021-07-29T20:47:28Z",
              "updatedAt": "2021-07-29T20:48:49Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIwOTQ5",
          "commit": {
            "abbreviatedOid": "2d3b361"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:48:54Z",
          "updatedAt": "2021-07-29T20:48:55Z",
          "comments": [
            {
              "originalPosition": 194,
              "body": "Nope, we can delete.",
              "createdAt": "2021-07-29T20:48:54Z",
              "updatedAt": "2021-07-29T20:48:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIxMTE2",
          "commit": {
            "abbreviatedOid": "2d3b361"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:49:07Z",
          "updatedAt": "2021-07-29T20:49:08Z",
          "comments": [
            {
              "originalPosition": 182,
              "body": "Less text is better :-) I'll remove it.",
              "createdAt": "2021-07-29T20:49:07Z",
              "updatedAt": "2021-07-29T20:49:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIxNzUz",
          "commit": {
            "abbreviatedOid": "13e1c02"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:50:02Z",
          "updatedAt": "2021-07-29T20:50:02Z",
          "comments": [
            {
              "originalPosition": 180,
              "body": "Hah, yeah, fair :-) That's a good suggestion!",
              "createdAt": "2021-07-29T20:50:02Z",
              "updatedAt": "2021-07-29T20:50:02Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTI3OTEz",
          "commit": {
            "abbreviatedOid": "13e1c02"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:57:57Z",
          "updatedAt": "2021-07-29T20:57:58Z",
          "comments": [
            {
              "originalPosition": 186,
              "body": "In fact, I'm not so sure we want to bolt ourselves to this term too much. First, it's not meaningful for every PDA protocol (for Hits in particular). Second, the term was coined in the original Prio paper, which we don't actually implement.",
              "createdAt": "2021-07-29T20:57:57Z",
              "updatedAt": "2021-07-29T20:57:58Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTI2MDI3",
          "commit": {
            "abbreviatedOid": "13e1c02"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-10T14:39:02Z",
          "updatedAt": "2021-08-10T14:39:03Z",
          "comments": [
            {
              "originalPosition": 93,
              "body": "I don't think that's necessary to do here.",
              "createdAt": "2021-08-10T14:39:02Z",
              "updatedAt": "2021-08-10T14:39:03Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTM1NjIz",
          "commit": {
            "abbreviatedOid": "13e1c02"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-10T14:46:51Z",
          "updatedAt": "2021-08-10T14:46:51Z",
          "comments": [
            {
              "originalPosition": 186,
              "body": "(Overcome by events)",
              "createdAt": "2021-08-10T14:46:51Z",
              "updatedAt": "2021-08-10T14:46:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTM3NjQ3",
          "commit": {
            "abbreviatedOid": "13e1c02"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-10T14:48:24Z",
          "updatedAt": "2021-08-10T14:48:24Z",
          "comments": [
            {
              "originalPosition": 36,
              "body": "Yeah, this seems reasonable.",
              "createdAt": "2021-08-10T14:48:24Z",
              "updatedAt": "2021-08-10T14:48:25Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTUxMTU0",
          "commit": {
            "abbreviatedOid": "873a725"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Some nits, and a question: At this point, isn't it our intent to spell out the anti-replay mechanism? These changes suggest *some* mechanism is required, but isn't more prescriptive than that.",
          "createdAt": "2021-08-10T14:59:07Z",
          "updatedAt": "2021-08-10T15:05:36Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "Agreed.",
              "createdAt": "2021-08-10T14:59:07Z",
              "updatedAt": "2021-08-10T15:05:36Z"
            },
            {
              "originalPosition": 93,
              "body": "```suggestion\r\n- Publish and manage an HPKE configuration that can be used for the upload protocol.\r\n```",
              "createdAt": "2021-08-10T15:00:02Z",
              "updatedAt": "2021-08-10T15:05:36Z"
            },
            {
              "originalPosition": 102,
              "body": "```suggestion\r\n  replay attack mitigation. One replay mitigation strategy is described in {{anti-replay}}.\r\n```\r\nAlso, isn't the strategy in {{anti-replay}} *the* strategy? Do we intend to allow deployments to skip this, or do something else?",
              "createdAt": "2021-08-10T15:01:17Z",
              "updatedAt": "2021-08-10T15:05:36Z"
            },
            {
              "originalPosition": 110,
              "body": "+1",
              "createdAt": "2021-08-10T15:02:21Z",
              "updatedAt": "2021-08-10T15:05:36Z"
            },
            {
              "originalPosition": 119,
              "body": "Again, at this point it seems like there is one and only one replay mitigation, and it's spelled out in the doc.",
              "createdAt": "2021-08-10T15:03:05Z",
              "updatedAt": "2021-08-10T15:05:36Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTYyODcz",
          "commit": {
            "abbreviatedOid": "ad987ff"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-10T15:08:42Z",
          "updatedAt": "2021-08-10T15:08:43Z",
          "comments": [
            {
              "originalPosition": 119,
              "body": "(Commented below.)",
              "createdAt": "2021-08-10T15:08:43Z",
              "updatedAt": "2021-08-10T15:08:43Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTc3NTE0",
          "commit": {
            "abbreviatedOid": "1acb0ff"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-10T15:21:31Z",
          "updatedAt": "2021-08-10T16:36:21Z",
          "comments": [
            {
              "originalPosition": 119,
              "body": "```suggestion\r\n- Implement and store state for the form of inter- and intra-batch replay mitigation in {{anti-replay}}; and\r\n```",
              "createdAt": "2021-08-10T15:21:32Z",
              "updatedAt": "2021-08-10T16:36:21Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2ODk4NDU4",
          "commit": {
            "abbreviatedOid": "0fe6750"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I flagged one typo and would like to see the text referred to in [this comment](https://github.com/abetterinternet/prio-documents/pull/93/files#r679473812) clarified.",
          "createdAt": "2021-08-10T22:32:50Z",
          "updatedAt": "2021-08-10T22:34:40Z",
          "comments": [
            {
              "originalPosition": 125,
              "body": "```suggestion\r\ninput to the protocol is the PDAParam structure, configured out of band, which contains\r\n```",
              "createdAt": "2021-08-10T22:32:50Z",
              "updatedAt": "2021-08-10T22:34:40Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2OTA0NDE5",
          "commit": {
            "abbreviatedOid": "22e040b"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-10T22:45:08Z",
          "updatedAt": "2021-08-10T22:45:08Z",
          "comments": [
            {
              "originalPosition": 110,
              "body": "This is indeed somewhat confusing, and probably not needed, so dropped.",
              "createdAt": "2021-08-10T22:45:08Z",
              "updatedAt": "2021-08-10T22:45:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2OTM3MTAz",
          "commit": {
            "abbreviatedOid": "ff840dc"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-11T00:01:50Z",
          "updatedAt": "2021-08-11T00:01:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 94,
      "id": "MDExOlB1bGxSZXF1ZXN0Njk5Nzk0NDg5",
      "title": "Add an extension slot to the upload request.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/94",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "One possible extension would be an opaque blob to carry client attestation information (for #89), if desired. The current set is empty, but I suspect we'll need freedom to extend here as this gets used without totally changing the protocol version.\r\n\r\nIf folks are generally supportive of this, it should probably land after #90, which touches and renames PDAUploadReq to PDAReport. ",
      "createdAt": "2021-07-29T19:41:21Z",
      "updatedAt": "2021-12-30T02:10:09Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "51efe71f37bb2d09acef59dde80326acb2e22b8f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/upload-extensions",
      "headRefOid": "61be64d144f5afbf49b61d4b30b20600b7abb616",
      "closedAt": "2021-07-30T19:15:37Z",
      "mergedAt": "2021-07-30T19:15:37Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "37925a8b7ee811f5e5abacdb57dcb299ff96660b"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm supportive of solving #89 in a generic way, but it's not clear that this change has enough plumbing. In particular, there doesn't appear to be a way for the helper to verify either property of the report (1 - the report was generated by a trusted client; 2 - the report was generated in a trusted environment)\r\n\r\nBoth (1) and (2) are _external to the protocol_. The purpose of extensions is to allow whatever mechanism exists for addressing them to be bound to the report, as is done here. \r\n\r\n> Copy the extension into the aggregate sub-request \r\n\r\nThis is done in the change. \r\n\r\n> and add an OPEN ISSUE for deciding whether this is sufficient plumbing for client attestation\r\n\r\nThis is _not meant_ to close the attestation issue. It's meant to give us the mechanics to address it later. ",
          "createdAt": "2021-07-29T20:29:45Z",
          "updatedAt": "2021-07-29T20:32:40Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NDkyNjI5",
          "commit": {
            "abbreviatedOid": "cbe12d4"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-07-29T20:11:25Z",
          "updatedAt": "2021-07-29T20:11:40Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Isn't this actually 4..2^16-1",
              "createdAt": "2021-07-29T20:11:26Z",
              "updatedAt": "2021-07-29T20:11:40Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NDkzNTY5",
          "commit": {
            "abbreviatedOid": "cbe12d4"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:12:35Z",
          "updatedAt": "2021-07-29T20:12:36Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "```suggestion\r\n  Extension extensions<4..2^16-1>;\r\n```",
              "createdAt": "2021-07-29T20:12:35Z",
              "updatedAt": "2021-07-29T20:12:36Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NDk1NzE5",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I'm supportive of solving #89 in a generic way, but it's not clear that this change has enough plumbing. In particular, there doesn't appear to be a way for the helper to verify either property of the report (1 - the report was generated by a trusted client; 2 - the report was generated in a trusted environment) Minimally, I think we would need to echo the extension in the aggregate sub-request.\r\n\r\nI'd be happy with one of of the following outcomes:\r\n- Copy the extension into the aggregate sub-request and add an OPEN ISSUE for deciding whether this is sufficient plumbing for client attestation\r\n- Close this PR and park the issue",
          "createdAt": "2021-07-29T20:15:18Z",
          "updatedAt": "2021-07-29T20:25:13Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "In fact, when the extension is used for the purpose of attestation, it probably *needs* to be echoed in each aggregate sub-request. ",
              "createdAt": "2021-07-29T20:15:18Z",
              "updatedAt": "2021-07-29T20:25:13Z"
            },
            {
              "originalPosition": 29,
              "body": "```suggestion\r\nadditional, authenticated[TODO: Specify how] information in the report. Each extension is a tag-length\r\n```",
              "createdAt": "2021-07-29T20:16:02Z",
              "updatedAt": "2021-07-29T20:25:13Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTA1Nzk4",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:28:33Z",
          "updatedAt": "2021-07-29T20:28:33Z",
          "comments": [
            {
              "originalPosition": 29,
              "body": "The contents of the extensions are presumably fed into the AAD, so closing.",
              "createdAt": "2021-07-29T20:28:33Z",
              "updatedAt": "2021-07-29T20:28:33Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTA2MTU1",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:29:00Z",
          "updatedAt": "2021-07-29T20:29:00Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "That is where I lean, too, but it does mean extension bloat in each request. That's probably fine, though.",
              "createdAt": "2021-07-29T20:29:00Z",
              "updatedAt": "2021-07-29T20:29:00Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTIyMDQ2",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Right you are. What threw me off was the following statement:\r\n> [OPEN ISSUE: if we include extensions in the AAD then these extensions are required to be\r\n> echoed in each aggregate subrequest. We could either not include this in the AAD, or replace\r\n> the list with a hash of the list (that's sent separately), or something else.]\r\n\r\nThe \"then these extensions are required to be echoed...\" makes it sound as if they're *not* echoed, which they are. I'm happy with this PR modulo fixing this statement.",
          "createdAt": "2021-07-29T20:50:21Z",
          "updatedAt": "2021-07-29T20:50:21Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTI4MTM2",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T20:58:14Z",
          "updatedAt": "2021-07-29T20:59:44Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "Any reason this is `<8..2^16-1>` here but `<4..2^16-1>` in `PDAUploadReq`?",
              "createdAt": "2021-07-29T20:58:14Z",
              "updatedAt": "2021-07-29T20:59:44Z"
            },
            {
              "originalPosition": 77,
              "body": "Do we need to provide this much structure for the extensions? IIUC at the PDA protocol layer, we will never examine or modify these extensions so why can't they be completely opaque blobs?",
              "createdAt": "2021-07-29T20:59:22Z",
              "updatedAt": "2021-07-29T20:59:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTM5MzQ0",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T21:12:51Z",
          "updatedAt": "2021-07-29T21:12:51Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "Me not making the change in two places :)",
              "createdAt": "2021-07-29T21:12:51Z",
              "updatedAt": "2021-07-29T21:12:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTM5NTI1",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T21:13:06Z",
          "updatedAt": "2021-07-29T21:13:06Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "```suggestion\r\n  Extension extensions<4..2^16-1>;\r\n```",
              "createdAt": "2021-07-29T21:13:06Z",
              "updatedAt": "2021-07-29T21:13:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE4NTQwMDU0",
          "commit": {
            "abbreviatedOid": "f3001a7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-29T21:13:49Z",
          "updatedAt": "2021-07-29T21:13:49Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "This just creates the registry where extensions will go. We don't need to do anything more here, unless we decide we want to introduce some mandatory extensions.",
              "createdAt": "2021-07-29T21:13:49Z",
              "updatedAt": "2021-07-29T21:13:49Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE5MzkyNjk5",
          "commit": {
            "abbreviatedOid": "61be64d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-30T19:13:03Z",
          "updatedAt": "2021-07-30T19:13:03Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "Well, my point applies more to the `struct Extension` definition. The extension just be an opaque blob of bytes, and the responsibility of protocol implementations would be to relay the blob unmodified from `PDAReport` to `PDAAggregateSubReq`. Then the format of that blob is something to be agreed upon by the client that generated it and the aggregator that is examining it, out of band from the PDA protocol.",
              "createdAt": "2021-07-30T19:13:03Z",
              "updatedAt": "2021-07-30T19:13:03Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzE5Mzk0NTIz",
          "commit": {
            "abbreviatedOid": "61be64d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-07-30T19:15:31Z",
          "updatedAt": "2021-07-30T19:15:31Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "The contents of the extension, and how they're used, are up to the thing that defines the extension. So, yeah, I think what we have here aligns with what you envision.",
              "createdAt": "2021-07-30T19:15:31Z",
              "updatedAt": "2021-07-30T19:15:31Z"
            }
          ]
        }
      ]
    },
    {
      "number": 96,
      "id": "MDExOlB1bGxSZXF1ZXN0NzAzODE0NTI4",
      "title": "Draft charter",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/96",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-04T18:17:14Z",
      "updatedAt": "2021-08-27T21:07:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2590598856adae8dd386d8e6762c42de89f83161",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "charter",
      "headRefOid": "8ed99d50d7e9a31e52b6b69223c1722de8d8cb91",
      "closedAt": "2021-08-27T21:07:10Z",
      "mergedAt": "2021-08-27T21:07:10Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "560afbee40ac7d13154d2ea536273e28ea319e7e"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNTk5OTg3",
          "commit": {
            "abbreviatedOid": "96e0894"
          },
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T18:22:18Z",
          "updatedAt": "2021-08-04T18:22:19Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "```suggestion\r\nconventional methods require collecting individual responses and then\r\n```",
              "createdAt": "2021-08-04T18:22:18Z",
              "updatedAt": "2021-08-04T18:22:19Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNjAwMjI0",
          "commit": {
            "abbreviatedOid": "96e0894"
          },
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T18:22:37Z",
          "updatedAt": "2021-08-04T18:22:37Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "```suggestion\r\naggregating them, thus representing a threat to user privacy and\r\n```",
              "createdAt": "2021-08-04T18:22:37Z",
              "updatedAt": "2021-08-04T18:22:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNjE2NzUy",
          "commit": {
            "abbreviatedOid": "96e0894"
          },
          "author": "stpeter",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T18:41:49Z",
          "updatedAt": "2021-08-04T18:45:19Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "```suggestion\r\n- Client submission of individual reports, including proofs of validity, to a primary server.\r\n```",
              "createdAt": "2021-08-04T18:41:49Z",
              "updatedAt": "2021-08-04T18:45:19Z"
            },
            {
              "originalPosition": 19,
              "body": "```suggestion\r\n- Verification of validity proofs by two or more servers\r\n```",
              "createdAt": "2021-08-04T18:43:15Z",
              "updatedAt": "2021-08-04T18:45:19Z"
            },
            {
              "originalPosition": 20,
              "body": "```suggestion\r\n- Computation of aggregate values by two or more servers\r\n```",
              "createdAt": "2021-08-04T18:44:30Z",
              "updatedAt": "2021-08-04T18:45:19Z"
            },
            {
              "originalPosition": 21,
              "body": "```suggestion\r\n- Reporting of aggregate results to the entity taking the measurement\r\n```",
              "createdAt": "2021-08-04T18:45:06Z",
              "updatedAt": "2021-08-04T18:45:19Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNjUxNzE4",
          "commit": {
            "abbreviatedOid": "2017b1e"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T19:18:37Z",
          "updatedAt": "2021-08-04T19:18:37Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "I actually don't think we should commit to this in the charter (even though I think it's a good idea).",
              "createdAt": "2021-08-04T19:18:37Z",
              "updatedAt": "2021-08-04T19:18:37Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNjUxOTc3",
          "commit": {
            "abbreviatedOid": "2017b1e"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T19:18:56Z",
          "updatedAt": "2021-08-04T19:18:57Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Hmm.... I think if we're going to say >1 server somewhere it should be above this.",
              "createdAt": "2021-08-04T19:18:56Z",
              "updatedAt": "2021-08-04T19:18:57Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNjUyMDYz",
          "commit": {
            "abbreviatedOid": "2017b1e"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T19:19:02Z",
          "updatedAt": "2021-08-04T19:19:02Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "Same here.",
              "createdAt": "2021-08-04T19:19:02Z",
              "updatedAt": "2021-08-04T19:19:02Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNjUyMzIz",
          "commit": {
            "abbreviatedOid": "93d9ca0"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T19:19:21Z",
          "updatedAt": "2021-08-04T19:19:21Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Maybe \"by the servers\" here and below.",
              "createdAt": "2021-08-04T19:19:21Z",
              "updatedAt": "2021-08-04T19:19:21Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNzAxMzk0",
          "commit": {
            "abbreviatedOid": "93d9ca0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T20:19:29Z",
          "updatedAt": "2021-08-04T20:21:27Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "\"Heavy Hitters\" isn't the name of the protocol in ePrint 2021/017. \"Prio\" is the name of a protocol, but we don't implement it. (We actually implement ePrint 2019/088.) Rather than name the protocols, I suggest we just cite the papers.",
              "createdAt": "2021-08-04T20:19:29Z",
              "updatedAt": "2021-08-04T20:21:27Z"
            },
            {
              "originalPosition": 28,
              "body": "```suggestion\r\nThe WG will deliver a protocol which can accommodate multiple PDA\r\ntasks, with the initial deliverable supporting both simple\r\n```",
              "createdAt": "2021-08-04T20:21:16Z",
              "updatedAt": "2021-08-04T20:21:27Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNzA5ODE5",
          "commit": {
            "abbreviatedOid": "93d9ca0"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T20:29:46Z",
          "updatedAt": "2021-08-04T20:29:46Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "I think perhaps we just say \"Prio\". We're not saying we're going to standardize that, we're giving an example.",
              "createdAt": "2021-08-04T20:29:46Z",
              "updatedAt": "2021-08-04T20:29:46Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzIyNzEwMTE1",
          "commit": {
            "abbreviatedOid": "93d9ca0"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-04T20:30:06Z",
          "updatedAt": "2021-08-04T20:30:06Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "Also, I'm trying to get HCG to just rename all this stuff Prio",
              "createdAt": "2021-08-04T20:30:06Z",
              "updatedAt": "2021-08-04T20:30:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI1ODg5NzEx",
          "commit": {
            "abbreviatedOid": "93d9ca0"
          },
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-09T23:30:41Z",
          "updatedAt": "2021-08-09T23:30:41Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "a protocol?  seems like something that a working group might decide, but HH and Prio are very different things functionally.  my take: make two, share as many design elements as possible",
              "createdAt": "2021-08-09T23:30:41Z",
              "updatedAt": "2021-08-09T23:30:41Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI4MDE3Nzkz",
          "commit": {
            "abbreviatedOid": "93d9ca0"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T22:46:04Z",
          "updatedAt": "2021-08-11T22:46:04Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "> Also, I'm trying to get HCG to just rename all this stuff Prio\r\n\r\nThis would be great for ISRG since [we have already invested in \"Prio Services\" as a brand](https://www.abetterinternet.org/prio/)!",
              "createdAt": "2021-08-11T22:46:04Z",
              "updatedAt": "2021-08-11T22:46:04Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQwODY3MDc2",
          "commit": {
            "abbreviatedOid": "5f1d278"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-27T20:21:47Z",
          "updatedAt": "2021-08-27T20:31:34Z",
          "comments": [
            {
              "originalPosition": 3,
              "body": "```suggestion\r\none might want to measure the most common sites that people visit\r\n```",
              "createdAt": "2021-08-27T20:21:47Z",
              "updatedAt": "2021-08-27T20:31:34Z"
            },
            {
              "originalPosition": 7,
              "body": "```suggestion\r\nrather in aggregated data (e.g., how many users visit URL X).\r\nConventional methods require collecting individual measurements and then\r\n```",
              "createdAt": "2021-08-27T20:22:31Z",
              "updatedAt": "2021-08-27T20:31:34Z"
            },
            {
              "originalPosition": 11,
              "body": "```suggestion\r\nNew cryptographic techniques such as Prio address this gap by splitting\r\n```",
              "createdAt": "2021-08-27T20:22:59Z",
              "updatedAt": "2021-08-27T20:31:34Z"
            },
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nmeasurements. The Privacy Preserving Measurement (PPM) work will standardize\r\n```",
              "createdAt": "2021-08-27T20:23:22Z",
              "updatedAt": "2021-08-27T20:31:34Z"
            },
            {
              "originalPosition": 12,
              "body": "```suggestion\r\nmeasurements between multiple, non-colluding servers which can jointly compute the\r\n```",
              "createdAt": "2021-08-27T20:25:04Z",
              "updatedAt": "2021-08-27T20:31:34Z"
            },
            {
              "originalPosition": 18,
              "body": "```suggestion\r\n- Client submission of individual measurements, including proofs of validity.\r\n```",
              "createdAt": "2021-08-27T20:25:34Z",
              "updatedAt": "2021-08-27T20:31:35Z"
            },
            {
              "originalPosition": 25,
              "body": "```suggestion\r\nPPM service. \r\n```",
              "createdAt": "2021-08-27T20:25:47Z",
              "updatedAt": "2021-08-27T20:31:34Z"
            },
            {
              "originalPosition": 28,
              "body": "```suggestion\r\nPPM algorithms. The initial deliverable will support measurements of simple\r\n```",
              "createdAt": "2021-08-27T20:25:57Z",
              "updatedAt": "2021-08-27T20:31:35Z"
            },
            {
              "originalPosition": 32,
              "body": "```suggestion\r\nset of arbitrary strings submitted by users.  The PPM protocols will use\r\ncryptographic algorithms defined by the CFRG.\r\n```",
              "createdAt": "2021-08-27T20:27:18Z",
              "updatedAt": "2021-08-27T20:31:35Z"
            },
            {
              "originalPosition": 29,
              "body": "```suggestion\r\npredefined statistical aggregates such as averages, as well as measurement of \"heavy hitters\" out of the\r\n```",
              "createdAt": "2021-08-27T20:31:31Z",
              "updatedAt": "2021-08-27T20:31:35Z"
            }
          ]
        }
      ]
    },
    {
      "number": 97,
      "id": "MDExOlB1bGxSZXF1ZXN0NzAzODk2NTEx",
      "title": "Define \"valid\" collect requests",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/97",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This partially addresses the problem of preventing inter-batch replay attacks described by @chris-wood in #82. Along the way, clean up the description of collect requests so that it talks about protocol messages rather than API calls (as @ekr suggested).",
      "createdAt": "2021-08-04T19:23:04Z",
      "updatedAt": "2021-12-30T02:10:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2590598856adae8dd386d8e6762c42de89f83161",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/inter-batch",
      "headRefOid": "8b39cf50f094ee19d203316dfe33b76c7816a553",
      "closedAt": "2021-08-10T14:37:35Z",
      "mergedAt": "2021-08-10T14:37:35Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "da975086d315cfd9e41d8ee7243a19df1fd6c736"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2NTI0MTIx",
          "commit": {
            "abbreviatedOid": "8b39cf5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-10T14:37:30Z",
          "updatedAt": "2021-08-10T14:37:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 99,
      "id": "MDExOlB1bGxSZXF1ZXN0NzA3OTU0MDYw",
      "title": "typos, formatting errors, obsolete todos",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/99",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes a miscellany of typos, formatting errors and obsolete todos I encountered while reviewing the document.",
      "createdAt": "2021-08-11T01:19:35Z",
      "updatedAt": "2021-12-30T00:53:26Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "67ac55bf0cd06ad755dc0096f01bfebbbf27717d",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/misc",
      "headRefOid": "4f70e808b980e47afae48878a30fd0273ad4c0f1",
      "closedAt": "2021-08-11T22:24:34Z",
      "mergedAt": "2021-08-11T22:24:34Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "0baeb82d199db7c929dde0c6c3e769bb917cd005"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2OTY2MTc3",
          "commit": {
            "abbreviatedOid": "034144c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T01:20:45Z",
          "updatedAt": "2021-08-11T01:20:45Z",
          "comments": [
            {
              "originalPosition": 121,
              "body": "We have a section in security considerations about OHTTP and anonymizing proxies so I think this is settled.",
              "createdAt": "2021-08-11T01:20:45Z",
              "updatedAt": "2021-08-11T01:20:45Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2OTY2OTUz",
          "commit": {
            "abbreviatedOid": "034144c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T01:22:47Z",
          "updatedAt": "2021-08-11T01:22:48Z",
          "comments": [
            {
              "originalPosition": 144,
              "body": "IIUC the concern here is that leaders or helpers can learn which clients are uploading data, and I believe deployments can address that by using OHTTP.",
              "createdAt": "2021-08-11T01:22:47Z",
              "updatedAt": "2021-08-11T01:22:48Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2OTcwMTY4",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T01:31:30Z",
          "updatedAt": "2021-08-11T01:31:31Z",
          "comments": [
            {
              "originalPosition": 174,
              "body": "I don't think this requirement is correct. If the minimum batch window is 8 hours, why does it matter if the collector wants to aggregate over 0400-1200 instead of 0000-0800? Maybe the intent here was that `batch_end - batch_start` be a multiple of `PDAParam.batch_window`, but I don't think that's necessary, either. I think it's OK if a collector wants to aggregate over a window larger than the minimum value in the PDAParams, so long as the next aggregate doesn't overlap. The aggregators implement anti-replay by keeping track of the newest report timestamp aggregated for a PDATask, so they don't need all the batch windows to be the same or to be multiples of the minimum, do they?",
              "createdAt": "2021-08-11T01:31:31Z",
              "updatedAt": "2021-08-11T01:31:31Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI2OTkwNDM1",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-11T02:27:00Z",
          "updatedAt": "2021-08-11T02:29:01Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "```suggestion\r\n1. The clients, aggregators, and collector agree on a set of PDA tasks, as well\r\n```",
              "createdAt": "2021-08-11T02:27:00Z",
              "updatedAt": "2021-08-11T02:29:01Z"
            },
            {
              "originalPosition": 174,
              "body": "Assuming the batch size criteria is met, this seems right.",
              "createdAt": "2021-08-11T02:28:03Z",
              "updatedAt": "2021-08-11T02:29:01Z"
            },
            {
              "originalPosition": 272,
              "body": "We might consider just punting this to the underlying CFRG document. Maybe we can move this section to the parking lot?",
              "createdAt": "2021-08-11T02:28:52Z",
              "updatedAt": "2021-08-11T02:29:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI3NDk1NTE1",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "My only blocking comment is that I think we want (something like) the requirement you're removing. It's useful for the aggregators to be able to know the possible batch intervals before a collect request is made.",
          "createdAt": "2021-08-11T13:37:18Z",
          "updatedAt": "2021-08-11T13:55:24Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "```suggestion\r\n* `batch_window`: The window of time covered by a batch, i.e., the maximum interval\r\n   between the oldest and newest report in a batch.\r\n```",
              "createdAt": "2021-08-11T13:37:18Z",
              "updatedAt": "2021-08-11T13:55:24Z"
            },
            {
              "originalPosition": 174,
              "body": "Yeah, this maybe wasn't very clear. The intent of this requirement is to divide time into intervals that the aggregators and collector agree on in advance of the collect request. This has the advantage of allowing aggregators to aggregate input shares in each window as they arrive, thereby reducing the aggregators' storage requirements. Note that this isn't possible for Hits, but it is for Prio.",
              "createdAt": "2021-08-11T13:51:08Z",
              "updatedAt": "2021-08-11T13:55:24Z"
            },
            {
              "originalPosition": 253,
              "body": "What changed here? Looks like we're just inserting a line break?",
              "createdAt": "2021-08-11T13:52:32Z",
              "updatedAt": "2021-08-11T13:55:24Z"
            },
            {
              "originalPosition": 272,
              "body": "Yeah, I think we're going to end up removing the Prio stuff altogether. (See #98.) But we can do so in a future PR.",
              "createdAt": "2021-08-11T13:53:31Z",
              "updatedAt": "2021-08-11T13:55:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI3NjY2MjY0",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T15:56:32Z",
          "updatedAt": "2021-08-11T15:56:32Z",
          "comments": [
            {
              "originalPosition": 253,
              "body": "I'm putting `` around the math terms to make them fixed-width. I find it more readable if those stand out from the regular text, but I'm happy to revert.",
              "createdAt": "2021-08-11T15:56:32Z",
              "updatedAt": "2021-08-11T15:56:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI3NjY4OTU4",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T15:58:58Z",
          "updatedAt": "2021-08-11T15:58:58Z",
          "comments": [
            {
              "originalPosition": 253,
              "body": "Ack, no need!",
              "createdAt": "2021-08-11T15:58:58Z",
              "updatedAt": "2021-08-11T15:58:58Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI3Njc1ODE5",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T16:05:24Z",
          "updatedAt": "2021-08-11T16:05:24Z",
          "comments": [
            {
              "originalPosition": 174,
              "body": "Aggregators need to jointly evaluate the validity proof on a Prio input before it can be included in an aggregate, and proofs are not verified until the collect phase, which doesn't start until a collect request is emitted by the collector. I'm not sure how aggregators could speculatively aggregate input shares unless they are capable of removing an input share from their output if the proof is later found to be invalid, which would have its own storage requirements.",
              "createdAt": "2021-08-11T16:05:24Z",
              "updatedAt": "2021-08-11T16:05:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI3Njg2Njg5",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T16:15:45Z",
          "updatedAt": "2021-08-11T16:15:45Z",
          "comments": [
            {
              "originalPosition": 174,
              "body": "> Aggregators need to jointly evaluate the validity proof on a Prio input before it can be included in an aggregate, and proofs are not verified until the collect phase, which doesn't start until a collect request is emitted by the collector.\r\n\r\nI don't this is quite right. My understanding is that the aggregators can verify an input's validity as long as they have the PDAParam. Waiting on the collect request is only necessary for Hits, where you need to know the set of candidate prefixes before you know how to proceed.\r\n",
              "createdAt": "2021-08-11T16:15:45Z",
              "updatedAt": "2021-08-11T16:15:45Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI3Njg5MDI5",
          "commit": {
            "abbreviatedOid": "c0b46ef"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T16:18:01Z",
          "updatedAt": "2021-08-11T16:18:01Z",
          "comments": [
            {
              "originalPosition": 174,
              "body": "In general you're right: the first aggregate request needs to block on a collect request. But with Prio it's perfectly fine to do aggregation (including input validation) in advance of the collect request.",
              "createdAt": "2021-08-11T16:18:01Z",
              "updatedAt": "2021-08-11T16:18:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI4MDA0MzI2",
          "commit": {
            "abbreviatedOid": "4f70e80"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-11T22:19:03Z",
          "updatedAt": "2021-08-11T22:19:04Z",
          "comments": [
            {
              "originalPosition": 174,
              "body": "Following from our discussion in the design call earlier today, I've reverted this specific change so that @cjpatton can revisit this language later. I believe that we concluded that we want the batch windows in the collect requests to have the same size as the batch window in the PDAParams.",
              "createdAt": "2021-08-11T22:19:04Z",
              "updatedAt": "2021-08-11T22:19:04Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI4MDA2NTkw",
          "commit": {
            "abbreviatedOid": "4f70e80"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-11T22:23:24Z",
          "updatedAt": "2021-08-11T22:23:24Z",
          "comments": []
        }
      ]
    },
    {
      "number": 100,
      "id": "MDExOlB1bGxSZXF1ZXN0NzA5NzQ3OTEx",
      "title": "Rename",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/100",
      "state": "CLOSED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-11T22:34:51Z",
      "updatedAt": "2021-08-11T22:35:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0baeb82d199db7c929dde0c6c3e769bb917cd005",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "rename",
      "headRefOid": "53aa7057e7e8d3d02e5320b0dbd630e203c0c57a",
      "closedAt": "2021-08-11T22:35:11Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [],
      "reviews": []
    },
    {
      "number": 101,
      "id": "MDExOlB1bGxSZXF1ZXN0NzA5NzUwMjkz",
      "title": "Rename",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/101",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-11T22:36:48Z",
      "updatedAt": "2021-08-11T22:44:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0baeb82d199db7c929dde0c6c3e769bb917cd005",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "rename",
      "headRefOid": "610d9d2f000281c528f7415239e902a54cc4e51b",
      "closedAt": "2021-08-11T22:44:33Z",
      "mergedAt": "2021-08-11T22:44:33Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "120515ccebfc5454d564d220370ebd4d124d580a"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI4MDE3MDY3",
          "commit": {
            "abbreviatedOid": "610d9d2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-11T22:44:29Z",
          "updatedAt": "2021-08-11T22:44:29Z",
          "comments": []
        }
      ]
    },
    {
      "number": 103,
      "id": "MDExOlB1bGxSZXF1ZXN0NzA5NzgyMjMw",
      "title": "clarify `PPMParam.collector_config` field",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/103",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Removes some TODOs, clarifies why we have `HpkeConfig\r\ncollector_config` in `struct PPMParam` instead of `Url collector_url`,\r\nand amends collect protocol so that leader output shares are\r\nencrypted to collector's HPKE config. Several response structures\r\nno longer have `PPMProto` or `PPMTaskID` fields if the appropriate\r\ntask (and hence `PPMParam` and hence `PPMProto`) can be inferred\r\nfrom the request.",
      "createdAt": "2021-08-11T23:03:22Z",
      "updatedAt": "2021-12-30T00:53:27Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a801f7f6018ec929fa6199baa4e3caea7642609c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/collector-hpke",
      "headRefOid": "757f4b197dff2d05035d21989bfc7cdcf4f5e4f6",
      "closedAt": "2021-08-19T14:19:00Z",
      "mergedAt": "2021-08-19T14:19:00Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "84ddcbc789b0e4475358dc27f2379998b2484cb4"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I want to address #95 in this PR as well, so downgrading to draft for now.",
          "createdAt": "2021-08-11T23:07:22Z",
          "updatedAt": "2021-08-11T23:07:22Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood @cjpatton This is ready; please review at your convenience.",
          "createdAt": "2021-08-13T17:54:36Z",
          "updatedAt": "2021-08-13T17:54:36Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Thank you for the review, @cjpatton. This picked up some merge conflicts so I will wait for all of @ekr's changes to land and then rebase.",
          "createdAt": "2021-08-17T23:34:43Z",
          "updatedAt": "2021-08-17T23:34:43Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "You should be able to rebase now. I am mostly done.\n\nOn Tue, Aug 17, 2021 at 4:34 PM Tim Geoghegan ***@***.***>\nwrote:\n\n> Thank you for the review, @cjpatton <https://github.com/cjpatton>. This\n> picked up some merge conflicts so I will wait for all of @ekr\n> <https://github.com/ekr>'s changes to land and then rebase.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/pull/103#issuecomment-900699786>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIKRFMTJT63D74Y6FXDT5LWZ3ANCNFSM5B7VXOCA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n",
          "createdAt": "2021-08-17T23:39:05Z",
          "updatedAt": "2021-08-17T23:39:05Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased, reordered, ready for re-review",
          "createdAt": "2021-08-18T01:12:53Z",
          "updatedAt": "2021-08-18T01:12:53Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI4MDU5Mzg1",
          "commit": {
            "abbreviatedOid": "22d0a06"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-12T00:28:53Z",
          "updatedAt": "2021-08-12T00:28:53Z",
          "comments": [
            {
              "originalPosition": 94,
              "body": "Besides moving this definition to here from the \"Aggregate Request\" section, I also appended `server_role` to the `info` string passed to `SetupBaseS`.",
              "createdAt": "2021-08-12T00:28:53Z",
              "updatedAt": "2021-08-12T00:28:53Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzI4MDU5NTEw",
          "commit": {
            "abbreviatedOid": "22d0a06"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-12T00:29:19Z",
          "updatedAt": "2021-08-12T00:29:19Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "Changed the verb to avoid confusion since this section discusses the \"collect\" protocol.",
              "createdAt": "2021-08-12T00:29:19Z",
              "updatedAt": "2021-08-12T00:29:19Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwODIwNDAw",
          "commit": {
            "abbreviatedOid": "22d0a06"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-16T15:00:42Z",
          "updatedAt": "2021-08-16T15:03:32Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "In `PPMReport` the encrypted input shares have type `PDAEncryptedInputShare encrypted_input_shares<1..2^16-1>`, which would allow PPM tasks to specify more than one helper. The current \r\nprotocol doesn't support this right now, of course, but it might be a good idea to mirror what we have there.\r\n\r\n```suggestion\r\n  PPMEncryptedOutputShare encrypted_output_shares<0..2^16-1>;\r\n```\r\n\r\nIf you take this suggestion, remember to update the text below.",
              "createdAt": "2021-08-16T15:00:42Z",
              "updatedAt": "2021-08-16T15:03:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzQ4Mzg2",
          "commit": {
            "abbreviatedOid": "470f14a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T01:06:00Z",
          "updatedAt": "2021-08-18T01:06:01Z",
          "comments": [
            {
              "originalPosition": 97,
              "body": "Besides fixing the order of the arguments to `context.Seal`, I also appended `server_role` to the `info` string passed to `SetupBaseS`, since both helper and leader now encrypt output shares.",
              "createdAt": "2021-08-18T01:06:01Z",
              "updatedAt": "2021-08-18T01:06:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzQ4NzUw",
          "commit": {
            "abbreviatedOid": "470f14a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T01:07:05Z",
          "updatedAt": "2021-08-18T01:07:05Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "I'd prefer to punt this to #117, so that the document is updated in one step to handle multiple helpers.",
              "createdAt": "2021-08-18T01:07:05Z",
              "updatedAt": "2021-08-18T01:07:05Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MDUwMDQw",
          "commit": {
            "abbreviatedOid": "757f4b1"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-19T14:18:54Z",
          "updatedAt": "2021-08-19T14:18:54Z",
          "comments": []
        }
      ]
    },
    {
      "number": 115,
      "id": "MDExOlB1bGxSZXF1ZXN0NzEyNjU1NDQ1",
      "title": "Restructure the document",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/115",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This PR restructures the document to have a more traditional IETF flow, including an overview of operation and then serially going through each operation. It also moves the crypto detail to an appendix in preparation for moving it to a separate CFRG document.",
      "createdAt": "2021-08-13T22:23:02Z",
      "updatedAt": "2021-08-17T19:37:05Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "120515ccebfc5454d564d220370ebd4d124d580a",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "ekr_restructure",
      "headRefOid": "f4a201b792457a39b0f6530dc59407828ddd2e95",
      "closedAt": "2021-08-17T19:37:05Z",
      "mergedAt": "2021-08-17T19:37:05Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "0102342a28ab5006bc76172d99b06914d1a7cdd2"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm very happy with this restructure. However, I think there are a couple of things that get lost, which should be addressed:\r\n\r\n>    One thing that's no longer clear is that, in general (and for Hits in particular), an aggregate or output-share request cannot be made until the collector has issued a collect request. This is discussed a bit in {{pa-collect}}, but I think it needs to be made clear much earlier. In particular, in the Aggregate Request section, we should be clear that the parameters of the collect request must be known before the aggregate request can be made.\r\n\r\nAs noted above, I think this is an unnecessary limitation in the current text. There is no a priori reason why it needs to be so for *either* protocol. Even for Hits, one might imagine having the leader preconfigured with a specific heavy hitters measurement strategy.\r\n\r\n\r\n>   Something that gets a bit lost is the idea that there are two processes being executed simultaneously: report uploading and collection. This idea is important to understand where the potential bottle necks are.\r\n\r\nI actually think there are *three* things going on:\r\n\r\n1. Clients sending in reports.\r\n1. The leader issuing aggregation requests\r\n1. The collector asking for results.\r\n\r\nThat's how this text is structure.\r\n\r\n>  For example, as collect request can't be completed until a sufficient number of shares have been uploaded, validated, and aggregated.\r\n\r\nSure, but this is just one instance of where the collector may have to wait. For instance, the computation may just take some time.\r\n\r\n\r\n",
          "createdAt": "2021-08-16T18:10:34Z",
          "updatedAt": "2021-08-16T18:10:34Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton I added some clarifying text. Maybe we're now at the point where this is probably close enough and we should merge and then fix other things later. WDYT?",
          "createdAt": "2021-08-17T17:43:07Z",
          "updatedAt": "2021-08-17T17:43:07Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwODI5NjIz",
          "commit": {
            "abbreviatedOid": "5dcb6ab"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I'm very happy with this restructure. However, I think there are a couple of things that get lost, which should be addressed:\r\n\r\n* One thing that's no longer clear is that, in general (and for Hits in particular), an aggregate or output-share request _cannot_ be made until the collector has issued a collect request. This is discussed a bit in `{{pa-collect}}`, but I think it needs to be made clear much earlier. In particular, in the Aggregate Request section, we should be clear that the parameters of the collect request must be known before the aggregate request can be made.\r\n\r\n* Something that gets a bit lost is the idea that there are two processes being executed simultaneously: report uploading and collection. This idea is important to understand where the potential bottle necks are. For example, as collect request can't be completed until a sufficient number of shares have been uploaded, validated, and aggregated.",
          "createdAt": "2021-08-16T15:08:58Z",
          "updatedAt": "2021-08-16T16:18:08Z",
          "comments": [
            {
              "originalPosition": 258,
              "body": "The architecture diagram shows two helpers. We intend allow each PPM protocol to specify any number of helpers, so maybe it makes sense to codify that here in the intro/overview. The wrapper protocol doesn't quite permit this yet, but it should suffice to add TODOs as needed, or file an issue.",
              "createdAt": "2021-08-16T15:08:58Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 271,
              "body": "nit: Remove duplicate blank line",
              "createdAt": "2021-08-16T15:09:40Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 275,
              "body": "The meaning of measurement is somewhat ambiguous at this point. On the one hand it sounds like a measurement is a single event from which a report is constructed. On the other hand, it sounds as if a measurement is an on-going progress that results in multiple reports generated by a single client for the same task.\r\n\r\nI think the right definition is the former, i.e., a \"mesurement\" is the value used by the client to generate a report for a task. Therefore, a \"task\" encompasses the process of taking multiple measurements over time. If we want to change the meaning of these terms, we need to be consistent throughout.",
              "createdAt": "2021-08-16T15:12:20Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 283,
              "body": "```suggestion\r\n* The minimum \"batch size\" of reports which can be aggregated.\r\n```",
              "createdAt": "2021-08-16T15:20:23Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 283,
              "body": "I'd also add:\r\n* The rate at which measurements can be taken, i.e., the \"minimum batch window\".",
              "createdAt": "2021-08-16T15:21:20Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 286,
              "body": "Is the task ID important at this stage?",
              "createdAt": "2021-08-16T15:21:45Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 291,
              "body": "```suggestion\r\nthough they pass through the leader, the leader is unable to see or modify\r\n```",
              "createdAt": "2021-08-16T15:22:53Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 300,
              "body": "```suggestion\r\nuntil the entire batch of reports is received. \r\n```",
              "createdAt": "2021-08-16T15:24:43Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 305,
              "body": "I think it's confusing to say the protocol is \"compatible\" with both a push and pull deployment mode. It's more accurate to say that the protocol is comprised of two processes: one in which clients push reports to the aggregators; and another in which the collector pulls results from the aggregators.",
              "createdAt": "2021-08-16T15:26:35Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 407,
              "body": "I think this description of how a PPM task works  would be useful, even in a condensed form. The important takeways are:\r\n1. The upload process is carried out by the clients and aggregators.\r\n2. The collect process is carried out by the collector and aggregators.\r\n3. These processes are carried out _simultaneously_.",
              "createdAt": "2021-08-16T15:31:50Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 462,
              "body": "```suggestion\r\nwhere `task_id` is the associated PPM task (this value is always known) and\r\n```",
              "createdAt": "2021-08-16T15:33:44Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 447,
              "body": "We need to define task ID before we get to this section. Perhaps we should wait to discuss error handling until task ID is introduced below?",
              "createdAt": "2021-08-16T15:35:05Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 597,
              "body": "TODO: Check that this is reflected elsewhere.",
              "createdAt": "2021-08-16T15:41:37Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 730,
              "body": "```suggestion\r\nThis process is illustrated below in {{pa-aggregate-flow}}. In this example,\r\n```",
              "createdAt": "2021-08-16T15:44:19Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 722,
              "body": "s/AggregateReq/PPMAggregateReq/, here and below.",
              "createdAt": "2021-08-16T15:48:45Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 741,
              "body": "```suggestion\r\nIn order to allow the helpers to minimize the state they need to run the protocol, the helper can attach a\r\n```",
              "createdAt": "2021-08-16T15:49:31Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 743,
              "body": "```suggestion\r\nvalue in the next request, thus offloading the state to the\r\n```",
              "createdAt": "2021-08-16T15:49:55Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 745,
              "body": "`{{helper-state}}` isn't a section. Also, how the state is used is currently up the helper. Without specifying a  protection mechanism I don't see what we're trying to enforce with this MUST.",
              "createdAt": "2021-08-16T15:52:48Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 781,
              "body": "s/AggregateResp/PPMAggregateResp/ (here and below)",
              "createdAt": "2021-08-16T15:53:34Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 827,
              "body": "I don't think there's a need to restrict it this way.",
              "createdAt": "2021-08-16T15:55:06Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 646,
              "body": "```suggestion\r\n### Upload Request\r\n```",
              "createdAt": "2021-08-16T15:58:52Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 900,
              "body": "s/CollectReq/PPMCollectReq/ (here and, in all likelihood, below)",
              "createdAt": "2021-08-16T16:03:04Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            },
            {
              "originalPosition": 997,
              "body": "It's not merely a matter of how the leader is configured. In fact, it mostly depends on which PPM protocol is in use. For instance, Prio lets you start validating and aggregating inputs right away, whereas Hits requires you to wait until you have full batch.",
              "createdAt": "2021-08-16T16:10:56Z",
              "updatedAt": "2021-08-16T16:18:08Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTU4NTQw",
          "commit": {
            "abbreviatedOid": "5dcb6ab"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:22:11Z",
          "updatedAt": "2021-08-16T17:22:12Z",
          "comments": [
            {
              "originalPosition": 271,
              "body": "```suggestion\r\n```",
              "createdAt": "2021-08-16T17:22:11Z",
              "updatedAt": "2021-08-16T17:22:12Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTY1MTUx",
          "commit": {
            "abbreviatedOid": "2776fe7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:29:56Z",
          "updatedAt": "2021-08-16T17:29:57Z",
          "comments": [
            {
              "originalPosition": 997,
              "body": "```suggestion\r\nDepending on the PPM scheme and how the leader is configured, the CollectReq may cause\r\n```",
              "createdAt": "2021-08-16T17:29:56Z",
              "updatedAt": "2021-08-16T17:30:10Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTY2Njgz",
          "commit": {
            "abbreviatedOid": "6dacfe5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:31:43Z",
          "updatedAt": "2021-08-16T17:31:44Z",
          "comments": [
            {
              "originalPosition": 900,
              "body": "Let's punt to #116?",
              "createdAt": "2021-08-16T17:31:44Z",
              "updatedAt": "2021-08-16T17:31:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTY3ODAy",
          "commit": {
            "abbreviatedOid": "de3df67"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:33:05Z",
          "updatedAt": "2021-08-16T17:33:05Z",
          "comments": [
            {
              "originalPosition": 511,
              "body": "```suggestion\r\n## Task Configuration {#task-configuration}\r\n```",
              "createdAt": "2021-08-16T17:33:05Z",
              "updatedAt": "2021-08-16T17:33:05Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTY4NTEx",
          "commit": {
            "abbreviatedOid": "de3df67"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:33:57Z",
          "updatedAt": "2021-08-16T17:33:58Z",
          "comments": [
            {
              "originalPosition": 452,
              "body": "```suggestion\r\nwhere `task_id` is the associated PPM task (this value is always known, see {{task-configuration}}) and\r\n```",
              "createdAt": "2021-08-16T17:33:57Z",
              "updatedAt": "2021-08-16T17:33:58Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTY5MTA4",
          "commit": {
            "abbreviatedOid": "5dcb6ab"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:34:40Z",
          "updatedAt": "2021-08-16T17:34:40Z",
          "comments": [
            {
              "originalPosition": 447,
              "body": "(forward pointer inserted)",
              "createdAt": "2021-08-16T17:34:40Z",
              "updatedAt": "2021-08-16T17:34:40Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTY5NTQ5",
          "commit": {
            "abbreviatedOid": "effc468"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:35:12Z",
          "updatedAt": "2021-08-16T17:35:13Z",
          "comments": [
            {
              "originalPosition": 722,
              "body": "Punt to #116?",
              "createdAt": "2021-08-16T17:35:12Z",
              "updatedAt": "2021-08-16T17:35:13Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTcxMDEx",
          "commit": {
            "abbreviatedOid": "123df9c"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:37:01Z",
          "updatedAt": "2021-08-16T17:37:02Z",
          "comments": [
            {
              "originalPosition": 745,
              "body": "We're requiring that the state be encrypted, which is the purpose of this MUST, no?",
              "createdAt": "2021-08-16T17:37:01Z",
              "updatedAt": "2021-08-16T17:37:02Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTcxMTM1",
          "commit": {
            "abbreviatedOid": "123df9c"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:37:11Z",
          "updatedAt": "2021-08-16T17:37:11Z",
          "comments": [
            {
              "originalPosition": 781,
              "body": "Punt to #116?",
              "createdAt": "2021-08-16T17:37:11Z",
              "updatedAt": "2021-08-16T17:37:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMwOTQ0NDU2",
          "commit": {
            "abbreviatedOid": "5dcb6ab"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T17:05:30Z",
          "updatedAt": "2021-08-16T18:06:31Z",
          "comments": [
            {
              "originalPosition": 286,
              "body": "It's a forward reference so I won't have the dependency issue you mention later.",
              "createdAt": "2021-08-16T17:05:30Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 305,
              "body": "Well, I think I'm saying something different here, which is that there ought to be a mode in which the leader just computes the results and automatically hands them off to the collector; the only thing preventing that is some sort of HTTP notification mechanics which we're actually going to need anyway to make long collection activities work.\r\n\r\nIf you want, I can make this an open issue.\r\n\r\n",
              "createdAt": "2021-08-16T17:07:27Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 447,
              "body": "This is why I mentioned above that the task ID was a 32-byte value. I think that's sufficient to understand this section.\r\n\r\n",
              "createdAt": "2021-08-16T17:08:10Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 462,
              "body": "Note that this is a defect in the previous text, which just moved.",
              "createdAt": "2021-08-16T17:08:51Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 597,
              "body": "I deliberately removed this text. I don't think it's useful to have this kind of laundry list. Better to let the requirements become clear.",
              "createdAt": "2021-08-16T17:09:42Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 646,
              "body": "I think we should name these sections by the function they perform. It's just HTTP mechanics that this is a \"Request\"",
              "createdAt": "2021-08-16T17:10:32Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 722,
              "body": "I'm actually ambivalent on this",
              "createdAt": "2021-08-16T17:10:46Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 730,
              "body": "What's wrong with \"shown\"?",
              "createdAt": "2021-08-16T17:11:00Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 745,
              "body": "IETF markdown automatically turns heds into section markers by doing tr/A-Z /a-z\\-/.\r\n\r\nI agree that the section there is imprecise, but I think that that section should describe the security requirements.",
              "createdAt": "2021-08-16T17:12:48Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 827,
              "body": "I think it's going to be incredibly confusing if not. You could easily end up in a situation where batch A is ready to read but batch B is not. It's much clearer to have multiple concurrent state machines.\r\n\r\n",
              "createdAt": "2021-08-16T17:13:35Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            },
            {
              "originalPosition": 275,
              "body": "I actually think some imprecision is helpful here. There are actually three things:\r\n\r\n- A given client report\r\n- A given set of client reports fed to the collector.\r\n- The configuration [task]\r\n\r\nI don't think it's that problematic to use \"measurement\" informally for 1 and 2. If we just think about this in a standard telemetry setting, one could say that the operator takes a measurement by collecting all the client measurements. I agree that PDUs need to be clearly defined, but this is expository text.\r\n\r\n",
              "createdAt": "2021-08-16T18:06:21Z",
              "updatedAt": "2021-08-16T18:06:31Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMTE2NzY4",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T20:42:37Z",
          "updatedAt": "2021-08-16T20:56:01Z",
          "comments": [
            {
              "originalPosition": 305,
              "body": "Oh, I see. By \"push\" you mean there might be some alternative to an HTTP request for gathering a CollectResp? Remember that, in general, thee collector needs to convey CollectReq to the aggregators somehow. (For instance, in Hits, the output can't be produced until the collector hands the aggregators the set of candidate prefixes.)\r\n\r\nIf that's what you mean, an open issue sounds fine.",
              "createdAt": "2021-08-16T20:42:37Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            },
            {
              "originalPosition": 646,
              "body": "That's cool with me, but it's a bit odd that this section's super section is also called \"Uploading Reports\".",
              "createdAt": "2021-08-16T20:43:28Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            },
            {
              "originalPosition": 722,
              "body": "SGTM",
              "createdAt": "2021-08-16T20:43:42Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            },
            {
              "originalPosition": 745,
              "body": "IMO the behavior that a MUST enforces should be fully specified. My suggestion is to drop this MUST and replace it with an open issue:\r\n> [OPEN ISSUE: Security requires the helper state to be encrypted, but we haven't said how.]",
              "createdAt": "2021-08-16T20:45:52Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            },
            {
              "originalPosition": 827,
              "body": "Yeah, that'd be pretty terrible. It seems like this would be addressed by https://github.com/abetterinternet/prio-documents/issues/109, right? Mind referring to that issue here?",
              "createdAt": "2021-08-16T20:48:32Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            },
            {
              "originalPosition": 286,
              "body": "It seems out of place here. Could we at least provide a bit more information about the ID's purpose?",
              "createdAt": "2021-08-16T20:54:12Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            },
            {
              "originalPosition": 597,
              "body": "Could you at least move it down to the dead text section below? It's useful to have this documented somewhere, at least temporarily. ",
              "createdAt": "2021-08-16T20:55:40Z",
              "updatedAt": "2021-08-16T20:56:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMTMwNzU0",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T21:01:29Z",
          "updatedAt": "2021-08-16T21:01:30Z",
          "comments": [
            {
              "originalPosition": 827,
              "body": "Sure.",
              "createdAt": "2021-08-16T21:01:29Z",
              "updatedAt": "2021-08-16T21:01:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMTMwODU3",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T21:01:36Z",
          "updatedAt": "2021-08-16T21:01:37Z",
          "comments": [
            {
              "originalPosition": 646,
              "body": "Oh, this could be an error on my part. Will check.",
              "createdAt": "2021-08-16T21:01:37Z",
              "updatedAt": "2021-08-16T21:02:02Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMTMwOTU4",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T21:01:45Z",
          "updatedAt": "2021-08-16T21:01:45Z",
          "comments": [
            {
              "originalPosition": 597,
              "body": "Sure.",
              "createdAt": "2021-08-16T21:01:45Z",
              "updatedAt": "2021-08-16T21:01:45Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMTMxMjcx",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T21:02:12Z",
          "updatedAt": "2021-08-16T21:02:12Z",
          "comments": [
            {
              "originalPosition": 283,
              "body": "SG",
              "createdAt": "2021-08-16T21:02:12Z",
              "updatedAt": "2021-08-16T21:02:12Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMTMyMjA3",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T21:03:23Z",
          "updatedAt": "2021-08-16T21:25:33Z",
          "comments": [
            {
              "originalPosition": 745,
              "body": "As a general matter, we often require MUSTs that aren't fully specified (e.g., MUST be cryptographically random) but I agree that given the state of things, an OPEN ISSUE makes sense.",
              "createdAt": "2021-08-16T21:03:24Z",
              "updatedAt": "2021-08-16T21:25:33Z"
            },
            {
              "originalPosition": 305,
              "body": "Wood and I spent a while discussing this on IM and it seems\r\nto me that there are a few pieces here:\r\n\r\n- Incremental computation\r\n- Proactively producing results\r\n\r\nTo take Prio as our example, it's quite possible for the leader to\r\njust send every report to the helpers as it comes in, then issue\r\nOutputShareReq when the batch window + jitter window expires and then\r\njust store the data for CollectReq.  It seems like that has some\r\nobvious advantages, though note that it's soft state and is invisible\r\nto the collector except for timing.\r\n\r\nMoving to Hits, the situation seems a bit more complicated.  As I\r\nunderstand it, it's not possible to collect all hits above a given\r\nthreshold and have the output concealed from the leader [0]. It\r\nmight, however, be possible to verify the proofs first, or at\r\nleast be theoretically possible even if Hits doesn't allow it\r\nnow. Again, this would be soft state.\r\n\r\n\r\nConversely, I think it's clear we're going to need some way for the\r\ncollector to set off a long running computation and get the results\r\nwhen its done. This could either be via a long poll/ websockets or\r\nsome sort of notification hook. It's possible that the leader could\r\nuse this mechanism whatever it is to proactively inform the collector\r\nof results (i.e., publish/subscribe), though maybe that's premature\r\nhere.\r\n\r\nIOW I think it's important that we don't say that this protocol\r\nrequires that all AggregateReq/OutputShare operations are triggered\r\nby a CollectReq. I think it's less important that there be some way\r\nfor it to automatically do that and deliver the results. I can remove\r\nthat and/or add an OPEN ISSUE.\r\n\r\n\r\n[0] Though presumably the pattern of queries allows the\r\nleader to infer which strings are of interest, right?\r\n\r\n          \r\n\r\n\r\n\r\n\r\n\r\n",
              "createdAt": "2021-08-16T21:25:26Z",
              "updatedAt": "2021-08-16T21:25:33Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMjE4MzA1",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T23:48:51Z",
          "updatedAt": "2021-08-16T23:48:51Z",
          "comments": [
            {
              "originalPosition": 258,
              "body": "https://github.com/abetterinternet/prio-documents/issues/117",
              "createdAt": "2021-08-16T23:48:51Z",
              "updatedAt": "2021-08-16T23:48:51Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMjE4NjYz",
          "commit": {
            "abbreviatedOid": "8be8a64"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-16T23:49:44Z",
          "updatedAt": "2021-08-16T23:49:44Z",
          "comments": [
            {
              "originalPosition": 646,
              "body": "Fixed.",
              "createdAt": "2021-08-16T23:49:44Z",
              "updatedAt": "2021-08-16T23:49:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMjQ3OTUx",
          "commit": {
            "abbreviatedOid": "b715ef6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-17T01:04:46Z",
          "updatedAt": "2021-08-17T01:04:47Z",
          "comments": [
            {
              "originalPosition": 305,
              "body": "> To take Prio as our example, it's quite possible for the leader to\r\n> just send every report to the helpers as it comes in, then issue\r\n> OutputShareReq when the batch window + jitter window expires and then\r\n> just store the data for CollectReq. It seems like that has some\r\n> obvious advantages, though note that it's soft state and is invisible\r\n> to the collector except for timing.\r\n\r\nYup!\r\n\r\n> Moving to Hits, the situation seems a bit more complicated. As I\r\n> understand it, it's not possible to collect all hits above a given\r\n> threshold and have the output concealed from the leader [0]. It\r\n> might, however, be possible to verify the proofs first, or at\r\n> least be theoretically possible even if Hits doesn't allow it\r\n> now. Again, this would be soft state.\r\n\r\nThis isn't possible for the current Hits protocol, sadly. The input validation protocol involves first evaluating the IDPF on the set of candidate prefixes, then running the \"secure sketching\" protocol of Section 4 to make sure that at most of one of these prefixes evaluates to 1 (and the others evaluate to 0). That means that input validation has to wait until the candidate prefixes are disseminated (from the collector to the aggregators). Maybe there's an alternative protocol for which input validation doesn't depend on the prefixes, but we don't have it yet.\r\n\r\n> Conversely, I think it's clear we're going to need some way for the\r\n> collector to set off a long running computation and get the results\r\n> when its done. This could either be via a long poll/ websockets or\r\n> some sort of notification hook. It's possible that the leader could\r\n> use this mechanism whatever it is to proactively inform the collector\r\n> of results (i.e., publish/subscribe), though maybe that's premature\r\n> here.\r\n\r\nAgreed. it's definitely going to be the case that the time between a collect request is issued and the response is generated could be a matter of hours. Though I wonder if it's as simple as responding to as collect request by saying \"call back in two hours, I should have your answer ready by then\"? Do we really need something fancier than this?\r\n\r\n> IOW I think it's important that we don't say that this protocol\r\n> requires that all AggregateReq/OutputShare operations are triggered\r\n> by a CollectReq.\r\n\r\nIf we want to implement Hits, then we have to accept that, in general, input validation and aggregation depends on parameters disseminated by the collector. Saying otherwise means trying to fit Hits into a box it doesn't fit in.\r\n\r\nOf course, we could consider changing the shape of the box. In particular, we might remove the encryption from the output share request and have the leader combine the output shares to get the candidate prefixes and initiate the next round itself, without involving the collector. This way the collector would only be involved at the very end, once all `n` collect rounds (i.e., rounds of input validation and aggregation) are completed. (Is this what you're suggesting?)\r\n\r\nIt's not clear to me that this results in any significant speed-up. It seems like the main advantage of this approach is that it eliminates the interaction with the collector at each round. The downside is that it rules out uses cases brought up @csharrison in which the collector wants the opportunity to \"prune\" the search space at each round based on knowledge it has of the input space. Wherever this optimization is possible, it is sure to bring a significant performance boost.\r\n\r\n> I think it's less important that there be some way\r\n> for it to automatically do that and deliver the results. I can remove\r\n> that and/or add an OPEN ISSUE.\r\n\r\nBy \"less\" do you mean \"more\" here?\r\n\r\n> [0] Though presumably the pattern of queries allows the\r\n> leader to infer which strings are of interest, right?\r\n\r\nRight, there's no way to avoid this with the current Hits protocol.\r\n\r\nBy the way, this conversation raises an interesting question. Right now we have just two PPM protocols, one in which input validation and aggregation are triggered by parameters that need to be disseminated to the aggregators (Hits), and another in which these steps can proceed immediately (Prio). The question is: _For other PPM protocols, which of these scenario is more likely?_ I don't think we have an answer right now.",
              "createdAt": "2021-08-17T01:04:47Z",
              "updatedAt": "2021-08-17T01:12:43Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMjQ5MTkz",
          "commit": {
            "abbreviatedOid": "b715ef6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-17T01:08:04Z",
          "updatedAt": "2021-08-17T01:08:05Z",
          "comments": [
            {
              "originalPosition": 275,
              "body": "Ok. I'm not sure I agree but I'm happy to differ to your experience and expertise here :)",
              "createdAt": "2021-08-17T01:08:05Z",
              "updatedAt": "2021-08-17T01:08:05Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMxMjUwMTc2",
          "commit": {
            "abbreviatedOid": "b715ef6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-17T01:10:59Z",
          "updatedAt": "2021-08-17T01:10:59Z",
          "comments": [
            {
              "originalPosition": 745,
              "body": "Ah, good point. Though I think \"MUST use a cryptographically secure random number generator\" and \"MUST use some encryption mechanism\" are qualitatively a bit different. Namely, the former statement can be made fairly precise, whereas the latter statement leaves a lot of room for misinterpretation. (Do you use an AEAD scheme? How do you pick the nonce? etc.)",
              "createdAt": "2021-08-17T01:10:59Z",
              "updatedAt": "2021-08-17T01:17:58Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMTUwMjg4",
          "commit": {
            "abbreviatedOid": "f4a201b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T19:34:39Z",
          "updatedAt": "2021-08-17T19:34:39Z",
          "comments": []
        }
      ]
    },
    {
      "number": 119,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NTE3MzM3",
      "title": "PPMFoo->Foo. Fixed #116",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/119",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-17T19:41:54Z",
      "updatedAt": "2021-08-17T19:49:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0102342a28ab5006bc76172d99b06914d1a7cdd2",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "issue116_ppm",
      "headRefOid": "704888136f7bfe1cb7c30f502cf5bfe60f47bbe1",
      "closedAt": "2021-08-17T19:49:32Z",
      "mergedAt": "2021-08-17T19:49:32Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "37248d64b5e2b2124e1c2815fb77371fdb5d7df8"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton I'm struggling with Github here and can't figure out what change you want.",
          "createdAt": "2021-08-17T19:43:50Z",
          "updatedAt": "2021-08-17T19:43:50Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks. ",
          "createdAt": "2021-08-17T19:46:45Z",
          "updatedAt": "2021-08-17T19:46:45Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMTU3MTM0",
          "commit": {
            "abbreviatedOid": "a796b9a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Yesterday I spotted an old \"PA\" prefix that was missed when we did s/PA/PDA/ would you mind finding it and fixing it?",
          "createdAt": "2021-08-17T19:43:06Z",
          "updatedAt": "2021-08-17T19:43:06Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMTU3MjQx",
          "commit": {
            "abbreviatedOid": "a796b9a"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T19:43:14Z",
          "updatedAt": "2021-08-17T19:43:14Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMTU5NTM0",
          "commit": {
            "abbreviatedOid": "7048881"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Disregard, I found it and pushed myself.",
          "createdAt": "2021-08-17T19:45:55Z",
          "updatedAt": "2021-08-17T19:45:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 120,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NTQ2MTYz",
      "title": "Problem documents",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/120",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-17T20:30:44Z",
      "updatedAt": "2021-08-17T23:25:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "37248d64b5e2b2124e1c2815fb77371fdb5d7df8",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "problem_documents",
      "headRefOid": "13f1c0e2902556ca99f2ce4d83036998e8706aed",
      "closedAt": "2021-08-17T23:25:44Z",
      "mergedAt": "2021-08-17T23:25:44Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "c477edb1dc5c6cb82dd0545d9f41118517839de9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMjAxMzMy",
          "commit": {
            "abbreviatedOid": "f50ed20"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T20:36:40Z",
          "updatedAt": "2021-08-17T20:38:37Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "```suggestion\r\n| unrecognizedMessage     | The message type for a response was incorrect or the payload was malformed. |\r\n```",
              "createdAt": "2021-08-17T20:36:40Z",
              "updatedAt": "2021-08-17T20:38:37Z"
            },
            {
              "originalPosition": 41,
              "body": "```suggestion\r\nchallenge objects as defined in {{iana-considerations}}.  PPM servers can return\r\n```",
              "createdAt": "2021-08-17T20:37:13Z",
              "updatedAt": "2021-08-17T20:38:37Z"
            },
            {
              "originalPosition": 81,
              "body": "```suggestion\r\nfollowing the template in {{!RFC3553}}:\r\n```",
              "createdAt": "2021-08-17T20:37:42Z",
              "updatedAt": "2021-08-17T20:38:37Z"
            },
            {
              "originalPosition": 58,
              "body": "```suggestion\r\nThis list is not exhaustive.  The server MAY return errors\r\n```",
              "createdAt": "2021-08-17T20:38:05Z",
              "updatedAt": "2021-08-17T20:38:37Z"
            }
          ]
        }
      ]
    },
    {
      "number": 121,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NTU2MjE3",
      "title": "Add bof request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/121",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-17T20:48:46Z",
      "updatedAt": "2021-08-22T23:33:27Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "37248d64b5e2b2124e1c2815fb77371fdb5d7df8",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "bof_request",
      "headRefOid": "b2a4a44b95ba447e9823390c541c0dd5f24154ff",
      "closedAt": "2021-08-22T23:33:27Z",
      "mergedAt": "2021-08-22T23:33:27Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "6a13c7909cdaa65b51fc32351fb11d5829e0cb3b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzA3MTQ3",
          "commit": {
            "abbreviatedOid": "e6b4f54"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-17T23:30:07Z",
          "updatedAt": "2021-08-17T23:31:44Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "```suggestion\r\nNew cryptographic techniques such as Prio and, more recently, a protocol for privacy preserving heavy hitters, address\r\n```",
              "createdAt": "2021-08-17T23:30:07Z",
              "updatedAt": "2021-08-17T23:31:44Z"
            },
            {
              "originalPosition": 43,
              "body": "```suggestion\r\n   - Key Participant Conflict: Chris Wood, Eric Rescorla, Christopher Patton, Martin Thomson,\r\n```",
              "createdAt": "2021-08-17T23:31:27Z",
              "updatedAt": "2021-08-17T23:31:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzEyMjI0",
          "commit": {
            "abbreviatedOid": "1269961"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "You might want to wrap the long line but otherwise :+1: ",
          "createdAt": "2021-08-17T23:42:29Z",
          "updatedAt": "2021-08-17T23:42:29Z",
          "comments": []
        }
      ]
    },
    {
      "number": 122,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NTY2Mzc0",
      "title": "Add abstract",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/122",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-17T21:06:25Z",
      "updatedAt": "2021-08-17T21:21:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "37248d64b5e2b2124e1c2815fb77371fdb5d7df8",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "abstract",
      "headRefOid": "ad849cc9a5de80d6b9ef471e79f4c292043a8a36",
      "closedAt": "2021-08-17T21:21:16Z",
      "mergedAt": "2021-08-17T21:21:16Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "917910995e6915ca2ccf188b5460a98cb61826ab"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMjMzMzUz",
          "commit": {
            "abbreviatedOid": "fb8c1b0"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T21:16:01Z",
          "updatedAt": "2021-08-17T21:19:14Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nrevealing any individual user's data.\r\n```\r\n(just to avoid repeated \"collect\" in the same sentence)",
              "createdAt": "2021-08-17T21:16:01Z",
              "updatedAt": "2021-08-17T21:19:14Z"
            },
            {
              "originalPosition": 11,
              "body": "```suggestion\r\npeople's individual responses but rather in aggregated data. \r\nConventional methods require collecting individual responses and then\r\naggregating them, thus representing a threat to user privacy and\r\nrendering many such measurements difficult and impractical.\r\n```",
              "createdAt": "2021-08-17T21:16:41Z",
              "updatedAt": "2021-08-17T21:19:14Z"
            },
            {
              "originalPosition": 12,
              "body": "```suggestion\r\nThis document describes a multi-party privacy preserving measurement (PPM)\r\n```\r\nTake it or leave it, but IMO clarifying that this involves multiple entities seems like a useful thing to be up front about.",
              "createdAt": "2021-08-17T21:19:11Z",
              "updatedAt": "2021-08-17T21:19:14Z"
            }
          ]
        }
      ]
    },
    {
      "number": 123,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjEzMjY5",
      "title": "Remove/restructure attic stuff",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/123",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-17T22:48:07Z",
      "updatedAt": "2021-08-18T00:14:21Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "cb9ec06593f99dbb89fa9e518366349e2c990d1b",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "cleanup",
      "headRefOid": "9d4107dc80e5992fbe95cfcca767b72a6eddd514",
      "closedAt": "2021-08-18T00:14:21Z",
      "mergedAt": "2021-08-18T00:14:21Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "a801f7f6018ec929fa6199baa4e3caea7642609c"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzA4MDU2",
          "commit": {
            "abbreviatedOid": "5628724"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T23:32:26Z",
          "updatedAt": "2021-08-17T23:32:26Z",
          "comments": []
        }
      ]
    },
    {
      "number": 124,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjE4OTA1",
      "title": "Convert to definition lists",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/124",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This is just converting to defn. list format. I'm going to submit a separate PR to clean up the contents.",
      "createdAt": "2021-08-17T23:03:35Z",
      "updatedAt": "2021-08-17T23:04:51Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "917910995e6915ca2ccf188b5460a98cb61826ab",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "glossary",
      "headRefOid": "1958c0eadb8072689f2b742dd166a3f9f3d483ee",
      "closedAt": "2021-08-17T23:04:51Z",
      "mergedAt": "2021-08-17T23:04:51Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "60d64dde0e5cf7f7330cb645ea8348771cf47fff"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMjk2MDYy",
          "commit": {
            "abbreviatedOid": "1958c0e"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T23:04:47Z",
          "updatedAt": "2021-08-17T23:04:47Z",
          "comments": []
        }
      ]
    },
    {
      "number": 125,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjIyMzkx",
      "title": "Triage the glossary:",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/125",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "- Remove terms which are infrequently used (and in a few cases,\r\n  remove the unnecessary references)\r\n\r\n- Clean up a few terms which appear to be defined incorrectly.",
      "createdAt": "2021-08-17T23:12:13Z",
      "updatedAt": "2021-08-17T23:24:20Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "60d64dde0e5cf7f7330cb645ea8348771cf47fff",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "glossary_triage",
      "headRefOid": "91cd66393215d113eed14daaa2854d4773389b41",
      "closedAt": "2021-08-17T23:24:20Z",
      "mergedAt": "2021-08-17T23:24:20Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "7d002ffddde5e7c36e6a39410560eccb7099e7e9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzA0NzAy",
          "commit": {
            "abbreviatedOid": "91cd663"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T23:24:15Z",
          "updatedAt": "2021-08-17T23:24:15Z",
          "comments": []
        }
      ]
    },
    {
      "number": 126,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjIzMTcw",
      "title": "Add open issue",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/126",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-17T23:14:06Z",
      "updatedAt": "2021-08-17T23:25:05Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "60d64dde0e5cf7f7330cb645ea8348771cf47fff",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "batch_window_issue",
      "headRefOid": "c9dfb53b6038b1e84004e5fcb789a59070b26af4",
      "closedAt": "2021-08-17T23:25:05Z",
      "mergedAt": "2021-08-17T23:25:05Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "54246a63987049b909ce20c0dce63ac3fc21a728"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzA0OTcz",
          "commit": {
            "abbreviatedOid": "c9dfb53"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T23:24:54Z",
          "updatedAt": "2021-08-17T23:24:54Z",
          "comments": []
        }
      ]
    },
    {
      "number": 127,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjM0NDM4",
      "title": "s/jitter/nonce, and file an open issue to track removing nonce altogether",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/127",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Apply `make fix-lint` to clean up some other whitespace stuff, too.\r\n\r\nCloses #113.",
      "createdAt": "2021-08-17T23:48:59Z",
      "updatedAt": "2021-12-30T02:10:14Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c477edb1dc5c6cb82dd0545d9f41118517839de9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/rename-jitter",
      "headRefOid": "ab0aa5e72987c185058f787d7970e81cf1c6cdf7",
      "closedAt": "2021-08-18T00:10:23Z",
      "mergedAt": "2021-08-18T00:10:23Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "cb9ec06593f99dbb89fa9e518366349e2c990d1b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzE1NTAy",
          "commit": {
            "abbreviatedOid": "48afcf2"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-17T23:51:03Z",
          "updatedAt": "2021-08-17T23:51:03Z",
          "comments": []
        }
      ]
    },
    {
      "number": 128,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjQyMzcw",
      "title": "Add boilerplate media type definitions for each protocol message.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/128",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #105",
      "createdAt": "2021-08-18T00:10:02Z",
      "updatedAt": "2021-12-30T02:10:17Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a801f7f6018ec929fa6199baa4e3caea7642609c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/media-types",
      "headRefOid": "d21113d3da57302cb21246f41f396665b51d8e34",
      "closedAt": "2021-08-19T16:21:11Z",
      "mergedAt": "2021-08-19T16:21:11Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "0aa718758829395915806b531060b3680593a10c"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@mnot -- is it better practice to define media types per protocol message, or to define a single media type for all protocol messages?",
          "createdAt": "2021-08-18T00:17:15Z",
          "updatedAt": "2021-08-18T00:17:15Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "Generally, if they're different formats -- e.g., they have different schema, or just different syntax -- they need different media types (or an explicitly generic one like `application/octet-stream`). If it's one format that has different uses, you can give it a single media type and then switch contexts based upon internal flags.\r\n\r\nThe real question, though, is whether you have any potential for use cases where it's important for generic software that's handling these messages to do different things based upon the media type. ",
          "createdAt": "2021-08-18T00:37:44Z",
          "updatedAt": "2021-08-18T00:37:44Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Generally, if they're different formats -- e.g., they have different schema, or just different syntax -- they need different media types (or an explicitly generic one like application/octet-stream). If it's one format that has different uses, you can give it a single media type and then switch contexts based upon internal flags.\r\n>\r\n> The real question, though, is whether you have any potential for use cases where it's important for generic software that's handling these messages to do different things based upon the media type.\r\n\r\nI don't think there'd ever be generic software using these types (beyond the HpkeConfig?). Most messages are meant to be consumed by PPM and PPM alone. I moved protocol messages to the \"message\" registry space and kept the HpkeConfig in the application type space, and also left all intended usage fields as COMMON. I also added an open issue to have someone more familiar with these registries give these allocation requests a review.\r\n\r\n@ekr, this should be good to go based on the above.\r\n",
          "createdAt": "2021-08-18T12:28:53Z",
          "updatedAt": "2021-08-18T12:28:53Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MjEwNjUy",
          "commit": {
            "abbreviatedOid": "ad650c7"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM except for the hed.",
          "createdAt": "2021-08-19T16:14:42Z",
          "updatedAt": "2021-08-19T16:15:19Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "This hed seems a bit short. What's your thinking here.",
              "createdAt": "2021-08-19T16:14:42Z",
              "updatedAt": "2021-08-19T16:15:19Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MjE1NTAy",
          "commit": {
            "abbreviatedOid": "ad650c7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T16:19:34Z",
          "updatedAt": "2021-08-19T16:19:34Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Only that it was more clear, but it's a separate change, so I'll just revert.",
              "createdAt": "2021-08-19T16:19:34Z",
              "updatedAt": "2021-08-19T16:19:34Z"
            }
          ]
        }
      ]
    },
    {
      "number": 129,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE0NjU0NDA0",
      "title": "Cache using HTTP headers, invalidate and retry upon upload.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/129",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #106.",
      "createdAt": "2021-08-18T00:34:28Z",
      "updatedAt": "2021-12-30T02:10:15Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a801f7f6018ec929fa6199baa4e3caea7642609c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/config-cache-and-invalidate",
      "headRefOid": "47ee415bcae18486c33a77194386e5cbdb1631e0",
      "closedAt": "2021-08-18T20:33:17Z",
      "mergedAt": "2021-08-18T20:33:17Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "44b507c61a5b15f36e354b0c937e0f05601361ea"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzUxMjY1",
          "commit": {
            "abbreviatedOid": "a71f7c0"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-08-18T01:14:18Z",
          "updatedAt": "2021-08-18T01:20:50Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "```suggestion\r\nAggregators SHOULD use HTTP caching to permit client-side caching of this\r\n```",
              "createdAt": "2021-08-18T01:14:18Z",
              "updatedAt": "2021-08-18T01:20:50Z"
            },
            {
              "originalPosition": 59,
              "body": "`HpkeConfig.id` is a uint8, meaning that any server can have 256 different keys, ever. If the lifetime of any HPKE key is on the order of days (which is what I infer from the suggested `Cache-Control` value above), and we don't allow recycling of `HpkeConfig.id`, then won't servers run out of key configs in just a few years? Which is a roundabout way of asking: should `HpkeConfig.id` be bigger than 8 bits?",
              "createdAt": "2021-08-18T01:19:55Z",
              "updatedAt": "2021-08-18T01:20:50Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyMzYzNjQ2",
          "commit": {
            "abbreviatedOid": "a71f7c0"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T01:47:54Z",
          "updatedAt": "2021-08-18T01:47:54Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "That seems like probably the right approach, though I suppose we could recycle.",
              "createdAt": "2021-08-18T01:47:54Z",
              "updatedAt": "2021-08-18T01:47:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyNzc5MzQ5",
          "commit": {
            "abbreviatedOid": "bd955b9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T12:08:30Z",
          "updatedAt": "2021-08-18T12:08:30Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "Indeed -- the current design requires aggregators to recycle. Since this is orthogonal to caching the config, let's file an open issue to address separately?",
              "createdAt": "2021-08-18T12:08:30Z",
              "updatedAt": "2021-08-18T12:08:30Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMyNzgxMjUx",
          "commit": {
            "abbreviatedOid": "bd955b9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T12:10:28Z",
          "updatedAt": "2021-08-18T12:10:28Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "(We had an issue tracking HpkeConfig updates, so I just updated that issue accordingly.)",
              "createdAt": "2021-08-18T12:10:28Z",
              "updatedAt": "2021-08-18T12:10:29Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMzMDI2OTEy",
          "commit": {
            "abbreviatedOid": "47ee415"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-18T15:17:30Z",
          "updatedAt": "2021-08-18T15:17:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 132,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE1MzI3ODMw",
      "title": "Added an author list.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/132",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This adds a list of authors based on the main contributors in alphabetical order.\r\n\r\n@cjpatton, @tgeoghegan I would recommend changing your email addresses to something that's not your employer so that you get emails even if you change jobs.",
      "createdAt": "2021-08-18T18:38:54Z",
      "updatedAt": "2021-08-18T22:22:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a801f7f6018ec929fa6199baa4e3caea7642609c",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "authors",
      "headRefOid": "792904f4a1435210abf956ce0bcc33637ea46b15",
      "closedAt": "2021-08-18T22:22:01Z",
      "mergedAt": "2021-08-18T22:22:01Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "d271f629357d3d7c0bcb7a90ee25cf6fd1cdf295"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMzMjk3NDgy",
          "commit": {
            "abbreviatedOid": "7aed549"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T19:58:17Z",
          "updatedAt": "2021-08-18T19:58:18Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "```suggestion\r\n       email: timgeog+ietf@gmail.com\r\n```",
              "createdAt": "2021-08-18T19:58:17Z",
              "updatedAt": "2021-08-18T19:58:18Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMzMjk3NzM3",
          "commit": {
            "abbreviatedOid": "7aed549"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-18T19:58:36Z",
          "updatedAt": "2021-08-18T19:58:36Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMzMzkwNzg5",
          "commit": {
            "abbreviatedOid": "b16e12e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-18T22:03:36Z",
          "updatedAt": "2021-08-18T22:03:37Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "```suggestion\r\n       email: chrispatton+ietf@gmail.com\r\n```",
              "createdAt": "2021-08-18T22:03:36Z",
              "updatedAt": "2021-08-18T22:03:37Z"
            }
          ]
        }
      ]
    },
    {
      "number": 134,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE1Mzk3NDQ0",
      "title": "use `urn:ietf:params:ppm` sub-namespace",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/134",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "I think this is correct given the namespace reserved in `#ppm-urn-space`.",
      "createdAt": "2021-08-18T20:32:16Z",
      "updatedAt": "2021-12-30T00:53:28Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a801f7f6018ec929fa6199baa4e3caea7642609c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/urn-ietf-ppm-params",
      "headRefOid": "ad5865654c5cd7e2d6d48b6e37e110f4bb987bc4",
      "closedAt": "2021-08-18T20:33:06Z",
      "mergedAt": "2021-08-18T20:33:06Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "7abf55dc9bc07ec3e86c3df00bd792cdbf5cc7d4"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Good catch!",
          "createdAt": "2021-08-18T20:33:02Z",
          "updatedAt": "2021-08-18T20:33:02Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzMzMzI1MzAw",
          "commit": {
            "abbreviatedOid": "ad58656"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-18T20:32:56Z",
          "updatedAt": "2021-08-18T20:32:56Z",
          "comments": []
        }
      ]
    },
    {
      "number": 135,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE1NDk2NTkx",
      "title": "Amend PDUs to support multiple helpers",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/135",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Modify `struct Param`, `struct Report` and `struct CollectResp` to\r\naccommodate more than one helper. Note that the PPM protocol doesn't\r\nactually support multiple helpers to enhance privacy (#68) but we\r\nnonetheless want to define the PDUs so it is possible to introduce that\r\nfeature in the future. We do this by introducing an `AggregatorId` type\r\ninto various struct definitions, allowing participants to tell which\r\naggregator an input share or output share corresponds to. We also\r\nintroduce a special, reserved value of `AggregatorId` that allows\r\nclients to tell which aggregator is the leader to which reports should\r\nbe uploaded. Finally, we use the aggregator ID in HPKE context\r\nconstruction instead of the existing constants 0x00 and 0x01 for leader\r\nand helper, respectively.\r\n\r\nAlong the way, we introduce the `HpkeConfigId` type alias used in few\r\ndifferent places.\r\n\r\nResolves #117, #133",
      "createdAt": "2021-08-19T00:13:44Z",
      "updatedAt": "2021-12-30T00:53:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "6a13c7909cdaa65b51fc32351fb11d5829e0cb3b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/many-helpers",
      "headRefOid": "b891bdd145fe2c46da04ae356efea79ddbb502cf",
      "closedAt": "2021-08-26T22:31:56Z",
      "mergedAt": "2021-08-26T22:31:56Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "0ba2398d5cd14c916261ae8f3ae2f8c3e1ed302b"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Note that this change is stacked on #103 (sorry).\r\n\r\n- [x] make sure this PR targets `main` before merging",
          "createdAt": "2021-08-19T00:14:19Z",
          "updatedAt": "2021-08-19T15:33:26Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan I merged #103, so we can retarget this one now.",
          "createdAt": "2021-08-19T14:20:25Z",
          "updatedAt": "2021-08-19T14:20:25Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> @tgeoghegan I merged #103, so we can retarget this one now.\r\n\r\nThanks! This is retargeted onto `main` and ready for review.",
          "createdAt": "2021-08-19T15:34:06Z",
          "updatedAt": "2021-08-19T15:34:06Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> With that in mind, my high level ask is to be stricter about how ids are assigned. In particular, the ids should be contiguous (i.e., 0, 1, ..., len(Param.aggregators)-1) and Param.aggregators should be sorted by id.\r\n\r\nWhat value does sorting these add?",
          "createdAt": "2021-08-19T21:17:24Z",
          "updatedAt": "2021-08-19T21:17:24Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "After discussion with cjpatton, I tacked on a commit that attempts to\r\n    specify this stuff without defining `AggregatorId` and inserting it into\r\n    various messages. Instead, we now require a consistent ordering between\r\n    `Param.aggregator_endpoints` and `Report.encrypted_input_shares` (the\r\n    order of `CollectResp.encrypted_output_shares` doesn't matter; the\r\n    collector doesn't need to do anything special with the leader's share).\r\n    \r\nI'm not sure if this is better than the `AggregatorId` approach but now we can compare them.\r\n",
          "createdAt": "2021-08-20T00:58:53Z",
          "updatedAt": "2021-08-20T00:59:06Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Taking a step back, is it true that we're simply trying to address aggregator routing at the leader here? In particular, upon receipt of a Report, the leader needs to know what aggregators should get the shares contained therein. The latest commit does this by saying \"send the i-th share to the i-th helper that was agreed upon in the Param structure.\" Is this a correct interpretation of the problem?",
          "createdAt": "2021-08-20T16:53:39Z",
          "updatedAt": "2021-08-20T16:53:39Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Taking a step back, is it true that we're simply trying to address aggregator routing at the leader here? In particular, upon receipt of a Report, the leader needs to know what aggregators should get the shares contained therein.\r\n\r\nThis PR is about addressing #117: putting enough structure into the PDUs so that we could extend the protocol in the future to support multiple helpers without changing the wire format of messages, even if actually supporting >1 helper might require more rounds of communication and new, yet undefined messages. IMO, that also requires solving #133, the problem of matching an encrypted input share to an aggregator. I also think that now that we have introduced a means of distinguishing arbitrary numbers of aggregators from each other, we should use it to diversify key material (but see my question below).\r\n\r\nThe actual protocol extensions are #68, which this PR does not address, nor do I think we intend to do anything about that issue anytime soon.\r\n\r\n> The latest commit does this by saying \"send the i-th share to the i-th helper that was agreed upon in the Param structure.\" Is this a correct interpretation of the problem?\r\n\r\nYes.\r\n\r\nI apologize for the back and forth on this. I think we have two important questions to resolve in this PR:\r\n\r\n## Question 1\r\nWe currently diversify keys by \"server role\", which is either \"leader\" or \"helper\". Now that we admit that there can be more than two server roles, do we want to further diversify key material by that role? \r\n\r\n### Question 1.(a)\r\nIf yes to 1., do we want to do that in this PR or should we punt to a subsequent change and leave `server_role = 0x00` for leader, `0x01` for helper?\r\n\r\n## Question 2\r\nWe want some protocol messages to be able to contain a vector of sub-messages, one for each participating aggregator. Do we want to achieve that with a vector of labeled tuples `(aggregator_id, value)` (as in commit d510e04ee4843b37e774cf3826af70f051272f7a), or with a simple vector of values, where the index into the vector is effectively the aggregator ID (as in commit 351d2a941369c048bc515cd8df32f91a7c48a3a8)?\r\n\r\nI think both are workable, with different tradeoffs. I prefer the labeled tuples approach because I think it is more explicit and  imposing fewer requirements on the order of lists in different messages means less validation code and removes opportunities for implementation bugs. @cjpatton argues for the simple vector approach.\r\n\r\nI'm not sure how to resolve that disagreement, except that consensus matters more to me than which one we choose. Should we, uh, vote on it or something?",
          "createdAt": "2021-08-20T20:00:22Z",
          "updatedAt": "2021-08-20T20:00:22Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I apologize for the back and forth on this. \r\n\r\nNo worries -- we're in alignment on the problem here. \r\n\r\nAs for the questions:\r\n\r\n1) Further diversification seems not useful, and appears like it only complicates matters. In particular, a helper doesn't necessarily need to know that it's helper i or j -- it just needs to know that it's not the leader. Sticking with what we currently have makes sense.\r\n2) I claim this is about routing messages to aggregators. As a silly example, imagine we split up Report into multiple HTTP requests, where each request carried one share for each aggregator. The request could identify the aggregator. This would also solve the problem here, but perhaps in a (much) worse way. \r\n\r\nBoth proposals -- tagging each share by an ID, or sorting the report based on the parameters -- seem fine, but I lean towards something like the former. Sorting the report means that everyone needs to agree on the order of helpers everywhere, which seems like an unnecessary constraint. I would prefer a design wherein each share unambiguously, and without additional coordination, identifies the intended aggregator.",
          "createdAt": "2021-08-20T20:20:01Z",
          "updatedAt": "2021-08-20T20:20:34Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "SGTM\n\nOn Mon, Aug 23, 2021 at 11:27 AM Tim Geoghegan ***@***.***>\nwrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In draft-pda-protocol.md\n> <https://github.com/abetterinternet/prio-documents/pull/135#discussion_r694209946>\n> :\n>\n> > @@ -419,12 +418,15 @@ Duration uint64; /* Number of seconds elapsed between two instants */\n>  Time uint64; /* seconds elapsed since start of UNIX epoch */\n>  ~~~\n>\n> -* `uuid`: A unique sequence of bytes used  to ensure that two otherwise\n> +* `uuid`: A unique sequence of bytes used to ensure that two otherwise\n>\n> I agree, but I think struct Param will go away altogether when someone\n> (perhaps me, later this week) addresses #104\n> <https://github.com/abetterinternet/prio-documents/issues/104>, and we\n> will be left with nothing but task_id. So I prefer to leave this text\n> alone until it gets deleted in that PR (except, I guess, to delete this\n> extra space).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/prio-documents/pull/135#discussion_r694209946>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLILWUDR7YM562RUZ73LT6KHK7ANCNFSM5CNCZXCA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n",
          "createdAt": "2021-08-23T18:37:11Z",
          "updatedAt": "2021-08-23T18:37:11Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "In the interest of continuing to move forward, I am going to keep the version where we keep the ordering consistent across different messages. Per @chris-wood's observation, I've punted the question of aggregator IDs or server roles beyond the existing 0x01/0x00 constants to #138. ",
          "createdAt": "2021-08-23T19:00:38Z",
          "updatedAt": "2021-08-23T19:00:38Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MjIyODE3",
          "commit": {
            "abbreviatedOid": "24be953"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T16:27:03Z",
          "updatedAt": "2021-08-19T16:31:15Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "I would consider making this larger. I could imagine there being > 256 aggregators.",
              "createdAt": "2021-08-19T16:27:03Z",
              "updatedAt": "2021-08-19T16:31:15Z"
            },
            {
              "originalPosition": 55,
              "body": "```suggestion\r\n  when constructing other messages containing vectors with one value for each\r\n```",
              "createdAt": "2021-08-19T16:27:20Z",
              "updatedAt": "2021-08-19T16:31:15Z"
            },
            {
              "originalPosition": 57,
              "body": "Why do aggregators need to know their ID?",
              "createdAt": "2021-08-19T16:27:35Z",
              "updatedAt": "2021-08-19T16:31:16Z"
            },
            {
              "originalPosition": 57,
              "body": "```suggestion\r\n  Each aggregator knows its own `id`. `id` values must be unique in a `Param` but may have different meanings in different Params.\r\n```",
              "createdAt": "2021-08-19T16:28:28Z",
              "updatedAt": "2021-08-19T16:31:16Z"
            },
            {
              "originalPosition": 45,
              "body": "Would it actually make sense to not have an integer in this field and only in the submissions, and just have those values refer to indexes here.",
              "createdAt": "2021-08-19T16:30:21Z",
              "updatedAt": "2021-08-19T16:31:16Z"
            },
            {
              "originalPosition": 160,
              "body": "Actually, why do we need this field at all?",
              "createdAt": "2021-08-19T16:31:04Z",
              "updatedAt": "2021-08-19T16:31:16Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MjE5MzUx",
          "commit": {
            "abbreviatedOid": "24be953"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nice! This really cleans stuff up.",
          "createdAt": "2021-08-19T16:23:33Z",
          "updatedAt": "2021-08-19T16:31:41Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "```suggestion\r\n  Each aggregator knows its own `id`. `id` values MUST be unique in a `Param`.\r\n  A `Param` structure with duplicate `id` values is considered malformed and MUST\r\n  be ignored by clients.\r\n```",
              "createdAt": "2021-08-19T16:23:34Z",
              "updatedAt": "2021-08-19T16:31:41Z"
            },
            {
              "originalPosition": 62,
              "body": "```suggestion\r\n  `Param`. A `Param` structure without the `leader_aggregator_id` is considered malformed\r\n   and MUST be ignored by clients.\r\n```",
              "createdAt": "2021-08-19T16:24:29Z",
              "updatedAt": "2021-08-19T16:31:41Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MjI5NzEy",
          "commit": {
            "abbreviatedOid": "24be953"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T16:34:12Z",
          "updatedAt": "2021-08-19T16:34:13Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "> Why do aggregators need to know their ID?\r\n\r\nThis is a fair question -- it seems like the aggregator ID is only used to coordinate messages, and might not need to be included in the AAD.",
              "createdAt": "2021-08-19T16:34:12Z",
              "updatedAt": "2021-08-19T16:34:13Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MzQ1ODMy",
          "commit": {
            "abbreviatedOid": "24be953"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T18:43:22Z",
          "updatedAt": "2021-08-19T18:43:22Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "Yes, an aggregator only needs to know its own ID to put in the AAD when establishing HPKE context. That is also why I made that field `uint8`, to match the previous leader/helper `server_role` values. However we could also make the aggregator ID into a longer string of bytes, allowing the values to be human readable strings, which I think will be nice for logging purposes. But it sounds like you are doubting the value of including server role in the AAD at all? I would think that since a given aggregator could be acting as either a leader or a helper in different protocol instantiations, and maybe using the same HPKE config for both, including server role diversifies keys across protocol instantiations. Then again, if that's the goal, then we should probably also mix in the `task_id` since `aggregator_id` values could be re-used across different protocol instantiations.\r\n\r\nSo my take here is we should:\r\n\r\n* define `opaque AggregatorId<1..256>;`\r\n* use `enc, context = SetupBase(S|R)(pk, \"pda input share\" || task_id || aggregator_id)` and `enc, context = SetupBaseS(pk, \"pda output share\" || task_id || aggregator_id)`\r\n\r\nBut I will defer to the superior crypto brains of @chris-wood and @ekr ... should we just drop `server_role`/`aggregator_id` from the HPKE application info?",
              "createdAt": "2021-08-19T18:43:22Z",
              "updatedAt": "2021-08-19T19:50:04Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MzYwMzQ5",
          "commit": {
            "abbreviatedOid": "24be953"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T19:00:12Z",
          "updatedAt": "2021-08-19T19:00:12Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "Or we could go back to leader = `0x00` / helper = `0x01` in the info, which has the virtue of not entangling our mechanism for coordinating messages with our mechanism for key diversity, but then we will have to revisit the question of what application info should be provided by helpers beyond the first one, if and when we introduce multiple helpers. My goal in introducing the aggregator_id into the AAD was to avoid such a disruptive change.",
              "createdAt": "2021-08-19T19:00:12Z",
              "updatedAt": "2021-08-19T19:00:12Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0MzYzMzA5",
          "commit": {
            "abbreviatedOid": "24be953"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T19:03:34Z",
          "updatedAt": "2021-08-19T19:03:34Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "I agree, there's no reason not to make this a 64 bit integer (or even a much longer byte string as I suggest in another thread on this PR). However I'm not sure if we want the IDs in other messages to refer to indexes here, because that imposes the new constraint on aggregator IDs that they be sequential, which is not required with the current `Aggregator` definition.\r\n",
              "createdAt": "2021-08-19T19:03:34Z",
              "updatedAt": "2021-08-19T19:03:34Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0NDQ1NDg3",
          "commit": {
            "abbreviatedOid": "49ce09c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "This change is basically right. The main thing I would change is to make more clear the intended meaning of aggregator ID. While it was intended to solve the indexing-into-input-shares-array problem, this PR also addresses the more fundamental problem of assigning roles to each of the aggregators in the protocol. I think this is a great idea, and that's what this should be about.\r\n\r\nWith that in mind, my high level ask is to be stricter about how ids are assigned. In particular, the ids should be contiguous (i.e., `0, 1, ..., len(Param.aggregators)-1`) and `Param.aggregators` should be sorted by id.",
          "createdAt": "2021-08-19T20:36:49Z",
          "updatedAt": "2021-08-19T21:02:55Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "256 or more aggregators sounds crazy to me, but maybe we have a different mental model for AggregatorId. I would think that AggregatorId identifies the *role* of an endpoint of the protocol. Having more than, say, 10 different roles (1 leader and 9 helpers, all doing the same thing, but maybe in a particular order) seems inconceivable to me.",
              "createdAt": "2021-08-19T20:36:49Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 87,
              "body": "```suggestion\r\nBefore the client can upload its report to the leader, it must know the public key of\r\n```",
              "createdAt": "2021-08-19T20:42:03Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 89,
              "body": "This is hard to read because it uses some non-standard notation. Is there anything technically wrong with what we had before? It seems to me that nothing has changed semantically.",
              "createdAt": "2021-08-19T20:45:13Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 47,
              "body": "Instead of reserving a special value, could we just insist that each `Aggregator` in `Param.aggregators` is sorted by `Aggregator.id`? While at it, I think we should insist that the sequence of ids is always `0, 1, ..., len(Param.aggregators)-1`. (Maybe we already have this?)",
              "createdAt": "2021-08-19T20:48:21Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 137,
              "body": "If we insist that the aggregator ids are unique, contiguous, and start at 0, then here we could just say `aggregator_id in [0, num_aggregators)`.",
              "createdAt": "2021-08-19T20:49:37Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 221,
              "body": "It would be cleaner if we could just insist that `agregator.id > 0`.",
              "createdAt": "2021-08-19T20:51:46Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 222,
              "body": "Here again is some non-standard notation. Can we express this differently?",
              "createdAt": "2021-08-19T20:52:05Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 317,
              "body": "```suggestion\r\nID of the aggregator. `output_share` is the serialized `OutputShare`.\r\n```",
              "createdAt": "2021-08-19T20:54:48Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            },
            {
              "originalPosition": 336,
              "body": "```suggestion\r\n* `aggregator_id` is the ID of the aggregator constructing the\r\n```",
              "createdAt": "2021-08-19T20:55:32Z",
              "updatedAt": "2021-08-19T21:02:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0NDc2Njk2",
          "commit": {
            "abbreviatedOid": "4cd6bd2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T21:18:19Z",
          "updatedAt": "2021-08-19T21:18:19Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "How would this value (of zero) different from having the leader appear first (also a value of zero)?",
              "createdAt": "2021-08-19T21:18:19Z",
              "updatedAt": "2021-08-19T21:18:19Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0NDc3NTEz",
          "commit": {
            "abbreviatedOid": "4cd6bd2"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T21:19:29Z",
          "updatedAt": "2021-08-19T21:19:29Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "FWIW, I'd drop it from the AAD, and then mark an open issue asking whether or not we believe it's important to authenticate. (My impression is that this is not important to authenticate.)",
              "createdAt": "2021-08-19T21:19:29Z",
              "updatedAt": "2021-08-19T21:19:29Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0NTUzOTcx",
          "commit": {
            "abbreviatedOid": "4cd6bd2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-19T23:38:01Z",
          "updatedAt": "2021-08-19T23:38:02Z",
          "comments": [
            {
              "originalPosition": 89,
              "body": "I guess I'm not sure how clearly an RFC needs to spell out things like this for implementors. I agree it was pretty clear before so I think I'll put it back.",
              "createdAt": "2021-08-19T23:38:02Z",
              "updatedAt": "2021-08-19T23:38:02Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM1NjIyMTAx",
          "commit": {
            "abbreviatedOid": "351d2a9"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I'm also a bit concerned about universal sorting, but I think we can resolve this separately, b/c it's really just about what goes into the HPKE functions. I suggest we land this once the more minor comments have been resolved.",
          "createdAt": "2021-08-22T23:38:56Z",
          "updatedAt": "2021-08-22T23:56:27Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "I wouldn't call this \"uuid\" because then some IETF nitpicker will complain that it's not actually a UUID.\r\n\r\nMaybe ```task_nonce```",
              "createdAt": "2021-08-22T23:38:56Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 25,
              "body": "I would rewrite this because it's the reports which have to be consistent with this:\r\n```suggestion\r\n  endpoints can be found. The leader's endpoint MUST be the first in the vector.\r\n  The order in encrypted input shares in a `Report` (see\r\n  {{uploading-reports}}) MUST be the same as the\r\n  order in which aggregators appear in this vector.\r\n```",
              "createdAt": "2021-08-22T23:40:19Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 34,
              "body": "FYI, you don't need this. kramdown automatically creates anchors by doing tr/A-Z /a-z\\-/.",
              "createdAt": "2021-08-22T23:41:24Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 49,
              "body": "```suggestion\r\naggregator's endpoint URL, provided in the `Param` for the task. The aggregator responds to\r\n```",
              "createdAt": "2021-08-22T23:42:24Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 73,
              "body": "```suggestion\r\nThe client MUST abort if any of the following happen for any `key_config` request:\r\n```",
              "createdAt": "2021-08-22T23:42:46Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 84,
              "body": "FWIW, I prefer the phrasing before. \"Let\" is a bit stilted.",
              "createdAt": "2021-08-22T23:43:14Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 96,
              "body": "```suggestion\r\n  should be for the first helper, and so on).\r\n```",
              "createdAt": "2021-08-22T23:44:59Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            },
            {
              "originalPosition": 208,
              "body": "This bit of mathiness seems a bit confusing. Instead of expanding the formula, I would consider \"where each subresponse corresponds to the sub-request in the same position in AggregateReq\"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
              "createdAt": "2021-08-22T23:51:30Z",
              "updatedAt": "2021-08-22T23:56:27Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM2NDM3NjY2",
          "commit": {
            "abbreviatedOid": "351d2a9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-23T18:27:48Z",
          "updatedAt": "2021-08-23T18:27:48Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "I agree, but I think `struct Param` will go away altogether when someone (perhaps me, later this week) addresses #104, and we will be left with nothing but `task_id`. So I prefer to leave this text alone until it gets deleted in that PR (except, I guess, to delete this extra space).",
              "createdAt": "2021-08-23T18:27:48Z",
              "updatedAt": "2021-08-23T18:27:48Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM3ODA0OTUw",
          "commit": {
            "abbreviatedOid": "ce83f1b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Yup! This is looking good to me. A couple more questions in-line.",
          "createdAt": "2021-08-25T00:53:43Z",
          "updatedAt": "2021-08-25T00:57:18Z",
          "comments": [
            {
              "originalPosition": 108,
              "body": "Does it make sense to replace `server_role` with the index of the input share in the report struct? This may be useful for protocols with more than two aggregators.",
              "createdAt": "2021-08-25T00:53:44Z",
              "updatedAt": "2021-08-25T00:57:18Z"
            },
            {
              "originalPosition": 185,
              "body": "Do you intend to start a new paragraph here? It's usually OK to inline OPEN ISSUEs with surrounding text.",
              "createdAt": "2021-08-25T00:55:08Z",
              "updatedAt": "2021-08-25T00:57:18Z"
            },
            {
              "originalPosition": 213,
              "body": "See comment above.",
              "createdAt": "2021-08-25T00:55:48Z",
              "updatedAt": "2021-08-25T00:57:18Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM3ODIwOTIw",
          "commit": {
            "abbreviatedOid": "ce83f1b"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T01:38:17Z",
          "updatedAt": "2021-08-25T01:38:17Z",
          "comments": [
            {
              "originalPosition": 108,
              "body": "We punted this to a separate issue. ",
              "createdAt": "2021-08-25T01:38:17Z",
              "updatedAt": "2021-08-25T01:38:17Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4ODA2NTQ4",
          "commit": {
            "abbreviatedOid": "ce83f1b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T20:37:40Z",
          "updatedAt": "2021-08-25T20:37:40Z",
          "comments": [
            {
              "originalPosition": 108,
              "body": "Namely #138. I do agree that server role will end up being the index of the aggregator in the vector.",
              "createdAt": "2021-08-25T20:37:40Z",
              "updatedAt": "2021-08-25T20:38:56Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4ODA4ODg0",
          "commit": {
            "abbreviatedOid": "e42452b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T20:40:33Z",
          "updatedAt": "2021-08-25T20:40:33Z",
          "comments": [
            {
              "originalPosition": 185,
              "body": "I think I did that because I rewrote this paragraph so many times that I got tired of re-word-wrapping the [OPEN ISSUE]. I put it back inline.",
              "createdAt": "2021-08-25T20:40:33Z",
              "updatedAt": "2021-08-25T20:40:33Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4ODA5MDYz",
          "commit": {
            "abbreviatedOid": "e42452b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T20:40:45Z",
          "updatedAt": "2021-08-25T20:40:45Z",
          "comments": [
            {
              "originalPosition": 213,
              "body": "Also #138. ",
              "createdAt": "2021-08-25T20:40:45Z",
              "updatedAt": "2021-08-25T20:40:45Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4OTM2ODE2",
          "commit": {
            "abbreviatedOid": "e42452b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Just a few more comments, one of which is blocking.",
          "createdAt": "2021-08-26T00:36:39Z",
          "updatedAt": "2021-08-26T00:50:01Z",
          "comments": [
            {
              "originalPosition": 129,
              "body": "nit\r\n```suggestion\r\n`extensions` are the corresponding fields of `Report`.\r\n```",
              "createdAt": "2021-08-26T00:36:40Z",
              "updatedAt": "2021-08-26T00:50:01Z"
            },
            {
              "originalPosition": 150,
              "body": "```suggestion\r\ncan send them to the helpers to be validated and aggregated. In order\r\n```",
              "createdAt": "2021-08-26T00:37:35Z",
              "updatedAt": "2021-08-26T00:50:01Z"
            },
            {
              "originalPosition": 166,
              "body": "Hmm, this is a bit hairy. We haven't yet fleshed out the communication pattern for protocols with multiple helpers. This seems to imply that the leader always starts by making an aggregate request to each helper. But in what order? Do you wait for the response form the first helper before making a request to the next?\r\n\r\nI think we need to punt on this problem for now by writing this section as if there's just one helper. Here for example you would say something like\r\n> The leader sends a POST request to `[helper]/aggregate`, where `[helper]` is the helper URL specified by the PPM parameters for task ID `AggregateReq.task_id`.",
              "createdAt": "2021-08-26T00:44:26Z",
              "updatedAt": "2021-08-26T00:50:01Z"
            },
            {
              "originalPosition": 236,
              "body": "Here again it might be best to speak in terms of 1 leader and 1 helper. I can't imagine that the order in which output-share requests are made would matter, but maybe we should err on the side of simplicity.",
              "createdAt": "2021-08-26T00:49:23Z",
              "updatedAt": "2021-08-26T00:50:01Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4OTY4MDUw",
          "commit": {
            "abbreviatedOid": "e42452b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-26T02:02:26Z",
          "updatedAt": "2021-08-26T02:02:26Z",
          "comments": [
            {
              "originalPosition": 166,
              "body": "Sure, seems reasonable. The goal of this change was just to make it possible to admit more helpers, so we can certainly make this language simpler without losing anything.",
              "createdAt": "2021-08-26T02:02:26Z",
              "updatedAt": "2021-08-26T02:02:26Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM5OTU0MTAy",
          "commit": {
            "abbreviatedOid": "4b6797f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-26T21:30:34Z",
          "updatedAt": "2021-08-26T21:30:34Z",
          "comments": []
        }
      ]
    },
    {
      "number": 136,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE2Mjk4Njkz",
      "title": "s/pda/ppm",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/136",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The draft name still has the acronym \"pda\" in it. Let's kill this for good.",
      "createdAt": "2021-08-19T22:16:25Z",
      "updatedAt": "2021-12-30T02:10:18Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0aa718758829395915806b531060b3680593a10c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/pda2ppm",
      "headRefOid": "a2541ee2928fcb70fc134564085be9092c878522",
      "closedAt": "2021-08-20T15:02:24Z",
      "mergedAt": "2021-08-20T15:02:24Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "7c14a1f6c535662376033749d8636440ac74ff82"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "You also need to fix .targets.mk I believe",
          "createdAt": "2021-08-19T22:19:14Z",
          "updatedAt": "2021-08-19T22:19:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "nice catch, @ekr",
          "createdAt": "2021-08-19T22:39:14Z",
          "updatedAt": "2021-08-19T22:39:14Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM0NTQzNTE3",
          "commit": {
            "abbreviatedOid": "a2541ee"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-19T23:12:11Z",
          "updatedAt": "2021-08-19T23:12:11Z",
          "comments": []
        }
      ]
    },
    {
      "number": 137,
      "id": "MDExOlB1bGxSZXF1ZXN0NzE2MzczMDgx",
      "title": "Resolve open batch boundary issues",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/137",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We have the following security/operational considerations, not all of\r\nwhich can be addressed at the same time:\r\n\r\n  - Enforcing privacy budgets for DP requires us to limit the number of\r\n  batches the same report is used.\r\n  - Hits requires the same report to be used in some number of batches.\r\n\r\nTo resolve these issues, this change adds a `max_report_lifetime`\r\nparameter, which specifies the maximum number of times a report may be\r\naggregated into an output. This enables Hits while allowing PPM\r\nprotocols to enforce a privacy budget.\r\n\r\nThis change also seeks to clarify some ambiguities in how the batch\r\nboundaries are defined, and why they're defined that way. Along the way,\r\nit cleans up some protocol bits that got lost in the last refactor.\r\n\r\nCloses #118 by doing nothing. Hopefully the new text makes the issue more\r\nclear.",
      "createdAt": "2021-08-20T01:46:13Z",
      "updatedAt": "2021-12-30T02:10:20Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "14625bf507ab723949271d78f4667d74aedfaaa7",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/batch-boundaries",
      "headRefOid": "f4168a161758c111d6d5487819b7f4c43c8bbaa2",
      "closedAt": "2021-09-09T14:12:05Z",
      "mergedAt": "2021-09-09T14:12:05Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "fc7dfe824f1586d76bde80aea403c98615a5ce92"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased.",
          "createdAt": "2021-09-07T23:32:35Z",
          "updatedAt": "2021-09-07T23:32:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2021-09-09T14:12:00Z",
          "updatedAt": "2021-09-09T14:12:00Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM1MzgwNTU2",
          "commit": {
            "abbreviatedOid": "a467f99"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-08-20T20:55:14Z",
          "updatedAt": "2021-08-20T22:02:29Z",
          "comments": [
            {
              "originalPosition": 66,
              "body": "I think the idea is that if you have an extension whose purpose is client->server authentication, then the authenticity and/or integrity and/or confidentiality of the contents of the extension are internal to the structure of the extension and opaque to PPM. But I don't think we want to get into that conversation in this PR, and your `OPEN ISSUE` as written is valid.",
              "createdAt": "2021-08-20T20:55:14Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 7,
              "body": "Eight usages of the term \"batch interval\" are introduced by this change. Should it be defined here?",
              "createdAt": "2021-08-20T21:03:27Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 167,
              "body": " \"each input share\" -> this means that helper has to track privacy budget per-input, which I think is a significant increase in the storage requirements on the helper (and/or the size of the opaque state stored for helper by leader). Isn't the idea that the helper can just maintain a few tuples of `(timestamp, consumed_privacy)`, meaning that `consumed_privacy` worth of privacy budget has been used up for all the reports up to `timestamp`? e.g.:\r\n\r\n```\r\ntimestamp | consumed_privacy\r\n1000      | 20\r\n2000      | 10\r\n```\r\nmeans reports with timestamp in range `[0, 1000)` have consumed 20 points of privacy, `[1001, 2000)` have consumed 10 points of privacy, `[2000, end)` have consumed 0 points of privacy.",
              "createdAt": "2021-08-20T21:24:22Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 168,
              "body": "Is an integer count of inclusions in an output sufficient? I think that assumes that every query against a set of input shares consumes the same \"amount\" of privacy budget. Is that true in all schemes? In heavy hitters, the idea is that the collector will iteratively make queries with longer and longer string prefixes. Does varying prefix length change \"how much\" privacy is consumed by a query?",
              "createdAt": "2021-08-20T21:25:42Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 169,
              "body": "`lifetime` does not seem like the right word, particularly because it implies an interval or duration of time. Why not `Param.privacy_budget`?",
              "createdAt": "2021-08-20T21:31:28Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 174,
              "body": "I'm not sure this is correct. I think \"except as required\" is referring to a case like heavy hitters where the collector may do multiple queries against a single batch with longer and longer string prefixes, right? But even then, then I think aggregators still need to enforce that an input only ever appears in a single batch, and appears at most once in that batch.\r\n\r\nPut differently, our notions of anti-replay are about how inputs are assigned to batches, and apply uniformly regardless of PPM scheme. The question of how many queries are allowed against a batch by some scheme is orthogonal.",
              "createdAt": "2021-08-20T21:46:09Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 274,
              "body": "See my comment in #{{anti-replay}} -- I don't think this is correct.",
              "createdAt": "2021-08-20T21:50:09Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 157,
              "body": "To double confirm: this means that given a `Param.min_batch_duration` of eight hours, then batch intervals of 8, 16, 24 hours are valid, but 12 or 17 are not, right? And the aim here is to allow aggregators to pre-compute output shares before a collect request, since servicing a collect request over `0800-2400` just requires summing the already computed output shares for `0800-1600` and `1600-2400`?",
              "createdAt": "2021-08-20T21:54:24Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 292,
              "body": "```suggestion\r\nclients, but are not robust against malicious servers. Any aggregator can\r\n```",
              "createdAt": "2021-08-20T21:55:32Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            },
            {
              "originalPosition": 243,
              "body": "This would also be a good place to discuss efficient means of tracking privacy budgets.",
              "createdAt": "2021-08-20T22:02:04Z",
              "updatedAt": "2021-08-20T22:02:29Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM3NzkxNDQ2",
          "commit": {
            "abbreviatedOid": "a467f99"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T00:15:29Z",
          "updatedAt": "2021-08-25T00:50:06Z",
          "comments": [
            {
              "originalPosition": 157,
              "body": "Yes.",
              "createdAt": "2021-08-25T00:15:29Z",
              "updatedAt": "2021-08-25T00:50:06Z"
            },
            {
              "originalPosition": 167,
              "body": "There is indeed an increase of storage requirements compared to the status quo. However, the status quo doesn't account for privacy budgets at all. Hence, I claim this is a cost we have to eat no matter what.\r\n\r\nBut how big is the cost? The key thing to notice is that the increase in storage requirements is linear in the number of batch intervals _that can still be queried_. This number depends on the minimum batch duration and the rate at which the collector makes requests. Crucially, the storage increase _does not depend on the number of reports_.",
              "createdAt": "2021-08-25T00:21:20Z",
              "updatedAt": "2021-08-25T00:50:06Z"
            },
            {
              "originalPosition": 168,
              "body": "Interesting question. To put it another way: Is the max report lifetime too coarse-grained?\r\n\r\nThe interesting thing about Hits is that, without DP, _some amount of leakage_ is inevitable, but as you point out, how much is leaked depends on the distribution of inputs. (As an extreme example, suppose a large number of non-heavy-hitters share a common prefix.)\r\n\r\nVanilla Hits doesn't try to mitigate this at all, so we would set the max report lifetime to be the length of the input. (This is the maximum number of collect requests that would be needed to compute the heavy hitters.) The current proposal for mitigating this leakage is via DP. Interestingly, if the aggregators are adding the noise, then the distribution of the noise -- and therefore the privacy budget -- can only depend on the candidate prefixes of the previous rounds. I don't imagine it would, it seems reasonable to just pick a max report lifetime. But we don't want to preclude the possibility of someone doing something fancier.\r\n\r\nOf course, nothing about the protocol precludes the aggregators from bailing out _before_ the max report lifetime is reached... what's important is that they agree on when to bail out.\r\n\r\nWhich leads me to this conclusion: the max report lifetime parameter seems to wear to hats:\r\n1. It provides a coarse-grained way of enforcing a privacy budget. In particular, it should suffice for \"simple\" schemes in which the privacy budget is independent of the input distribution.\r\n2. It provides a way for aggregators to know how long they have to keep around reports (or the output shares derived from them). Namely, if a report has reached its max lifetime, it's OK to throw it away.\r\n\r\nThis PR talks about (1.), but perhaps it also makes sense to talk about (2.).",
              "createdAt": "2021-08-25T00:43:20Z",
              "updatedAt": "2021-08-25T00:50:06Z"
            },
            {
              "originalPosition": 174,
              "body": "You're right, this needs to be rephrased. What do you think about\r\n\r\n> Using a report multiple times withing a single batch, or using the same report in multiple batches, is considered a privacy violation. To prevent such attacks...",
              "createdAt": "2021-08-25T00:46:34Z",
              "updatedAt": "2021-08-25T00:50:06Z"
            },
            {
              "originalPosition": 243,
              "body": "We already suggest that such a scheme exists:\r\n>  each aggregator only needs to store the aggregate output share for each possible batch interval,\r\nalong with the number of times the output share was used in a batch.\r\n\r\nI suggest we hold off on describing this until we've implemented it. This will give us a chance to validate any assumptions we're making here. If you'd like I can flag an OPEN ISSUE here.",
              "createdAt": "2021-08-25T00:48:21Z",
              "updatedAt": "2021-08-25T00:50:06Z"
            },
            {
              "originalPosition": 274,
              "body": "Yup, good call. Maybe this distinction would be more clear if we renamed \"max_report_lifetime\" to \"max_batch_lifetime\", since what we're really talking about is how many times a batch of reports can be queried.",
              "createdAt": "2021-08-25T00:49:44Z",
              "updatedAt": "2021-08-25T00:50:06Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM3ODAzODA2",
          "commit": {
            "abbreviatedOid": "c28c713"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T00:50:24Z",
          "updatedAt": "2021-08-25T00:50:24Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Yeah, I can do that.",
              "createdAt": "2021-08-25T00:50:24Z",
              "updatedAt": "2021-08-25T00:50:24Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4NTg0MzY0",
          "commit": {
            "abbreviatedOid": "c28c713"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-08-25T16:38:49Z",
          "updatedAt": "2021-08-25T17:01:11Z",
          "comments": [
            {
              "originalPosition": 174,
              "body": "Yes, that seems better to me, though \"within\" and not \"withing\".",
              "createdAt": "2021-08-25T16:38:49Z",
              "updatedAt": "2021-08-25T17:01:11Z"
            },
            {
              "originalPosition": 168,
              "body": "On question 1, my intuition is that the amount of privacy budget consumed by a query should be decided by aggregator servicing the query. How to compute the amount of privacy consumed is part of the specification of a PPM scheme (so for Prio, you always consume one unit of privacy, and for Hits it's more complicated). However as you point out:\r\n>what's important is that [aggregators] agree on when to bail out.\r\n\r\nIf leader and aggregator disagree on how much privacy was consumed by queries, then they might not bail out in lockstep. But perhaps we can accommodate that thanks to the leader model? The helper could indicate to the leader in `AggregateSubResp` that the privacy budget for a report has been exhausted, at which point the leader can fail the corresponding `CollectResp`. Similarly if the leader decides a report's privacy budget is exhausted, then it can stop executing the aggregate protocol for that report with the helper.\r\n\r\nHowever I acknowledge that's more complicated, and it's not worth doing if we strongly believe that the coarse-grained privacy budgeting (i.e. one query always consumes one unit of privacy) will suffice for all or nearly all future schemes.\r\n\r\nOn question 2: now I understand why you used the word lifetime. I still think that's the wrong term, for these reasons:\r\n\r\n- \"lifetime\" implies, well, time, and there are several other places where we discuss spans of time or points in time (e.g., batch window or report timestamp, respectively), so I'd prefer to express this in a way that doesn't imply a relation to other time concepts in the protocol. Plus the unit of this lifetime isn't even measured in seconds but rather a number of queries.\r\n- I don't think PPM wants to specify anything about how long reports are retained, beyond the implicit requirement that they be available long enough to service all the collector's queries and a suggestion that implementations could delete them afterward. The observable behavior our protocol cares about is limiting how many queries may be made against a report, and I think message fields should be named accordingly. For that reason, `privacy_budget` makes more sense to me than `max_report_lifetime`.",
              "createdAt": "2021-08-25T16:57:26Z",
              "updatedAt": "2021-08-26T22:31:43Z"
            },
            {
              "originalPosition": 243,
              "body": "Good point. Plus privacy budgets and their efficient representations will vary with PPM schemes, so such discussion should perhaps be in the Prio or Hits specific docs.",
              "createdAt": "2021-08-25T16:58:19Z",
              "updatedAt": "2021-08-25T17:01:11Z"
            },
            {
              "originalPosition": 169,
              "body": "We're discussing this name in a different comment thread, so resolving this one",
              "createdAt": "2021-08-25T16:59:31Z",
              "updatedAt": "2021-08-25T17:01:11Z"
            },
            {
              "originalPosition": 167,
              "body": ">Crucially, the storage increase does not depend on the number of reports.\r\n\r\nThis is the key property I am concerned with, and I agree that nothing in this text precludes this or suggests otherwise, so resolving this.",
              "createdAt": "2021-08-25T17:00:05Z",
              "updatedAt": "2021-09-08T20:49:04Z"
            },
            {
              "originalPosition": 66,
              "body": "Nothing further to say about this in this PR except: could you please make sure we have a GH issue with label parking-lot for this question?",
              "createdAt": "2021-08-25T17:00:59Z",
              "updatedAt": "2021-08-25T17:01:11Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM4ODA0MTUw",
          "commit": {
            "abbreviatedOid": "c28c713"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-08-25T20:34:42Z",
          "updatedAt": "2021-08-25T20:34:43Z",
          "comments": [
            {
              "originalPosition": 168,
              "body": "On the question of naming the field: this might be one of those things that changes a few more times as we go through the standardization process and we get more eyes and commentators on the protocol, so if you disagree with me about `privacy_budget`, I'm happy to move forward with `max_report_lifetime` (or `max_batch_lifetime` as you suggested elsewhere).",
              "createdAt": "2021-08-25T20:34:42Z",
              "updatedAt": "2021-08-25T20:34:43Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ4NTEyMzIw",
          "commit": {
            "abbreviatedOid": "6aeb674"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-07T23:44:10Z",
          "updatedAt": "2021-09-08T00:12:48Z",
          "comments": [
            {
              "originalPosition": 168,
              "body": "> If leader and aggregator disagree on how much privacy was consumed by queries, then they might not bail out in lockstep. But perhaps we can accommodate that thanks to the leader model? The helper could indicate to the leader in `AggregateSubResp` that the privacy budget for a report has been exhausted, at which point the leader can fail the corresponding `CollectResp`. Similarly if the leader decides a report's privacy budget is exhausted, then it can stop executing the aggregate protocol for that report with the helper.\r\n\r\nYeah, this is basically what I mean by \"bail out at the same time\". I.e., an honest aggregator ensures the privacy budget isn't violated, even in the presence of a cheating peer.\r\n\r\n\r\n> However I acknowledge that's more complicated, and it's not worth doing if we strongly believe that the coarse-grained privacy budgeting (i.e. one query always consumes one unit of privacy) will suffice for all or nearly all future schemes.\r\n\r\nI don't think we have enough information to say for sure. I think it suffices to say that `max_report_lifetime` is an upperbound only.\r\n\r\n\r\n> On question 2: now I understand why you used the word lifetime. I still think that's the wrong term, for these reasons:\r\n> \r\n>     * \"lifetime\" implies, well, time, and there are several other places where we discuss spans of time or points in time (e.g., batch window or report timestamp, respectively), so I'd prefer to express this in a way that doesn't imply a relation to other time concepts in the protocol. Plus the unit of this lifetime isn't even measured in seconds but rather a number of queries.\r\n\r\nAgreed here.\r\n\r\n>     * I don't think PPM wants to specify anything about how long reports are retained, beyond the implicit requirement that they be available long enough to service all the collector's queries and a suggestion that implementations could delete them afterward. The observable behavior our protocol cares about is limiting how many queries may be made against a report, and I think message fields should be named accordingly. For that reason, `privacy_budget` makes more sense to me than `max_report_lifetime`.\r\n\r\nHere I disagree. This operational parameter is a kind of contract between collectors and aggregators that stipulates how long data has to be available. Without it, aggregators would have to keep state indefinitely. I suppose we could do this in a different way, but we'll need to have something like this in the spec.\r\n",
              "createdAt": "2021-09-07T23:44:10Z",
              "updatedAt": "2021-09-08T00:12:48Z"
            },
            {
              "originalPosition": 168,
              "body": "I'm going to rename it to `max_batch_lifetime` for now.",
              "createdAt": "2021-09-07T23:44:46Z",
              "updatedAt": "2021-09-08T00:12:48Z"
            },
            {
              "originalPosition": 7,
              "body": "Done.",
              "createdAt": "2021-09-07T23:51:33Z",
              "updatedAt": "2021-09-08T00:12:48Z"
            },
            {
              "originalPosition": 174,
              "body": "Done.",
              "createdAt": "2021-09-07T23:52:47Z",
              "updatedAt": "2021-09-08T00:12:48Z"
            },
            {
              "originalPosition": 274,
              "body": "Removed the line that caused confusion.",
              "createdAt": "2021-09-07T23:55:45Z",
              "updatedAt": "2021-09-08T00:12:48Z"
            },
            {
              "originalPosition": 66,
              "body": "IMO this is covered by https://github.com/abetterinternet/prio-documents/issues/89. Added a reference.",
              "createdAt": "2021-09-07T23:59:44Z",
              "updatedAt": "2021-09-08T00:12:48Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5NjAwNzIw",
          "commit": {
            "abbreviatedOid": "a5d6356"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "A few more nits but basically LGTM once any occurrence of \"batch window\" is corrected.",
          "createdAt": "2021-09-08T20:48:08Z",
          "updatedAt": "2021-09-08T22:13:36Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "```suggestion\r\n: A parameter of the collect or output-share request that specifies the time\r\n  range of the reports in the batch.\r\n```",
              "createdAt": "2021-09-08T20:48:09Z",
              "updatedAt": "2021-09-08T22:13:36Z"
            },
            {
              "originalPosition": 64,
              "body": "```suggestion\r\n  boundaries with which the batch interval of each collect request must be\r\n```\r\nIIUC we should replace occurrences of \"batch window\" with \"batch interval\", right? I flagged a couple other misuses of \"batch window\" in this PR but we* should scrub the whole document for occurrences of that term.\r\n\r\n\\* you",
              "createdAt": "2021-09-08T20:53:52Z",
              "updatedAt": "2021-09-08T22:13:36Z"
            },
            {
              "originalPosition": 92,
              "body": "Not a blocker but maybe we should introduce a type:\r\n\r\n```\r\nstruct {\r\n  Time start;\r\n  Time End;\r\n} Interval;\r\n```\r\nAnd use it in `OutputShareReq` and other places where we represent time intervals.",
              "createdAt": "2021-09-08T20:55:53Z",
              "updatedAt": "2021-09-08T22:13:36Z"
            },
            {
              "originalPosition": 149,
              "body": "```suggestion\r\ninput shares that fall within the batch interval. Finally, it responds with HTTP\r\n```",
              "createdAt": "2021-09-08T20:57:10Z",
              "updatedAt": "2021-09-08T22:13:36Z"
            },
            {
              "originalPosition": 167,
              "body": "```suggestion\r\nthat `batch_end - batch_start >= min_batch_duration`. Unless both these\r\n```",
              "createdAt": "2021-09-08T21:00:13Z",
              "updatedAt": "2021-09-08T22:13:36Z"
            },
            {
              "originalPosition": 168,
              "body": ">Here I disagree. This operational parameter is a kind of contract between collectors and aggregators that stipulates how long data has to be available. Without it, aggregators would have to keep state indefinitely. I suppose we could do this in a different way, but we'll need to have something like this in the spec.\r\n\r\nI'm not arguing for getting rid of this parameter, just for changing its name to better reflect what its units are (a number of queries rather than a duration). I would phrase it as \"once a batch's `privacy_budget` is exhausted, aggregators MUST refuse further queries on the batch and MAY delete the inputs in the batch\" (note that the MUST part is something observable by other parties whereas it's not meaningfully possible for an aggregator to prove to other participants that it has deleted some input).\r\n\r\nBut as I said, we should move forward with `max_batch_lifetime`.",
              "createdAt": "2021-09-08T22:11:25Z",
              "updatedAt": "2021-09-08T22:13:36Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzUwNDA2MzQ5",
          "commit": {
            "abbreviatedOid": "f4168a1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-09T14:11:30Z",
          "updatedAt": "2021-09-09T14:11:30Z",
          "comments": [
            {
              "originalPosition": 92,
              "body": "Done.",
              "createdAt": "2021-09-09T14:11:30Z",
              "updatedAt": "2021-09-09T14:11:30Z"
            }
          ]
        }
      ]
    },
    {
      "number": 142,
      "id": "MDExOlB1bGxSZXF1ZXN0NzIwMDY4Mzky",
      "title": "Harmonize `Encrypted(Input|Output)Share`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/142",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "These two structs are virually identical, so this commit makes their\r\nfield naming consistent.",
      "createdAt": "2021-08-25T23:30:00Z",
      "updatedAt": "2021-12-30T00:53:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "timg/many-helpers",
      "baseRefOid": "b891bdd145fe2c46da04ae356efea79ddbb502cf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/encrypted-share-fields",
      "headRefOid": "cad77b1e46439916a79ef16a7367f594add7726d",
      "closedAt": "2021-08-27T00:19:00Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Stacked on #135, so marking as a draft for now.",
          "createdAt": "2021-08-25T23:30:15Z",
          "updatedAt": "2021-08-25T23:30:15Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 143,
      "id": "MDExOlB1bGxSZXF1ZXN0NzIwMDc1MjI5",
      "title": "remove `struct Param`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/143",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "`Param` is never transmitted over the wire, nor do we otherwise depend\r\non is structure, so this commit removes that struct definition and\r\ninstead enumerates the pre-negotiated task parameters each participant\r\nmust hold, leaving implementations free to store, represent and transmit\r\nthose parameters any way they wish. `TaskId` is still around, but\r\ninstead of being a SHA-256 over the serialiation of Param, it's now just\r\n32 random bytes.\r\n\r\nResolves #104",
      "createdAt": "2021-08-25T23:52:22Z",
      "updatedAt": "2021-12-30T00:53:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "81516b772789719fe3a3281abad7ad6e1edb6ea0",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/no-param",
      "headRefOid": "4e3482b0c88d6e7c1cffb452672c62bae49a7ece",
      "closedAt": "2021-09-15T20:20:51Z",
      "mergedAt": "2021-09-15T20:20:51Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "8609c88d63b6364378377e47e2f25a5db89caf2c"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Mind rebasing, @tgeoghegan?",
          "createdAt": "2021-09-08T00:16:35Z",
          "updatedAt": "2021-09-08T00:16:35Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> This seems fine, although I'm a little concerned that the text is going to get a lot more confusing by dropping named parameters. PR #137 refers to the named parameters in `Param` quite a lot. Could we hold off on merging this until that PR is merged and see what it looks like after?\r\n\r\nSounds reasonable to me. I'll rebase after #137 is merged.",
          "createdAt": "2021-09-08T16:38:04Z",
          "updatedAt": "2021-09-08T16:38:04Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased on main to incorporate #137.",
          "createdAt": "2021-09-09T21:26:22Z",
          "updatedAt": "2021-09-09T21:26:22Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ4NTIzMzI4",
          "commit": {
            "abbreviatedOid": "7e98bfb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-08T00:14:18Z",
          "updatedAt": "2021-09-08T00:25:44Z",
          "comments": [
            {
              "originalPosition": 68,
              "body": "```suggestion\r\nbe set to a random string output by a cryptographically secure pseudorandom\r\n```",
              "createdAt": "2021-09-08T00:14:18Z",
              "updatedAt": "2021-09-08T00:25:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ4NTM5NDM0",
          "commit": {
            "abbreviatedOid": "143cf83"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "This seems fine, although I'm a little concerned that the text is going to get a lot more confusing by dropping named parameters. PR https://github.com/abetterinternet/prio-documents/pull/137 refers to the named parameters in `Param` quite a lot. Could we hold off on merging this until that PR is merged and see what it looks like after?",
          "createdAt": "2021-09-08T01:00:19Z",
          "updatedAt": "2021-09-08T01:00:19Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4tBbkl",
          "commit": {
            "abbreviatedOid": "a90673b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "A few nits, but otherwise I'm good with landing this.",
          "createdAt": "2021-09-15T16:30:53Z",
          "updatedAt": "2021-09-15T16:38:12Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "```suggestion\r\nPrior to the start of the protocol, each participant in the protocol must agree on\r\n```",
              "createdAt": "2021-09-15T16:30:54Z",
              "updatedAt": "2021-09-15T16:38:12Z"
            },
            {
              "originalPosition": 103,
              "body": "These requirements make me think we should define `struct` for the list of aggregators that defines the order and which endpoint is the leader.",
              "createdAt": "2021-09-15T16:32:47Z",
              "updatedAt": "2021-09-15T16:38:12Z"
            },
            {
              "originalPosition": 120,
              "body": "```suggestion\r\n* protocol: The core PPM protocol, e.g., Prio or Hits.\r\n```",
              "createdAt": "2021-09-15T16:34:28Z",
              "updatedAt": "2021-09-15T16:38:12Z"
            },
            {
              "originalPosition": 100,
              "body": "Bullets describing parameters with names used elsewhere in the doc should begin with the name. E.g.,:\r\n> * `max_batch_lifetime` is the ...",
              "createdAt": "2021-09-15T16:35:15Z",
              "updatedAt": "2021-09-15T16:38:12Z"
            },
            {
              "originalPosition": 181,
              "body": "```suggestion\r\n                          \"pda input share\" || task_id || server_role)\r\n```\r\nIf this is too long, consider reducing the size of the tab.",
              "createdAt": "2021-09-15T16:36:34Z",
              "updatedAt": "2021-09-15T16:38:12Z"
            },
            {
              "originalPosition": 120,
              "body": "This is a \"named parameter\".",
              "createdAt": "2021-09-15T16:37:43Z",
              "updatedAt": "2021-09-15T16:38:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tCSit",
          "commit": {
            "abbreviatedOid": "a90673b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-15T20:10:15Z",
          "updatedAt": "2021-09-15T20:10:16Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "I think you're right, but I think that will come with #138 when we formalize the notion of aggregator roles. Once we have that, I think this language and the discussion of `Report` gets much simpler.",
              "createdAt": "2021-09-15T20:10:16Z",
              "updatedAt": "2021-09-15T20:10:16Z"
            }
          ]
        }
      ]
    },
    {
      "number": 145,
      "id": "MDExOlB1bGxSZXF1ZXN0NzIxNjE2OTEx",
      "title": "Charter and links",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/145",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-08-27T21:12:50Z",
      "updatedAt": "2021-08-27T21:14:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "560afbee40ac7d13154d2ea536273e28ea319e7e",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "bof-request2",
      "headRefOid": "fc407c624c75fe5ec7247779645d32d90defddd2",
      "closedAt": "2021-08-27T21:14:44Z",
      "mergedAt": "2021-08-27T21:14:44Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "6b91cb28c9713a41d65f858cbab3a1110cd406eb"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQwODk4MDI5",
          "commit": {
            "abbreviatedOid": "fc407c6"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-08-27T21:14:37Z",
          "updatedAt": "2021-08-27T21:14:37Z",
          "comments": []
        }
      ]
    },
    {
      "number": 147,
      "id": "MDExOlB1bGxSZXF1ZXN0NzI0NzE0Mjcz",
      "title": "Add share-counting attacks to threat model",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/147",
      "state": "MERGED",
      "author": "csharrison",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Aggregators merely counting the number of input shares could compromise\r\nuser privacy in some circumstances (namely, it can break differential privacy\r\nif the number of records per user is not constant).\r\n\r\nThis attack only applies to aggregators who can count the number of shares,\r\nso any protection still holds on the collector side.\r\n\r\nTo address this attack, there are two possible mitigations, which both involve\r\nadding noise to the total number of input shares. This should be possible to\r\ndo with \"null\" shares that don't affect aggregate output (i.e. inserting 0 into\r\nsums).",
      "createdAt": "2021-09-01T17:34:47Z",
      "updatedAt": "2021-09-10T17:52:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "14625bf507ab723949271d78f4667d74aedfaaa7",
      "headRepository": "csharrison/prio-documents",
      "headRefName": "counting-attack",
      "headRefOid": "c8d1691b60aeff426371cf33904fc061cdcd6e66",
      "closedAt": "2021-09-10T17:51:52Z",
      "mergedAt": "2021-09-10T17:51:52Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "81516b772789719fe3a3281abad7ad6e1edb6ea0"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Specifically, this is about being able to count the number of contributions from an individual client, right? If so, this would also be mitigated by putting an anonymizing proxy between clients and the leader which would scrub identifying information from reports.",
          "createdAt": "2021-09-01T17:42:42Z",
          "updatedAt": "2021-09-01T17:42:42Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "An anonymizing proxy mitigates attacks but not in the worst case. I didn't mention in the PR (I could if it is necessary), but the worst case attack involves the attackers having some side knowledge (e.g. via client collusion) such that if N reports arrive, then Tim did not contribute to this batch, and if N+1 reports arrive, then Tim did contribute to the batch. This is related to the Sybil style attacks which do the same thing but with the value embedded in the report. \r\n\r\nIn these kinds of attacks, no auxiliary information in the report (like IP, etc) is needed.\r\n\r\nIn practice, I don't believe this attack does much to meaningfully reduce user privacy. However, I think calling it out is important if we want to make differential privacy claims since DP requires us to think through worst-case attacks.",
          "createdAt": "2021-09-01T17:50:34Z",
          "updatedAt": "2021-09-01T17:50:34Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Can you say more about what adding \"null\" shares does for DP? As a working example, suppose we're computing the sum of the inputs `S = X[1] + ... + X[m]`, where `X[i]` is an element of a finite field and `m` is the number of real inputs.\r\n> \r\n> It sounds like the scheme is to estimate `S` as\r\n> \r\n> ```\r\n>   S' = X[1] + ... + X[m] + // Real inputs\r\n>        e[1] + ... + e[m] + // error term for each real input\r\n>        e[m+1] ... + e[n]   // error term for each \"null\" input \r\n> ```\r\n> \r\n> where `n` is the number of inputs, including the \"null\" inputs generated by the clients (resp. the leader). So we are including an error term for each real input and generating an error team for each \"null\" (i.e., `0`-valued) input. Is that the idea?\r\n\r\nNo this is not what I am talking about. This issue is not necessarily about attacks on `S`, but rather about attacks on revealing `m` to the helpers. In some settings, `m` could be just as sensitive as `S`. In your example you are using some local noise for each input (just one possible way of implementing DP). I don't think we would need this to protect `S`. However, to protect `m` we would only need to add false shares that truly sum to 0 (i.e. no noise embedded in them at all). So that `m` is hard to learn from `n` the total # of input records.\r\n\r\n> > [T]he worst case attack involves the attackers having some side knowledge (e.g. via client collusion) such that if N reports arrive, then Tim did not contribute to this batch, and if N+1 reports arrive, then Tim did contribute to the batch. This is related to the Sybil style attacks which do the same thing but with the value embedded in the report.\r\n> \r\n> Suppose I have some a priori knowledge about the distribution of the inputs `X[i]`. (Maybe I have corrupted some small subset of the clients, and I know that each input is independently distributed.) Can't I make a pretty good guess about the value of `m` from `S'`? In fact, if i know that the expected value of `X[i]` is 1, and each `X[i]` is independent, then `S'` is going to be close to `m`.\r\n\r\nGreat question. Let's simplify this and say that you know for a fact that the true dataset contains either:\r\n- `m = N` records summing to `S = N`, OR\r\n- `m = N+1` records summing to `S = N+1`\r\ni.e. each record will always just have value 1. Also let's say that we've properly hidden `m` directly, and the adversary wants to learn `m` from `S`. I think it should be clear that if we can protect the true value of `S`, we can protect the true value of `m` as they are the same.\r\n\r\nTrue, `S` will be close to N or N+1, but the noise added to the sum will be enough that both cases are reasonably probable, protecting both `S` and `m`. For example (with Laplace noise with epsilon ln2):\r\n- `P(output >= N+1 | S = N) = 25%` ([link](https://www.wolframalpha.com/input/?i=LaplaceDistribution%5B0%2C+1%2FLog%5B2%5D%5D+%3E+1))\r\n- `P(output >= N+1 | S = N+1) = 50%`",
          "createdAt": "2021-09-08T16:08:28Z",
          "updatedAt": "2021-09-08T16:28:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> No this is not what I am talking about. This issue is not necessarily about attacks on `S`, but rather about attacks on revealing `m` to the helpers. In some settings, `m` could be just as sensitive as `S`. In your example you are using some local noise for each input (just one possible way of implementing DP). I don't think we would need this to protect `S`. However, to protect `m` we would only need to add false shares that truly sum to 0 (i.e. no noise embedded in them at all). So that `m` is hard to learn from `n` the total # of input records.\r\n\r\nSharpening this a bit more: The concern is not whether the collector learns `m` but whether the aggregators learn `m`. The clients can add `n-m` bogus reports to hide `m` from the aggregators, or the leader can add `n-m` bogus reports to hide `m` from the helper. Is that right? \r\n\r\nThis would be a tad inconsistent with the current threat model, which assumes the collector may collude with an aggregator and, in particular, provide it with the final output. That said, I'm fine with adding it as a security consideration. We just want to make it clear that it only appears to be achievable under stronger non-collusion assumptions.\r\n\r\n\r\n> > Suppose I have some a priori knowledge about the distribution of the inputs `X[i]`. (Maybe I have corrupted some small subset of the clients, and I know that each input is independently distributed.) Can't I make a pretty good guess about the value of `m` from `S'`? In fact, if i know that the expected value of `X[i]` is 1, and each `X[i]` is independent, then `S'` is going to be close to `m`.\r\n> \r\n> Great question. Let's simplify this and say that you know for a fact that the true dataset contains either:\r\n> \r\n>     * `m = N` records summing to `S = N`, OR\r\n> \r\n>     * `m = N+1` records summing to `S = N+1`\r\n>       i.e. each record will always just have value 1. Also let's say that we've properly hidden `m` directly, and the adversary wants to learn `m` from `S`. I think it should be clear that if we can protect the true value of `S`, we can protect the true value of `m` as they are the same.\r\n> \r\n> True, `S` will be close to N or N+1, but the noise added to the sum will be enough that both cases are reasonably probable, protecting both `S` and `m`. For example (with Laplace noise with epsilon ln2):\r\n> \r\n>     * `P(output >= N+1 | S = N) = 25%`\r\n> \r\n>     * `P(output >= N+1 | S = N+1) = 50%`\r\n\r\nI see, thanks for clarifying. I think we need to be careful about what we mean when we say that the number of real (i.e., non-bogus) inputs is \"protected\". The output of the distributed aggregation function may be differentially private, but it doesn't hide the number of real inputs.",
          "createdAt": "2021-09-08T16:43:12Z",
          "updatedAt": "2021-09-08T16:43:12Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> > No this is not what I am talking about. This issue is not necessarily about attacks on `S`, but rather about attacks on revealing `m` to the helpers. In some settings, `m` could be just as sensitive as `S`. In your example you are using some local noise for each input (just one possible way of implementing DP). I don't think we would need this to protect `S`. However, to protect `m` we would only need to add false shares that truly sum to 0 (i.e. no noise embedded in them at all). So that `m` is hard to learn from `n` the total # of input records.\r\n> \r\n> Sharpening this a bit more: The concern is not whether the collector learns `m` but whether the aggregators learn `m`. The clients can add `n-m` bogus reports to hide `m` from the aggregators, or the leader can add `n-m` bogus reports to hide `m` from the helper. Is that right?\r\n\r\nYes that's right\r\n\r\n> This would be a tad inconsistent with the current threat model, which assumes the collector may collude with an aggregator and, in particular, provide it with the final output. That said, I'm fine with adding it as a security consideration. We just want to make it clear that it only appears to be achievable under stronger non-collusion assumptions.\r\n\r\nThe collector colluding with an aggregator and providing the final output should be OK (similarly, the aggregators providing the noisy count of raw records to the collector). In this case, the adversary has 2 views of the data (noisy count and noisy output). You should still be able to prove DP protections in this case if each view is protected with DP (this comes from the fact that DP composes well: https://en.wikipedia.org/wiki/Differential_privacy#Composability).\r\n\r\n> I see, thanks for clarifying. I think we need to be careful about what we mean when we say that the number of real (i.e., non-bogus) inputs is \"protected\". The output of the distributed aggregation function may be differentially private, but it doesn't hide the number of real inputs.\r\n\r\nYes, this is exactly the purpose of this PR. By default, we can't protect `m` from the aggregators unless we add the bogus records. However, if we can directly hide the number of real records (by adding bogus ones), _and also_ ensure `S` is DP, we can hide the true values of both `S` and `m`.",
          "createdAt": "2021-09-08T16:54:24Z",
          "updatedAt": "2021-09-08T16:54:24Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton take another look. I've tried to keep this short + sweet but let me know if you think it can be compressed further.",
          "createdAt": "2021-09-08T18:17:21Z",
          "updatedAt": "2021-09-08T18:17:21Z"
        },
        {
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This should be ready to go",
          "createdAt": "2021-09-10T17:42:16Z",
          "updatedAt": "2021-09-10T17:42:16Z"
        }
      ],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ4NTMwMzA5",
          "commit": {
            "abbreviatedOid": "b187505"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Can you say more about what adding \"null\" shares does for DP? As a working example, suppose we're computing the sum of the inputs `S = X[1] + ... + X[m]`, where `X[i]` is an element of a finite field and `m` is the number of real inputs.\r\n\r\nIt sounds like the scheme is to estimate `S` as\r\n```\r\n  S' = X[1] + ... + X[m] + // Real inputs\r\n       e[1] + ... + e[m] + // error term for each real input\r\n       e[m+1] ... + e[n]   // error term for each \"null\" input \r\n```\r\nwhere `n` is the number of inputs, including the \"null\" inputs generated by the clients (resp. the leader). So we are including an error term for each real input and generating an error team for each \"null\" (i.e., `0`-valued) input. Is that the idea?\r\n\r\n> [T]he worst case attack involves the attackers having some side knowledge (e.g. via client collusion) such that if N reports arrive, then Tim did not contribute to this batch, and if N+1 reports arrive, then Tim did contribute to the batch. This is related to the Sybil style attacks which do the same thing but with the value embedded in the report.\r\n\r\nSuppose I have some a priori knowledge about the distribution of the inputs `X[i]`. (Maybe I have corrupted some small subset of the clients, and I know that each input is independently distributed.) Can't I make a pretty good guess about the value of `m` from `S'`? In fact, if i know that the expected value of `X[i]` is 1, and each `X[i]` is independent, then `S'` is going to be close to `m`.\r\n",
          "createdAt": "2021-09-08T00:33:22Z",
          "updatedAt": "2021-09-08T00:55:53Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "- The idea is pretty clear if you're as deep into this as we are, but I think someone new would have a hard time figuring out what a `bogus \"null\" share` is. We might stipulate that the core PPM protocol define this. If you'd like we can leave it as an open issue for #98.\r\n- It sounds like what you want is that the clients or servers contribute \"null\" shares + some noise for DP. If this is correct, then let's say it.\r\n- Instead of `inject bogus \"null\" shares` let's say \"generate bogus reports that encode \"null\" shares`, as this is a bit more precise.",
              "createdAt": "2021-09-08T00:33:22Z",
              "updatedAt": "2021-09-08T00:55:53Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5MzUxMTk4",
          "commit": {
            "abbreviatedOid": "b187505"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-08T16:12:31Z",
          "updatedAt": "2021-09-08T16:12:32Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "> It sounds like what you want is that the clients or servers contribute \"null\" shares + some noise for DP. If this is correct, then let's say it.\r\n\r\nNo, the null shares should be effectively \"no-ops\", and not introduce any change in the visible output. They only serve to protect the helpers from learning the true count.\r\n\r\nYour other suggestions sound good, I can make an update",
              "createdAt": "2021-09-08T16:12:32Z",
              "updatedAt": "2021-09-08T16:12:32Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5NDg4MTE3",
          "commit": {
            "abbreviatedOid": "0509aa2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Nice! Just a nit to add a reference to https://github.com/abetterinternet/prio-documents/issues/98.",
          "createdAt": "2021-09-08T18:35:13Z",
          "updatedAt": "2021-09-08T18:35:44Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "```suggestion\r\ninserting null shares into an aggregation is effectively a no-op. See issue#98.]\r\n```",
              "createdAt": "2021-09-08T18:35:13Z",
              "updatedAt": "2021-09-08T18:35:44Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5NjA2MjAy",
          "commit": {
            "abbreviatedOid": "d7c0617"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-09-08T20:54:45Z",
          "updatedAt": "2021-09-08T20:54:45Z",
          "comments": []
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5NjkyNjkw",
          "commit": {
            "abbreviatedOid": "d7c0617"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Left a couple of suggestions that I don't think should block merging. @csharrison let us know when you've rebased or otherwise finished with this and we can merge it for you.",
          "createdAt": "2021-09-08T23:06:46Z",
          "updatedAt": "2021-09-08T23:11:55Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "```suggestion\r\n       indistinguishable from true inputs (metadata, etc), especially when\r\n       constructing timestamps on reports.\r\n```\r\nThe size will necessarily be the same as a true input as we discussed earlier.",
              "createdAt": "2021-09-08T23:06:46Z",
              "updatedAt": "2021-09-08T23:11:55Z"
            },
            {
              "originalPosition": 15,
              "body": "```suggestion\r\n   input independently of user behavior. For example, a client should periodically\r\n   upload a report even if the event that the task is tracking has not occurred, so\r\n   that the absence of reports cannot be distinguished from their presence.\r\n```",
              "createdAt": "2021-09-08T23:10:47Z",
              "updatedAt": "2021-09-08T23:11:55Z"
            }
          ]
        },
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5Nzc4MjY4",
          "commit": {
            "abbreviatedOid": "d7c0617"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-09T02:49:05Z",
          "updatedAt": "2021-09-09T02:49:06Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "Yes that's right",
              "createdAt": "2021-09-09T02:49:06Z",
              "updatedAt": "2021-09-09T02:49:06Z"
            }
          ]
        }
      ]
    },
    {
      "number": 149,
      "id": "MDExOlB1bGxSZXF1ZXN0NzMwODgzMDg5",
      "title": "update references to \"prio-documents\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/149",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The repo was renamed to \"ppm-specification\". This commit replaces\r\nvarious occurences of the old repo's name, and also updated README.md to\r\nreflect the current name of the protocol.",
      "createdAt": "2021-09-09T19:47:36Z",
      "updatedAt": "2021-12-30T00:53:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "fc7dfe824f1586d76bde80aea403c98615a5ce92",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/rename-repo",
      "headRefOid": "58d5e3a65c5435b8ad24c7bc5208325710fb1096",
      "closedAt": "2021-09-09T19:51:03Z",
      "mergedAt": "2021-09-09T19:51:03Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "7a4f38b0984cf67a88e2ff9c0051d2bab3f79d92"
      },
      "comments": [],
      "reviews": [
        {
          "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzUwNzU3ODA0",
          "commit": {
            "abbreviatedOid": "58d5e3a"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-09-09T19:48:23Z",
          "updatedAt": "2021-09-09T19:48:23Z",
          "comments": []
        }
      ]
    },
    {
      "number": 151,
      "id": "PR_kwDOFEJYQs4sFmoA",
      "title": "House cleaning",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/151",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Since there's no PR pending, I thought it would be a good time to fix up the doc a bit. No spec or even editorial changes, just some line breaks and a few typos.",
      "createdAt": "2021-09-21T22:04:31Z",
      "updatedAt": "2021-12-30T02:10:21Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8609c88d63b6364378377e47e2f25a5db89caf2c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/editorial",
      "headRefOid": "9b061ace4aaecf63d589b1a13655d44d4835cb32",
      "closedAt": "2021-09-22T19:49:48Z",
      "mergedAt": "2021-09-22T19:49:48Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "7cf4db3f2ec7605a87ef79a52611eb544b0b0f33"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm assuming everyone trusts that this makes no substantive changes. time to merge.",
          "createdAt": "2021-09-22T19:49:45Z",
          "updatedAt": "2021-09-22T19:49:45Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 152,
      "id": "PR_kwDOFEJYQs4sKJKU",
      "title": "Specify the protocol in terms of VDAF evaluation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/152",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #98.\r\n\r\nAlong the way, this commit cleans up some ambiguous language that allows\r\nfor multiple helpers, but without fully specifying the protocol flow.",
      "createdAt": "2021-09-22T20:49:07Z",
      "updatedAt": "2021-09-28T18:20:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7cf4db3f2ec7605a87ef79a52611eb544b0b0f33",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/vdaf",
      "headRefOid": "69c7d0d9aea2b5741a7847ef36a673c636f15b24",
      "closedAt": "2021-09-24T00:13:27Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Hi @tgeoghegan, @ekr, thanks for the super useful feedback! Based on your feedback, and in light of a super helpful conversation with @chris-wood today, I'm going to close this PR and open a new one after some major revisions. Specifically, @chris-wood came up with some changes to the VDAF syntax that will make the PPM spec a lot cleaner. I'll get cracking on this next Tuesday.",
          "createdAt": "2021-09-24T00:13:27Z",
          "updatedAt": "2021-09-24T00:13:27Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4tYmNP",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I'm pleased to see the `protocol` named parameter go away as that simplifies several protocol messages.",
          "createdAt": "2021-09-22T21:26:21Z",
          "updatedAt": "2021-09-22T21:52:41Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "Is this an appropriate place to discuss the PPM meta-protocol that would capture Prio's query randomness exchange, or is that a later PR?",
              "createdAt": "2021-09-22T21:26:21Z",
              "updatedAt": "2021-09-22T21:52:41Z"
            },
            {
              "originalPosition": 146,
              "body": "```suggestion\r\naggregate requests to the helper, the first of which contains the helper's\r\n```",
              "createdAt": "2021-09-22T21:28:43Z",
              "updatedAt": "2021-09-22T21:52:41Z"
            },
            {
              "originalPosition": 394,
              "body": "I think this only makes sense if changed to:\r\n```suggestion\r\nto compute their share of the output. Alternately, if `output_param` is empty or\r\n```\r\nAlso, what should the collector put in `output_param` if it is \"empty or a well-known value\"? Empty byte sequence?",
              "createdAt": "2021-09-22T21:38:09Z",
              "updatedAt": "2021-09-22T21:52:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tYwm6",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I think it would be helpful to separate out the change to talk about VBAF from the other technical changes you have made here.",
          "createdAt": "2021-09-22T22:35:23Z",
          "updatedAt": "2021-09-22T22:44:41Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "Can we instead make this the index in the array?",
              "createdAt": "2021-09-22T22:35:23Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 99,
              "body": "This kind of comes out of nowhere. I think you need a section describing the functions in the VDAF in overview.",
              "createdAt": "2021-09-22T22:35:53Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 146,
              "body": "```suggestion\r\naggregate requests to the helper, the first of which contains the helper's\r\n```",
              "createdAt": "2021-09-22T22:36:07Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 146,
              "body": "Why are you writing this as if it were one helper? Let's write all this text as if it were many and then have a single place where we say it's not.",
              "createdAt": "2021-09-22T22:36:47Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 155,
              "body": "This seems like an unnecessary assumption.",
              "createdAt": "2021-09-22T22:37:10Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 149,
              "body": "I think this clearly reveals that these subsequent ones just shouldn't be aggregate requests but something else.\r\n\r\nAggregateContinue perhaps.",
              "createdAt": "2021-09-22T22:38:38Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 158,
              "body": "Is this the first time a \"collect\" request has been mentioned.",
              "createdAt": "2021-09-22T22:39:03Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 157,
              "body": "I would not call this \"output\" but rather \"state\"",
              "createdAt": "2021-09-22T22:39:19Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 178,
              "body": "Why are we letting this span multiple batches? That seems weird.",
              "createdAt": "2021-09-22T22:39:57Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 185,
              "body": "See above about cardinality.",
              "createdAt": "2021-09-22T22:40:14Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 199,
              "body": "See above. I think these should just be different messages.",
              "createdAt": "2021-09-22T22:40:32Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 240,
              "body": "I don't understand \"each sub-request for a given report\". Is there some other structure?\r\n\r\n\"Each subrequest corresponds to a single report\"",
              "createdAt": "2021-09-22T22:42:39Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            },
            {
              "originalPosition": 244,
              "body": "This is not the first sub-request in the list but rather the first time you send one.",
              "createdAt": "2021-09-22T22:43:06Z",
              "updatedAt": "2021-09-22T22:44:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tY3JF",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-22T23:34:05Z",
          "updatedAt": "2021-09-22T23:34:06Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": "I agree with Chris here. I think there's no reason to require leaders to construct `AggregateReq`s around batch boundaries, and doing it this way enables really simple leader implementations that send an `AggregateReq` to helper every time some fixed number of reports is received.\r\n\r\nA helper will consider all the `AggregateSubReq`s independently, and it's easy for it to decide what batch a particular subreq belongs to (it just has to truncate the subreq timestamp to the batch interval) so there's no need for all the subreqs in a req to belong to the same batch.",
              "createdAt": "2021-09-22T23:34:05Z",
              "updatedAt": "2021-09-22T23:34:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tY48I",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-22T23:53:08Z",
          "updatedAt": "2021-09-22T23:53:08Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": "I don't understand the alleged semantics of this. Suppose that I send reqs from batch 1 and batch 2, what does the helper do?\r\n",
              "createdAt": "2021-09-22T23:53:08Z",
              "updatedAt": "2021-09-22T23:53:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tY7y8",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-23T00:24:48Z",
          "updatedAt": "2021-09-23T00:24:48Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": "Let's say it's Prio, in which scenario the aggregators can sum inputs as soon as the proofs check out without waiting for the collect protocol to begin. Further let's say an aggregator sends an `AggregateReq` that contains `AggregateSubReq`s whose timestamps span multiple batches.\r\n\r\nA helper implementation would have a map where the keys are batch intervals and the values are the accumulated value for that batch window. Upon receipt of an `AggregateReq`, the helper iterates over the `AggregateSubReq`s and for each:\r\n\r\n- decrypts the input share and proof share;\r\n- verifies the proof (it has its own proof share and got leader's proof share in the `AggregateSubReq`);\r\n- truncates the report timestamp to the batch interval (e.g. with [`chrono::DurationRound::duration_trunc`](https://docs.rs/chrono/0.4.19/chrono/trait.DurationRound.html#tymethod.duration_trunc) or [`time.Time.Truncate`](https://pkg.go.dev/time#Time.Truncate)) to figure out what batch the input belongs to;\r\n- sums the input share into  the accumulated value for the batch interval from the map described earlier.\r\n\r\nLater, a collector issues a `CollectReq` to the leader which specifies some `batch_interval`, and the leader sends a corresponding `OutputShareReq` to helper. Helper just looks up the `batch_interval` in its map of accumulators and responds with the value.\r\n\r\nEnabling this kind of eager aggregation is a design goal, and I don't see that it requires that `AggregateReq`s be constructed on batch interval boundaries.",
              "createdAt": "2021-09-23T00:24:48Z",
              "updatedAt": "2021-09-23T00:24:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tY8KD",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-23T00:28:51Z",
          "updatedAt": "2021-09-23T00:28:51Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": "This is going to interact very badly with any kind of state offloading because you will end up with the state token containing multiple batches and it will be very unclear when you can abandon it or do anything in parallel.\r\n\r\nMore apropos to the current moment, the current text has an open issue here and this PR just decides that in the middle of making some other totally orthogonal change. If people want there to be an open issue where we discuss this design issue, then fine, but it doesn't belong in this PR.\r\n\r\n\r\n",
              "createdAt": "2021-09-23T00:28:51Z",
              "updatedAt": "2021-09-23T00:28:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tY9Kj",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-23T00:42:25Z",
          "updatedAt": "2021-09-23T00:42:26Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": ">This is going to interact very badly with any kind of state offloading because you will end up with the state token containing multiple batches and it will be very unclear when you can abandon it or do anything in parallel.\r\n\r\nYes, I encountered this when implementing a prototype of this and tried to capture these questions in #150. My conclusion is that the opaque `helper_state` blob has to be per-task, and we need the PPM protocol to make that explicit so that the leader knows which state blob to send to which helper in which aggregate request.\r\n\r\nYour point about parallelism is a really good one, though. My 2c is that we should put `OPEN ISSUE: And the same batch, right?` back and hash this question out in #150 + a later PR.",
              "createdAt": "2021-09-23T00:42:25Z",
              "updatedAt": "2021-09-23T00:42:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpSN6",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:06:23Z",
          "updatedAt": "2021-09-28T18:06:23Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "This comment is solved by having the outputs of the VDAF setup algorithm be part of the task parameters.",
              "createdAt": "2021-09-28T18:06:23Z",
              "updatedAt": "2021-09-28T18:06:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpSv9",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:08:43Z",
          "updatedAt": "2021-09-28T18:08:43Z",
          "comments": [
            {
              "originalPosition": 155,
              "body": "It's not an assumption. This follows from the VDAF definition and how its execution for two aggregators is specified here.",
              "createdAt": "2021-09-28T18:08:43Z",
              "updatedAt": "2021-09-28T18:08:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpTDd",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:10:05Z",
          "updatedAt": "2021-09-28T18:10:05Z",
          "comments": [
            {
              "originalPosition": 178,
              "body": "I'm reverting this change and we'll punt.",
              "createdAt": "2021-09-28T18:10:05Z",
              "updatedAt": "2021-09-28T18:10:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpTK6",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:10:35Z",
          "updatedAt": "2021-09-28T18:10:35Z",
          "comments": [
            {
              "originalPosition": 155,
              "body": "\"Assumption\" in this case means \"that's how you've defined VDAF\". But suppose I define a VDAF' which is like VDAF but has an indeterminate number of rounds.\r\n\r\n",
              "createdAt": "2021-09-28T18:10:35Z",
              "updatedAt": "2021-09-28T18:10:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpUdw",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:16:18Z",
          "updatedAt": "2021-09-28T18:16:18Z",
          "comments": [
            {
              "originalPosition": 149,
              "body": "This will be a somewhat major refactor, so I'll wait to address until the next PR.",
              "createdAt": "2021-09-28T18:16:18Z",
              "updatedAt": "2021-09-28T18:16:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpUqJ",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:17:09Z",
          "updatedAt": "2021-09-28T18:17:09Z",
          "comments": [
            {
              "originalPosition": 158,
              "body": "No, it's mentioned above. I think it should already be mentioned in the overview section.",
              "createdAt": "2021-09-28T18:17:09Z",
              "updatedAt": "2021-09-28T18:17:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpUxa",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:17:37Z",
          "updatedAt": "2021-09-28T18:17:37Z",
          "comments": [
            {
              "originalPosition": 157,
              "body": "\"Output parameter\" is the same term used in the VDAF syntax. \"State\" refers to the aggregator's state.",
              "createdAt": "2021-09-28T18:17:37Z",
              "updatedAt": "2021-09-28T18:17:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpVDs",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:18:38Z",
          "updatedAt": "2021-09-28T18:18:38Z",
          "comments": [
            {
              "originalPosition": 240,
              "body": "Good catch. Should be \"Each sub-request includes the report's ...\"",
              "createdAt": "2021-09-28T18:18:38Z",
              "updatedAt": "2021-09-28T18:18:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpVO4",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:19:25Z",
          "updatedAt": "2021-09-28T18:19:25Z",
          "comments": [
            {
              "originalPosition": 244,
              "body": "I think this might be fixed by splitting up the first aggregate request from the others, as you suggested above.",
              "createdAt": "2021-09-28T18:19:25Z",
              "updatedAt": "2021-09-28T18:19:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpVYl",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:20:06Z",
          "updatedAt": "2021-09-28T18:20:07Z",
          "comments": [
            {
              "originalPosition": 157,
              "body": "I don't really think you can refer to whatever choices you happen to have made in the VDAF draft -- which has even less consensus than this document -- as authoritative for this kind of issue. My point is that this is misleading nomenclature and we should change it both here and in that document. It would be a different situation of the VDAF document were an RFC and so we were trying to conform to that, but it's not.",
              "createdAt": "2021-09-28T18:20:07Z",
              "updatedAt": "2021-09-28T18:20:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tpVhz",
          "commit": {
            "abbreviatedOid": "69c7d0d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-28T18:20:42Z",
          "updatedAt": "2021-09-28T18:20:42Z",
          "comments": [
            {
              "originalPosition": 146,
              "body": "Punting this discussion to the new PR.",
              "createdAt": "2021-09-28T18:20:42Z",
              "updatedAt": "2021-09-28T18:20:42Z"
            }
          ]
        }
      ]
    },
    {
      "number": 154,
      "id": "PR_kwDOFEJYQs4saOfn",
      "title": "Specify the protocol in terms of VDAF evaluation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/154",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #98.\r\n\r\n",
      "createdAt": "2021-09-28T18:39:19Z",
      "updatedAt": "2021-12-30T02:09:26Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2112f391f64b13f209b26cdd8721f05d070c2d28",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/vdaf-2",
      "headRefOid": "04bb97cd3aa9cf450a21caa7278af48b106366f0",
      "closedAt": "2021-12-09T15:24:21Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan note that I'm closing this in favor of #168. Let me know if you'd like me to recreate it so that we have something to reference for abetterinternet/ppm-prototype.",
          "createdAt": "2021-12-09T15:24:21Z",
          "updatedAt": "2021-12-09T15:24:21Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4tut85",
          "commit": {
            "abbreviatedOid": "cad4c63"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "First bunch of comments. I want to (re-)read https://github.com/cjpatton/ppm and then possibly comment some more!",
          "createdAt": "2021-09-29T21:51:29Z",
          "updatedAt": "2021-09-29T22:33:22Z",
          "comments": [
            {
              "originalPosition": 137,
              "body": "The remainder of the text uses 0x00 for leader and 0x01 for helper, which I think is a good change because it aligns the server role values with the position at which each server's share appears in `Report.encrypted_input_shares`.",
              "createdAt": "2021-09-29T21:56:30Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 192,
              "body": "```suggestion\r\nrequests to the helper, the first of which contains the helper's encrypted input\r\n```",
              "createdAt": "2021-09-29T21:57:28Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 263,
              "body": "```suggestion\r\nThe *verify-start request* is used by the leader to send a set of\r\n```",
              "createdAt": "2021-09-29T21:59:30Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 265,
              "body": "A link to #150 would be appropriate in the `[[OPEN ISSUE]]`",
              "createdAt": "2021-09-29T22:00:11Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 280,
              "body": "Are there any risks with letting the leader insert arbitrary `output_param` into requests sent to helper? In the Priov3 case, it seems this should always be empty or absent (because the output param is already known to all the parties participating in a task).\r\n\r\nIn the Hits case, though, a leader could tamper with the string prefixes in the collector's queries. This might be beyond the scope of this particular PR, but maybe the helper and collector should establish a mutually authenticated channel using each other's HPKE configs to ensure leader can't tamper with the output param.",
              "createdAt": "2021-09-29T22:06:19Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 320,
              "body": "```suggestion\r\nwith `leader_share.aggregator_config_id`. Next, it runs\r\n```",
              "createdAt": "2021-09-29T22:08:15Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 489,
              "body": "```suggestion\r\noutput_share = vdaf_finish(state, inbound_message)\r\n```\r\n",
              "createdAt": "2021-09-29T22:13:02Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 484,
              "body": "The helper has nowhere but the `helper_state` blob to keep track of how many rounds have been executed. How does it know whether to execute `vdaf_next` or `vdaf_finish` for any `VerifyNextSubReq`? Perhaps we are missing something like `vdaf_is_finished(state) -> int` (but with a good name instead of that bad one)?",
              "createdAt": "2021-09-29T22:20:16Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 599,
              "body": "```suggestion\r\nDepending on the VDAF and how the leader is configured, the collect request may\r\n```",
              "createdAt": "2021-09-29T22:23:48Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            },
            {
              "originalPosition": 580,
              "body": "What should a collector put here in the Prio case? Empty string?",
              "createdAt": "2021-09-29T22:27:12Z",
              "updatedAt": "2021-09-29T22:33:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txUxD",
          "commit": {
            "abbreviatedOid": "cad4c63"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:00:41Z",
          "updatedAt": "2021-09-30T14:00:41Z",
          "comments": [
            {
              "originalPosition": 137,
              "body": "Good catch.",
              "createdAt": "2021-09-30T14:00:41Z",
              "updatedAt": "2021-09-30T14:00:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txVlm",
          "commit": {
            "abbreviatedOid": "cad4c63"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:03:09Z",
          "updatedAt": "2021-09-30T14:12:26Z",
          "comments": [
            {
              "originalPosition": 580,
              "body": "Yes.",
              "createdAt": "2021-09-30T14:03:09Z",
              "updatedAt": "2021-09-30T14:12:26Z"
            },
            {
              "originalPosition": 484,
              "body": "The VDAF fixes the number of rounds. The first round uses vdaf_start and subsequent rounds use vdaf_next.",
              "createdAt": "2021-09-30T14:04:34Z",
              "updatedAt": "2021-09-30T14:12:26Z"
            },
            {
              "originalPosition": 280,
              "body": "In terms of soundness, there is a risk of a network attacker tampering with this. For that we'll need some sort of sender authentication in the leader<->helper channel. (We've discussed this type of threat before, but I'm not sure we have a ticket for it.)\r\n\r\nIn terms of privacy, mutual authentication doesn't buy us anything because the leader can still cheat. That said, privacy ought to hold here regardless of what the cheating leader does (so long as the helper itself is honest). Of course, we need to prove this for every VDAF we want to implement. (For Prio this is trivial since the only valid output parameter is \"\".)",
              "createdAt": "2021-09-30T14:11:15Z",
              "updatedAt": "2021-09-30T14:12:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txjj-",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:45:19Z",
          "updatedAt": "2021-09-30T14:45:20Z",
          "comments": [
            {
              "originalPosition": 280,
              "body": "I wrote this up in #155. I think we should just put a TODO or a NOTE here referencing this issue and move on with this PR.",
              "createdAt": "2021-09-30T14:45:19Z",
              "updatedAt": "2021-09-30T14:45:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txlkS",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:51:26Z",
          "updatedAt": "2021-09-30T14:51:27Z",
          "comments": [
            {
              "originalPosition": 484,
              "body": "I buy that the number of rounds is a constant determined by the VDAF. But how does helper know what round it's on when it receives a `VerifyNextReq`? Implicitly a helper would have to keep a count of executed rounds in its `helper_state`, or we could require the VDAF to encode that into the `state` that gets passed into `vdaf_next`.",
              "createdAt": "2021-09-30T14:51:26Z",
              "updatedAt": "2021-09-30T14:51:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txlt2",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:51:54Z",
          "updatedAt": "2021-09-30T14:51:55Z",
          "comments": [
            {
              "originalPosition": 580,
              "body": "Sounds fine, but I think we should have explicit text instructing implementations to do this.",
              "createdAt": "2021-09-30T14:51:54Z",
              "updatedAt": "2021-09-30T14:51:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txoF_",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:58:40Z",
          "updatedAt": "2021-09-30T14:58:40Z",
          "comments": [
            {
              "originalPosition": 484,
              "body": "I'd also be fine with adding a \"round_number\" field to the request body.",
              "createdAt": "2021-09-30T14:58:40Z",
              "updatedAt": "2021-09-30T14:58:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txoZw",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T14:59:37Z",
          "updatedAt": "2021-09-30T14:59:37Z",
          "comments": [
            {
              "originalPosition": 580,
              "body": "The instruction is \"the leader copies the output parameter verbatim from the collect request to verify-start request\". Is that not already explicitly there?",
              "createdAt": "2021-09-30T14:59:37Z",
              "updatedAt": "2021-09-30T14:59:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txra5",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T15:09:08Z",
          "updatedAt": "2021-09-30T15:09:08Z",
          "comments": [
            {
              "originalPosition": 484,
              "body": "That could work, but then we have to consider what if any risks there are to allowing the leader to control the round number. If we put it in helper-controlled state, then it's protected by the encryption we already require from helper implementations.",
              "createdAt": "2021-09-30T15:09:08Z",
              "updatedAt": "2021-09-30T15:09:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txr-P",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T15:10:57Z",
          "updatedAt": "2021-09-30T15:10:57Z",
          "comments": [
            {
              "originalPosition": 580,
              "body": "So it is! I did notice one typo in the paragraph below. Resolving this thread.",
              "createdAt": "2021-09-30T15:10:57Z",
              "updatedAt": "2021-09-30T15:10:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4txsCb",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T15:11:08Z",
          "updatedAt": "2021-09-30T15:11:09Z",
          "comments": [
            {
              "originalPosition": 602,
              "body": "```suggestion\r\ntheir share of the output. Alternately, if `output_param` is empty or a\r\n```",
              "createdAt": "2021-09-30T15:11:09Z",
              "updatedAt": "2021-09-30T15:11:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4tyQMF",
          "commit": {
            "abbreviatedOid": "80e23fe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-09-30T17:27:53Z",
          "updatedAt": "2021-09-30T17:27:53Z",
          "comments": [
            {
              "originalPosition": 484,
              "body": "Good point! Either way the helper is going to have to keep track of its state, and thus what message to expect next. I think we should make no changes here and just make it clear that the helper has to keep track of this.",
              "createdAt": "2021-09-30T17:27:53Z",
              "updatedAt": "2021-09-30T17:27:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4t7EIr",
          "commit": {
            "abbreviatedOid": "848d112"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-10-04T15:34:08Z",
          "updatedAt": "2021-10-04T15:35:36Z",
          "comments": [
            {
              "originalPosition": 345,
              "body": "Do we not need a parameter indicating whether the entity running `vdaf_start` is the leader or helper, so that it can tell whether the input share is compressed or not (if the VDAF is Prio)? Or do we encode that information into `input_share`?",
              "createdAt": "2021-10-04T15:34:08Z",
              "updatedAt": "2021-10-04T15:35:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uEoOn",
          "commit": {
            "abbreviatedOid": "848d112"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-06T16:26:14Z",
          "updatedAt": "2021-10-06T16:26:15Z",
          "comments": [
            {
              "originalPosition": 345,
              "body": "It's encoded by the `input_share`. I know you didn't like this redundancy, but it makes the VDAF syntax a lot simpler.",
              "createdAt": "2021-10-06T16:26:14Z",
              "updatedAt": "2021-10-06T16:26:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uErQW",
          "commit": {
            "abbreviatedOid": "848d112"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Only got partway through, but I have a number of comments.",
          "createdAt": "2021-10-06T16:37:39Z",
          "updatedAt": "2021-10-06T16:54:10Z",
          "comments": [
            {
              "originalPosition": 61,
              "body": "Please file an issue for this rounds question. I am not convinced that we should commit to a fixed number of rounds as a protocol invariant.",
              "createdAt": "2021-10-06T16:37:40Z",
              "updatedAt": "2021-10-06T16:54:10Z"
            },
            {
              "originalPosition": 84,
              "body": "This restriction should appear in precisely one place so that we do not have to change it everywhere if we add multiple helpers. The first location is probably the right one.",
              "createdAt": "2021-10-06T16:38:21Z",
              "updatedAt": "2021-10-06T16:54:10Z"
            },
            {
              "originalPosition": 97,
              "body": "```suggestion\r\n  The client uses the public parameter to split its input into input shares, and\r\n```",
              "createdAt": "2021-10-06T16:38:36Z",
              "updatedAt": "2021-10-06T16:54:10Z"
            },
            {
              "originalPosition": 101,
              "body": "It's quite hard to understand the role of the verification parameter without a description of the vdaf system",
              "createdAt": "2021-10-06T16:39:24Z",
              "updatedAt": "2021-10-06T16:54:10Z"
            },
            {
              "originalPosition": 125,
              "body": "```suggestion\r\ntransforms the measurement into a set of input shares, one for each aggregator. To encrypt each input share,\r\nthe client first generates an HPKE {{!I-D.irtf-cfrg-hpke}} context for the\r\n```",
              "createdAt": "2021-10-06T16:40:10Z",
              "updatedAt": "2021-10-06T16:54:10Z"
            },
            {
              "originalPosition": 139,
              "body": "```suggestion\r\n`server_role` is a byte whose value is the index of the aggregator in the aggregator list. The bytestring `enc` is the\r\n```\r\n\r\nOr some such...",
              "createdAt": "2021-10-06T16:41:13Z",
              "updatedAt": "2021-10-06T16:54:10Z"
            },
            {
              "originalPosition": 191,
              "body": "```suggestion\r\nhelper(s) begin verifying and aggregating them. In order to enable the system to\r\n```",
              "createdAt": "2021-10-06T16:42:03Z",
              "updatedAt": "2021-10-06T16:54:11Z"
            },
            {
              "originalPosition": 194,
              "body": "```suggestion\r\nrequests to the helpers, the first of which contains the helper's encrypted input\r\n```",
              "createdAt": "2021-10-06T16:42:14Z",
              "updatedAt": "2021-10-06T16:54:11Z"
            },
            {
              "originalPosition": 195,
              "body": "```suggestion\r\nshares. After a number of successful requests, the aggregators have recovered\r\n```",
              "createdAt": "2021-10-06T16:42:33Z",
              "updatedAt": "2021-10-06T16:54:11Z"
            },
            {
              "originalPosition": 271,
              "body": "This is a regression. Please don't hardcode that it's two all over this PR.",
              "createdAt": "2021-10-06T16:51:28Z",
              "updatedAt": "2021-10-06T16:54:11Z"
            },
            {
              "originalPosition": 327,
              "body": "Isn't this operation the same for every endpoint? Why have it be leader-specific.",
              "createdAt": "2021-10-06T16:52:48Z",
              "updatedAt": "2021-10-06T16:54:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uE0zj",
          "commit": {
            "abbreviatedOid": "848d112"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-06T17:15:26Z",
          "updatedAt": "2021-10-06T17:15:27Z",
          "comments": [
            {
              "originalPosition": 345,
              "body": "If I complained about it in the past I have forgotten why! I'm OK with a VDAF-specific detail like this being encoded into `input_share`, i.e. in a way that's opaque to the PPM layer.",
              "createdAt": "2021-10-06T17:15:27Z",
              "updatedAt": "2021-10-06T17:15:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uJMde",
          "commit": {
            "abbreviatedOid": "848d112"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-07T16:47:45Z",
          "updatedAt": "2021-10-07T17:00:00Z",
          "comments": [
            {
              "originalPosition": 84,
              "body": "Good call. I removed this MUST here and the paranthetical above saying the protocol is only defined for two aggregators. I added a MUST for this at the end of {{task-configuration}}.",
              "createdAt": "2021-10-07T16:47:45Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            },
            {
              "originalPosition": 61,
              "body": "Before I do, I'd like to chat in person about the implication of this restriction and why I feel it's a safe assumption. Just to be clear: the intention is that the VDAF scheme determines the number of rounds, not the wrapper protocol (i.e., this specification).",
              "createdAt": "2021-10-07T16:48:55Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            },
            {
              "originalPosition": 139,
              "body": "Works for me, I thought we were still blocking on this change.",
              "createdAt": "2021-10-07T16:50:36Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            },
            {
              "originalPosition": 191,
              "body": "I think if we're going to make this change, we'll have to actually specify how multiple helpers works. We can do this here, but my preference would be to wait for antoher PR.",
              "createdAt": "2021-10-07T16:51:44Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            },
            {
              "originalPosition": 327,
              "body": "`SetupBaseR` has a different info string, but yeah, we should unify this when we allow for multiple helpers.",
              "createdAt": "2021-10-07T16:52:40Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            },
            {
              "originalPosition": 101,
              "body": "Ack, I'll add a description of VDAFs in the overview for this PR.",
              "createdAt": "2021-10-07T16:56:52Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            },
            {
              "originalPosition": 194,
              "body": "This is not currently specified.",
              "createdAt": "2021-10-07T16:57:41Z",
              "updatedAt": "2021-10-07T17:00:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uNqy9",
          "commit": {
            "abbreviatedOid": "6afbda1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-08T19:12:05Z",
          "updatedAt": "2021-10-08T19:12:06Z",
          "comments": [
            {
              "originalPosition": 61,
              "body": "Changed \"constant\" to \"maximum\" number of rounds throughtout the doc.",
              "createdAt": "2021-10-08T19:12:06Z",
              "updatedAt": "2021-10-08T19:12:06Z"
            }
          ]
        }
      ]
    },
    {
      "number": 156,
      "id": "PR_kwDOFEJYQs4sm2d6",
      "title": "Clarify timing of report aggregations & purpose of anti-replay.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/156",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "As previously written, the timing of report aggregations would allow\r\naggregation of reports with timestamps far into the future, which would\r\nallow a single client reporting a timestamp of INT_MAX to DOS the\r\naggregation system due to the way timestamps are used in the anti-replay\r\nmechanism. Given the severity of not following this portion of the\r\nspecification, I upgrade the SHOULD to a MUST as well.\r\n\r\nFixes #153.",
      "createdAt": "2021-10-03T18:54:54Z",
      "updatedAt": "2022-09-16T00:29:59Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7cf4db3f2ec7605a87ef79a52611eb544b0b0f33",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "clarify-replay-and-buffering",
      "headRefOid": "fcbdb035a113897b4f131598a781bca6527388bc",
      "closedAt": "2021-10-25T19:54:13Z",
      "mergedAt": "2021-10-25T19:54:13Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "d1e323f806d499760818619aa6223fc9b59c9879"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "At a high level, there are two ways for this to work:\r\n\r\n1. The leader and the helper agree precisely on which reports are acceptable, in which case (a) they need to have agreement on how much slack there is and (b) have precisely synchronized clocks. If the leader sends the helper an invalid share, then this can be a fatal error.\r\n2. The leader and the helper. In this case, they do not need to have agreement on slack or have synchronized clocks, but we then need to deal with the situation in which some shares are unacceptable. This requires a way for the helper to report that and then to know how the leader handles that.",
          "createdAt": "2021-10-06T17:26:15Z",
          "updatedAt": "2021-10-06T17:26:15Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> At a high level, there are two ways for this to work:\r\n> \r\n> 1. The leader and the helper agree precisely on which reports are acceptable, in which case (a) they need to have agreement on how much slack there is and (b) have precisely synchronized clocks. If the leader sends the helper an invalid share, then this can be a fatal error.\r\n> 2. The leader and the helper. In this case, they do not need to have agreement on slack or have synchronized clocks, but we then need to deal with the situation in which some shares are unacceptable. This requires a way for the helper to report that and then to know how the leader handles that.\r\n\r\nI took another look at the spec and I think we can actually get away without any new agreed-upon parameters, nor synchronized clocks; nor is there a requirement for (new) agreement on shares being unacceptable. (Sorry, I should have caught this during the discussion yesterday.)\r\n\r\nWith the changes in this PR, the leader now needs to know two new parameters: (a) how long into the past to keep reports before including them in an aggregate request, (b) how far into the future a client-provided timestamp can be.\r\n\r\nThe leader uses these parameters to decide, respectively, (a) which client reports to include in a given aggregate request, and (b) which client reports to drop at time of reception. It is capable of performing both of these functions on its own, without need for communication/coordination with the helper.\r\n\r\nThe helper's function for this part of the protocol is to receive aggregate requests, perform the existing anti-replay mechanism (based on ordering of `(timestamp, nonce)`) to avoid the leader being able to replay reports, then do whatever aggregation is necessary based on the VDAF in use. The helper does not actually need to know the parameters the leader is using to perform client report selection/filtering, nor does it need to have a clock synchronized with the leader (unless the VDAF needs a clock, I suppose the helper does not need a clock at all for the aggregation portion of the protocol!). Malformed aggregation requests, including (by my reading) those that trigger the anti-replay mechanism, ~~are handled as per section 3.1~~. [edit: actually, currently out-of-order reports are silently dropped: \"[The helper] then filters out out-of-order sub-requests by ignoring any sub-request that does not follow the previous one (See Section 4.4.2.)\"]",
          "createdAt": "2021-10-07T18:17:51Z",
          "updatedAt": "2021-10-07T19:46:31Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "There was some discussion of whether we need to accept client reports timestamped in the future during yesterday's meeting. I wanted to circle back on this.\r\n\r\nFirst: an implementation can choose not to accept client timestamps that are in the future. The tradeoff is that clients whose clocks have skewed even a small amount of time into the future will no longer be able to provide reports.\r\n\r\nFor that reason, I think implementations should accept reports timestamped into the future, for a time period on the order of \"minutes\" or \"hours\" (but perhaps not \"days\" or \"weeks\"?). The leader does have to store these reports -- but since the leader already has to store all reports for some time to allow reports to arrive out-of-order, this is not a totally new requirement.\r\n\r\nThe text in the PR suggests accepting reports timestamped into the future up to a \"time proportional to the batch window size\"; to be honest, I simply copied forward the previous text's suggestion here. Perhaps there is a better suggestion to be made?",
          "createdAt": "2021-10-07T18:25:46Z",
          "updatedAt": "2021-10-07T18:25:46Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Based on last week's discussion, I dropped the `grace_window` behavior from a `MUST` to a `SHOULD`.\r\n\r\nPlease note this now-allowed bad behavior, copied from the commit message:\r\n> Specifically, an implementation that chooses to ignore the `grace_window` SHOULD, and accepts reports clock-skewed a few minutes in the future, could end up dropping non-skewed reports if a skewed report arrives just before an aggregate request is made.\r\n\r\nI don't think this caveat is bad enough to be worthy of specifying grace_window with a MUST, I only note it for posterity.",
          "createdAt": "2021-10-20T00:01:30Z",
          "updatedAt": "2021-10-20T00:01:30Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I won't have time to look at this by noon, so please merge without my approval.",
          "createdAt": "2021-10-25T16:45:55Z",
          "updatedAt": "2021-10-25T16:45:55Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "LGTM! I'll merge at noon unless someone adds a review asking for more changes.",
          "createdAt": "2021-10-25T18:36:26Z",
          "updatedAt": "2021-10-25T18:36:26Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I'd like to look first.\n\nOn Mon, Oct 25, 2021 at 11:36 AM Tim Geoghegan ***@***.***>\nwrote:\n\n> LGTM! I'll merge at noon unless someone adds a review asking for more\n> changes.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/ppm-specification/pull/156#issuecomment-951197698>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIJZ2AT6XUETLILVI2LUIWPTJANCNFSM5FH42HCQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n",
          "createdAt": "2021-10-25T18:37:31Z",
          "updatedAt": "2021-10-25T18:37:31Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4uEo69",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Yup, I agree this is a good idea. ",
          "createdAt": "2021-10-06T16:28:43Z",
          "updatedAt": "2021-10-06T16:28:43Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4uEplP",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-10-06T16:31:14Z",
          "updatedAt": "2021-10-06T16:31:14Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4uE2L5",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "@ekr pointed out on that the call that this needs to be more prescriptive for interop.",
          "createdAt": "2021-10-06T17:21:10Z",
          "updatedAt": "2021-10-06T17:21:10Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4uSY8B",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-10-11T18:41:39Z",
          "updatedAt": "2021-10-11T19:04:00Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "nit: I liked the explicit mention of \"privacy violation\", because in the \"Security Considerations\" section, we discuss the \"privacy\" and \"soundness\" properties which are provided by the secret sharing and distributed proof components of the system, respectively. I think it's helpful to highlight where we can which of those properties is protected by a particular part of the protocol.",
              "createdAt": "2021-10-11T18:41:39Z",
              "updatedAt": "2021-10-11T19:04:00Z"
            },
            {
              "originalPosition": 12,
              "body": "I think we can be more prescriptive here and simply state that reports whose timestamps are in the future MUST be rejected, while allowing for clock skew. My rationale is that rejecting reports from the future is simple to specify and simple to implement, and lets us unambiguously specify behavior, except with respect to clock skew.\r\n\r\nI found some precedent for discussing clock skew this way. [RFC7519 section 4.1.4 and 4.1.5](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.4) discusses clock skew in the context of token expiration and not-before:\r\n```\r\nThe \"exp\" (expiration time) claim identifies the expiration time on\r\n   or after which the JWT MUST NOT be accepted for processing.  The\r\n   processing of the \"exp\" claim requires that the current date/time\r\n   MUST be before the expiration date/time listed in the \"exp\" claim.\r\n   Implementers MAY provide for some small leeway, usually no more than\r\n   a few minutes, to account for clock skew.\r\n```\r\n\r\nSimilarly in [the section of RFC7523 discussing expiration of JWTs in Oauth 2.0](https://www.rfc-editor.org/rfc/rfc7523.html#section-3):\r\n```\r\nThe JWT MUST contain an \"exp\" (expiration time) claim that\r\n        limits the time window during which the JWT can be used.  The\r\n        authorization server MUST reject any JWT with an expiration time\r\n        that has passed, subject to allowable clock skew between\r\n        systems.\r\n```\r\n\r\nMy takeaway is that it's acceptable for a standard to acknowledge clock skew but not be totally specific about what to do about it.",
              "createdAt": "2021-10-11T18:50:24Z",
              "updatedAt": "2021-10-11T19:04:00Z"
            },
            {
              "originalPosition": 9,
              "body": "Given that this is a `MUST`, I'd like us to be more explicit than saying \"proportional\". I wonder if we should have an explicit grace period parameter, or perhaps a submission deadline, so that clients can decide how long to hold onto inputs before uploading them. I can imagine clients wanting to defer uploads until they are on a WiFi network, or maybe until they are not drawing power from a battery, but if they know that the deadline is just about to elapse then they might opt to do the uploads a little sooner.\r\n\r\nI also wonder if we should define an error code and error document so that the leader can explicitly reject a report for being too old, and then tell the client what the oldest report it would accept is. This would allow the client to skip over potentially many queued reports that will be rejected by leader for being too old.",
              "createdAt": "2021-10-11T19:03:25Z",
              "updatedAt": "2021-10-11T19:04:00Z"
            },
            {
              "originalPosition": 9,
              "body": "I'm open to punting some or all of the above to a later PR in the interest of getting this change merged.",
              "createdAt": "2021-10-11T19:03:57Z",
              "updatedAt": "2021-10-11T19:04:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uWb6f",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-12T16:50:04Z",
          "updatedAt": "2021-10-12T16:50:04Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Ah, I agree that it would be good to keep this explicit -- I didn't realize the intended reference here. I added some wording that is as clear as possible.",
              "createdAt": "2021-10-12T16:50:04Z",
              "updatedAt": "2021-10-12T16:50:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uWeI2",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-12T16:59:31Z",
          "updatedAt": "2021-10-12T16:59:31Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Thanks for digging up the prior art -- I like both of these better than what's written, since the intent is to protect against clock skew specifically (and the previous wording in the PR didn't make that clear). I went with something like the first, since I want to make the \"clock skew leeway\" as hard to miss as possible for implementers, since I suspect it will be practically necessary to avoid unnecessarily dropping a nontrivial fraction of client reports.",
              "createdAt": "2021-10-12T16:59:31Z",
              "updatedAt": "2021-10-12T17:50:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uWtWQ",
          "commit": {
            "abbreviatedOid": "b0d536b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-12T18:08:29Z",
          "updatedAt": "2021-10-12T18:08:30Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "I like the idea of an explicit task parameter: even though only the leader _needs_ to know this parameter for the protocol to work, as you point out, clients might wish to know the parameter to make decisions about which reports are worthy of submission. With an explicit `grace_window` parameter, clients could assume that any reports more than a `grace_window` into the past is unlikely to be accepted and drop them, without dropping too many reports that would have been accepted.\r\n\r\nAn explicit error type would work too, and would permit clients to make more precise report-or-drop decisions at the cost of more coordination/specification. I'll punt on specifying that unless someone objects.",
              "createdAt": "2021-10-12T18:08:29Z",
              "updatedAt": "2021-10-12T18:08:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4u_KkI",
          "commit": {
            "abbreviatedOid": "2ff8c56"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-10-25T15:49:31Z",
          "updatedAt": "2021-10-25T15:49:36Z",
          "comments": [
            {
              "originalPosition": 23,
              "body": "The consensus in the call the other week was to not prescribe much of anything around this, so I think we should remove the explicit `grace_window` parameter (which I know I asked for in the first place, so sorry to demand its deletion now) and go back to the SHOULD text we had (though we should keep your MUST NOT about future timestamps and MAY about clock skew).",
              "createdAt": "2021-10-25T15:49:31Z",
              "updatedAt": "2021-10-25T15:49:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4u_WbB",
          "commit": {
            "abbreviatedOid": "fcbdb03"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-25T16:32:17Z",
          "updatedAt": "2021-10-25T16:32:18Z",
          "comments": [
            {
              "originalPosition": 23,
              "body": "Done -- effectively, this drops the explicit `grace_window` parameter for the implicit \"time period proportional to the batch window\", and choosing such a window means that we effectively won't ever aggregate something with a future timestamp even if we have a clock-skew grace window, so no complaints from me.\r\n\r\nI kept the wording as \"...before including them in an aggregate request\" rather than the original \"...before issuing the first aggregate request\". I _think_ this is what's intended (since a given report will only be included in a single successful aggregate request). Let me know if I'm missing something there.",
              "createdAt": "2021-10-25T16:32:18Z",
              "updatedAt": "2021-10-25T16:32:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4u_z8c",
          "commit": {
            "abbreviatedOid": "fcbdb03"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-10-25T18:35:47Z",
          "updatedAt": "2021-10-25T18:35:47Z",
          "comments": []
        }
      ]
    },
    {
      "number": 157,
      "id": "PR_kwDOFEJYQs4s9h5i",
      "title": "Some proposed edits",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/157",
      "state": "MERGED",
      "author": "coopdanger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I think this could still use some more words about what is meant by \"configuration\" but I was not sure myself so I didn't suggest anything.",
      "createdAt": "2021-10-08T20:50:20Z",
      "updatedAt": "2021-10-08T20:52:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7cf4db3f2ec7605a87ef79a52611eb544b0b0f33",
      "headRepository": "coopdanger/ppm-specification",
      "headRefName": "patch-1",
      "headRefOid": "d02f24b453a0677774abf442d3f69ffdefde76b7",
      "closedAt": "2021-10-08T20:52:33Z",
      "mergedAt": "2021-10-08T20:52:33Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "5e866c1225bf679c8e46a3464833f36b3cb44239"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 159,
      "id": "PR_kwDOFEJYQs4s9ieZ",
      "title": "Clarify configuration",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/159",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-10-08T20:55:38Z",
      "updatedAt": "2022-09-16T00:29:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5e866c1225bf679c8e46a3464833f36b3cb44239",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "configuration",
      "headRefOid": "006dedd6a9ec05e11374c52192b44b5ec90d8f72",
      "closedAt": "2021-10-12T12:25:27Z",
      "mergedAt": "2021-10-12T12:25:27Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "6602a39dc70fcf9bcf57b8acb50144d784a1ab09"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4uROWc",
          "commit": {
            "abbreviatedOid": "e6c9fa4"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-11T13:55:46Z",
          "updatedAt": "2021-10-11T13:56:07Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "```suggestion\r\nare configured with each other's identities and details of the types of\r\n```",
              "createdAt": "2021-10-11T13:55:46Z",
              "updatedAt": "2021-10-11T13:56:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uRSI8",
          "commit": {
            "abbreviatedOid": "e6c9fa4"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM pending Tim's nit",
          "createdAt": "2021-10-11T14:08:54Z",
          "updatedAt": "2021-10-11T14:08:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4uReI4",
          "commit": {
            "abbreviatedOid": "006dedd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-11T14:51:47Z",
          "updatedAt": "2021-10-11T14:51:47Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "s/PRIV/PPM/? This is a new acronym to me",
              "createdAt": "2021-10-11T14:51:47Z",
              "updatedAt": "2021-10-11T14:51:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uRgIH",
          "commit": {
            "abbreviatedOid": "006dedd"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-11T14:57:33Z",
          "updatedAt": "2021-10-11T14:57:33Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "Some IETF leadership folks objected to the collision with IPPM, so PRIV was a fallback: https://datatracker.ietf.org/group/priv/about/",
              "createdAt": "2021-10-11T14:57:33Z",
              "updatedAt": "2021-10-11T14:57:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uRnvh",
          "commit": {
            "abbreviatedOid": "006dedd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-10-11T15:10:30Z",
          "updatedAt": "2021-10-11T15:10:30Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "Ack, thanks!",
              "createdAt": "2021-10-11T15:10:30Z",
              "updatedAt": "2021-10-11T15:10:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4uRn2A",
          "commit": {
            "abbreviatedOid": "006dedd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-10-11T15:10:41Z",
          "updatedAt": "2021-10-11T15:10:41Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4uSYaW",
          "commit": {
            "abbreviatedOid": "006dedd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-10-11T18:38:19Z",
          "updatedAt": "2021-10-11T18:38:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 160,
      "id": "PR_kwDOFEJYQs4tpSgk",
      "title": "Rename again",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/160",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-10-25T20:04:02Z",
      "updatedAt": "2021-10-25T20:15:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d1e323f806d499760818619aa6223fc9b59c9879",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "rename_again",
      "headRefOid": "68eab4c1c5375079cf2d3bc11119948324acd24b",
      "closedAt": "2021-10-25T20:15:12Z",
      "mergedAt": "2021-10-25T20:15:12Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "bb88916823c47966f2ef139c248a17d7fb5d2170"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4vAJIH",
          "commit": {
            "abbreviatedOid": "27e710f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This should update `.gitignore` to replace `draft-ppm-protocol.xml` with `draft-gpew-priv-ppm.xml` but otherwise LGTM",
          "createdAt": "2021-10-25T20:12:58Z",
          "updatedAt": "2021-10-25T20:12:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 162,
      "id": "PR_kwDOFEJYQs4uGNiT",
      "title": "Editorial clean-up",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/162",
      "state": "MERGED",
      "author": "coopdanger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-11-04T14:41:38Z",
      "updatedAt": "2021-11-04T15:51:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "bb88916823c47966f2ef139c248a17d7fb5d2170",
      "headRepository": "coopdanger/ppm-specification",
      "headRefName": "patch-2",
      "headRefOid": "1ad7bfc85df5aaefb44bffdc32a55319a05beeeb",
      "closedAt": "2021-11-04T15:51:38Z",
      "mergedAt": "2021-11-04T15:51:38Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "7e2fbd9e671c0bd31868a06e1f257c6ed9b83223"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4vj3g2",
          "commit": {
            "abbreviatedOid": "1ad7bfc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-11-04T15:51:34Z",
          "updatedAt": "2021-11-04T15:51:34Z",
          "comments": []
        }
      ]
    },
    {
      "number": 164,
      "id": "PR_kwDOFEJYQs4uPpi5",
      "title": "Ietf112 slides",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/164",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-11-08T18:41:09Z",
      "updatedAt": "2021-11-12T15:57:51Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "de1b61b1d6480a965648dcfad52879d9b7a9997b",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "ietf112-slides",
      "headRefOid": "a6d6d9a513cd95c23b739b595a42b4b1b2f2832a",
      "closedAt": "2021-11-12T15:57:51Z",
      "mergedAt": "2021-11-12T15:57:51Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "2112f391f64b13f209b26cdd8721f05d070c2d28"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm going to claim they are encrypted, with a one time pad.\n\nOn Mon, Nov 8, 2021 at 1:52 PM Tim Geoghegan ***@***.***>\nwrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In ietf112-slides/ppm-ietf-112.tex\n> <https://github.com/abetterinternet/ppm-specification/pull/164#discussion_r745120762>\n> :\n>\n> > +\n> +\\begin{frame}{What about bogus data?}\n> +\n> +  \\begin{itemize}\n> +  \\item Plausible but false\n> +    \\begin{itemize}\n> +    \\item ``I am 180cm tall'' when I am actually 175cm\n> +    \\item A problem with any surveying technique\n> +    \\item Solution: live with somewhat noisy data\n> +    \\end{itemize}\n> +  \\item Completely ridiculous\n> +    \\begin{itemize}\n> +    \\item ``I am 1km tall'' (or worse, ``I am -1km tall'')\n> +    \\item Easy to remove with standard systems by filtering\n> +      \\begin{itemize}\n> +      \\item ... but with Prio the data is encrypted\n>\n> \"encrypted\" isn't the right word here, especially since the secret shares\n> constructed by clients *do* get encrypted, but not to conceal them from\n> aggregators. I can't come up with a better single word to express this,\n> though.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/ppm-specification/pull/164#pullrequestreview-800643076>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIPKEINCRXOPLIVT2TDULBBB3ANCNFSM5HTMIKPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n",
          "createdAt": "2021-11-08T21:54:02Z",
          "updatedAt": "2021-11-08T21:54:02Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4vuNgE",
          "commit": {
            "abbreviatedOid": "ecb2ead"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-11-08T21:52:17Z",
          "updatedAt": "2021-11-08T21:52:17Z",
          "comments": [
            {
              "originalPosition": 329,
              "body": "\"encrypted\" isn't the right word here, especially since the secret shares constructed by clients *do* get encrypted, but not to conceal them from aggregators. I can't come up with a better single word to express this, though.",
              "createdAt": "2021-11-08T21:52:17Z",
              "updatedAt": "2021-11-08T21:52:17Z"
            }
          ]
        }
      ]
    },
    {
      "number": 165,
      "id": "PR_kwDOFEJYQs4ucIME",
      "title": "Update charter.md",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/165",
      "state": "MERGED",
      "author": "ShivanKaul",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Amend charter to allow for alternative privacy-preserving measurement techniques that address some desired use cases.",
      "createdAt": "2021-11-12T06:29:48Z",
      "updatedAt": "2021-11-29T20:12:49Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "de1b61b1d6480a965648dcfad52879d9b7a9997b",
      "headRepository": "ShivanKaul/ppm-specification",
      "headRefName": "patch-1",
      "headRefOid": "c6c435333b50ebdd9c3a85bc215a951484418e32",
      "closedAt": "2021-11-29T20:12:49Z",
      "mergedAt": "2021-11-29T20:12:49Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "892ad83780df387bd0ffbb2916ebbfd79f5a3e1b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4v-sXx",
          "commit": {
            "abbreviatedOid": "39ca0bd"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-11-12T15:57:41Z",
          "updatedAt": "2021-11-12T15:57:41Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4wOBMJ",
          "commit": {
            "abbreviatedOid": "39ca0bd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-11-17T18:20:53Z",
          "updatedAt": "2021-11-17T18:20:53Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4wUwt-",
          "commit": {
            "abbreviatedOid": "39ca0bd"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "LGTM with one change ",
          "createdAt": "2021-11-19T04:23:03Z",
          "updatedAt": "2021-11-19T04:23:17Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "```suggestion\r\n- Client submission of individual measurements, potentially along with proofs of validity\r\n```\r\n\r\nAvoiding the non sequiter later.",
              "createdAt": "2021-11-19T04:23:03Z",
              "updatedAt": "2021-11-19T04:23:17Z"
            }
          ]
        }
      ]
    },
    {
      "number": 167,
      "id": "PR_kwDOFEJYQs4uue9n",
      "title": "Abuse cases",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/167",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-11-18T16:56:25Z",
      "updatedAt": "2021-11-18T17:06:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2112f391f64b13f209b26cdd8721f05d070c2d28",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "abuse_cases",
      "headRefOid": "5657790aecc1f84f8d2bcfce530316ca61cfd0fa",
      "closedAt": "2021-11-18T17:06:16Z",
      "mergedAt": "2021-11-18T17:06:16Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "65ff82c2cd9e5ef05e3b2e1b0c6a7fb716e3d2c5"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "@coopdanger ",
          "createdAt": "2021-11-18T16:56:31Z",
          "updatedAt": "2021-11-18T16:56:31Z"
        },
        {
          "author": "coopdanger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "LGTM, thanks",
          "createdAt": "2021-11-18T17:05:43Z",
          "updatedAt": "2021-11-18T17:05:43Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4wSto9",
          "commit": {
            "abbreviatedOid": "5657790"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-11-18T17:05:36Z",
          "updatedAt": "2021-11-18T17:05:36Z",
          "comments": []
        }
      ]
    },
    {
      "number": 168,
      "id": "PR_kwDOFEJYQs4vhdci",
      "title": "Refer to VDAF instead of \"PPM scheme\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/168",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The spec vaguely refers to the \"concrete PPM scheme\" whenever we need to talk about protocol specific parameters. This change replaces each such reference with a reference to VDAF. It also adds the aggregation parameter to the aggregate request, which will be needed for hits.\r\n\r\nThis is an alternative #154 that aims to be less invasive. Partially addresses #98.",
      "createdAt": "2021-12-07T22:06:45Z",
      "updatedAt": "2021-12-29T17:45:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "892ad83780df387bd0ffbb2916ebbfd79f5a3e1b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/vdaf-unspecified",
      "headRefOid": "1378cbd94f2c318bb6bfa5a1bb173e595125ab90",
      "closedAt": "2021-12-09T15:23:24Z",
      "mergedAt": "2021-12-09T15:23:24Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "93032f7f3e1ed8cd1d785e68598d680a344c9813"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2021-12-09T15:22:46Z",
          "updatedAt": "2021-12-09T15:22:46Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4xRKCQ",
          "commit": {
            "abbreviatedOid": "0629e12"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Much cleaner \ud83d\udc4d ",
          "createdAt": "2021-12-08T15:27:01Z",
          "updatedAt": "2021-12-08T15:27:01Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4xRl8A",
          "commit": {
            "abbreviatedOid": "0629e12"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM, modulo one typo",
          "createdAt": "2021-12-08T17:00:39Z",
          "updatedAt": "2021-12-08T17:01:27Z",
          "comments": [
            {
              "originalPosition": 167,
              "body": "```suggestion\r\n  opaque agg_param<0..2^16-1>;  // VDAF aggregation parameter\r\n```",
              "createdAt": "2021-12-08T17:00:40Z",
              "updatedAt": "2021-12-08T17:01:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xTAws",
          "commit": {
            "abbreviatedOid": "0629e12"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This is generally good.\r\n\r\n@cjpatton can you somehow the F(p, ...) thing before you land?",
          "createdAt": "2021-12-08T23:43:13Z",
          "updatedAt": "2021-12-08T23:49:49Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "I think you need to say something more about \"p\" here. Like, what is it?\r\n\r\nAnd why can't we just bury that in F. I.e., \\mathcal{F} is a family of functions parametrized by p and F is a specific instance?\r\n\r\n",
              "createdAt": "2021-12-08T23:43:13Z",
              "updatedAt": "2021-12-08T23:49:49Z"
            },
            {
              "originalPosition": 13,
              "body": "```suggestion\r\nschemes that implement the VDAF\r\n```",
              "createdAt": "2021-12-08T23:44:35Z",
              "updatedAt": "2021-12-08T23:49:49Z"
            },
            {
              "originalPosition": 56,
              "body": "Can we remove the semicolons here?",
              "createdAt": "2021-12-08T23:45:10Z",
              "updatedAt": "2021-12-08T23:49:49Z"
            },
            {
              "originalPosition": 84,
              "body": "```suggestion\r\nthe input's validity [BBCGGI19] which the aggregators can jointly verify\r\n```",
              "createdAt": "2021-12-08T23:47:35Z",
              "updatedAt": "2021-12-08T23:49:49Z"
            },
            {
              "originalPosition": 85,
              "body": "```suggestion\r\nreject the report if it cannot be verified. However, they do not\r\n```",
              "createdAt": "2021-12-08T23:47:45Z",
              "updatedAt": "2021-12-08T23:49:49Z"
            },
            {
              "originalPosition": 213,
              "body": "```suggestion\r\nencodes the nonce and a VDAF-specific `message`:\r\n```",
              "createdAt": "2021-12-08T23:48:40Z",
              "updatedAt": "2021-12-09T01:03:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xTKMH",
          "commit": {
            "abbreviatedOid": "0629e12"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-09T01:04:33Z",
          "updatedAt": "2021-12-09T01:15:24Z",
          "comments": [
            {
              "originalPosition": 213,
              "body": "Took this but merged \"timestamp\" into \"nonce\".",
              "createdAt": "2021-12-09T01:04:33Z",
              "updatedAt": "2021-12-09T01:15:24Z"
            },
            {
              "originalPosition": 56,
              "body": "Replaced with periods.",
              "createdAt": "2021-12-09T01:05:59Z",
              "updatedAt": "2021-12-09T01:15:24Z"
            },
            {
              "originalPosition": 9,
              "body": "Two reasons not to bury this:\r\n1. `F` is implied by the VDAF in use\r\n2. `p` is a parameter chosen by the collector and distributed to the aggregators.\r\n\r\nI agree we should say more about what this is. I've added a paragraph to add a bit more color.\r\n\r\nBy the way, the canonical example of an aggregation parameter is the set of candidate prefixes for each round of heavy hitters.",
              "createdAt": "2021-12-09T01:11:40Z",
              "updatedAt": "2021-12-09T01:15:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xTVal",
          "commit": {
            "abbreviatedOid": "edede2b"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-12-09T02:47:40Z",
          "updatedAt": "2021-12-09T02:48:11Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "I can live with your text here, but I don't really agree with this rationale. This is introductory text and doesn't need to map 1:1 with the VDAF spec.",
              "createdAt": "2021-12-09T02:47:40Z",
              "updatedAt": "2021-12-09T02:48:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xV6lj",
          "commit": {
            "abbreviatedOid": "edede2b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-09T15:21:44Z",
          "updatedAt": "2021-12-09T15:21:44Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "Yeah I agree. Happy to iterate on it later on.",
              "createdAt": "2021-12-09T15:21:44Z",
              "updatedAt": "2021-12-09T15:21:44Z"
            }
          ]
        }
      ]
    },
    {
      "number": 169,
      "id": "PR_kwDOFEJYQs4vhlq-",
      "title": "Revise anti-replay mechanism",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/169",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "As proposed on the list: https://mailarchive.ietf.org/arch/msg/ppm/WP7PF6y_dr_VY9yHWc8hZOuIZWQ/",
      "createdAt": "2021-12-07T23:16:45Z",
      "updatedAt": "2022-09-16T00:30:02Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "48bc21ce7f1c7e446618bfe53cd9470511e130bc",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/replay",
      "headRefOid": "6b5c09b0dcc92698d0b0630dd41f46bda7a2ab66",
      "closedAt": "2022-01-15T00:20:15Z",
      "mergedAt": "2022-01-15T00:20:15Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "47c5b696a0c12e9c5da9cfc12c40d1786e0ce1fc"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> If we're revisiting the helper's storage capabilities to the extent of expecting them to maintain a set of nonces, should we also revisit the notion of the leader storing the helper's encrypted input shares, too?\r\n\r\nYeah, I think we could do that, but let's punt to a future PR.",
          "createdAt": "2021-12-08T15:15:22Z",
          "updatedAt": "2021-12-08T15:15:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2021-12-10T20:42:00Z",
          "updatedAt": "2021-12-10T20:42:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Removed the time-sensitive parameters",
          "createdAt": "2021-12-10T22:57:58Z",
          "updatedAt": "2021-12-10T22:57:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "There seems to be consensus for the main change introduced by this PR, which is to not require the Leader to sort reports. The only outstanding question is how to mitigate data loss that results from aggregators ignoring reports \"at will\". This is definitely a solvable problem, but there's not yet consensus on how to solve it. I'd like to suggest that we merge this PR and open an issue to hash out this problem.\r\n\r\n@chris-wood, @ekr: Please review and approve.",
          "createdAt": "2022-01-10T18:26:24Z",
          "updatedAt": "2022-01-10T18:26:24Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> First, as a practical matter, if clocks aren't roughly synced, then we are going to have some serious problems about out of window rejection.\r\n\r\nI'm not sure I agree with this. My understanding of the design principle here -- which I support -- is that aggregators just agree on a list of timestamps, just like they agree on other parameters for configuration of a measurement task. That means the only real requirement for clock sync then falls on the leader, as they control what is effectively \"the current batch.\" The leader controls what things are sent to the aggregators, so they can track the current batch based on what the leader asks them to aggregate and then output. (I _think_ they could do this without any notion of a clock whatsoever, but I may be missing something.)",
          "createdAt": "2022-01-14T23:08:56Z",
          "updatedAt": "2022-01-14T23:08:56Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood I think you may be correct, at least in the \"everything gets sent to the leader\" model. The helper can just accept whatever the leader says as the \"right edge\" of the window. And if the leader is dumb enough to do something way in the future, then it can't aggregate for a long time.\r\n",
          "createdAt": "2022-01-14T23:16:07Z",
          "updatedAt": "2022-01-14T23:16:07Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Yeah, this seems like it gets more complicated if we move to the \"send directly to aggregators\" model. \ud83e\udd37 ",
          "createdAt": "2022-01-14T23:17:14Z",
          "updatedAt": "2022-01-14T23:17:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "For my part, I actually think @ekr's suggestion is quite nice. However not everyone agrees that we should require the aggregators to be roughly time-synced, and we'll need to discuss it further. I'm trying to avoid scope creep with the current PR. @ekr I've addressed your inline comments ...  please let me know if you're happy merging this without your suggestion and we can discuss it in a new issue.",
          "createdAt": "2022-01-14T23:28:27Z",
          "updatedAt": "2022-01-14T23:28:27Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm not sure why it makes sense to land a PR changing things when we\ndon't have consensus.\n\nI would suggest instead that we just land the protocol piece that\nincludes the nonce and say that the precise replay suppression\nalgorithm is out of scope. As a practical matter, this will just mean\nthat interop is a little messy because the helper may silently\nreject, but that can happen anyway for other reasons, and none\nof this will be much of a problem if clocks are synced, which we\ncan just do for now.\n\n\n\nOn Fri, Jan 14, 2022 at 3:28 PM Christopher Patton ***@***.***>\nwrote:\n\n> For my part, I actually think @ekr <https://github.com/ekr>'s suggestion\n> is quite nice. However not everyone agrees that we should require the\n> aggregators to be roughly time-synced, and we'll need to discuss it\n> further. I'm trying to avoid scope creep with the current PR. @ekr\n> <https://github.com/ekr> I've addressed your inline comments ... please\n> let me know if you're happy merging this without your suggestion and we can\n> discuss it in a new issue.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/ppm-specification/pull/169#issuecomment-1013540322>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLIKQZHTH4ADMI7QDWQTUWCWSNANCNFSM5JSNFTXQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
          "createdAt": "2022-01-14T23:32:39Z",
          "updatedAt": "2022-01-14T23:32:39Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think we disagree on the replay-suppression algorithm. In fact, the only piece we haven't converged on is how to avoid storing nonce sets indefinitely while minimizing data loss.\r\n\r\nWe already require that the nonces be stored as long as they might be used (i.e., for as long as a batch isn't collected) It seems like all we need to do is remove the following text:\r\n\r\n> Depending on the rate of upload requests, the size of the nonce set can grow to     \r\n> be quite large. To ensure they do not need to store these indefinitely, each     \r\n> aggregator MAY at its own discretion ignore reports with timestamps that are too     \r\n> far in the past or too far in the future (see {{batch-parameter-validation}}).     \r\n\r\nand replace it with an open issue where we discuss this problem.\r\n",
          "createdAt": "2022-01-14T23:54:54Z",
          "updatedAt": "2022-01-14T23:54:54Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> I don't think we disagree on the replay-suppression algorithm. In fact, the only piece we haven't converged on is how to avoid storing nonce sets indefinitely while minimizing data loss.\r\n\r\nI was including that as part of the algorithm, but OK.\r\n\r\n\r\n> We already require that the nonces be stored as long as they might be used (i.e., for as long as a batch isn't collected) It seems like all we need to do is remove the following text:\r\n> \r\n> > Depending on the rate of upload requests, the size of the nonce set can grow to\r\n> > be quite large. To ensure they do not need to store these indefinitely, each\r\n> > aggregator MAY at its own discretion ignore reports with timestamps that are too\r\n> > far in the past or too far in the future (see {{batch-parameter-validation}}).\r\n> \r\n> and replace it with an open issue where we discuss this problem.\r\n\r\nSeems like you should also remove the rest of the OPEN ISSUE text following.\r\n\r\n",
          "createdAt": "2022-01-14T23:57:55Z",
          "updatedAt": "2022-01-14T23:57:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Done, and squashed the commits.",
          "createdAt": "2022-01-15T00:01:21Z",
          "updatedAt": "2022-01-15T00:01:21Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Here's the issue: https://github.com/abetterinternet/ppm-specification/issues/180\r\nI'll merge this once @ekr approves.",
          "createdAt": "2022-01-15T00:14:06Z",
          "updatedAt": "2022-01-15T00:14:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Done.",
          "createdAt": "2022-01-15T00:18:46Z",
          "updatedAt": "2022-01-15T00:18:46Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4xOf4o",
          "commit": {
            "abbreviatedOid": "e709ef8"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-07T23:58:19Z",
          "updatedAt": "2021-12-08T00:06:33Z",
          "comments": [
            {
              "originalPosition": 192,
              "body": "The previous strategy had the \"nonce time window evaluation\" (i.e. checking that each client report is not too new/too old) done solely by the leader; if I'm reading this PR correctly, now this time window evaluation is done on a per-aggregator basis.\r\n\r\nThis means that skewed clocks (or non-zero network latency) between different aggregators could lead to the aggregators disagreeing about which reports are acceptable and which should be ignored. My understanding is that such a disagreement can be catastrophic to the accuracy of the eventual aggregation.\r\n\r\nTo avoid the need to keep precisely-synchronized clocks between the leader & helper, I propose that the time-based filtering continue to be done by the leader alone. (The nonce-based filtering can/should still be done by both the leader & the helper.)",
              "createdAt": "2021-12-07T23:58:19Z",
              "updatedAt": "2021-12-08T00:06:33Z"
            },
            {
              "originalPosition": 184,
              "body": "nit: `noncee` -> `nonces`",
              "createdAt": "2021-12-07T23:58:41Z",
              "updatedAt": "2021-12-08T00:06:33Z"
            },
            {
              "originalPosition": 193,
              "body": "In previous discussion, the \"look-back\" window we considered was on the order of a few hours; the \"look-forward\" window was effectively as small as tolerable given likely clock skews between the clients & the leader, on the order of a few minutes.\r\n\r\nIn this PR, the look-forward & look-back are identical (i.e. `max_report_offset`).\r\n\r\nWDYT about making the look-forward and look-back windows into two separate parameters, or otherwise not prescribing that they be identical?",
              "createdAt": "2021-12-08T00:02:05Z",
              "updatedAt": "2021-12-08T00:07:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xOjB-",
          "commit": {
            "abbreviatedOid": "e709ef8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "If we're revisiting the helper's storage capabilities to the extent of expecting them to maintain a set of nonces, should we also revisit the notion of the leader storing the helper's encrypted input shares, too?",
          "createdAt": "2021-12-08T00:25:09Z",
          "updatedAt": "2021-12-08T00:26:59Z",
          "comments": [
            {
              "originalPosition": 199,
              "body": "```suggestion\r\nstrategy may result in dropping reports that happen to have the same nonce.\r\n```\r\nThis is unlikely given that the nonce is a 64 bit integer though, right?",
              "createdAt": "2021-12-08T00:25:09Z",
              "updatedAt": "2021-12-08T00:26:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xOqKu",
          "commit": {
            "abbreviatedOid": "e709ef8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-08T01:30:01Z",
          "updatedAt": "2021-12-08T01:38:16Z",
          "comments": [
            {
              "originalPosition": 192,
              "body": "> This means that skewed clocks (or non-zero network latency) between different aggregators could lead to the aggregators disagreeing about which reports are acceptable and which should be ignored. My understanding is that such a disagreement can be catastrophic to the accuracy of the eventual aggregation.\r\n\r\nClock skew is indeed a problem, but the worst that can happen if a clock is off is that a report gets dropped needlessly. The kind of catastrophic failure you're talking about would happen only if one aggregator decided to aggregate its input share but the other did not. If a report is ignored, then an input share cannot be validated and therefore cannot be aggregated.\r\n\r\n> To avoid the need to keep precisely-synchronized clocks between the leader & helper, I propose that the time-based filtering continue to be done by the leader alone. (The nonce-based filtering can/should still be done by both the leader & the helper.)\r\n\r\nBoth aggregators need to enforce anti-replay, so the helper would have to store the entire nonce set for the task indefinitely. This is the main problem we're trying to avoid with time-based filtering.",
              "createdAt": "2021-12-08T01:30:01Z",
              "updatedAt": "2021-12-08T01:38:16Z"
            },
            {
              "originalPosition": 193,
              "body": "Good catch, I forgot about this! (This is why we have reviews!) I'll break this into two parameters.",
              "createdAt": "2021-12-08T01:30:47Z",
              "updatedAt": "2021-12-08T01:38:16Z"
            },
            {
              "originalPosition": 199,
              "body": "It's not terribly unlikely. Say you have an upload rate of 10,000,000 reports/sec. Then the probability of a nonce collision is roughly 1,000,000^2/2^64, i.e., not negligible.",
              "createdAt": "2021-12-08T01:37:37Z",
              "updatedAt": "2021-12-08T01:38:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xRfWA",
          "commit": {
            "abbreviatedOid": "7b906f0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-08T16:37:28Z",
          "updatedAt": "2021-12-08T16:37:29Z",
          "comments": [
            {
              "originalPosition": 187,
              "body": "I suppose we could make this a MAY.",
              "createdAt": "2021-12-08T16:37:28Z",
              "updatedAt": "2021-12-08T16:37:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xc-Nr",
          "commit": {
            "abbreviatedOid": "73d7673"
          },
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-12T22:35:07Z",
          "updatedAt": "2021-12-12T22:48:02Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "You shouldn't be checking this file in.  Especially as it is listed in .gitignore.",
              "createdAt": "2021-12-12T22:35:07Z",
              "updatedAt": "2021-12-12T22:48:02Z"
            },
            {
              "originalPosition": 182,
              "body": "This note being attached to the AAD statement is confusing.  It's a note about the whole scheme, not the AAD construction.",
              "createdAt": "2021-12-12T22:40:36Z",
              "updatedAt": "2021-12-12T22:48:02Z"
            },
            {
              "originalPosition": 147,
              "body": "The point isn't that it is too far into the future, it is that you either track nonces, or you reject the input.",
              "createdAt": "2021-12-12T22:47:53Z",
              "updatedAt": "2021-12-12T22:48:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xh094",
          "commit": {
            "abbreviatedOid": "73d7673"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-14T02:22:14Z",
          "updatedAt": "2021-12-14T02:22:23Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Ugh, thanks for flagging! Reverted.\r\n\r\nHowever, after `make clean && make` I see the file in `git status`, despite the fact that it's listed as ignored:\r\n```\r\n$ git status\r\nOn branch cjpatton/replay\r\nYour branch is up to date with 'origin/cjpatton/replay'.\r\n\r\nChanges not staged for commit:\r\n  (use \"git add <file>...\" to update what will be committed)\r\n  (use \"git restore <file>...\" to discard changes in working directory)\r\n\tmodified:   .targets.mk\r\n\r\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n```",
              "createdAt": "2021-12-14T02:22:14Z",
              "updatedAt": "2021-12-14T02:22:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xh1v4",
          "commit": {
            "abbreviatedOid": "0b0eb0c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-14T02:29:36Z",
          "updatedAt": "2021-12-14T02:33:18Z",
          "comments": [
            {
              "originalPosition": 182,
              "body": "Good call. I replaced this with a paragraph that, hopefully, makes it more clear what we're trying to accomplish here.",
              "createdAt": "2021-12-14T02:29:36Z",
              "updatedAt": "2021-12-14T02:33:18Z"
            },
            {
              "originalPosition": 147,
              "body": "Fair enough, but this text isn't new to this PR. What's changed here is the need to re-order reports.",
              "createdAt": "2021-12-14T02:32:03Z",
              "updatedAt": "2021-12-14T02:33:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKjYj",
          "commit": {
            "abbreviatedOid": "b884da9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-12-29T21:23:47Z",
          "updatedAt": "2021-12-29T21:49:06Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Do we need to spell out how a `struct Nonce` gets mashed into an HPKE context? i.e. endianness",
              "createdAt": "2021-12-29T21:23:47Z",
              "updatedAt": "2021-12-29T21:49:06Z"
            },
            {
              "originalPosition": 71,
              "body": "```suggestion\r\nhas been aggregated at least once. Otherwise, comparing the aggregate result to\r\n```",
              "createdAt": "2021-12-29T21:24:00Z",
              "updatedAt": "2021-12-29T21:49:06Z"
            },
            {
              "originalPosition": 124,
              "body": "```suggestion\r\ninterval that has been aggregated at least once.\r\n```",
              "createdAt": "2021-12-29T21:35:40Z",
              "updatedAt": "2021-12-29T21:49:06Z"
            },
            {
              "originalPosition": 71,
              "body": "What does it mean for a *batch interval* to have been *aggregated*? I don't think that verb is defined for that noun in the protocol right now. As a reader, if I want to see where things become \"aggregated\", then the `AggregateReq/AggregateResp` exchange seems like the obvious place, but an `AggregateReq` is not defined in terms of a batch interval. `AggregateSubReq` does include parameters that uniquely identify a `Report`, which I think lets us conclude that a `Report` is aggregated when the leader gets an `AggregateSubResp` from the helper for a `Report`. `OutputShareReq` does have a `batch_interval` member, so is the receipt of the corresponding `EncryptedOutputShare` when a batch interval becomes \"aggregated\"?\r\n\r\nI think we should write out a state machine for reports and batch intervals, especially since it seems like they enter the \"aggregated\" state at different moments. We maybe want to use a different verb than \"aggregate\" for batch interval to avoid confusion, too. How about \"collect\", since the `OutputShareReq/EncryptedOutputShare` process should be triggered by a collector issuing `CollectReq` to leader?\r\n\r\nAlso I think we should not do something like add a `batch_interval` member to `AggregateReq` and declare that receipt of `AggregateResp` is when a batch interval is aggregated, because I think that would prevent leaders and helpers from summing Prio inputs ahead of receipt of all the reports in a batch interval.",
              "createdAt": "2021-12-29T21:43:37Z",
              "updatedAt": "2021-12-29T21:49:06Z"
            },
            {
              "originalPosition": 202,
              "body": "Similarly to my point about the verb \"aggregate\" and the noun \"batch interval\", I think the notion of a report being \"processed\" is ambiguous. Given that there are two processing stages for an aggregator, the `AggregateReq` process and then the `OutputShareReq` process, when is a report \"processed\"? I suspect it's the former. Once again I think spelling out the state machine for reports and batch intervals would be helpful.",
              "createdAt": "2021-12-29T21:46:53Z",
              "updatedAt": "2021-12-29T21:49:06Z"
            },
            {
              "originalPosition": 223,
              "body": "Could you write up a GH issue for this for visibility?",
              "createdAt": "2021-12-29T21:47:31Z",
              "updatedAt": "2021-12-29T21:49:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKm70",
          "commit": {
            "abbreviatedOid": "b884da9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:16:06Z",
          "updatedAt": "2021-12-29T22:16:06Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "TLS-syntax specifies the byte encoding of `struct Nonce`, including endianness. See also my comment here: https://github.com/abetterinternet/ppm-specification/issues/139#issuecomment-1002795321. (If I've understood you correctly, we should be able to close that issue without action.)",
              "createdAt": "2021-12-29T22:16:06Z",
              "updatedAt": "2021-12-29T22:16:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKmVH",
          "commit": {
            "abbreviatedOid": "b884da9"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-12-29T22:06:32Z",
          "updatedAt": "2021-12-29T22:19:45Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I don't think so. The HPKE function takes a blob and the TLS language specifies a bigendian serialization.",
              "createdAt": "2021-12-29T22:06:32Z",
              "updatedAt": "2021-12-29T22:19:46Z"
            },
            {
              "originalPosition": 71,
              "body": "Why are you saying \"ignore\" as opposed to \"generate error\"",
              "createdAt": "2021-12-29T22:11:05Z",
              "updatedAt": "2021-12-29T22:19:46Z"
            },
            {
              "originalPosition": 124,
              "body": "Again, why ignore and not error?",
              "createdAt": "2021-12-29T22:14:11Z",
              "updatedAt": "2021-12-29T22:19:46Z"
            },
            {
              "originalPosition": 217,
              "body": "It's not unbounded. It's of size 2^{96}.",
              "createdAt": "2021-12-29T22:18:03Z",
              "updatedAt": "2021-12-29T22:19:46Z"
            },
            {
              "originalPosition": 221,
              "body": "It's not at your discretion whether you ignore those which have been aggregated.",
              "createdAt": "2021-12-29T22:19:01Z",
              "updatedAt": "2021-12-29T22:19:46Z"
            },
            {
              "originalPosition": 234,
              "body": "Why is it useful not to require clock sync? The leader and the helper are servers, and they're quite capable of having accurate clocks.",
              "createdAt": "2021-12-29T22:19:37Z",
              "updatedAt": "2021-12-29T22:19:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKnN3",
          "commit": {
            "abbreviatedOid": "b884da9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:20:53Z",
          "updatedAt": "2021-12-29T22:20:53Z",
          "comments": [
            {
              "originalPosition": 223,
              "body": "Sure thing, however I want to wait until folks are happy with this change. After all, the issue is triggered by this PR and only makes sense if it's merged.",
              "createdAt": "2021-12-29T22:20:53Z",
              "updatedAt": "2021-12-29T22:20:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKn4G",
          "commit": {
            "abbreviatedOid": "e0b8fc9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:32:42Z",
          "updatedAt": "2021-12-29T22:32:43Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "@tgeoghegan good call. I've changed \"aggregated\" to be more precise. (Similarly below.)",
              "createdAt": "2021-12-29T22:32:43Z",
              "updatedAt": "2021-12-29T22:32:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKn5s",
          "commit": {
            "abbreviatedOid": "e0b8fc9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:33:10Z",
          "updatedAt": "2021-12-29T22:33:10Z",
          "comments": [
            {
              "originalPosition": 202,
              "body": "I made this more precise along the same lines as above.",
              "createdAt": "2021-12-29T22:33:10Z",
              "updatedAt": "2021-12-29T22:33:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4xh7re",
          "commit": {
            "abbreviatedOid": "6effbf2"
          },
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-14T03:26:49Z",
          "updatedAt": "2021-12-29T22:34:01Z",
          "comments": [
            {
              "originalPosition": 147,
              "body": "My point is that you need to be very clear about what your anti-replay protections are doing and how.  This requirement without that context is not going to help people understand what the rule exists for and how it provides that protection.",
              "createdAt": "2021-12-14T03:26:49Z",
              "updatedAt": "2021-12-29T22:34:01Z"
            },
            {
              "originalPosition": 234,
              "body": "Good sync != perfect sync.  Aggregation will fail if anti-replay is independently enforced by helpers (which is something I argue we want) and any amount of relative skew could produce disagreements.",
              "createdAt": "2021-12-29T22:25:51Z",
              "updatedAt": "2021-12-29T22:34:01Z"
            },
            {
              "originalPosition": 238,
              "body": "I don't see how this is difficult.  The leader starts a run and tells helpers what t_min and t_max are as part of that.",
              "createdAt": "2021-12-29T22:26:24Z",
              "updatedAt": "2021-12-29T22:34:01Z"
            },
            {
              "originalPosition": 221,
              "body": "Definitely.  This is an important point on which all helpers (and the leader) need to agree.  Discretion can't come into it or aggregation will fail.",
              "createdAt": "2021-12-29T22:27:30Z",
              "updatedAt": "2021-12-29T22:34:01Z"
            },
            {
              "originalPosition": 75,
              "body": "```suggestion\r\n```\r\n\r\nThis is not a helpful statement.  It could be misinterpreted as saying that differential privacy is enough.  But anti-replay is critical to ensuring that differential privacy protections are adequate.  This says the exact opposite.",
              "createdAt": "2021-12-29T22:31:04Z",
              "updatedAt": "2021-12-29T22:34:01Z"
            },
            {
              "originalPosition": 124,
              "body": "You should include all the anti-replay text in the one place.  I read the previous piece and guessed that only the leader looks at timestamps.  But that isn't it.\r\n\r\nNote also s/timestamp/nonce or \"nonce contains a timestamp that\"",
              "createdAt": "2021-12-29T22:32:20Z",
              "updatedAt": "2021-12-29T22:34:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKoAU",
          "commit": {
            "abbreviatedOid": "b7400d6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:35:07Z",
          "updatedAt": "2021-12-29T22:43:49Z",
          "comments": [
            {
              "originalPosition": 217,
              "body": "Actually it's 2^128, but point well taken! Fixed to not imply unboundedness.",
              "createdAt": "2021-12-29T22:35:07Z",
              "updatedAt": "2021-12-29T22:43:49Z"
            },
            {
              "originalPosition": 221,
              "body": "Nice catch.",
              "createdAt": "2021-12-29T22:36:28Z",
              "updatedAt": "2021-12-29T22:43:49Z"
            },
            {
              "originalPosition": 234,
              "body": "> The leader and the helper are servers, and they're quite capable of having accurate clocks.\r\n\r\nI tend to agree. If we align `d_max` and `d_min` with `min_batch_duration`, then all they have to do is ensure their clocks are within `min_batch_duration` seconds of one another. However I think @martinthomson's broader point is this requirement isn't strictly necessary. The true requirement is that they agree on what set of nonces they have to track at any given time.\r\n\r\nThis was discussed extensively here: https://mailarchive.ietf.org/arch/msg/ppm/oDCOJ-cUR8NQFG6At3byH1fdcd4/",
              "createdAt": "2021-12-29T22:42:23Z",
              "updatedAt": "2021-12-29T22:43:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKo60",
          "commit": {
            "abbreviatedOid": "b7400d6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:53:42Z",
          "updatedAt": "2021-12-29T22:53:42Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "@ekr good question.\r\n\r\nOne reason to not *require* an error here is that, if we did, the leader would have to check if the batch has been collected before completing the HTTP request. The current text allows it to make this determination later on.\r\n\r\nOne reason to *not permit* an error at all is that it volunteers information to the client that the client doesn't really need, namely, that the collector has requested the aggregate result for the interval. This doesn't seem all that damaging right now, but who knows.",
              "createdAt": "2021-12-29T22:53:42Z",
              "updatedAt": "2021-12-29T22:53:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpIY",
          "commit": {
            "abbreviatedOid": "c17a911"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T22:58:11Z",
          "updatedAt": "2021-12-29T22:58:12Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "And one reason to provide an error is so clients can debug themselves.",
              "createdAt": "2021-12-29T22:58:12Z",
              "updatedAt": "2021-12-29T22:58:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpOG",
          "commit": {
            "abbreviatedOid": "b7400d6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:00:01Z",
          "updatedAt": "2021-12-29T23:00:02Z",
          "comments": [
            {
              "originalPosition": 234,
              "body": "@martinthomson AFAICT a disagreement only results in needlessly dropping reports, I don't see how it can result in a replay (i.e., privacy violation). I'm probably missing something though.",
              "createdAt": "2021-12-29T23:00:02Z",
              "updatedAt": "2021-12-29T23:13:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpSM",
          "commit": {
            "abbreviatedOid": "c17a911"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:01:06Z",
          "updatedAt": "2021-12-29T23:01:06Z",
          "comments": [
            {
              "originalPosition": 234,
              "body": "I'm not sure what you mean by \"perfect sync\". NTP is capable of providing 10ms error. Given that these timestamps are in seconds, this seems fine. I don't see a real problem here.",
              "createdAt": "2021-12-29T23:01:06Z",
              "updatedAt": "2021-12-29T23:01:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpVM",
          "commit": {
            "abbreviatedOid": "b7400d6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:01:58Z",
          "updatedAt": "2021-12-29T23:01:58Z",
          "comments": [
            {
              "originalPosition": 75,
              "body": "Great point, I will just axe this.\r\n\r\nThe broader point is that we've uncovered a number of potential \"batch faults\", all of which fall more or less into the umbrella of privacy leaks that DP aims to mitigate. To be clear, I don't think we should *require* DP to be used in the core PPM protocol, but I thought it might be worth noting the connection for later on when we really think through how to incorporate DP.",
              "createdAt": "2021-12-29T23:01:58Z",
              "updatedAt": "2021-12-29T23:01:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpVu",
          "commit": {
            "abbreviatedOid": "c17a911"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2021-12-29T23:02:09Z",
          "updatedAt": "2021-12-29T23:06:24Z",
          "comments": [
            {
              "originalPosition": 125,
              "body": "```suggestion\r\ninterval for which it has received at least one output-share request from the\r\n```",
              "createdAt": "2021-12-29T23:02:09Z",
              "updatedAt": "2021-12-29T23:06:24Z"
            },
            {
              "originalPosition": 202,
              "body": "Is there perhaps a change here you forgot to push here? This text is unchanged since I reviewed. I think at least we should change \"processed\" to \"aggregated\".",
              "createdAt": "2021-12-29T23:06:20Z",
              "updatedAt": "2021-12-29T23:06:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpnR",
          "commit": {
            "abbreviatedOid": "52926db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:07:51Z",
          "updatedAt": "2021-12-29T23:07:59Z",
          "comments": [
            {
              "originalPosition": 238,
              "body": "Shouldn't be too bad, but you do have to negotiate `t_min, t_max` that works for all of the helpers. You have to make sure that whatever `t_min` you pick is newer than than the oldest value that is tolerated by any aggregator. (Similarly for `t_max`, you'd have to pick it so that it's older than the newest value tolerated by any aggregator.)",
              "createdAt": "2021-12-29T23:07:51Z",
              "updatedAt": "2021-12-29T23:07:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKprV",
          "commit": {
            "abbreviatedOid": "52926db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:08:53Z",
          "updatedAt": "2021-12-29T23:08:53Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "The bug in this case is clockskew, right? ",
              "createdAt": "2021-12-29T23:08:53Z",
              "updatedAt": "2021-12-29T23:08:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKpyc",
          "commit": {
            "abbreviatedOid": "ec6274a"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:11:20Z",
          "updatedAt": "2021-12-29T23:11:20Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "Or any number of other things that cause the clock to be wrong.",
              "createdAt": "2021-12-29T23:11:20Z",
              "updatedAt": "2021-12-29T23:11:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKp1l",
          "commit": {
            "abbreviatedOid": "8c82034"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:12:26Z",
          "updatedAt": "2021-12-29T23:12:26Z",
          "comments": [
            {
              "originalPosition": 202,
              "body": "Woops, totally missed this. I thought this was about something else. s/processed/aggregated/g is the right change here.",
              "createdAt": "2021-12-29T23:12:26Z",
              "updatedAt": "2021-12-29T23:12:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKqQX",
          "commit": {
            "abbreviatedOid": "8c82034"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:21:40Z",
          "updatedAt": "2021-12-29T23:21:40Z",
          "comments": [
            {
              "originalPosition": 124,
              "body": "> You should include all the anti-replay text in the one place. I read the previous piece and guessed that only the leader looks at timestamps. But that isn't it.\r\n\r\nAgreed. I tried but couldn't figure out how to do so without a more major refactor. One idea I had was to change the {{anti-replay}} section to speak more generally about \"batch faults\", i.e., instances where a report was included or excluded from a batch in a way that leaked more information about the measurement than I intended.\r\n\r\nWhat I did instead was note above that the helper enforces the same thing.\r\n\r\nNote that this text gets de-duplicated in https://github.com/abetterinternet/ppm-specification/pull/174.\r\n\r\n> Note also s/timestamp/nonce or \"nonce contains a timestamp that\"\r\n\r\nDone.",
              "createdAt": "2021-12-29T23:21:40Z",
              "updatedAt": "2021-12-29T23:21:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yKq4G",
          "commit": {
            "abbreviatedOid": "57ffdc3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2021-12-29T23:37:30Z",
          "updatedAt": "2021-12-29T23:37:30Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "In any case, the benefit seems marginal to me. But I don't feel that strongly about it. I'd go for permitting an error here if you'd prefer that, but I don't think we should require it.",
              "createdAt": "2021-12-29T23:37:30Z",
              "updatedAt": "2021-12-29T23:38:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yPkHu",
          "commit": {
            "abbreviatedOid": "82fc2c7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "A few editorial nits but this LGTM.",
          "createdAt": "2022-01-03T18:14:27Z",
          "updatedAt": "2022-01-03T18:29:26Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "nit: since the `Nonce` type appears in a few places, I think the discussion of its `time` and `rand` members should be moved to the definition of `struct Nonce`, around line 417.",
              "createdAt": "2022-01-03T18:14:27Z",
              "updatedAt": "2022-01-03T18:29:26Z"
            },
            {
              "originalPosition": 73,
              "body": "nit: this adjective suggests that we have some kind of classification of privacy violations.\r\n```suggestion\r\nthe previous aggregate result may result in a privacy violation.\r\n```",
              "createdAt": "2022-01-03T18:17:23Z",
              "updatedAt": "2022-01-03T18:29:26Z"
            },
            {
              "originalPosition": 111,
              "body": "I find this confusing because the leader should be applying the anti-replay logic at the time of receiving uploads. By the time it gets to constructing an `AggregateReq`, it should already have discarded any reports with duplicate nonces by this point. It's also confusing to state that the leader should \"preprocess the set of reports carried by the AggregateReq\" because the leader constructs the `AggregateReq`.",
              "createdAt": "2022-01-03T18:19:43Z",
              "updatedAt": "2022-01-03T18:29:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yPxjU",
          "commit": {
            "abbreviatedOid": "7f3b319"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-03T19:46:09Z",
          "updatedAt": "2022-01-03T19:46:20Z",
          "comments": [
            {
              "originalPosition": 111,
              "body": "Addressed the latter request. For the former, an implementation of the leader might opt to wait to filter reports until it's ready to begin the aggregation flow. This makes sense if it wants to limit the amount of work it has to do for each upload request, where latency matters a lot more.",
              "createdAt": "2022-01-03T19:46:09Z",
              "updatedAt": "2022-01-03T19:46:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y2Qhh",
          "commit": {
            "abbreviatedOid": "0406fa0"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "_Really sorry_ for the delay here. This LGTM. I think we need to address #141 next.",
          "createdAt": "2022-01-14T15:59:00Z",
          "updatedAt": "2022-01-14T16:13:56Z",
          "comments": [
            {
              "originalPosition": 73,
              "body": "```suggestion\r\n(Note that the helpers enforce this as well; see {{aggregate-request}}.)\r\nThe leader responds to ignored requests with status 400 and an error of\r\ntype 'staleReport'.\r\n```",
              "createdAt": "2022-01-14T15:59:00Z",
              "updatedAt": "2022-01-14T16:13:56Z"
            },
            {
              "originalPosition": 123,
              "body": "```suggestion\r\noutput-share request from the leader. (See {{output-share-request}}.)\r\nThis means leaders cannot interleave a sequence of aggregate and \r\noutput-share requests for a single batch.\r\n```\r\n\r\nWe should probably add guidance for the leader here as to _when_ they should issue their output-share request. If they do it as soon as the minimum batch count is hit, then nothing else in the batch can be aggregated. (We can do this in a followup PR, I think.)",
              "createdAt": "2022-01-14T16:07:59Z",
              "updatedAt": "2022-01-14T16:13:56Z"
            },
            {
              "originalPosition": 171,
              "body": "```suggestion\r\nleader SHOULD NOT accept reports whose timestamps are too far in the future.\r\n```\r\n\r\nI don't see how we can enforce a MUST without a specific definition of \"too far in the future.\"",
              "createdAt": "2022-01-14T16:08:40Z",
              "updatedAt": "2022-01-14T16:13:56Z"
            },
            {
              "originalPosition": 234,
              "body": "I agree with @martinthomson here -- it's best if we can simply avoid skew altogether, and that seems doable with the agreed upon min/max limits.",
              "createdAt": "2022-01-14T16:12:07Z",
              "updatedAt": "2022-01-14T16:13:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y2VCq",
          "commit": {
            "abbreviatedOid": "0406fa0"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T16:14:15Z",
          "updatedAt": "2022-01-14T16:14:15Z",
          "comments": [
            {
              "originalPosition": 73,
              "body": "(And we would need to add this new error, if this suggestion lands.)",
              "createdAt": "2022-01-14T16:14:15Z",
              "updatedAt": "2022-01-14T16:14:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3j5C",
          "commit": {
            "abbreviatedOid": "0406fa0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:15:09Z",
          "updatedAt": "2022-01-14T22:23:18Z",
          "comments": [
            {
              "originalPosition": 234,
              "body": "I'm not convinced is as easy as that. Would you be OK with merging without a change and moving the discussion to an issue? (As I proposed above.)",
              "createdAt": "2022-01-14T22:15:09Z",
              "updatedAt": "2022-01-14T22:23:18Z"
            },
            {
              "originalPosition": 171,
              "body": "I agree, but the spec currently says MUST here, so this isn't something that's changed in this PR. Can we resolve this by leaving an OPEN ISSUE here?",
              "createdAt": "2022-01-14T22:16:31Z",
              "updatedAt": "2022-01-14T22:23:19Z"
            },
            {
              "originalPosition": 73,
              "body": "This is a duplicate of @ekr's point here: https://github.com/abetterinternet/ppm-specification/pull/169#discussion_r776503058. ",
              "createdAt": "2022-01-14T22:22:35Z",
              "updatedAt": "2022-01-14T22:23:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3nwt",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:44:22Z",
          "updatedAt": "2022-01-14T22:44:22Z",
          "comments": [
            {
              "originalPosition": 171,
              "body": "Why not just make the change here? It's obviously related to this PR.",
              "createdAt": "2022-01-14T22:44:22Z",
              "updatedAt": "2022-01-14T22:44:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3n2S",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:45:13Z",
          "updatedAt": "2022-01-14T22:45:13Z",
          "comments": [
            {
              "originalPosition": 73,
              "body": "I agree, and I think we need to be clear with the protocol interaction here. I am not supportive of \"ignore behavior is left unspecified.\"",
              "createdAt": "2022-01-14T22:45:13Z",
              "updatedAt": "2022-01-14T22:45:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3oIo",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:47:43Z",
          "updatedAt": "2022-01-14T22:47:43Z",
          "comments": [
            {
              "originalPosition": 171,
              "body": "People often ask to keep spec PRs minimal, but I don't think anyone will object so I'll just change this. Done.",
              "createdAt": "2022-01-14T22:47:43Z",
              "updatedAt": "2022-01-14T22:47:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3oKs",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:48:03Z",
          "updatedAt": "2022-01-14T22:48:03Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "I see no reason to just let this behavior go unspecified. What if some leaders reply with 200 OK and some reply with an error in this case? At the end of the day, the report upload was unsuccessful, and the signal sent back to the client should not be 200 OK.",
              "createdAt": "2022-01-14T22:48:03Z",
              "updatedAt": "2022-01-14T22:48:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3oTQ",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:49:14Z",
          "updatedAt": "2022-01-14T22:49:14Z",
          "comments": [
            {
              "originalPosition": 234,
              "body": "I don't really see how it would be complicated. This is the same as aggregators agreeing on batch interval boundaries, no? In any case, yes, I'm fine merging while this is an OPEN ISSUE.",
              "createdAt": "2022-01-14T22:49:14Z",
              "updatedAt": "2022-01-14T22:49:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3olz",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:51:15Z",
          "updatedAt": "2022-01-14T22:51:16Z",
          "comments": [
            {
              "originalPosition": 73,
              "body": "I'm going to close this thread and quote you in the other thread. That way we don't have two threads on the same topic.",
              "createdAt": "2022-01-14T22:51:15Z",
              "updatedAt": "2022-01-14T22:51:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3jE4",
          "commit": {
            "abbreviatedOid": "0406fa0"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "OK. I've gone through this again and I'm no more convinced than I was the last time.\r\n\r\nI think the basic structure of the nonce as time + random ID is fine,\r\nbut the actual description of how endpoints are supposed to behave is\r\nnot.\r\n\r\nFirst, as a practical matter, if clocks aren't roughly synced, then we\r\nare going to have some serious problems about out of window\r\nrejection. So, I don't think trying to avoid clock sync is\r\nhelpful. Second, I think we can simply not worry about a small rate of\r\nlossage from data too far in the future. IOW,\r\n\r\nI think this leads to a simpler design in which only the leader needs\r\nto keep buffered state outside of its window.\r\n\r\n1. Aggregators are expected to have accurate, synced clocks.\r\n\r\n2. You can't aggregate an event of time T till T + epsilon. This is\r\n   enforced by the helper, which rejects if it sees it. The leader\r\n   needs to wait till T + epsilon + skew where skew is the allowed\r\n   clock skew.\r\n\r\n3. You can't aggregate the block T1 - T2 after T3 - T4 where T3 > T2.\r\n\r\nThis means that the leader buffers data between the end of the last\r\naggregation block and now() + Buffer where Buffer is configurable, but\r\non the order of the minimum aggregation block window. The helper\r\ndoesn't know Buffer because it will never see anything that's out of\r\nwindow. The helper just needs to know when the last aggregation block\r\nended, the data it is currently working on, and the current time and\r\nreject (with an error) anything out of that window or that is\r\nduplicated.\r\n\r\nThis is a more straightforward design and also detect errors, whereas\r\nthe current design just fails silently.\r\n\r\n\r\nIf we want to allow data to be sent directly to the helper. In that\r\ncase, each aggregator needs to buffer data between the end of the\r\nlast aggregation block and now + Buffer(). This means that there\r\nis some possibility of disagreement on both edges (because of\r\nrace conditions and clock sync). This means that we need a way\r\nfor each side to tell the other that there was an error, and we\r\njust need a \"rejected because too early\" or \"rejected because too late\"\r\nerror, just like for any other error. However, with this much buffer,\r\nthis should occur infrequently. Note that this easily handles the\r\ncase where the helper rejects but if the leader does, it may never\r\nlearn there is something the helper accepted.\r\n",
          "createdAt": "2022-01-14T22:09:17Z",
          "updatedAt": "2022-01-14T23:00:19Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "This seems like a recipe for silent failure in case of clock skew.",
              "createdAt": "2022-01-14T22:09:17Z",
              "updatedAt": "2022-01-14T22:51:28Z"
            },
            {
              "originalPosition": 122,
              "body": "This also is a recipe for silent error.",
              "createdAt": "2022-01-14T22:10:26Z",
              "updatedAt": "2022-01-14T22:51:28Z"
            },
            {
              "originalPosition": 171,
              "body": "+1",
              "createdAt": "2022-01-14T22:10:38Z",
              "updatedAt": "2022-01-14T22:51:28Z"
            },
            {
              "originalPosition": 234,
              "body": "This seems much more complicated than simply having sync.",
              "createdAt": "2022-01-14T22:12:25Z",
              "updatedAt": "2022-01-14T22:51:28Z"
            },
            {
              "originalPosition": 238,
              "body": "This just seems unspecified.",
              "createdAt": "2022-01-14T22:13:04Z",
              "updatedAt": "2022-01-14T22:51:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3pEf",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:55:30Z",
          "updatedAt": "2022-01-14T22:55:30Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "@chris-wood supports the idea of having the leader issue an error here. Quote: \r\n> I agree, and I think we need to be clear with the protocol interaction here. I am not supportive of \"ignore behavior is left unspecified.\"\r\n\r\nAs I mentioned above, it may not always be feasible for the leader to check for the error condition on-the-fly. It requires less internal-state coordination to just ingest the report and prune it later. I am fine with a MAY or even a SHOULD here but I don't think it should be a MUST. I'm fine being in the rough, just let me know if you think this absolutely needs to be a MUST and I will change it.",
              "createdAt": "2022-01-14T22:55:30Z",
              "updatedAt": "2022-01-14T22:56:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3pUH",
          "commit": {
            "abbreviatedOid": "9a0f719"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T22:57:44Z",
          "updatedAt": "2022-01-14T22:57:44Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "Sure, I'm operating under the assumption that the leader can detect this immediately upon upload. If the leader defers the check to some point later on, then obviously it can't reply with an error on the fly. A SHOULD seems like it'd work here.",
              "createdAt": "2022-01-14T22:57:44Z",
              "updatedAt": "2022-01-14T22:57:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3qGV",
          "commit": {
            "abbreviatedOid": "9950b87"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T23:04:34Z",
          "updatedAt": "2022-01-14T23:04:34Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "Updated so that the leader SHOULD send an error.",
              "createdAt": "2022-01-14T23:04:34Z",
              "updatedAt": "2022-01-14T23:04:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3qUG",
          "commit": {
            "abbreviatedOid": "9950b87"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T23:06:23Z",
          "updatedAt": "2022-01-14T23:06:24Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "Added text saying the leader SHOULD reply with an error. As I noted in a previous thread, it may not be feasible for the leader to decide if the batch interval has been collected. It's much simpler if all the server has to do is ingest the report at this step and wait to prune it when it's ready to start aggregating.",
              "createdAt": "2022-01-14T23:06:24Z",
              "updatedAt": "2022-01-14T23:06:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3sHX",
          "commit": {
            "abbreviatedOid": "9950b87"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T23:25:56Z",
          "updatedAt": "2022-01-14T23:25:56Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "Yeah I think you're right that failing silently here is a bad idea. There are other ways in which the helper might fail for a given report, and all of these should be handled by relaying an error back to the leader. I have an idea for how to address this in a way that handles all such errors in a uniform manner (see #179 if you're curious), but it would be hard to specify very concretely given the state the document is in. I've updated the text to make sure the helper sends an error here. It's a bit awkward, but it'll do the job for now.",
              "createdAt": "2022-01-14T23:25:56Z",
              "updatedAt": "2022-01-14T23:25:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3sj-",
          "commit": {
            "abbreviatedOid": "0f20c7a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-14T23:31:12Z",
          "updatedAt": "2022-01-14T23:31:12Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "Totally agree here. I've added some text saying that the helper MUST reply with an error in this case rather than its next VDAF message. The text is a bit awkward, but I think it's hard to do much better in the current state of the document. In fact, there are a number of errors that might happen between now and when the helper is ready to reply. In #179 I've tried to solve this problem in a more generic way.",
              "createdAt": "2022-01-14T23:31:12Z",
              "updatedAt": "2022-01-14T23:31:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4y3wBl",
          "commit": {
            "abbreviatedOid": "3f61b0a"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM but can you put in an open issue marker for #180. Just like one line should be fine.",
          "createdAt": "2022-01-15T00:16:41Z",
          "updatedAt": "2022-01-15T00:16:41Z",
          "comments": []
        }
      ]
    },
    {
      "number": 170,
      "id": "PR_kwDOFEJYQs4vn3bS",
      "title": "Simplify overview text about the aggregation function",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/170",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-12-09T16:14:05Z",
      "updatedAt": "2021-12-29T17:45:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "93032f7f3e1ed8cd1d785e68598d680a344c9813",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/vdaf-unspecified-nits",
      "headRefOid": "6c17491f8a165cf60f3652c72904ffd2613e21df",
      "closedAt": "2021-12-09T16:32:29Z",
      "mergedAt": "2021-12-09T16:32:29Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "09f78f242bc708fbc0005c242f9e226058267375"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4xWRpT",
          "commit": {
            "abbreviatedOid": "6c17491"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2021-12-09T16:32:25Z",
          "updatedAt": "2021-12-09T16:32:25Z",
          "comments": []
        }
      ]
    },
    {
      "number": 171,
      "id": "PR_kwDOFEJYQs4vxPY0",
      "title": "Reformat charter.md and ignore more files",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/171",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "No changes to the text (modulo one spelling correction).",
      "createdAt": "2021-12-13T16:17:51Z",
      "updatedAt": "2021-12-29T17:45:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "09f78f242bc708fbc0005c242f9e226058267375",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/charter-edit",
      "headRefOid": "782d21baaef6026d1298584c916445c18f9930da",
      "closedAt": "2021-12-13T16:22:41Z",
      "mergedAt": "2021-12-13T16:22:41Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b497a64adbebb70581832e13e438a9c30dcb63b8"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4xf9pj",
          "commit": {
            "abbreviatedOid": "782d21b"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2021-12-13T16:22:10Z",
          "updatedAt": "2021-12-13T16:22:10Z",
          "comments": []
        }
      ]
    },
    {
      "number": 172,
      "id": "PR_kwDOFEJYQs4wBi-T",
      "title": "Final edits for Roman",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/172",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "PRIV -> PPM\r\nAdd abuse cases language",
      "createdAt": "2021-12-17T22:27:50Z",
      "updatedAt": "2021-12-17T22:35:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b497a64adbebb70581832e13e438a9c30dcb63b8",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "charter_final",
      "headRefOid": "e1e9cebd2515b4a678d5273de818eea61193534d",
      "closedAt": "2021-12-17T22:35:38Z",
      "mergedAt": "2021-12-17T22:35:38Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "4b45f71a6c631db33d23ae21ceb6fcbb9a8a2536"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4xz4QS",
          "commit": {
            "abbreviatedOid": "e1e9ceb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2021-12-17T22:32:03Z",
          "updatedAt": "2021-12-17T22:32:03Z",
          "comments": []
        }
      ]
    },
    {
      "number": 174,
      "id": "PR_kwDOFEJYQs4wYWzU",
      "title": "Have clients upload report shares to helpers",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/174",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #130 (cc/ @hostirosti).\r\nAddresses a bug pointed out in https://github.com/abetterinternet/ppm-specification/issues/183#issuecomment-1016975853 (cc/ @ekr).\r\n\r\nTaking #169 effectively increases the storage requirements for the helper to `O(n)`, where `n` is the number of reports. This PR would increase storage requirements for the helper by a constant factor, but would reduce the leader <-> helper communication overhead by the same factor.\r\n\r\n2022-05-06: Moved back to draft. There is not yet consensus on this change.\r\n2022-01-24: Moved from draft to review. The consensus reached on [this thread](https://mailarchive.ietf.org/arch/msg/ppm/sCJ1oKR0KMHlZ65FCPUbw3FrGxY/) seems to be to take this change.\r\n2021-12-29: This PR is currently marked as \"Draft\" because I don't yet intend for it to be merged. Rather the goal of this PR is to make the discussion about this trade-off more concrete.",
      "createdAt": "2021-12-29T21:56:16Z",
      "updatedAt": "2023-03-06T19:16:09Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "557b887eb02c641608e9e5eaab34615744fc0c57",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/split-upload",
      "headRefOid": "18baafb6dfcd7fdd4f3da9b4a234a58797320ee9",
      "closedAt": "2022-07-28T16:03:35Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "We can revisit this later, but this is pretty stale and it woiuld be nice to clear the decks. Closing.",
          "createdAt": "2022-07-28T16:03:35Z",
          "updatedAt": "2022-07-28T16:03:35Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4zV4L2",
          "commit": {
            "abbreviatedOid": "458bf8d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-24T18:39:12Z",
          "updatedAt": "2022-01-24T19:18:56Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "Seems like this should replace the diagram on line 232.",
              "createdAt": "2022-01-24T18:39:12Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 46,
              "body": "```suggestion\r\n   phase of the protocol as requested by the collector.\r\n```",
              "createdAt": "2022-01-24T18:39:28Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 61,
              "body": "```suggestion\r\nreport shares are handled by an intermediary, that intermediary cannot recover\r\n```",
              "createdAt": "2022-01-24T18:39:50Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 69,
              "body": "nit: not particularly clear what \"them\" refers to\r\n```suggestion\r\nThe leader orchestrates the process of verifying reports (see\r\n```",
              "createdAt": "2022-01-24T18:40:51Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 73,
              "body": "nit: It seems like what matters isn't having all the reports in a batch in hand but rather receiving the aggregation parameter from the collector. Is it possible to define a VDAF where verification of an individual report depends on other reports?",
              "createdAt": "2022-01-24T18:42:37Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 119,
              "body": "```suggestion\r\nClients periodically upload report shares to the aggregators. This involves two\r\nHTTP requests, one to retrieve the aggregator's HPKE configuration and\r\nanother to upload its report share.\r\n```",
              "createdAt": "2022-01-24T18:44:10Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 129,
              "body": "```suggestion\r\nBefore the client can upload a report share to an aggregator, it must first\r\nobtain the aggregator's public key. This is retrieved by sending a request to\r\n```",
              "createdAt": "2022-01-24T18:44:40Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 143,
              "body": "```suggestion\r\nReport shares are usually uploaded by clients directly, but MAY instead be\r\n```",
              "createdAt": "2022-01-24T18:45:09Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 145,
              "body": "Do we have an issue for defining an upload message with multiple reports in it, to allow intermediaries to efficiently submit many reports at once? I think we should do that independently from this change, though.",
              "createdAt": "2022-01-24T18:47:24Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 145,
              "body": "I'm not sure \"on behalf of the collector\" is helpful here. I think what you have in mind is a case where the intermediary and the collector are the same real world entity, but I can imagine deployments where an intermediary is used for reasons having nothing to do with the collector.",
              "createdAt": "2022-01-24T18:49:39Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 170,
              "body": "```suggestion\r\n  {{anti-replay}}.) Each share of a report MUST have the same nonce.\r\n```",
              "createdAt": "2022-01-24T18:51:00Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 227,
              "body": "```suggestion\r\nTo prevent reports from being misused, the aggregators filter out replayed or\r\n```",
              "createdAt": "2022-01-24T18:54:11Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 233,
              "body": "The reason we had upload extensions in the first place was so that they could be tunnelled through the leader into the helper. Now that we do direct uploads, can we remove these? The obvious remaining use case is tunnelling extensions through a client proxy/intermediary/batching client, but if deployments use OHTTP as we recommend, then PPM shouldn't need to specify anything as things like HTTP request headers will get securely relayed.",
              "createdAt": "2022-01-24T19:00:04Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 278,
              "body": "Now that the helper is responsible for its own nonce, report and aggregate accumulator storage, do we need aggregate requests to not span batch intervals anymore? I'm not sure but I think the reason we had this constraint on aggregate requests was so that the state blob could be \"sharded\" by batch interval and allow parallelism in the helper. Now, though, we might expect the helper to have its own storage where it keeps track of aggregate accumulators for several different batch intervals.",
              "createdAt": "2022-01-24T19:06:27Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 341,
              "body": "```suggestion\r\nsub-response rather than its next VDAF message.\r\n```\r\nTo make it clear that filtering out an individual report does not fail the overall aggregate transaction.",
              "createdAt": "2022-01-24T19:08:33Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 389,
              "body": "nit: \"extracting its aggregate share from its state\" implies the helper will have an aggregate share ready to go in its state, which won't be true for `poplar1`. We could just say \"The helper responds to this request with its aggregate share, encrypted under the collector's HPKE public key.\"",
              "createdAt": "2022-01-24T19:10:30Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 418,
              "body": "```suggestion\r\nerror for the leader to issue any more aggregate requests for reports in the\r\n```",
              "createdAt": "2022-01-24T19:11:06Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 420,
              "body": "It's an error for the leader to issue more than `max_batch_lifetime` requests for a given batch interval, right? As written this states the leader can issue exactly one request.",
              "createdAt": "2022-01-24T19:12:14Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 463,
              "body": "```suggestion\r\n  been collected, then the aggregator MUST ignore it. This prevents additional\r\n  reports from being aggregated after their batch has already been collected.\r\n```",
              "createdAt": "2022-01-24T19:13:18Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            },
            {
              "originalPosition": 467,
              "body": "I think but am not certain that this needs to reference the task's `max_batch_lifetime` rather than allowing exactly once, because successive collector queries will change the aggregation parameter (e.g., different prefixes in `poplar1`).",
              "createdAt": "2022-01-24T19:15:48Z",
              "updatedAt": "2022-01-24T19:18:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zXQaR",
          "commit": {
            "abbreviatedOid": "6109832"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T02:58:23Z",
          "updatedAt": "2022-01-25T04:33:37Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "That's the intent, but they show slightly different things so I thought it should be discussed. Would you prefer this diagram or something like the one above?",
              "createdAt": "2022-01-25T02:58:23Z",
              "updatedAt": "2022-01-25T04:33:37Z"
            },
            {
              "originalPosition": 73,
              "body": "No that's not possible. Yeah, it's the aggregation parameter that matters, but here (in the overview) we have so far avoided talking about the aggregation parameter.\r\n\r\nNote that this change isn't new to this PR.",
              "createdAt": "2022-01-25T04:20:37Z",
              "updatedAt": "2022-01-25T04:33:37Z"
            },
            {
              "originalPosition": 145,
              "body": "IMO this is out-of-scope. The way we should try to spell this PR is to allow either clients to upload report shares directly or allow report shares to be uploaded by an intermediary. How the report shares get to the intermediary isn't relevant for us.",
              "createdAt": "2022-01-25T04:22:23Z",
              "updatedAt": "2022-01-25T04:33:37Z"
            },
            {
              "originalPosition": 145,
              "body": "Good point, that's just one possible case. I dropped \"on behalf of the collector\".",
              "createdAt": "2022-01-25T04:23:26Z",
              "updatedAt": "2022-01-25T04:33:38Z"
            },
            {
              "originalPosition": 233,
              "body": "#89 will likely make use of upload extensions, hence we'll need to keep them around.",
              "createdAt": "2022-01-25T04:25:59Z",
              "updatedAt": "2022-01-25T04:33:38Z"
            },
            {
              "originalPosition": 278,
              "body": "Yeah, it may not be strictly necessary. However I think we should keep this as-is for now in order to keep this PR minimal. Plus, it may be helpful for something we're not seeing right now.",
              "createdAt": "2022-01-25T04:27:15Z",
              "updatedAt": "2022-01-25T04:33:38Z"
            },
            {
              "originalPosition": 389,
              "body": "Actually at this point it will have computed an aggregate share. However I took your suggestion because your text is a lot clearer.",
              "createdAt": "2022-01-25T04:29:22Z",
              "updatedAt": "2022-01-25T04:33:38Z"
            },
            {
              "originalPosition": 467,
              "body": "I don't think want to change the batch after it has been collected with *any* aggregation parameter. Imagine, for example, that the collector requests the aggregate for candidate prefixes [\"ab\"]. Then add another measurement and re-run with candidate prefixes [\"abc\"]. Then I know immediately whether the additional measurement was prefixed by \"abc\".",
              "createdAt": "2022-01-25T04:33:21Z",
              "updatedAt": "2022-01-25T04:33:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zbFi2",
          "commit": {
            "abbreviatedOid": "2634b20"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Thanks, @cjpatton! I have a couple questions and comments. The biggest unknown for me is the new set of report processing requirements, but I think we can work through that.",
          "createdAt": "2022-01-25T19:04:12Z",
          "updatedAt": "2022-01-25T19:34:33Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "Agree with Tim here -- I'm not seeing the value of keeping both.",
              "createdAt": "2022-01-25T19:04:12Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 107,
              "body": "Is this requirement necessary anymore? It seems like the client doesn't need to care which aggregator is the leader if both are treated as separate aggregators to store shares.",
              "createdAt": "2022-01-25T19:06:11Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 129,
              "body": "```suggestion\r\nBefore the client can upload a report share to an aggregator, it first fetches\r\nthe aggregator's public key. This is done by sending a request to\r\n```",
              "createdAt": "2022-01-25T19:11:21Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 233,
              "body": "We may also need general extensibility outside of new VDAFs in the future -- a lesson learned from TLS. ",
              "createdAt": "2022-01-25T19:13:30Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 145,
              "body": "This seems to preclude sending both reports to the same endpoint, even though they're encrypted under separate aggregator public keys. We could address this by allowing the task configuration to specify the upload endpoint(s) _separately_ from the aggregation endpoints. Would that work?",
              "createdAt": "2022-01-25T19:21:13Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 278,
              "body": "Agreed. Let's note this and revisit in a followup change. (Good catch, Tim!)",
              "createdAt": "2022-01-25T19:23:23Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 318,
              "body": "Hmm, now that the nonce<>client uniqueness is no longer enforced by a single party, what happens if, say, two benign clients happen to generate colliding nonce values, and each aggregator picks a different one while filtering for anti-replay? We could maybe increase the width of the nonce random value to hedge against this collision, but it's not clear to me that's sufficient. ",
              "createdAt": "2022-01-25T19:28:47Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            },
            {
              "originalPosition": 470,
              "body": "This new requirement is pretty subtle to me. Can we drop an open issue here to further investigate if this is indeed the correct thing to do?",
              "createdAt": "2022-01-25T19:33:37Z",
              "updatedAt": "2022-01-25T19:34:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zb4IY",
          "commit": {
            "abbreviatedOid": "b5ba4d0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T22:42:39Z",
          "updatedAt": "2022-01-25T22:42:39Z",
          "comments": [
            {
              "originalPosition": 420,
              "body": "Duplicate of https://github.com/abetterinternet/ppm-specification/pull/174#discussion_r791074329?",
              "createdAt": "2022-01-25T22:42:39Z",
              "updatedAt": "2022-01-25T22:42:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zb57S",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T22:48:17Z",
          "updatedAt": "2022-01-25T23:17:47Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "Fair point, although depending on the VDAF, the leader may need to know if it's a leader or a helper. This is true of prio3 and poplar1, for instance. In both of these schemes there are big, \"uncompressed\" shares that are sent to the leader. The helpers only get short, \"compressed\" shares.\r\n\r\nThere must be a better way to spell this requirement.",
              "createdAt": "2022-01-25T22:48:17Z",
              "updatedAt": "2022-01-25T23:17:47Z"
            },
            {
              "originalPosition": 145,
              "body": "If an ingestor is doing the uploading, it still needs to upload to the individual aggregator endpoints. The intention of this PR is that the client's interaction with the ingestor is out-of-scope. Perhaps this isn't clear enough? Or maybe you think this should be in-scope? ",
              "createdAt": "2022-01-25T22:53:41Z",
              "updatedAt": "2022-01-25T23:17:47Z"
            },
            {
              "originalPosition": 318,
              "body": "Oh interesting! To be clear, before #169 we still had both aggregators enforce anti-replay. The difference is that we no longer of single party pick the set of reports that are processed. Nice catch.\r\n\r\nThis basically amounts to an attack on correctness. What will happen for prio3 or poplar1 is that the aggregators will derive different verification randomness, which will cause them to deem the report invalid and reject it (with high probability).\r\n\r\nHowever, where this edge case *would* matter is if a VDAF were used that didn't have any verifiability. (We should probably not get in the habbit of calling such a scheme a VDAF: see https://github.com/cjpatton/vdaf/issues/20.) This was already requested in #45, so it's within the realm of possibility.\r\n\r\nNote that a collision fairly likely, since the nonce only contains 64 random bits. (The remaining bits are a timestamp.) We may consider bumping this to 128 just to be on the safe side.\r\n\r\nHow about addressing this by leaving an OPEN ISSUE in the text? It would reference #45 and we would leave a note there to make sure we keep track of this.\r\n\r\n",
              "createdAt": "2022-01-25T23:16:01Z",
              "updatedAt": "2022-01-25T23:17:47Z"
            },
            {
              "originalPosition": 470,
              "body": "I agree it's subtle. I'd be good with that outcome, although it was already discussed here in a different context: https://github.com/abetterinternet/ppm-specification/issues/183#issuecomment-1016978627.",
              "createdAt": "2022-01-25T23:17:34Z",
              "updatedAt": "2022-01-25T23:17:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zb_wg",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T23:24:37Z",
          "updatedAt": "2022-01-25T23:24:38Z",
          "comments": [
            {
              "originalPosition": 470,
              "body": "Let's at least leave an OPEN ISSUE referencing #183, then, since that's still open. (I will review #183 more carefully soon.)",
              "createdAt": "2022-01-25T23:24:38Z",
              "updatedAt": "2022-01-25T23:24:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zb_96",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T23:26:09Z",
          "updatedAt": "2022-01-25T23:26:09Z",
          "comments": [
            {
              "originalPosition": 318,
              "body": "That's a fine resolution, yep. Let's also note that this type of collision will cause verification failures with overwhelming probability?",
              "createdAt": "2022-01-25T23:26:09Z",
              "updatedAt": "2022-01-25T23:26:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcAQz",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T23:28:19Z",
          "updatedAt": "2022-01-25T23:28:20Z",
          "comments": [
            {
              "originalPosition": 145,
              "body": "Well, consider the case where there is an ingestor service. Certainly, that ingestor service can't be the _client_ in PPM, since that would require it see inputs and then encrypt them under the aggregator public keys. Rather, the ingestor sees encrypted things that it then relays onward to the aggregator endpoints. What we're specifying here is the client behavior, and I think we need to make sure that we the client can send both its encrypted shares to a single ingestor if needed. Right now that seems prohibited?",
              "createdAt": "2022-01-25T23:28:20Z",
              "updatedAt": "2022-01-25T23:28:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcAZB",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-25T23:29:20Z",
          "updatedAt": "2022-01-25T23:29:21Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "Oh, yeah, that was totally not clear to me! How is this expressed in the VDAF draft?",
              "createdAt": "2022-01-25T23:29:20Z",
              "updatedAt": "2022-01-25T23:29:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcGX3",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T00:17:58Z",
          "updatedAt": "2022-01-26T00:17:58Z",
          "comments": [
            {
              "originalPosition": 73,
              "body": "OK -- there's enough to chew on in this PR that we don't have to wordsmith this right now.",
              "createdAt": "2022-01-26T00:17:58Z",
              "updatedAt": "2022-01-26T00:17:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcGiL",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T00:19:23Z",
          "updatedAt": "2022-01-26T00:19:24Z",
          "comments": [
            {
              "originalPosition": 318,
              "body": "Will do.",
              "createdAt": "2022-01-26T00:19:24Z",
              "updatedAt": "2022-01-26T00:19:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcG2-",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T00:22:18Z",
          "updatedAt": "2022-01-26T00:22:19Z",
          "comments": [
            {
              "originalPosition": 145,
              "body": "This PR, sure, but I'm not the only person left wondering whether or how intermediates can batch up reports: ekr asked about this on the PPM list. Anyway, I filed #188 so we don't lose track of the idea.",
              "createdAt": "2022-01-26T00:22:19Z",
              "updatedAt": "2022-01-26T00:22:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcHt2",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T00:30:02Z",
          "updatedAt": "2022-01-26T00:30:03Z",
          "comments": [
            {
              "originalPosition": 233,
              "body": "Well, I don't want to open the can of worms of client attestation in the margins of this PR, but I'm happy to accept TLS as a precedent.",
              "createdAt": "2022-01-26T00:30:03Z",
              "updatedAt": "2022-01-26T00:30:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcI_x",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T00:42:11Z",
          "updatedAt": "2022-01-26T00:42:11Z",
          "comments": [
            {
              "originalPosition": 467,
              "body": "Then I think there's something I must not be understanding about how `max_batch_lifetime` works, or how PPM is meant to solve t-heavy-hitters, which I thought involved the collector sending a series of collect requests with longer candidate prefixes each time. But that doesn't need to be solved in this PR.",
              "createdAt": "2022-01-26T00:42:11Z",
              "updatedAt": "2022-01-26T00:42:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zetuV",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T15:17:06Z",
          "updatedAt": "2022-01-26T15:17:07Z",
          "comments": [
            {
              "originalPosition": 467,
              "body": "You understand correctly how PPM is supposed to solve heavy hitters. All `max_batch_lifetime` is supposed to say is that you can't make any more collect requests than `max_batch_lifetime`. Otherwise it doesn't make any restrictions on the aggregation parameter. In particular, Poplar can be implemented as you describe.\r\n\r\nAll that this line is saying is that you can't change the set of reports in a batch after you've collected it. Otherwise, this could lead to a privacy violation, regardless of the VDAF in use.",
              "createdAt": "2022-01-26T15:17:06Z",
              "updatedAt": "2022-01-26T15:17:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zeuSC",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T15:18:44Z",
          "updatedAt": "2022-01-26T15:18:44Z",
          "comments": [
            {
              "originalPosition": 107,
              "body": "It's not explicit in the syntax, but for schemes that needs to differentiate aggregator roles, they can do so via the verification parameter. For example, see https://cjpatton.github.io/vdaf/draft-patton-cfrg-vdaf.html#name-setup-2",
              "createdAt": "2022-01-26T15:18:44Z",
              "updatedAt": "2022-01-26T15:18:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zevBf",
          "commit": {
            "abbreviatedOid": "18baafb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T15:20:50Z",
          "updatedAt": "2022-01-26T15:20:50Z",
          "comments": [
            {
              "originalPosition": 145,
              "body": "Ok I see. Then I think this section would need to be clear that it specifies both the behavior of the client generating report shares and the entity (either the client or someone else) that uploads shares to the aggregators. Would you prefer these be different sections perhaps?",
              "createdAt": "2022-01-26T15:20:50Z",
              "updatedAt": "2022-01-26T15:20:50Z"
            }
          ]
        }
      ]
    },
    {
      "number": 175,
      "id": "PR_kwDOFEJYQs4wYddx",
      "title": "nits",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/175",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2021-12-29T23:51:36Z",
      "updatedAt": "2021-12-30T02:09:23Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "4b45f71a6c631db33d23ae21ceb6fcbb9a8a2536",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/nit",
      "headRefOid": "6884897b8ebc25b5f1a1b6a8e297666c8682b9c9",
      "closedAt": "2021-12-29T23:53:32Z",
      "mergedAt": "2021-12-29T23:53:32Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "5b46fbb96828ab4edf6222dca1cfd3e58d2dfab9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4yKrc7",
          "commit": {
            "abbreviatedOid": "6884897"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2021-12-29T23:53:28Z",
          "updatedAt": "2021-12-29T23:53:28Z",
          "comments": []
        }
      ]
    },
    {
      "number": 177,
      "id": "PR_kwDOFEJYQs4wiBQ9",
      "title": "Rename \"Heavy Hitters\" to \"Poplar\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/177",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #176.\r\n\r\nThe corresponding change in the VDAF draft is here: https://github.com/cjpatton/vdaf/pull/16",
      "createdAt": "2022-01-04T21:46:41Z",
      "updatedAt": "2022-09-16T00:30:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5b46fbb96828ab4edf6222dca1cfd3e58d2dfab9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/issue176",
      "headRefOid": "894cfdda72ef0ba4622f2bff637484793d1091d3",
      "closedAt": "2022-01-06T18:48:37Z",
      "mergedAt": "2022-01-06T18:48:37Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "50af0f5d48b849ba7aeca1e31b44f55eab6d82f7"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "SGTM.\n\nOn Tue, Jan 4, 2022 at 3:33 PM Christopher Patton ***@***.***>\nwrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In draft-gpew-priv-ppm.md\n> <https://github.com/abetterinternet/ppm-specification/pull/177#discussion_r778455672>\n> :\n>\n> > @@ -205,10 +205,11 @@ schemes that implement the VDAF interface specified in\n>    etc. This class of VDAFs is based on Prio [CGB17] and includes improvements\n>    described in [BBCGGI19].\n>\n> -* `hits`, which allows for finding the most common strings among a collection\n> +* `pops`, which allows for finding the most popular strings among a collection\n>\n> I like your thinking @eriktaubeneck <https://github.com/eriktaubeneck>\n> and @csharrison <https://github.com/csharrison> ! I vote for poplar1.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/abetterinternet/ppm-specification/pull/177#discussion_r778455672>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIPLILIXK7RLQXNMU6LNODUUN7UZANCNFSM5LINY26Q>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because your review was requested.Message ID:\n> ***@***.***>\n>\n",
          "createdAt": "2022-01-04T23:34:37Z",
          "updatedAt": "2022-01-04T23:34:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "s/pops/poplar1/ as discussed. @tgeoghegan and @ekr can I get you to approve this?",
          "createdAt": "2022-01-06T18:39:54Z",
          "updatedAt": "2022-01-06T18:39:54Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "s/Hits/Poplar/. Thanks @tgeoghegan !",
          "createdAt": "2022-01-06T18:45:41Z",
          "updatedAt": "2022-01-06T18:45:41Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4yTwrK",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-01-04T21:54:40Z",
          "updatedAt": "2022-01-04T21:54:44Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "As @bifurcation noted in https://github.com/cjpatton/vdaf/pull/16, I think `poplar` is nicer.",
              "createdAt": "2022-01-04T21:54:40Z",
              "updatedAt": "2022-01-04T21:54:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yTxQU",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-04T21:58:08Z",
          "updatedAt": "2022-01-04T21:58:08Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "\"Poplar\" is the name of the full-blown heavy hitters protocol, whereas this name is supposed to refer to the underlying VDAF. I'd like to avoid conflating the two.",
              "createdAt": "2022-01-04T21:58:08Z",
              "updatedAt": "2022-01-04T21:58:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yTyQI",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-04T22:04:08Z",
          "updatedAt": "2022-01-04T22:04:09Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I guess we're just voting now, but I would prefer poplar.",
              "createdAt": "2022-01-04T22:04:09Z",
              "updatedAt": "2022-01-04T22:04:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yT0ND",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-04T22:16:26Z",
          "updatedAt": "2022-01-04T22:16:27Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Well then, this just got interesting. I wonder if others have an opinion or ideas for alternatives. I don't care much about the name for the VDAF, my only criterion is that it is distinct from \"Poplar\". A more verbose possibility is `poplar-vdaf`. \r\ncc/ @csharrison @eriktaubeneck @martinthomson @stpeter @bifurcation @BranLwyd ",
              "createdAt": "2022-01-04T22:16:26Z",
              "updatedAt": "2022-01-04T22:16:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yT7mu",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-04T23:07:10Z",
          "updatedAt": "2022-01-04T23:07:10Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "You could align with the naming scheme for the Prio VDAF and call it `poplar1` :) Other than that I don't have a strong opinion.",
              "createdAt": "2022-01-04T23:07:10Z",
              "updatedAt": "2022-01-04T23:07:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yT-aM",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "eriktaubeneck",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-04T23:30:53Z",
          "updatedAt": "2022-01-04T23:30:53Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Keeping it consistent with the naming scheme for Prio seems positive, and a version number doesn't seem like a terrible addition as it seems likely the VDAF may evolve like the prio VDAF did.",
              "createdAt": "2022-01-04T23:30:53Z",
              "updatedAt": "2022-01-04T23:30:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yT-pP",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-04T23:33:21Z",
          "updatedAt": "2022-01-04T23:33:21Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I like your thinking @eriktaubeneck  and @csharrison ! I vote for `poplar1`. ",
              "createdAt": "2022-01-04T23:33:21Z",
              "updatedAt": "2022-01-04T23:33:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4yUonb",
          "commit": {
            "abbreviatedOid": "d95249f"
          },
          "author": "henrycg",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks good! Thanks.",
          "createdAt": "2022-01-05T07:04:18Z",
          "updatedAt": "2022-01-05T07:04:18Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4ybArC",
          "commit": {
            "abbreviatedOid": "3e12842"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-06T18:40:12Z",
          "updatedAt": "2022-01-06T18:40:13Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "s/pops/poplar1/.",
              "createdAt": "2022-01-06T18:40:12Z",
              "updatedAt": "2022-01-06T18:40:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4ybA6d",
          "commit": {
            "abbreviatedOid": "3e12842"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2022-01-06T18:41:20Z",
          "updatedAt": "2022-01-06T18:41:20Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4ybBYO",
          "commit": {
            "abbreviatedOid": "3e12842"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This change is good and consistent with the corresponding change to vdaf. However I see there are two other occurrences of \"Hits\" in the doc, on lines 1335 and 1356:\r\n\r\n```\r\nThe PPM parameters also specify the maximum number of times a report can be\r\nused. Some protocols, such as *Hits*, require reports to be used in multiple\r\nbatches spanning multiple collect requests.\r\n<...>\r\nMost PPM protocols, including Prio and *Hits*, are robust against malicious\r\nclients, but are not robust against malicious servers.\r\n```\r\nI think we could change those to \"Poplar\"",
          "createdAt": "2022-01-06T18:43:30Z",
          "updatedAt": "2022-01-06T18:43:30Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4ybCDr",
          "commit": {
            "abbreviatedOid": "894cfdd"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-01-06T18:46:50Z",
          "updatedAt": "2022-01-06T18:46:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 178,
      "id": "PR_kwDOFEJYQs4wn3VZ",
      "title": "Poplar, with one 'o'",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/178",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-01-06T18:51:51Z",
      "updatedAt": "2022-09-16T00:30:02Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "50af0f5d48b849ba7aeca1e31b44f55eab6d82f7",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/pooplar",
      "headRefOid": "550a583bb43ba48dacb0cc138b380972c9e06153",
      "closedAt": "2022-01-06T18:57:32Z",
      "mergedAt": "2022-01-06T18:57:32Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "48bc21ce7f1c7e446618bfe53cd9470511e130bc"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4ybESC",
          "commit": {
            "abbreviatedOid": "550a583"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-01-06T18:57:25Z",
          "updatedAt": "2022-01-06T18:57:25Z",
          "comments": []
        }
      ]
    },
    {
      "number": 179,
      "id": "PR_kwDOFEJYQs4xD_Nt",
      "title": "DO NOT MERGE Proposed interop target",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/179",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The current version of the protocol cannot be implemented in the sense\r\nthat it does not specify a mapping from the VDAF execution to HTTP\r\nrequests. The goal of this PR is to flesh out one possible mapping so\r\nthat we can begin work on implementation, with the understanding of\r\ncourse that the overall shape and the details are still subject to\r\nchange.\r\n\r\nThe design presumes we have reached consensus on a variety of open\r\nissues, all of which still need to be discussed. The purpose of this PR\r\nis *not* to facilitate these discussions, but instead to ensure sure\r\nthere is a solution to each issue that we're confident we can implement.\r\nThese issues include:\r\n\r\n* PR #169 will be merged, i.e., the anti-replay mechanism will be\r\nrelaxed to require the helper to store nonce sets.\r\n\r\n* PR #174 will not be merged, i.e., clients will continue to upload\r\nreports to the leader (and not the helpers). However, if we end up\r\nchoosing to take this PR, it shouldn't be too bad to change course.\r\n\r\n* Issue #45 will be resolved by adding support for VDAFs w/o\r\nverifiability. (We should probably call these \"DAFs\".)\r\n\r\n* Issue #68 will be resolved by adding support for multiple helpers (for\r\napplicable VDAFs). We don't do this quite yet, but the aggregate flow is\r\nmeant to be easily extendable to accommodate this.\r\n\r\n* Messages sent between aggregators need to be authenticated.\r\n\r\nWe have an answer for each of these, but we will discuss and merge PRs\r\nfor each of them separately.\r\n\r\n**Tips for reviewing:** Rather than look at the diff, I would read the\r\ndocument from /Work begins here/ to /Work ends here/. There is a big new\r\nsection in the middle, and GH didn't do a very good job of showing the\r\nchange.",
      "createdAt": "2022-01-14T22:07:24Z",
      "updatedAt": "2022-09-16T00:31:26Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c7ad24e2ea42dc9d3b125cb8228aaa4384ccc8b5",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "headRefOid": "79462d9f8703f908b6d4d21d9297cffb626b3ce2",
      "closedAt": "2022-05-11T14:25:27Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton @tgeoghegan, I believe we can now close this. Thanks for driving this work and teeing things up to land in `main`!",
          "createdAt": "2022-05-11T11:11:01Z",
          "updatedAt": "2022-05-11T11:11:01Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4zLuz0",
          "commit": {
            "abbreviatedOid": "7a76a72"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Reviewed! I have three categories of comments here:\r\n\r\n- trivial typos (hopefully these are self-evident)\r\n- editorial remarks that don't affect the viability of these changes as an implementable interop target (i.e. we can discuss them if and when we merge this to main) (these are tagged with \"EDITORIAL\")\r\n- actual discussion points\r\n\r\nI think we only need to dig into the last category immediately.",
          "createdAt": "2022-01-20T19:29:29Z",
          "updatedAt": "2022-01-21T04:12:59Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "EDITORIAL\r\n```suggestion\r\n* `vdaf_verify_param`: The aggregator's VDAF verification parameter output by\r\n  the setup algorithm computed jointly by the aggregators before the start of the \r\n  PPM protocol {{?I-D.draft-cfrg-patton-vdaf}}).\r\n```\r\nI keep having to remind myself that the verify param is the result of a multiparty computation separate from the one being standardized in PPM so I think it's helpful to accentuate that here.",
              "createdAt": "2022-01-20T19:29:29Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 133,
              "body": "The endpoint `[leader]` is already understood to be the endpoint for a task's leader. What does the extra `/leader` path component achieve?",
              "createdAt": "2022-01-20T23:05:54Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 165,
              "body": "EDITORIAL\r\n```suggestion\r\n  opaque context<1..2^16-1>;\r\n  opaque ciphertext<1..2^16-1>;\r\n```\r\nThis is unrelated to your change but I feel like `enc` is ambiguous since it could be an abbreviation of either \"encapsulated context\" or \"encrypted text\".",
              "createdAt": "2022-01-20T23:09:49Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 165,
              "body": "EDITORIAL\r\nI think it's a good idea to generalize the `HpkeCiphertext` structure and use it in several places. We should also hoist it up to the common definitions section with `Duration`.",
              "createdAt": "2022-01-20T23:11:11Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 184,
              "body": "EDITORIAL\r\nWe should explicitly state that the client needs to create a _new_ HPKE context for each report it uploads. When I first implemented this in `ppm-prototype`, I figured that since all the parameters used in the context generation are fixed for a given task, a client could re-use the same HPKE context for multiple messages. But of course HPKE includes a [message sequence number](https://www.ietf.org/archive/id/draft-irtf-cfrg-hpke-12.html#name-encryption-and-decryption) in its AEAD parameters so you can't do that without keeping track of the sequence number on both sides.\r\n\r\nAnyway, I think we can spare implementors the little journey of learning I went on by explicitly telling them to make a new HPKE context every time they upload a report.",
              "createdAt": "2022-01-20T23:23:39Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 190,
              "body": "EDITORIAL\r\nI'm not crazy about the `0x03` notation because it's not as clear as it could be that what should be concatenated into the context string is the 1 byte wide integer 3, not a 4 or 2 byte integer, nor the string \"0x03\". But that's orthogonal to what this PR is about and I don't have a better suggestion.",
              "createdAt": "2022-01-21T00:05:51Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 226,
              "body": "```suggestion\r\nindicated by `HpkeCiphertext.config_id`, with status 400 and an error of\r\n```",
              "createdAt": "2022-01-21T00:08:29Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 242,
              "body": "```suggestion\r\nalert the client with error \"staleReport\".\r\n```\r\nAlso the table of error types should be updated to include this.",
              "createdAt": "2022-01-21T00:09:58Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 240,
              "body": "We've talked about this elsewhere, but if we defined a state machine for reports and batch intervals, then we could rephrase stuff like this as \"the leader ignores any report whose nonce contains a timestamp that falls in a batch interval that is in the `COLLECTED` state.\" That's not needed in this change though.",
              "createdAt": "2022-01-21T00:11:34Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 254,
              "body": "Given that helpers must now store the full set of observed report nonces, is this true anymore? IIRC the problem with reports from the future was that it would advance the helper's last seen nonce and cause it to reject reports, but that's not the case anymore.",
              "createdAt": "2022-01-21T00:14:43Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 267,
              "body": "```suggestion\r\nwith multiple sets of reports in parallel. To aggregate a set of reports, the\r\n```",
              "createdAt": "2022-01-21T00:15:23Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 268,
              "body": "```suggestion\r\nleader sends a request to each helper containing those report shares. Each helper\r\n```",
              "createdAt": "2022-01-21T00:15:33Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 283,
              "body": "EDITORIAL\r\n```suggestion\r\n* Aggregating a set of reports -- especially validating them -- may require\r\n```\r\nI feel we should use more specific verbs than \"processing\" where possible",
              "createdAt": "2022-01-21T00:18:04Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 309,
              "body": "Some discussion of helper state scope and parallelism is in #150. IMO now that we have accepted that helpers have to persistently store nonces, we should make them persistently store reports, too, and axe the helper state. I think it would simplify leader implementations and the protocol text considerably.",
              "createdAt": "2022-01-21T00:23:23Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 322,
              "body": "Should we have `AggregateFinishReq` for symmetry? Such a message could be a good way to implement #141",
              "createdAt": "2022-01-21T00:24:39Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 361,
              "body": "EDITORIAL\r\n```suggestion\r\nshares\", one for each aggregator besides itself:\r\n```\r\nor it could say \"one for each helper\".",
              "createdAt": "2022-01-21T00:25:49Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 389,
              "body": "```suggestion\r\nIn order to aggregate its report share, an aggregator must first decrypt the\r\ninput share and then interact with the other aggregators. This involves executing the\r\n```",
              "createdAt": "2022-01-21T00:32:42Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 391,
              "body": "```suggestion\r\neach aggregator recovers and verifies the validity of its output share. Output\r\n```\r\n",
              "createdAt": "2022-01-21T00:35:05Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 415,
              "body": "When rendering this to a .txt, we get\r\n```\r\n[CP: To implement.] ~~~ struct { Nonce nonce; TransitionType\r\n   tran_type; select (Transition.tran_type) { case continued: opaque\r\n   payload<0..2^16-1>; // VDAF message case finished: Empty; case\r\n   failed: TransitionError; } } Transition; ~~~\r\n```\r\nand I _think_ it's because there's no newline.\r\n```suggestion\r\n[CP: To implement.]\r\n\r\n~~~\r\nstruct {\r\n```\r\n",
              "createdAt": "2022-01-21T00:36:55Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 397,
              "body": "EDITORIAL\r\nnitty nit: this implies that the entire helper or leader is in one of these states, but in fact a server will have many of these state machines running concurrently, one for each report being aggregated. Maybe:\r\n\r\n```suggestion\r\n{{prep-leader}} and {{prep-helper}} specify the state machines of a report in the leader\r\nand helper respectively. Both state machines consist of the same five states:\r\n```",
              "createdAt": "2022-01-21T00:41:45Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 475,
              "body": "AFAICT the helper would never receive `vdaf-prep-init-error` because if the leader errors out during prep, it just won't send that `ReportShare` to helper. Would the leader do anything differently if it received `vdaf-prep-init-error` or `vdaf-prep-next-error`? I think it just goes from `WAITING` to `FAILED` in either case.\r\n\r\nMore generally, I think we should avoid defining error codes unless we expect the recipient of the error code to do something specific with them, because otherwise we are just introducing ways that implementations can deviate from the protocol. If an error code is informative, then it could just as easily go into an opaque error message to get logged and observed by humans. ",
              "createdAt": "2022-01-21T00:45:21Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 499,
              "body": "```suggestion\r\n* If the report has been collected at least once, then the aggregator fails with error\r\n```",
              "createdAt": "2022-01-21T01:12:41Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 535,
              "body": "EDITORIAL:\r\n```suggestion\r\ninput_share = context.Open(ReportShare.nonce || ReportShare.extensions,\r\n                           ReportShare.encrypted_input_share.payload)\r\n```\r\nFor consistency with the specification of how the client uses `context.Seal`",
              "createdAt": "2022-01-21T01:17:02Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 545,
              "body": "EDITORIAL:\r\n```suggestion\r\nVariable `server_role` is the Role of the intended recipient,\r\n```\r\nThe `Role` enum already specifies the byte value, right?",
              "createdAt": "2022-01-21T01:17:53Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 549,
              "body": "EDITORIAL:\r\nWe don't introduce the aggregation parameter until the definition of `AggregateInitReq`, a few hundred lines below. A forward reference and/or some explanatory text explaining where it comes from would be helpful. As a reader, I have to admit I have trouble keeping the verification and aggregation parameters straight, so inline reminders of their provenance are helpful.",
              "createdAt": "2022-01-21T01:21:17Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 569,
              "body": "EDITORIAL:\r\nIf the aggregator that fails during preparation is the leader, what happens? Does it just never instruct the helper to aggregate the pertinent `ReportShare`?\r\n\r\nThis is stated explicitly in 4.3.2.3. Leader",
              "createdAt": "2022-01-21T01:23:14Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 596,
              "body": "EDITORIAL:\r\nI have a hard time reading these conditionals. It's not obvious whether the `failed` label applies to the vertical or horizontal edge, except that I can work backward from the `FAILED` vertex. How about:\r\n\r\n```suggestion\r\n                         |\r\n                         |\r\n                         V\r\n +----------failed--- prep_start <---------------------+\r\n |                    success                prep_next |\r\n |                       |             send Transition |\r\n```\r\n\r\nThough if this complies with a standard notation for RFC state machines then I can live with it.",
              "createdAt": "2022-01-21T01:31:05Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 618,
              "body": "```suggestion\r\na report share for aggregation.\"}\r\n```",
              "createdAt": "2022-01-21T01:35:55Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 627,
              "body": "```suggestion\r\nAfter this initial step, the following procedure is repeated until the leader\r\n```",
              "createdAt": "2022-01-21T01:36:06Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 629,
              "body": "I wonder if we should define a timeout for this state. If no transition message for the report arrives within X seconds, leader should transition to the failed state (perhaps after an appropriate number of retries).\r\n\r\nWe could declare the helper timing out to be equivalent to the helper sending a transition with type `FAILED`. I think that keeps the state machine simpler than adding dedicated timeout edges.",
              "createdAt": "2022-01-21T01:37:05Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 678,
              "body": "```suggestion\r\nwithout sending a message to the helper. Otherwise it interprets `out` as follows.\r\n```",
              "createdAt": "2022-01-21T01:40:20Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 735,
              "body": "```suggestion\r\nUpon receiving a Transition message from the leader, proceed as follows. If the\r\n```",
              "createdAt": "2022-01-21T01:43:12Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 762,
              "body": "```suggestion\r\nOnce processing of a report share reaches the FINISHED state, the aggregator\r\n```",
              "createdAt": "2022-01-21T01:44:13Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 763,
              "body": "```suggestion\r\nstores the the recovered output share until the batch to which it pertains is\r\n```",
              "createdAt": "2022-01-21T01:46:33Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 764,
              "body": "```suggestion\r\ncollected. To aggregate the output shares, the aggregator runs the aggregation\r\n```",
              "createdAt": "2022-01-21T01:47:18Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 785,
              "body": "```suggestion\r\nnumber of reports simultaneously:\r\n```",
              "createdAt": "2022-01-21T01:48:14Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 793,
              "body": "EDITORIAL:\r\nWe use `kebab-case` in `enum TransitionError`, so I think we should be consistent. That said, It seems like we're using Rust case convention in most places (i.e., `BumpyCase` for types and `snake_case` for names) and so continuing from that convention, both this and `TransitionError` should be in `BumpyCase`:\r\n\r\n```suggestion\r\n  AggInitReq(0),\r\n  AggReq(1),\r\n  AggResp(2),\r\n  AggShareReq(3),\r\n  AggShareResp(4),\r\n```\r\nHowever if there's an existing convention informed by the TLS RFC, we should do that.",
              "createdAt": "2022-01-21T01:51:10Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 815,
              "body": "If/when we merge this we should file a TODO to define this error.",
              "createdAt": "2022-01-21T01:55:31Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 811,
              "body": "Given that these messages will be transmitted over TLS, why do we need an additional MAC? My best guess is that it's to prove to the recipient that the message came from the other aggregator, but since the aggregators are already aware of each other's HPKE configs, could we use HPKE to [mutually authenticate](https://www.ietf.org/archive/id/draft-irtf-cfrg-hpke-12.html#name-authentication-using-an-asy)?\r\n\r\nMore broadly it feels wrong to put a transport level concern like integrity or authenticity in the message. Though I acknowledge that we need to do something about authenticating leader<->helper interactions and I'm now astonished we didn't think about this sooner! I think there's an analogy here to the problem of client<->aggregator authentication, in that I think the details of how a leader and helper authenticate to each other will be deployment specific and maybe we shouldn't try to specify it in PPM, but instead just abstractly declare that helper and leader must communicate over a confidential and mutually authenticated channel. mTLS works, but they could also use a PSK, or use a common identity provider, or who knows what.",
              "createdAt": "2022-01-21T01:59:50Z",
              "updatedAt": "2022-01-21T04:12:17Z"
            },
            {
              "originalPosition": 850,
              "body": "EDITORIAL:\r\n\ud83d\udc4d\ud83c\udffb . I think we would benefit from formalizing the definition of a batch as well as a few verbs that go with it (i.e., what does it mean for a batch to be _collected_).",
              "createdAt": "2022-01-21T02:56:55Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 862,
              "body": "```suggestion\r\n{{aggregate-message-auth}}, the helper handles the AggregateInitReq by computing\r\n```",
              "createdAt": "2022-01-21T02:58:14Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 878,
              "body": "I don't think that's necessary -- the `AggregateResp` is explicitly the response to an `Aggregate` message, so the leader should have the appropriate context when handling the response. However this gets trickier if we solve #111 by having helpers post `AggregateResp`s asynchronously to an endpoint exposed by the leader. Then your question becomes a case of #146.",
              "createdAt": "2022-01-21T03:01:03Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 881,
              "body": "Each `Transition` message contains a `Nonce` that would allow the leader to figure out which of its own `ReportShare`s it goes with. In many cases, a leader might store its `ReportShare`s in a key-value store keyed on the `Nonce`, so getting the responses in order might not make a difference. I wonder if we could relax this restriction.",
              "createdAt": "2022-01-21T03:04:01Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 881,
              "body": "Additionally:\r\n```suggestion\r\nthe AggregateInitReq. The order of these sequences MUST be the same (i.e.,\r\n```",
              "createdAt": "2022-01-21T03:04:16Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 884,
              "body": "```suggestion\r\nkeep track of state across aggregate requests. The helper's response to the\r\n```",
              "createdAt": "2022-01-21T03:04:37Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 962,
              "body": "I said elsewhere I think we should remove the helper state blob unless we can prove it's necessary. However if we keep it, we should put in a _SHOULD_ or _RECOMMENDED_ about versioning inside the state blob, because what if the helper deploys a new version while the leader is holding onto a state blob?\r\n\r\nIn passing, absolving helpers of dealing with state blob versioning and format migration is another strong argument for removing that feature from the protocol.",
              "createdAt": "2022-01-21T03:23:32Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 971,
              "body": "This doesn't sound right: the leader wouldn't emit an `AggregateShareReq` until it gets a `CollectReq`, would it? I can imagine that in a VDAF like `prio3`, the leader could eagerly send `AggregateShareReq`, but for `poplar1`, it can't do that until it gets the `agg_param`.",
              "createdAt": "2022-01-21T03:30:52Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 996,
              "body": "EDITORIAL:\r\nThis is awkward because the paragraph on line 1235-1237 refers to a singular helper but the next paragraph refers to multiple aggregators besides the leader. At one point we decided to write the protocol in a way that doesn't explicitly support multiple helpers but admits the possibility in the future. I'm open to rewriting the protocol explicitly assuming exactly one helper, but we should be consistent.",
              "createdAt": "2022-01-21T03:33:52Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1004,
              "body": "```suggestion\r\n{{out-to-agg-share}}, obtaining an opaque byte string `agg_share` whose structure is\r\n```",
              "createdAt": "2022-01-21T03:36:14Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1033,
              "body": "```suggestion\r\nciphertext `encrypted_agg_share` computed above.\r\n```",
              "createdAt": "2022-01-21T03:38:59Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1040,
              "body": "```suggestion\r\nerror for the leader to issue any more aggregate or aggregate-init requests for\r\n```",
              "createdAt": "2022-01-21T03:40:00Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1046,
              "body": "```suggestion\r\nThe collector sends a CollectReq message to the leader in order to collect\r\n```",
              "createdAt": "2022-01-21T03:40:23Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1084,
              "body": "```suggestion\r\nfail, then the leader aborts with \"XXX\". [TODO: Maybe convey the reason for\r\n```",
              "createdAt": "2022-01-21T03:41:44Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1087,
              "body": "EDITORIAL:\r\nnit: the leader may just be retrieving the aggregate share it has already computed",
              "createdAt": "2022-01-21T03:42:14Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 47,
              "body": "We have kind of been trying to keep the door open for multiple helpers in the future. If we were to add more helpers to this enum in the future, they would have to come after the client, which would introduce an inconsequential but slightly ugly discontinuity in the aggregator IDs. Maybe we should reorder this to:\r\n```\r\nenum {\r\n  collector(0),\r\n  client(1),\r\n  leader(2),\r\n  helper(3),\r\n} Role;\r\n```\r\n\r\nThat said (as noted elsewhere) I'm also comfortable with committing to exactly one helper and letting authors of some future RFC that obsoletes ours worry about this.",
              "createdAt": "2022-01-21T03:45:41Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1145,
              "body": "Participants should also check that `batch_interval.end >= batch_interval.start`. Maybe we should change the definition of `struct Interval` to\r\n\r\n```\r\nstruct {\r\n  Time start;\r\n  Duration length;\r\n} Interval;\r\n```\r\n\r\nThen, because `Duration` is an unsigned integer, it is impossible to define an invalid `Interval`.",
              "createdAt": "2022-01-21T03:49:55Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1162,
              "body": "EDITORIAL:\r\nultranit: if `X` is a set is `cardinality` or `size` more appropriate than `len`?",
              "createdAt": "2022-01-21T03:51:39Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1164,
              "body": "```suggestion\r\n* The aggregator keeps track of the number of times each report was aggregated.\r\n```",
              "createdAt": "2022-01-21T03:52:05Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1174,
              "body": "```suggestion\r\nshould keep it here until we figure out how to deal with nonce set\r\n```",
              "createdAt": "2022-01-21T03:52:28Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            },
            {
              "originalPosition": 1195,
              "body": "```suggestion\r\n[OPEN ISSUE: This has the potential to require aggregators to store nonce sets\r\n```",
              "createdAt": "2022-01-21T03:53:01Z",
              "updatedAt": "2022-01-21T04:12:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zPman",
          "commit": {
            "abbreviatedOid": "d878ff7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-21T16:01:38Z",
          "updatedAt": "2022-01-21T17:36:52Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "Whether we need to do something fancy here is TBD: https://github.com/abetterinternet/ppm-specification/issues/161. However, I like this suggestion, I'll just also add a pointer to the issue.",
              "createdAt": "2022-01-21T16:01:38Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 47,
              "body": "Great idea. I don't think this closes the door for multiple helpers, as long as their role in the PPM protocol is identical. However it does close the door on their being multiple \"types\" of helpers, which is still a possibility.",
              "createdAt": "2022-01-21T16:09:53Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 133,
              "body": "My thinking here is that the same endpoint might implement different roles (but never the same role for a given task). This allows the same domain to implement several roles.",
              "createdAt": "2022-01-21T16:20:10Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 165,
              "body": "I moved the HpkeCipertext to the section above. As for the suggested renaming: I agree that `enc` is ambiguous, but this naming convention matches other applications of HPKE, such as ECH and OHTTP. Consistent use of HPKE across specs is useful",
              "createdAt": "2022-01-21T16:24:34Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 184,
              "body": "Added \"Clients MUST NOT use the same `enc` for multiple reports.\" below.\r\n\r\nFYI, strictly speaking this comment isn't editorial, since it's somewhat normative.",
              "createdAt": "2022-01-21T16:27:36Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 190,
              "body": "I think it's reasonably clear that \"0x03\" isn't a byte string because it's not in quotes. Note that we're following the conventions of the TLS presentation language here. (Search \"0x0303\" in RFC 8446.)",
              "createdAt": "2022-01-21T16:32:00Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 254,
              "body": "This is still true: We don't have to require aggregators to keep around reports for the 24th century.",
              "createdAt": "2022-01-21T16:37:57Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 283,
              "body": "Ack. While I agree, this is somewhat out of scope for this PR.",
              "createdAt": "2022-01-21T16:39:12Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 309,
              "body": "I'd like to keep the helper state around, at least for the aggregation flow. In my own implementation, I'm using the helper state as a \"cookie\" that the helper uses to look up the state for the previous aggregate request.",
              "createdAt": "2022-01-21T16:40:57Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 322,
              "body": "Yeah, @chris-wood and I have discussed something like this. Let's see if we can't work it out on the call today and tack on a commit.",
              "createdAt": "2022-01-21T16:41:54Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 475,
              "body": "The protocol should specify exactly how any of these errors is to be handled. (If this isn't true then we need to fix it.) What the error codes are useful for is telemetry: It's good to know what errors are occuring because it gives you a starting point for diagnosing issues with your service. For instance, if I'm seeing `vdaf-prep-next-error` more than normal, this might indicate that the clients are misconfigured.\r\n\r\nNote that TLS does something similar: https://datatracker.ietf.org/doc/html/rfc8446#section-6",
              "createdAt": "2022-01-21T16:48:29Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 499,
              "body": "In the current context, \"at least once\" is implied by \"collected\".",
              "createdAt": "2022-01-21T16:49:10Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 569,
              "body": "That's right, though this will need to change if we go with #174. I've tried to minimize explanatory text that depends on decisions that haven't been made yet.",
              "createdAt": "2022-01-21T16:54:11Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 596,
              "body": "There's no \"standard\" here AFAIK. Feel free to hack on this. (just keep both state machines consistent.)",
              "createdAt": "2022-01-21T16:54:58Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 629,
              "body": "This would be need to be handled at the request level. A timeout is fatal, not only for a single report transition but for the entire aggregate request.",
              "createdAt": "2022-01-21T16:59:49Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 793,
              "body": "This follows the TLS presentation language.",
              "createdAt": "2022-01-21T17:03:18Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 811,
              "body": "> Given that these messages will be transmitted over TLS, why do we need an additional MAC? My best guess is that it's to prove to the recipient that the message came from the other aggregator, but since the aggregators are already aware of each other's HPKE configs, could we use HPKE to [mutually authenticate](https://www.ietf.org/archive/id/draft-irtf-cfrg-hpke-12.html#name-authentication-using-an-asy)?\r\n\r\nSome form of message authentication is needed because we don't assume mutually authenticated TLS. Yes, we could use one of the mutual auth modes for HPKE, but at the end of the day we would just use this to derive a MAC key.\r\n\r\n> More broadly it feels wrong to put a transport level concern like integrity or authenticity in the message.\r\n\r\nWhen we started working on this stuff, this was my inclination as well. However, conversations with Chris W., EKR, and others have lead me to think that there's value in minimizing the assumptions we make about what the underlying transport does, since it's evolution isn't tied to the evolution of our protocol.\r\n\r\n> I think there's an analogy here to the problem of client<->aggregator authentication, in that I think the details of how a leader and helper authenticate to each other will be deployment specific and maybe we shouldn't try to specify it in PPM, but instead just abstractly declare that helper and leader must communicate over a confidential and mutually authenticated channel. mTLS works, but they could also use a PSK, or use a common identity provider, or who knows what.\r\n\r\nI definitely see the value in keeping PPM agnostic about this. Many solutions are possible and not everyone is going to agree that a MAC is the best one. To be clear, I stuck a MAC here because the VDAF already requires distributing a shared secret key, so adding a MAC is cheap and convenient.",
              "createdAt": "2022-01-21T17:16:48Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 815,
              "body": "In draft PRs I write \"XXX\" as a placeholder for something I want to fill in before merging.",
              "createdAt": "2022-01-21T17:17:37Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 850,
              "body": "Agreed.",
              "createdAt": "2022-01-21T17:18:14Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 878,
              "body": "Another thing to consider: The leader's architecture might be a PubSub or Kafka queue that ingests AggregateResp's over multiple aggregation flows over multiple tasks. I suppose the HTTP client could just add the task ID?",
              "createdAt": "2022-01-21T17:20:01Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 881,
              "body": "The intention of this text is to allow implementations to parse and handle each Transition in-place, without sticking them in some intermediate data structure. ",
              "createdAt": "2022-01-21T17:25:59Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 971,
              "body": "Right, it's true that in general that you need to know the agg parameter before this point. But for prio3 there is no agg parameter (or, more precisely, the only valid agg parameter is the empty string).",
              "createdAt": "2022-01-21T17:27:56Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 996,
              "body": "I agree this is awkward, but EKR previously pushed back on changing any text to solve this ambiguity, which he viewed as a regression. We're just going to have to live with it until we make a decision about if/how to support multiple helpers.",
              "createdAt": "2022-01-21T17:29:06Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 1087,
              "body": "Changed \"computes\" to \"retrieves\".",
              "createdAt": "2022-01-21T17:31:47Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 1145,
              "body": "Ha, nice catch. Would you mind sending a PR for the main branch?",
              "createdAt": "2022-01-21T17:33:29Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            },
            {
              "originalPosition": 1162,
              "body": "I don't think so, I think it's clear. ",
              "createdAt": "2022-01-21T17:34:03Z",
              "updatedAt": "2022-01-21T17:36:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zQz1Q",
          "commit": {
            "abbreviatedOid": "830228f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-21T21:26:01Z",
          "updatedAt": "2022-01-21T21:26:02Z",
          "comments": [
            {
              "originalPosition": 815,
              "body": "Yeah we can deal with these before merging to main",
              "createdAt": "2022-01-21T21:26:01Z",
              "updatedAt": "2022-01-21T21:26:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zQ0b-",
          "commit": {
            "abbreviatedOid": "830228f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-21T21:29:32Z",
          "updatedAt": "2022-01-21T21:29:32Z",
          "comments": [
            {
              "originalPosition": 309,
              "body": "x-posting from another comment about helper state:\r\n\r\nI said elsewhere I think we should remove the helper state blob unless we can prove it's necessary. However if we keep it, we should put in a SHOULD or RECOMMENDED about versioning inside the state blob, because what if the helper deploys a new version while the leader is holding onto a state blob?\r\n\r\nIn passing, absolving helpers of dealing with state blob versioning and format migration is another strong argument for removing that feature from the protocol.",
              "createdAt": "2022-01-21T21:29:32Z",
              "updatedAt": "2022-01-21T21:29:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zQ0eD",
          "commit": {
            "abbreviatedOid": "830228f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-21T21:29:45Z",
          "updatedAt": "2022-01-21T21:29:46Z",
          "comments": [
            {
              "originalPosition": 962,
              "body": "Resolving this so we have a single thread about helper state",
              "createdAt": "2022-01-21T21:29:46Z",
              "updatedAt": "2022-01-21T21:29:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zQ203",
          "commit": {
            "abbreviatedOid": "830228f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-21T21:43:51Z",
          "updatedAt": "2022-01-21T21:43:52Z",
          "comments": [
            {
              "originalPosition": 1145,
              "body": "Here we are: https://github.com/abetterinternet/ppm-specification/pull/184",
              "createdAt": "2022-01-21T21:43:52Z",
              "updatedAt": "2022-01-21T21:43:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zRE4B",
          "commit": {
            "abbreviatedOid": "1870a9d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-21T23:27:13Z",
          "updatedAt": "2022-01-21T23:27:14Z",
          "comments": [
            {
              "originalPosition": 309,
              "body": "We agreed to deal with this out of band from this specific PR. I will file an issue about this. Resolving this particular item.",
              "createdAt": "2022-01-21T23:27:13Z",
              "updatedAt": "2022-01-21T23:27:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zRJBs",
          "commit": {
            "abbreviatedOid": "1870a9d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-22T00:13:41Z",
          "updatedAt": "2022-01-22T00:13:41Z",
          "comments": [
            {
              "originalPosition": 811,
              "body": "We're going to do the MAC with a pre shared secret for now, and will revisit this as #161 and other issues related to sharing entropy between aggregators mature.",
              "createdAt": "2022-01-22T00:13:41Z",
              "updatedAt": "2022-01-22T00:13:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zRPmE",
          "commit": {
            "abbreviatedOid": "1870a9d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-22T02:42:57Z",
          "updatedAt": "2022-01-22T02:42:57Z",
          "comments": [
            {
              "originalPosition": 322,
              "body": "We decided to park this. It's not clear that the extra round trip is needed, at least for the current set of VDAFs.",
              "createdAt": "2022-01-22T02:42:57Z",
              "updatedAt": "2022-01-22T02:42:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zcJsm",
          "commit": {
            "abbreviatedOid": "a9f6ed0"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T00:48:37Z",
          "updatedAt": "2022-01-26T00:48:37Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "Do we need separate `aggregate` and `aggregate_init` endpoints? `struct Aggregate` contains an `AggregateType` selector that allows the recipient to tell an `AggregateInitReq` from an `AggregateReq` anyway.",
              "createdAt": "2022-01-26T00:48:37Z",
              "updatedAt": "2022-01-26T00:48:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zewBK",
          "commit": {
            "abbreviatedOid": "a9f6ed0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-26T15:23:37Z",
          "updatedAt": "2022-01-26T15:23:38Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "Right, this doesn't seem strictly necessary. In fact the same is true for /aggregate_share.",
              "createdAt": "2022-01-26T15:23:37Z",
              "updatedAt": "2022-01-26T15:24:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs40ffZ6",
          "commit": {
            "abbreviatedOid": "b787cdd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-02-11T20:55:30Z",
          "updatedAt": "2022-02-11T21:12:30Z",
          "comments": [
            {
              "originalPosition": 92,
              "body": "I changed this to 16 because (1) I have a hard time imagining we'll ever get close to 65,536 input shares and (2) implementing encoding/decoding for `u24` is awkward!",
              "createdAt": "2022-02-11T20:55:31Z",
              "updatedAt": "2022-02-11T21:12:30Z"
            },
            {
              "originalPosition": 101,
              "body": "I keep going back and forth on whether there should be a separate endpoints for `aggreagate_init`, `aggregate` and `aggregate_share`. I think there's a disconnect between having separate endpoints, but having them all take `struct Aggregate`. The thing is, the only valid message to send to `[helper]/aggregate_init` is an `AggregateInitReq`, and the only valid response to send is `AggregateResp`, so why do we have a wrapper that allows sending or responding with any of `AggregateReq`, `AggregateReq`, `AggregateResp`, `AggregateShareReq` or `HpkeCiphertext`? I think the reason is so that we can specify the `opaque tag[32]` for the HMAC-SHA256 in one place but I don't think it's worth the extra error cases introduced by the enum wrapper.\r\n\r\nI think we should have:\r\n- `[helper]/aggregate_init` which _only_ accepts `AggregateInitReq` and _only_ responds with `AggregateResp`\r\n- `[helper]/aggregate` which _only_ accepts `AggregateReq` and _only_ responds with `AggregateResp`\r\n- `[helper]/aggregate_share` which _only_ accepts `AggregateShareReq` and _only_ responds with `HpkeCiphertext`\r\n- add `opaque tag [32]` fields to `AggregateInitReq`, `AggregateReq` and `AggregateShareReq` so that helper can verify that the message came from the leader.\r\n\r\nFinally, here's something spicy: do we need the HMAC tag on `AggregateResp` or the `HpkeCiphertext` that the helper responds to the leader with? If the leader connects to helper over TLS, then that provides sufficient authentication for responses.",
              "createdAt": "2022-02-11T21:04:26Z",
              "updatedAt": "2022-02-11T21:12:30Z"
            },
            {
              "originalPosition": 128,
              "body": "Let's say we are running a two round VDAF. In round 1, leader sends helper an `AggregateInitReq` over five reports ordered by nonce (each composed of `(time, rand)`) thus: `(1,1), (2,2), (3,3), (4,4), (5,5)`. Upon receipt, the helper extracts its prepare message and records that it has seen those nonces. The leader can verify that the `Transitions` in the `AggregateResp` it gets appear in the same order. So far so good.\r\n\r\nSuppose in the next round, the leader sends the next flight of prepare messages in a different order: `(5,5), (4,4), (3,3), (2,2), (1,1)`. How is helper supposed to know that this is \"out of order\" with respect to the previous round? Besides keeping track of whether or not it has ever seen an individual nonce, helper would have to keep track of the order in which they ever appeared in an `AggregateInitReq`. Is that reasonable? Instead of letting the leader dictate the order of reports/transitions, should we define a canonical order for reports and transitions (i.e., ascending order by nonce) so that both aggregators can statelessly enforce the desired ordering?",
              "createdAt": "2022-02-11T21:12:21Z",
              "updatedAt": "2022-02-11T21:12:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41iO61",
          "commit": {
            "abbreviatedOid": "3046435"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-02T21:02:20Z",
          "updatedAt": "2022-03-02T21:02:20Z",
          "comments": [
            {
              "originalPosition": 665,
              "body": "This doesn't match the pseudocode below\r\n```suggestion\r\nfollows. Let `leader_payload` denote the last VDAF message it computed and let\r\n`helper_payload` denote the last VDAF message it received from the helper. The\r\n```",
              "createdAt": "2022-03-02T21:02:20Z",
              "updatedAt": "2022-03-02T21:02:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41ijJ0",
          "commit": {
            "abbreviatedOid": "3046435"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-02T22:29:53Z",
          "updatedAt": "2022-03-02T22:30:02Z",
          "comments": [
            {
              "originalPosition": 787,
              "body": "There are currently two structures called `AggregateReq`: this one, and another defined on L1176 (which is embedded in this message if `msg_type == aqq_req`).\r\n\r\nMy strawman suggestion would be to keep this one named `AggregateReq`, and name the sub-message `AggregateContinueReq`. TBH I'm not super-happy with that name, maybe something better is apparent -- I don't care too much as long as they're named separately.",
              "createdAt": "2022-03-02T22:29:53Z",
              "updatedAt": "2022-03-02T22:30:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41w2is",
          "commit": {
            "abbreviatedOid": "d8c6677"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-07T17:14:30Z",
          "updatedAt": "2022-03-07T17:14:30Z",
          "comments": [
            {
              "originalPosition": 809,
              "body": "This requirement that the tag must be verified before interpreting the rest of the `AggregateReq` is impractical given our current structures and serializations of authenticated messages. The recipient must read a discriminant byte and one or two length fields inside the inner struct before it knows where the tag will be.\r\n\r\nPerhaps we should redefine the authenticated structures to contain two opaque fields, one with the encoded contents of a new inner struct, and one with the authentication tag over it. Then, we can read one length (of the `opaque inner<0..2^16-1>` field) and perform tag verification before parsing the nested contents.",
              "createdAt": "2022-03-07T17:14:30Z",
              "updatedAt": "2022-03-07T17:14:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4118su",
          "commit": {
            "abbreviatedOid": "d8c6677"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-08T17:17:42Z",
          "updatedAt": "2022-03-08T17:17:43Z",
          "comments": [
            {
              "originalPosition": 809,
              "body": "Indeed. We could also add a requirement that the size of the encoded values must be known (which is true for the top-layer messages that are being tagged, since they are being passed around as HTTP requests/responses); then the `tag` is just the last 32 bytes.",
              "createdAt": "2022-03-08T17:17:42Z",
              "updatedAt": "2022-03-08T17:17:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42DFO7",
          "commit": {
            "abbreviatedOid": "b787cdd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-11T05:24:22Z",
          "updatedAt": "2022-03-11T05:24:23Z",
          "comments": [
            {
              "originalPosition": 92,
              "body": "It's not 65,536 input shares, it's 65,536 *bytes* worth of input shares. I think it this should be kicked back up to 24.",
              "createdAt": "2022-03-11T05:24:22Z",
              "updatedAt": "2022-03-11T05:24:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42DFo0",
          "commit": {
            "abbreviatedOid": "d8c6677"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-11T05:27:48Z",
          "updatedAt": "2022-03-11T05:27:48Z",
          "comments": [
            {
              "originalPosition": 809,
              "body": "The same pattern exists for TLS, I don't think this is a big problem. On the other hand I would advocate for being conservative about what we MAC. In particular I wouldn't want the disccriminator or length bits to not be covered by the MAC.",
              "createdAt": "2022-03-11T05:27:48Z",
              "updatedAt": "2022-03-11T05:27:48Z"
            }
          ]
        }
      ]
    },
    {
      "number": 181,
      "id": "PR_kwDOFEJYQs4xN2On",
      "title": "Align some terminology with the VDAF draft",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/181",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "What PPM calls an \"output share\" is called an \"aggregate share\" in the\r\nVDAF draft. Furthermore, in the VDAF draft, an \"output share\" is what\r\nan aggregator recovers from running the MPC with its peers. This change\r\nfixes this ambiguity without changing the PPM protocol itself (modulo\r\nchanging the info string for HPKE).",
      "createdAt": "2022-01-18T17:20:33Z",
      "updatedAt": "2022-09-16T00:30:09Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "47c5b696a0c12e9c5da9cfc12c40d1786e0ce1fc",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/editorial-agg-share",
      "headRefOid": "58f903236300d61189d91fa1d739ab6c76c81068",
      "closedAt": "2022-01-19T18:39:52Z",
      "mergedAt": "2022-01-19T18:39:52Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b202eaaa4c5544bd3f9cac19af37bcdcadcf7996"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2022-01-19T18:38:03Z",
          "updatedAt": "2022-01-19T18:38:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks fellas!",
          "createdAt": "2022-01-19T18:39:48Z",
          "updatedAt": "2022-01-19T18:39:48Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4zBvN-",
          "commit": {
            "abbreviatedOid": "421753c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM, modulo typos",
          "createdAt": "2022-01-18T23:39:46Z",
          "updatedAt": "2022-01-18T23:56:36Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "```suggestion\r\nbatch interval for which it has completed at least one aggregate-share request\r\n```",
              "createdAt": "2022-01-18T23:39:46Z",
              "updatedAt": "2022-01-18T23:56:36Z"
            },
            {
              "originalPosition": 30,
              "body": "```suggestion\r\nAggregate result:\r\n```\r\nTo be consistent with other glossary entries",
              "createdAt": "2022-01-18T23:46:26Z",
              "updatedAt": "2022-01-18T23:56:36Z"
            },
            {
              "originalPosition": 69,
              "body": "```suggestion\r\nthe beginning of this computation, each aggregator is in possession of an input\r\n```",
              "createdAt": "2022-01-18T23:54:22Z",
              "updatedAt": "2022-01-18T23:56:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zE3gj",
          "commit": {
            "abbreviatedOid": "6f14a03"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-01-19T14:54:40Z",
          "updatedAt": "2022-01-19T14:55:12Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "```suggestion\r\ndistributed zero-knowledge proof of the input's validity {{BBCGGI19}} which the\r\n```",
              "createdAt": "2022-01-19T14:54:40Z",
              "updatedAt": "2022-01-19T14:55:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zE7Xr",
          "commit": {
            "abbreviatedOid": "6f14a03"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T15:05:13Z",
          "updatedAt": "2022-01-19T15:05:14Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "Hmm, the tool that renders the markdown into XML seems to interpret this correctly. Plus we've been using `{{XXX}}` rather than `{{blah}}` for paper references fairly consistently throughout the doc. Do you think, we should change this back?",
              "createdAt": "2022-01-19T15:05:13Z",
              "updatedAt": "2022-01-19T15:05:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zE8W-",
          "commit": {
            "abbreviatedOid": "6f14a03"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T15:07:58Z",
          "updatedAt": "2022-01-19T15:07:58Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "`[..]` does nothing in the translation process. It literally duplicates the text inline. You could write `[bananas]` and it would render `[bananas]` in the XML/TXT/HTML output. `{{...}}` actually invokes the translation process to cite things correctly. So, yeah, we should be using `{{...}}`. It helps us catch unused or missing references.",
              "createdAt": "2022-01-19T15:07:58Z",
              "updatedAt": "2022-01-19T15:08:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zE-5-",
          "commit": {
            "abbreviatedOid": "6f14a03"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T15:15:16Z",
          "updatedAt": "2022-01-19T15:15:16Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "That's not true, as far as I can tell. take a look at the rendered HTML. A link is put there that navigates to the informational references at the bottom. And we don't use the `{{...}}` syntax anywhere for this particular reference.",
              "createdAt": "2022-01-19T15:15:16Z",
              "updatedAt": "2022-01-19T15:15:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zE_KK",
          "commit": {
            "abbreviatedOid": "6f14a03"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T15:16:00Z",
          "updatedAt": "2022-01-19T15:16:01Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "Maybe there's more that's happening that I don't see? Regardless, I'm happy to make this change to all links in a follow-up PR.",
              "createdAt": "2022-01-19T15:16:01Z",
              "updatedAt": "2022-01-19T15:16:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zFBfx",
          "commit": {
            "abbreviatedOid": "6f14a03"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T15:22:38Z",
          "updatedAt": "2022-01-19T15:22:38Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "As discussed offline, I'll follow up with a PR that changes all the [...] style references with {{...}} style references.",
              "createdAt": "2022-01-19T15:22:38Z",
              "updatedAt": "2022-01-19T15:22:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zF1L9",
          "commit": {
            "abbreviatedOid": "c22133b"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Generally looks good. I would change the \"agg\" in the label if you can as it is confusing.",
          "createdAt": "2022-01-19T18:03:55Z",
          "updatedAt": "2022-01-19T18:23:11Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "This may be the VDAF term, but it seems kind of unfortunate. Why  isn't this called \"input\"?",
              "createdAt": "2022-01-19T18:03:56Z",
              "updatedAt": "2022-01-19T18:23:11Z"
            },
            {
              "originalPosition": 93,
              "body": "```suggestion\r\nvalidate their corresponding output shares. For example, `prio3` includes a\r\n```\r\n\r\nI think this might help",
              "createdAt": "2022-01-19T18:13:07Z",
              "updatedAt": "2022-01-19T18:23:11Z"
            },
            {
              "originalPosition": 89,
              "body": "```suggestion\r\nindication that a valid output share could not be computed.\r\n```\r\n\"recovered\" is a weird word.",
              "createdAt": "2022-01-19T18:13:33Z",
              "updatedAt": "2022-01-19T18:23:11Z"
            },
            {
              "originalPosition": 198,
              "body": "Is \"aggregate\" not gonna fit?",
              "createdAt": "2022-01-19T18:15:48Z",
              "updatedAt": "2022-01-19T18:23:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zF8h0",
          "commit": {
            "abbreviatedOid": "8fb3ba5"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T18:32:02Z",
          "updatedAt": "2022-01-19T18:32:03Z",
          "comments": [
            {
              "originalPosition": 198,
              "body": "So I'm thinking it might simplify analysis if we make the info string the same length for each invocation of HPKE the same length. I'm thinking \"ppm inp share\" for the encrypted input shares and \"ppm agg share\" for the encrypted aggregate shares. I sort of half-assed it here, but I was planning on doing this in a future PR. I'll just change it to \"aggregate\" in this PR and defer that discussion.",
              "createdAt": "2022-01-19T18:32:03Z",
              "updatedAt": "2022-01-19T18:32:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zF9HX",
          "commit": {
            "abbreviatedOid": "bca1a15"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T18:34:25Z",
          "updatedAt": "2022-01-19T18:34:25Z",
          "comments": [
            {
              "originalPosition": 198,
              "body": "Done",
              "createdAt": "2022-01-19T18:34:25Z",
              "updatedAt": "2022-01-19T18:34:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4zF-Xw",
          "commit": {
            "abbreviatedOid": "58f9032"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-01-19T18:39:28Z",
          "updatedAt": "2022-01-19T18:39:29Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "As discussed offline, the \"prepare step\" is the MPC that maps input shares to output shares (using the aggregation parameter to control the output).",
              "createdAt": "2022-01-19T18:39:28Z",
              "updatedAt": "2022-01-19T18:39:29Z"
            }
          ]
        }
      ]
    },
    {
      "number": 182,
      "id": "PR_kwDOFEJYQs4xRzJg",
      "title": "Convert [..]-style refs to {{..}}",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/182",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #181 (merge that first).\r\n\r\n@chris-wood pointed out that, if there is no reference for \"banana\",\r\nthen `[banana]` gets rendered as text. On the other hand `{{banana}}`\r\nwill fail to build because the reference is missing. The latter behavior\r\nis preferable.",
      "createdAt": "2022-01-19T17:39:43Z",
      "updatedAt": "2022-09-16T00:30:08Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton/editorial-agg-share",
      "baseRefOid": "6f14a03feb3c7c0d44a3d15f051d9c32509eb4be",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/editorial-fix-links",
      "headRefOid": "e1a91b530c2d920d76d0175e6f7837c0bccc1239",
      "closedAt": "2022-01-19T18:03:00Z",
      "mergedAt": "2022-01-19T18:03:00Z",
      "mergedBy": "ekr",
      "mergeCommit": {
        "oid": "c22133be23c391406d1dabfca602561057038eaa"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "LGTM",
          "createdAt": "2022-01-19T18:02:55Z",
          "updatedAt": "2022-01-19T18:02:55Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4zFuwh",
          "commit": {
            "abbreviatedOid": "e1a91b5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-01-19T17:40:08Z",
          "updatedAt": "2022-01-19T17:40:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 184,
      "id": "PR_kwDOFEJYQs4xaP-4",
      "title": "Redefine `Interval` to invalid values impossible",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/184",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "While reviewing #179, @cjpatton and I figured out a way to redefine\r\n`struct Interval` so that invalid values cannot be represented[1].\r\n\r\nThe existing representation contains `Time start` and `Time end`, which\r\nmeans an interval could be defined with `end < start`, which is invalid.\r\nRedefining `Interval` as a start instant and a duration makes it\r\nimpossible to represent invalid `Interval`s (since `Duration` is an\r\nunsigned integer).\r\n\r\nAdditionally, this implicitly fixes a bug in the batch interval validation\r\nlogic: we asserted that `batch_interval.end` should be divisible by\r\n`min_batch_interval`, but since `end` is _excluded_ from the `Interval`, I\r\nthink that was incorrect. Instead, `batch_interval.end - 1` should be divisible\r\nby `min_batch_duration`. In any case, this is simpler with the new\r\ndefinition of `Interval`: `Interval.duration` needs to be a multiple of\r\n`min_batch_duration`.\r\n\r\n[1]: https://github.com/abetterinternet/ppm-specification/pull/179#discussion_r789340693",
      "createdAt": "2022-01-21T21:43:40Z",
      "updatedAt": "2022-09-16T00:30:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b202eaaa4c5544bd3f9cac19af37bcdcadcf7996",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/errorproof-duration",
      "headRefOid": "a619f04a96ea7606addc38ebe677af5841509148",
      "closedAt": "2022-01-22T15:05:24Z",
      "mergedAt": "2022-01-22T15:05:24Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "0f0a2a22b6ee02c12244beb2e08b50699bd31214"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4zQ43z",
          "commit": {
            "abbreviatedOid": "a619f04"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-01-21T21:56:13Z",
          "updatedAt": "2022-01-21T21:56:13Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4zQ7Bw",
          "commit": {
            "abbreviatedOid": "a619f04"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nice!",
          "createdAt": "2022-01-21T22:09:44Z",
          "updatedAt": "2022-01-21T22:09:44Z",
          "comments": []
        }
      ]
    },
    {
      "number": 186,
      "id": "PR_kwDOFEJYQs4xci3z",
      "title": "Remove .targets.mk",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/186",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This never needed to be checked, and `make` overwrites it every time you\r\nbuild the doc.",
      "createdAt": "2022-01-22T17:43:42Z",
      "updatedAt": "2022-09-16T00:30:13Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0f0a2a22b6ee02c12244beb2e08b50699bd31214",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/remove-targets-mk",
      "headRefOid": "9d7b073ae063a77132154d309a03f184c9bdb178",
      "closedAt": "2022-01-24T16:08:49Z",
      "mergedAt": "2022-01-24T16:08:49Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "557b887eb02c641608e9e5eaab34615744fc0c57"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4zRgcW",
          "commit": {
            "abbreviatedOid": "9d7b073"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-01-22T17:47:24Z",
          "updatedAt": "2022-01-22T17:47:24Z",
          "comments": []
        }
      ]
    },
    {
      "number": 193,
      "id": "PR_kwDOFEJYQs4zssV8",
      "title": "Make collect requests asynchronous",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/193",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Make the handling of collect requests on the leader asynchronous, since\r\nservicing a collect request could entail running the preparation\r\nprotocol over all the reports that fall into the batch interval,\r\nespecially in VDAFs like poplar1 where the aggregation parameter is not\r\navailable until the collector provides it.\r\n\r\nI've only made the collect request handling asynchronous. My reasoning is that handling other requests synchronously should be feasible:\r\n\r\n- upload requests should only contain shares of a single report and thus never be all that big. Additionally, the client needs an explicit acknowledgement from the aggregator that the uploaded report was accepted and has been persisted.\r\n- the requests in the aggregation/preparation sub-protocol should also never get that big: even when preparing a large number of reports, the leader can choose to carve that work up into any number of aggregation jobs, each of which covers a small enough number of reports that the message sizes and processing times should be reasonable to handle in synchronous request/response exchanges. \r\n\r\nResolves #111",
      "createdAt": "2022-03-01T01:32:56Z",
      "updatedAt": "2022-09-16T00:30:25Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "29ce456d5b49020ab5af519ede0ab434f20c3827",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/async-collect",
      "headRefOid": "845207083f9622125ad22167b5ee3a1dc31ec599",
      "closedAt": "2022-03-07T18:52:42Z",
      "mergedAt": "2022-03-07T18:52:42Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "cb397732fa9edaba3050660927d81a9793d64377"
      },
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "With this change, we will have two differing definitions of structs named `CollectResp`. My understanding is that clients will know which one to expect based on the URL of the request, and the status code in the response. (they will receive a URL from /collect on success, and the `collect_job` URL will return a URL with status code 202, or shares with status code 200) I think we should change one of these for clarity to the reader, i.e. `CollectCompleteResp` or `CollectJobResp`.",
          "createdAt": "2022-03-01T21:53:39Z",
          "updatedAt": "2022-03-01T21:53:39Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs41dl9E",
          "commit": {
            "abbreviatedOid": "6367e49"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-03-01T23:46:33Z",
          "updatedAt": "2022-03-01T23:58:31Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "Q: what's the practical reason for allowing the `collect_job` to change?",
              "createdAt": "2022-03-01T23:46:33Z",
              "updatedAt": "2022-03-01T23:58:31Z"
            },
            {
              "originalPosition": 69,
              "body": "This seems low -- do we really only want to support shares of up to 255 bytes? Notably, each EncryptedAggregateShare structure has several fields (`enc`, `payload`) that may be up to 64 KiB each, so currently a maximally-sized EncryptedAggregateShare may not fit in this vector.",
              "createdAt": "2022-03-01T23:52:42Z",
              "updatedAt": "2022-03-01T23:58:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41doBc",
          "commit": {
            "abbreviatedOid": "6367e49"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-02T00:01:59Z",
          "updatedAt": "2022-03-02T00:01:59Z",
          "comments": [
            {
              "originalPosition": 69,
              "body": "Ah, that's the second time I've made that mistake! I thought this meant up to 2^8-1 `EncryptedAggregateShare` structures, but you're right, it means number of bytes. I'll make this 2^24-1 to be safe.",
              "createdAt": "2022-03-02T00:01:59Z",
              "updatedAt": "2022-03-02T00:01:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41doN7",
          "commit": {
            "abbreviatedOid": "6367e49"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-02T00:03:30Z",
          "updatedAt": "2022-03-02T00:03:31Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "I didn't see a reason to require that it be the same, but I suppose we could instead insist that it always be the same. Or we could say nothing, implicitly permitting `collect_job` to change.",
              "createdAt": "2022-03-02T00:03:30Z",
              "updatedAt": "2022-03-02T00:03:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41dpU6",
          "commit": {
            "abbreviatedOid": "6367e49"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-02T00:12:15Z",
          "updatedAt": "2022-03-02T00:12:59Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "WDYT about having the `collect_job` handler return an HTTP 202 with an unspecified (or empty) body if the results aren't yet available? This removes the unnecessary flexibility of changing `collect_jobs` without requiring specification language.\r\n\r\nThis is pretty nit-picky; I'd personally go with an unspecified body to leave things maximally flexible while allowing us to specify later, but LGTM either way.",
              "createdAt": "2022-03-02T00:12:15Z",
              "updatedAt": "2022-03-02T00:12:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41rJeK",
          "commit": {
            "abbreviatedOid": "838334e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Mostly editorial changes. One substantive question: Would it be more appropriate to add the collect job URL as an HTTP header?",
          "createdAt": "2022-03-04T17:19:30Z",
          "updatedAt": "2022-03-04T17:32:20Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "```suggestion\r\nguaranteed.  In fact, for some VDAFs, it is not be possible to begin preparing inputs\r\n```",
              "createdAt": "2022-03-04T17:19:30Z",
              "updatedAt": "2022-03-04T17:32:20Z"
            },
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nthese reasons, collect requests are handled asynchronously.\r\n```",
              "createdAt": "2022-03-04T17:20:03Z",
              "updatedAt": "2022-03-04T17:32:20Z"
            },
            {
              "originalPosition": 61,
              "body": "```suggestion\r\nWhen both aggregator's shares are successfully obtained, the leader\r\n```",
              "createdAt": "2022-03-04T17:26:27Z",
              "updatedAt": "2022-03-04T17:32:20Z"
            },
            {
              "originalPosition": 36,
              "body": "Is sending a binary-encoded response with the URL necessary/desirable? What about just adding the URL as an HTTP header? What does ACME do here?",
              "createdAt": "2022-03-04T17:27:50Z",
              "updatedAt": "2022-03-04T17:32:20Z"
            },
            {
              "originalPosition": 52,
              "body": "Allowing the collect job URL to change seems to add some complexity. If we don't need this complexity, than I suggest we remove it. In particular,  I'd propose removing this optional behavior.",
              "createdAt": "2022-03-04T17:29:48Z",
              "updatedAt": "2022-03-04T17:32:20Z"
            },
            {
              "originalPosition": 69,
              "body": "Why the size increase here? It seems unlikely to me that we'll ever have aggregate shares larger than 2^16 - 1 bytes.",
              "createdAt": "2022-03-04T17:31:01Z",
              "updatedAt": "2022-03-04T17:32:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41r2mT",
          "commit": {
            "abbreviatedOid": "838334e"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-03-04T20:25:57Z",
          "updatedAt": "2022-03-04T20:56:52Z",
          "comments": [
            {
              "originalPosition": 36,
              "body": "I would add a TODO just to get this done.",
              "createdAt": "2022-03-04T20:25:57Z",
              "updatedAt": "2022-03-04T20:56:52Z"
            },
            {
              "originalPosition": 40,
              "body": "This SHOULD seems odd. If this is a real attack then there should be a MUST requiring high entropy.\r\n\r\nI'm also not clear what an \"enumeration\" attack is. Presumably the leader has an access control mechanism, so what does it matter if I try /results/1 and get permission denied? And if it's supposed to prevent me knowing how many jobs there are, then I think this needs to be entirely unpredictable.",
              "createdAt": "2022-03-04T20:30:14Z",
              "updatedAt": "2022-03-04T20:56:52Z"
            },
            {
              "originalPosition": 52,
              "body": "I prefer @BranLwyd's suggestion here. Note that otherwise we have weird race conditions.",
              "createdAt": "2022-03-04T20:31:49Z",
              "updatedAt": "2022-03-04T20:56:52Z"
            },
            {
              "originalPosition": 85,
              "body": "I see why you are doing this and you have to for E2E reasons, but this seems hard to operationalize. I sugegest that instead we want a delete.",
              "createdAt": "2022-03-04T20:56:43Z",
              "updatedAt": "2022-03-04T20:56:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41rqgw",
          "commit": {
            "abbreviatedOid": "838334e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-04T19:43:30Z",
          "updatedAt": "2022-03-04T21:11:09Z",
          "comments": [
            {
              "originalPosition": 36,
              "body": "ACME delivers such URLs in response bodies, but it uses JSON to encode messages. For example, see the [`Order` object](https://datatracker.ietf.org/doc/html/rfc8555#section-7.1.3), which contains URLs that the client can hit for authorizations, to finalize an order and to eventually get the certificate.\r\n\r\nI agree that binary encoding of the URL seems unfortunate here. Maybe we can use an HTTP redirect with a `Location` header. [HTTP 303 See Other](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/303) seems it has the semantics we want. I'll read up on that and rewrite this to use that.",
              "createdAt": "2022-03-04T19:43:30Z",
              "updatedAt": "2022-03-04T21:11:09Z"
            },
            {
              "originalPosition": 52,
              "body": "I will incorporate Bran's idea.",
              "createdAt": "2022-03-04T21:10:10Z",
              "updatedAt": "2022-03-04T21:11:09Z"
            },
            {
              "originalPosition": 40,
              "body": "You're right, it makes more sense to assume this is authenticated. I'll delete this text.",
              "createdAt": "2022-03-04T21:10:30Z",
              "updatedAt": "2022-03-04T21:11:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41sHrh",
          "commit": {
            "abbreviatedOid": "1c7d715"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Per @chris-wood's observation, I put the `OPEN ISSUE` about collector to leader auth back in. I also went with HTTP 303 See Other over the previous CollectResp. I suspect we still need to say something about how long a leader should retain aggregate shares, in case the collector never sends the DELETE. @divergentdave suggested that we might put retention period parameters for reports, aggregate shares and other persistently stored data into a PPM task's parameters, which would allow us to punt the problem of picking retention problems to deployments and to write clear protocol text about what to do.",
          "createdAt": "2022-03-04T22:00:16Z",
          "updatedAt": "2022-03-04T22:06:29Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "I consulted ACME for inspiration here: [Section 7.4.2 Downloading the Certificate](https://datatracker.ietf.org/doc/html/rfc8555#section-7.4.2). ACME doesn't specify how long the certificate should be available for at all (and technically allows an ACME server to _never_ provide the cert). Maybe we can get away with saying nothing, too. In any case I wrote up the DELETE idea.",
              "createdAt": "2022-03-04T22:00:16Z",
              "updatedAt": "2022-03-04T22:06:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41sLwD",
          "commit": {
            "abbreviatedOid": "fb3c3bd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nicely done.",
          "createdAt": "2022-03-04T22:21:46Z",
          "updatedAt": "2022-03-04T22:21:46Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs41w74u",
          "commit": {
            "abbreviatedOid": "fb3c3bd"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-07T17:29:59Z",
          "updatedAt": "2022-03-07T17:31:21Z",
          "comments": [
            {
              "originalPosition": 100,
              "body": "```suggestion\r\n- CollectResult {{pa-collect}}: \"message/ppm-collect-result\"\r\n```",
              "createdAt": "2022-03-07T17:29:59Z",
              "updatedAt": "2022-03-07T17:31:21Z"
            }
          ]
        }
      ]
    },
    {
      "number": 194,
      "id": "PR_kwDOFEJYQs4zwlwG",
      "title": "Add checksum & report_count to aggregation share requests.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/194",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This is intended to allow PPM implementations to discover if the leader\r\n& helper(s) have fallen out-of-sync in their views of what shares are\r\nincluded in the aggregation.\r\n\r\n\r\nNotes: it would be trivial for an adversary to generate sets of nonces that collide with one another. Since the goal is to provide a check against server malfunction, rather than a security check against malicious clients or other actors in the system, I think this is acceptable. I evaluated a number of collision-resistant multiset-hash constructions[1], but none of them were currently standardized & specifying them would be more effort than seems necessary given the goals of this checksum.\r\n\r\n[1] the most promising of which was [`LtHash`](https://eprint.iacr.org/2019/227.pdf)\r\n\r\nCloses #163.",
      "createdAt": "2022-03-01T23:16:56Z",
      "updatedAt": "2022-09-16T00:30:18Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "557b887eb02c641608e9e5eaab34615744fc0c57",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "checksum",
      "headRefOid": "ccc6bb42388de5edbeb8b0fd703b6fc799f0d0c7",
      "closedAt": "2022-03-04T22:18:05Z",
      "mergedAt": "2022-03-04T22:18:05Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "29ce456d5b49020ab5af519ede0ab434f20c3827"
      },
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "The hash function wouldn't matter in that setting, as the attacker could generate a pool of a few hundred/thousand nonces and their hashes, fix one set of nonces, and use a system of equations over GF(2)^(digest length) to determine what the other set of nonces should be to match the checksum. Equivalently, they could start with generating a pool of nonces and hashes, treat them as vectors in GF(2)^(digest length), put them in a matrix, and find the kernel of that matrix. Pick two unique vectors that are in the kernel and have the same Hamming weight, then each entry determines whether the corresponding nonce from the pool should be included. These two vectors will give you two unique sets of nonces from the pool, such that they form two batches of the same size with the same checksum.\r\n\r\nUsing a MAC would require sorting at some point, and would preclude accumulating the checksum as each prepare request comes in, like with LtHash or this XOR construction.\r\n\r\nLong term, we need to more rigorously handle ordering of prepare and collect interactions, gracefully handling communication failures and retries between the leader and helper, and agreement on reports to be collected, but I agree this is fine for now as an error heuristic.",
          "createdAt": "2022-03-04T18:54:58Z",
          "updatedAt": "2022-03-04T18:54:58Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> > Notes: it would be trivial for an adversary to generate sets of nonces that collide with one another. Since the goal is to provide a check against server malfunction, rather than a security check against malicious clients or other actors in the system, I think this is acceptable.\r\n> \r\n> I agree that we don't care about a malicious aggregator here, but we do care about malicious clients. We need to think through what an attacker can do if it controls a subset of the clients and can disrupt the transmission of messages between the aggregators.\r\n> \r\n> Specifically, suppose the leader has computed the checksum\r\n> \r\n> ```\r\n> chucksum_leader = hash(N1) ^ ... ^ hash(Nb)\r\n> ```\r\n> \r\n> and the helper has computed the checksum\r\n> \r\n> ```\r\n> checksum_helper = hash(M1) ^ ... ^ hash(Mc]\r\n> ```\r\n> \r\n> where `N1, ..., Nb` and `M1, ..., Mc` are distinct sets of nonces (i.e., they differ by at least one element). Suppose further that the attacker controls the values of the nonces. The attacker's goal is to choose the nonce sets so that `checksum_leader == checksum_helper`. Thus the question is what we need to assume about `hash` to ensure that this is hard.\r\n> \r\n> It's not clear to me that this amounts to a standard assumption about cryptographic hash functions like SHA256. We might be better off using a MAC here instead.\r\n> \r\n> That said, I think it would be best to keep this change as simple as possible. I would be OK with merging as-is, so long as we add a note about the attack above in an \"OPEN ISSUE\" in the text.\r\n\r\nI think any hash function would permit an attacker to create a collision. Treating the hash function as a random oracle, the attacker could try random nonces until they find hash values that produce a basis for `Z_2^n` (where `n` is the output size of the hash in bits). I think this is expected to take right around `n` hash computations if I remember my linear algebra correctly. Then they can use this basis to select a set of nonces producing any desired checksum value. By repeating the process and finding a different basis, they can then produce a distinct set of nonces producing any desired checksum value. (I think this is similar to/the same as David's argument above.)\r\n\r\nSwapping out SHA256 for HMAC-SHA256 with the key kept secret by the aggregators would fix that particular issue since clients could no longer predict how a given nonce would contribute to the checksum; but as far as I'm aware there are no security proofs for this construction so there may be other attacks.\r\n\r\nThe lack of collision-resistance may be acceptable for two reasons IMO: (a) there are other mechanisms in PPM to make sure the aggregators are aggregating the same set of client reports, this is intended as a check against server malfunction/distributed systems errors; (b) since we are currently implementing the client-uploads-to-leader-only model, clients can't provide different sets of nonces to different aggregators. I'm still not entirely happy with it, however.\r\n\r\nI added an OPEN ISSUE and I think we should revisit this later.",
          "createdAt": "2022-03-04T22:12:43Z",
          "updatedAt": "2022-03-04T22:12:43Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs41dpOc",
          "commit": {
            "abbreviatedOid": "e8f6dd7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This would close #163. This LGTM, though I'm eager to hear what @cjpatton thinks about using XORed SHA256 here.",
          "createdAt": "2022-03-02T00:11:29Z",
          "updatedAt": "2022-03-02T00:11:29Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs41hI5v",
          "commit": {
            "abbreviatedOid": "e8f6dd7"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-02T17:03:45Z",
          "updatedAt": "2022-03-02T17:35:00Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "nit:\r\n```suggestion\r\nvalues with a bitwise-XOR operation.\r\n```",
              "createdAt": "2022-03-02T17:03:45Z",
              "updatedAt": "2022-03-02T17:35:00Z"
            },
            {
              "originalPosition": 45,
              "body": "I think we will want a separate error enumeration value to signify mismatches in either the report count or the checksum. However, I note that the rest of this aggregate share request section doesn't define how errors are signaled (i.e. for invalid batch parameters, or invalid helper state) so I think it's okay to punt on this for now.",
              "createdAt": "2022-03-02T17:32:05Z",
              "updatedAt": "2022-03-02T17:35:00Z"
            },
            {
              "originalPosition": 9,
              "body": "Slight rewording for clarity:\r\n```suggestion\r\nthe batch window. The checksum is computed by taking the SHA256 hash of each\r\nnonce from the client reports included in the aggregation, then combining the hash\r\n```",
              "createdAt": "2022-03-02T17:34:48Z",
              "updatedAt": "2022-03-02T17:35:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41hrta",
          "commit": {
            "abbreviatedOid": "e8f6dd7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-02T18:55:40Z",
          "updatedAt": "2022-03-02T18:55:40Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "Indeed, I worded this PR to continue to punt on error reporting as much as error reporting was previously punted on. (but FWIW, I agree we'd want to be able to discriminate the checksum/count-mismatch error case from other error cases)",
              "createdAt": "2022-03-02T18:55:40Z",
              "updatedAt": "2022-03-02T18:55:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41hr7b",
          "commit": {
            "abbreviatedOid": "e8f6dd7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-02T18:56:29Z",
          "updatedAt": "2022-03-02T18:56:29Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Huh, I always thought \"bytewise\" was more common than \"bitwise\", but a quick Googling shows this is not the case.",
              "createdAt": "2022-03-02T18:56:29Z",
              "updatedAt": "2022-03-02T18:56:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41rVuj",
          "commit": {
            "abbreviatedOid": "6791b15"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "> Notes: it would be trivial for an adversary to generate sets of nonces that collide with one another. Since the goal is to provide a check against server malfunction, rather than a security check against malicious clients or other actors in the system, I think this is acceptable.\r\n\r\nI agree that we don't care about a malicious aggregator here, but we do care about malicious clients. We need to think through what an attacker can do if it controls a subset of the clients and can disrupt the transmission of messages between the aggregators.\r\n\r\nSpecifically, suppose the leader has computed the checksum\r\n\r\n```\r\nchucksum_leader = hash(N1) ^ ... ^ hash(Nb)\r\n```\r\n\r\nand the helper has computed the checksum\r\n\r\n```\r\nchecksum_helper = hash(M1) ^ ... ^ hash(Mc]\r\n```\r\n\r\nwhere `N1, ..., Nb` and `M1, ..., Mc` are distinct sets of nonces (i.e., they differ by at least one element).  Suppose further that the attacker controls the values of the nonces. The attacker's goal is to choose the nonce sets so that `checksum_leader == checksum_helper`. Thus the question is what we need to assume about `hash` to ensure that this is hard.\r\n\r\nIt's not clear to me that this amounts to a standard assumption about cryptographic hash functions like SHA256. We might be better off using a MAC here instead.\r\n\r\nThat said, I think it would be best to keep this change as simple as possible. I would be OK with merging as-is, so long as we add a note about the attack above in an \"OPEN ISSUE\" in the text.",
          "createdAt": "2022-03-04T18:10:24Z",
          "updatedAt": "2022-03-04T18:10:24Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs41sCOc",
          "commit": {
            "abbreviatedOid": "6791b15"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM at least provisionally.",
          "createdAt": "2022-03-04T21:27:34Z",
          "updatedAt": "2022-03-04T21:28:06Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "I don't object to this, really, but it seems like kind of overkill to use SHA-256.",
              "createdAt": "2022-03-04T21:27:34Z",
              "updatedAt": "2022-03-04T21:28:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41sKGc",
          "commit": {
            "abbreviatedOid": "6791b15"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-04T22:12:40Z",
          "updatedAt": "2022-03-04T22:12:40Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "I agree, a non-cryptographic hash would be fine here IMO since we think we don't need a collision-resistant checksum, since the overall construction permits attackers to produce collisions anyway, and since using a cryptographic hash may give the false impression that the overall construction is actually secure.\r\n\r\nI'm going to leave the OPEN ISSUE for now but we should consider addressing this -- either switching to a `checksum` algorithm that actually does provide collision-resistance over the set of nonces, or switch away from using cryptographic-strength primitives entirely.",
              "createdAt": "2022-03-04T22:12:40Z",
              "updatedAt": "2022-03-04T22:14:21Z"
            }
          ]
        }
      ]
    },
    {
      "number": 196,
      "id": "PR_kwDOFEJYQs4z-tEm",
      "title": "Small terminology fixes",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/196",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Here are a few small editorial changes:\r\n\r\n- Refer to TLS's \"presentation language\", per section 3 of the RFC.\r\n- The output `enc` from the HPKE setup function is an encapsulated key, not an encapsulated context. The term context is defined as \"A context is an implementation-specific structure...\", whereas `enc` is alternately described as \"an encapsulated KEM shared secret\", an \"encapsulated key\", or a \"KEM encapsulated key\".\r\n- Remove stray whitespace from a hyphenated word.",
      "createdAt": "2022-03-04T23:08:28Z",
      "updatedAt": "2022-03-07T18:56:13Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "29ce456d5b49020ab5af519ede0ab434f20c3827",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "david/terminology-fixes",
      "headRefOid": "ceb09e158daa239ddd127349efab2c54daf14bc2",
      "closedAt": "2022-03-07T18:53:09Z",
      "mergedAt": "2022-03-07T18:53:09Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "74d02295233c8d222a25be78b9828751ff3642b2"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs41sfCO",
          "commit": {
            "abbreviatedOid": "ceb09e1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "One non-blocking comment.",
          "createdAt": "2022-03-05T01:37:50Z",
          "updatedAt": "2022-03-05T01:38:36Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "This is sometimes referred to as the \"encapsulated context\", since it's used to construct an HPKE context. That said, I'm fine with this change.",
              "createdAt": "2022-03-05T01:37:50Z",
              "updatedAt": "2022-03-05T01:38:36Z"
            }
          ]
        }
      ]
    },
    {
      "number": 197,
      "id": "PR_kwDOFEJYQs4z-wev",
      "title": "Add HMAC-SHA256 tag to all Request/Response messages.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/197",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Previously these were inconsistently applied between different messages.\r\nI introduce a new `collect_auth_key`, distinct from the existing\r\n`agg_auth_key`, for collect messages.\r\n\r\nI think we could get away with only authenticating the request messages\r\n(leaning on TLS to provide authentication for the responses); I'd be\r\namenable to that, but my understanding is that we don't want to rely on\r\nthe properties of the underlying channel, so I went ahead and specified\r\nauthentication tags for both request & response messages.",
      "createdAt": "2022-03-04T23:49:37Z",
      "updatedAt": "2022-09-16T00:30:24Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "d8c6677eb02284721195643fb95f4b07eb7be6dd",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "tags-for-everything",
      "headRefOid": "49110a3c3c77dc749e77fb76bb4d1908ec3969f1",
      "closedAt": "2022-03-08T17:01:21Z",
      "mergedAt": "2022-03-08T17:01:21Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "71b7eced785a5dada6d9b581652070dba023325c"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Tagging the AggregateShareReq with the key shared by the aggregators makes sense. (The fact that this was missing was an oversight.)\r\n> \r\n> However I think we should sync up before making changes to the collect flow. I envisioned using mutual TLS for this, Mozilla might have a different idea.\r\n\r\nOK, I've removed the `tag` from the `Collect{Req,Resp}` for now. mTLS would work too, and I'll be happy to go with whatever is determined by group consensus.",
          "createdAt": "2022-03-07T17:14:23Z",
          "updatedAt": "2022-03-07T17:14:23Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs41sfF9",
          "commit": {
            "abbreviatedOid": "6e627fa"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Tagging the AggregateShareReq with the key shared by the aggregators makes sense. (The fact that this was missing was an oversight.)\r\n\r\nHowever I think we should sync up before making changes to the collect flow. I envisioned using mutual TLS for this, Mozilla might have a different idea.",
          "createdAt": "2022-03-05T01:39:23Z",
          "updatedAt": "2022-03-05T01:46:02Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "This key would only need to be shared by the collector and leader. The helper doesn't need this key, does it?",
              "createdAt": "2022-03-05T01:39:23Z",
              "updatedAt": "2022-03-05T01:46:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41w1jK",
          "commit": {
            "abbreviatedOid": "6e627fa"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-07T17:11:17Z",
          "updatedAt": "2022-03-07T17:11:17Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "That's correct, this should have specified \"the leader and the collector\" rather than \"the aggregators and the collector\".",
              "createdAt": "2022-03-07T17:11:17Z",
              "updatedAt": "2022-03-07T17:11:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41xCU8",
          "commit": {
            "abbreviatedOid": "49110a3"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-07T17:52:25Z",
          "updatedAt": "2022-03-07T17:52:25Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs41xgVR",
          "commit": {
            "abbreviatedOid": "49110a3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-07T19:43:00Z",
          "updatedAt": "2022-03-07T19:43:00Z",
          "comments": []
        }
      ]
    },
    {
      "number": 198,
      "id": "PR_kwDOFEJYQs40M2Vd",
      "title": "fix HPKE info strings and specify agg share handling",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/198",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The info strings used in HPKE context construction used the prefix\r\n\"pda\", referring to the old \"Private Data Aggregation\" name of this\r\nprotocol. This commit also adds text instructing the collector how to\r\ndecrypt the aggregate shares it receives and to use the VDAF's\r\nunsharding algorithm to obtain aggregate results.",
      "createdAt": "2022-03-09T22:15:52Z",
      "updatedAt": "2022-09-16T00:30:28Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "74d02295233c8d222a25be78b9828751ff3642b2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/hpke-app-info",
      "headRefOid": "c62b2d3b69b493dcc332109a656a78e42114cedf",
      "closedAt": "2022-03-15T21:54:55Z",
      "mergedAt": "2022-03-15T21:54:55Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f469f5ed1e24eafe00b6cf533158c0c97610bb88"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Question for the reviewers: VDAF defines labels like `vdaf-00 prio3` ([VDAF sharding spec](https://cjpatton.github.io/vdaf/draft-patton-cfrg-vdaf.html#section-6.2.2)). Should we include the draft version number in info strings like this? e.g. `ppm-01 input share`?",
          "createdAt": "2022-03-09T22:23:37Z",
          "updatedAt": "2022-03-09T22:23:37Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs418kCu",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM!\r\n\r\n> Question for the reviewers: VDAF defines labels like `vdaf-00 prio3` ([VDAF sharding spec](https://cjpatton.github.io/vdaf/draft-patton-cfrg-vdaf.html#section-6.2.2)). Should we include the draft version number in info strings like this? e.g. `ppm-01 input share`?\r\n\r\nI'd be in favor of this, though we might want to go with `ppm-00` since `00` will be the first draft of the WG doc when it gets adopted.",
          "createdAt": "2022-03-09T22:35:02Z",
          "updatedAt": "2022-03-09T22:35:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs418ka9",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-09T22:35:48Z",
          "updatedAt": "2022-03-09T22:42:25Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "While this works, wouldn't it be more natural to specify `0x00` for the leader and `0x01` for the helper? This would permit a more natural extension to multiple helpers (which would be able to use `0x02`, `0x03`, ... without placing the leader between the first and second helpers) and would cause these role identifier numbers to match up with the ordering of the input shares in a client report, aggregate shares in an collect result, etc.\r\n\r\n(I think this may be somewhat beyond the scope of this individual PR, is this an issue that has been discussed? I notice the interop PR specifies a Role message that has not only the leader & helper but also the collector & client.)",
              "createdAt": "2022-03-09T22:35:48Z",
              "updatedAt": "2022-03-09T22:42:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs418mvT",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-09T22:44:29Z",
          "updatedAt": "2022-03-09T22:44:29Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "Yeah I think this would be a good change, but it should go in a different PR as you suggest.",
              "createdAt": "2022-03-09T22:44:29Z",
              "updatedAt": "2022-03-09T22:44:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs418tQb",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-09T23:23:21Z",
          "updatedAt": "2022-03-09T23:23:22Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "I'd be open to back-porting the language around this from PR #179. There we put leader and helper as `0x02` and `0x03` so that we can allow further helpers to be `0x04` and above, and also extend the info string to include the roles of both sender and recipient. I think that change should be fairly non-controversial, and it'll make it easier to eventually land #179 if we chip little bits of it out.",
              "createdAt": "2022-03-09T23:23:22Z",
              "updatedAt": "2022-03-09T23:23:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs418wAd",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-09T23:30:24Z",
          "updatedAt": "2022-03-09T23:30:25Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "Good catch, @tgeoghegan ",
              "createdAt": "2022-03-09T23:30:24Z",
              "updatedAt": "2022-03-09T23:30:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs41_-KJ",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-10T14:57:14Z",
          "updatedAt": "2022-03-10T15:08:41Z",
          "comments": [
            {
              "originalPosition": 41,
              "body": "```suggestion\r\n`server_role` is the role of the server that sent the aggregate share (`0x01`\r\n```",
              "createdAt": "2022-03-10T14:57:15Z",
              "updatedAt": "2022-03-10T15:08:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42F6Wh",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-11T17:28:21Z",
          "updatedAt": "2022-03-11T17:28:26Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "Backporting this from the interop PR when it is merged into main SGTM.",
              "createdAt": "2022-03-11T17:28:21Z",
              "updatedAt": "2022-03-11T17:28:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42SjPP",
          "commit": {
            "abbreviatedOid": "c62b2d3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-15T21:06:50Z",
          "updatedAt": "2022-03-15T21:06:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 199,
      "id": "PR_kwDOFEJYQs40NPJI",
      "title": "Editorial nits",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/199",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Nits for the interop PR.",
      "createdAt": "2022-03-10T00:55:56Z",
      "updatedAt": "2022-09-16T00:30:29Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "71b7eced785a5dada6d9b581652070dba023325c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatotn/interop-nits",
      "headRefOid": "8bd1d91fea1421c05e89517347a51a45014700c0",
      "closedAt": "2022-03-10T16:00:50Z",
      "mergedAt": "2022-03-10T16:00:49Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "02ce5110e30ab34c1969bdfcd3fe618e1ff3468a"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs41_3dz",
          "commit": {
            "abbreviatedOid": "8bd1d91"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-10T14:40:30Z",
          "updatedAt": "2022-03-10T14:40:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 200,
      "id": "PR_kwDOFEJYQs40Ql33",
      "title": "Allow the extensions list to be empty",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/200",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Currently, the minimum allowed length of the extensions vector is four bytes, which is exactly enough to encode one `Extension` containing a type and an `extension_data` of zero bytes. It appears this is in error, as we would like clients to be able to encode an empty vector of extensions in the normal case.",
      "createdAt": "2022-03-10T19:30:54Z",
      "updatedAt": "2022-03-10T21:11:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "74d02295233c8d222a25be78b9828751ff3642b2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "david/empty-extensions-list",
      "headRefOid": "49f7716f4800aa208871f290fa7436f06d9e65a3",
      "closedAt": "2022-03-10T21:10:57Z",
      "mergedAt": "2022-03-10T21:10:57Z",
      "mergedBy": "divergentdave",
      "mergeCommit": {
        "oid": "c7ad24e2ea42dc9d3b125cb8228aaa4384ccc8b5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs42BymY",
          "commit": {
            "abbreviatedOid": "49f7716"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-10T20:48:08Z",
          "updatedAt": "2022-03-10T20:48:08Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs42B4CO",
          "commit": {
            "abbreviatedOid": "49f7716"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-10T21:07:31Z",
          "updatedAt": "2022-03-10T21:07:31Z",
          "comments": []
        }
      ]
    },
    {
      "number": 201,
      "id": "PR_kwDOFEJYQs40Q1p2",
      "title": "Rename CollectResult to CollectResp.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/201",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "CollectResp is paired with CollectReq; naming things this way matches\r\nother {Req, Resp} message pairs.",
      "createdAt": "2022-03-10T20:56:23Z",
      "updatedAt": "2022-09-16T00:30:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "74d02295233c8d222a25be78b9828751ff3642b2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "collect-result-to-resp",
      "headRefOid": "defc48f8dbcd3e74dbaca3f23de9c6cd608ae77f",
      "closedAt": "2022-03-10T21:06:20Z",
      "mergedAt": "2022-03-10T21:06:20Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "8de0e04f5ec474a5224554921e299018b58e1f54"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs42B2K_",
          "commit": {
            "abbreviatedOid": "defc48f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-10T21:01:24Z",
          "updatedAt": "2022-03-10T21:01:24Z",
          "comments": []
        }
      ]
    },
    {
      "number": 203,
      "id": "PR_kwDOFEJYQs40a2-B",
      "title": "Interop: Error handling and editorial things",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/203",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-03-14T19:52:43Z",
      "updatedAt": "2022-09-16T00:30:36Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "97b6a3431b14953b1ea0ee19b5409cbcfb3b8faf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/interop-agg-order",
      "headRefOid": "0d7fdcc23cfc4ea8c4a2dfb671430171745d038e",
      "closedAt": "2022-03-15T21:54:28Z",
      "mergedAt": "2022-03-15T21:54:28Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f7017a203f4fe7f13a710f7cb6491923eed925c3"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs42M7cZ",
          "commit": {
            "abbreviatedOid": "10e8ef9"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-14T20:31:11Z",
          "updatedAt": "2022-03-14T20:48:25Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "I'm not sure I agree with the change of wording from \"observed\" to \"aggregated\" here.\r\n\r\nThere will be a gap of time from when a client upload completes to when it is first included in an aggregation job; my interpretation would be that those reports were \"observed\" (or perhaps \"received\") but not yet \"aggregated\". I think we want this error logic to cover reports that have not yet been included in an aggregation job, right?",
              "createdAt": "2022-03-14T20:31:12Z",
              "updatedAt": "2022-03-14T20:48:25Z"
            },
            {
              "originalPosition": 85,
              "body": "```suggestion\r\naggregator does not have to maintain this storage indefinitely, it MAY instead\r\n```\r\n\r\n(nit, typo fix)",
              "createdAt": "2022-03-14T20:36:55Z",
              "updatedAt": "2022-03-14T20:48:25Z"
            },
            {
              "originalPosition": 131,
              "body": "This splitting of VDAF output into two parts is new to me -- how is this to be done? e.g. is the definition of how to split the output into a tuple part of the VDAF?",
              "createdAt": "2022-03-14T20:42:12Z",
              "updatedAt": "2022-03-14T20:48:25Z"
            },
            {
              "originalPosition": 177,
              "body": "```suggestion\r\nits new preparation state and `outbound` is its next VDAF messaage, and\r\n```\r\n\r\n(nit, typo fix)",
              "createdAt": "2022-03-14T20:44:10Z",
              "updatedAt": "2022-03-14T20:48:25Z"
            },
            {
              "originalPosition": 178,
              "body": "```suggestion\r\ncontinues with `outbound` as its next VDAF message. Either way, it moves to\r\n```\r\n\r\n(nit, typo fix)",
              "createdAt": "2022-03-14T20:44:56Z",
              "updatedAt": "2022-03-14T20:48:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42NCs5",
          "commit": {
            "abbreviatedOid": "10e8ef9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-14T20:58:27Z",
          "updatedAt": "2022-03-14T21:00:59Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "Right, \"observed\" (or \"received\") is a a weaker condition than \"aggregated\": If a report has been aggregated, then it has also been observed; but if a report has been observed, it has not necessarily been aggregated.\r\n\r\nI think what we want here is \"aggregated\", for two reasons. First, it's more specific. Second, I suspect it'll be easier for servers to verify that they have agrgregated a report than it'll be for servers to verify they've observed a report, in particular because this is the aggregation endpoint.",
              "createdAt": "2022-03-14T20:58:27Z",
              "updatedAt": "2022-03-14T21:00:59Z"
            },
            {
              "originalPosition": 131,
              "body": "Yeah, this is just a matter of aligning the notation with the current draft: https://cjpatton.github.io/vdaf/draft-patton-cfrg-vdaf.html#name-preparation",
              "createdAt": "2022-03-14T21:00:49Z",
              "updatedAt": "2022-03-14T21:00:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42NPId",
          "commit": {
            "abbreviatedOid": "35b1c8a"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-14T21:49:01Z",
          "updatedAt": "2022-03-14T21:53:30Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "That makes sense; my only note is that the client's Upload Request section (4.2.2) refers to this section as part of a `SHOULD`, so I suppose this logic may also apply to client upload endpoints. I think this is OK since the relevant bullet point (this one) is not meaningfully changed by the wording change. This would matter for the `report-replayed` point, but that's not mentioned by the client upload section for now; we might need to deal with it later but it's fine for now.",
              "createdAt": "2022-03-14T21:49:01Z",
              "updatedAt": "2022-03-14T21:53:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42NM4G",
          "commit": {
            "abbreviatedOid": "35b1c8a"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-14T21:38:36Z",
          "updatedAt": "2022-03-14T22:12:21Z",
          "comments": [
            {
              "originalPosition": 32,
              "body": "Should this be referring to the response here?\r\n```suggestion\r\n* If a fatal error is encountered while processing an HTTP response, the HTTP\r\n  client... [TODO: Figure out what behavior is called for here. An HTTP request\r\n```",
              "createdAt": "2022-03-14T21:38:36Z",
              "updatedAt": "2022-03-14T22:12:21Z"
            },
            {
              "originalPosition": 176,
              "body": "typo fix:\r\n```suggestion\r\ninterprets `out` as the tuple `(new_state, outbound)`, where `new_state` is\r\n```",
              "createdAt": "2022-03-14T21:55:53Z",
              "updatedAt": "2022-03-14T22:12:21Z"
            },
            {
              "originalPosition": 179,
              "body": "I think this is incorrect, the sentence two sentences prior is about the leader finishing. (and, moreover, it should probably say that the leader moves to state `FINISHED`)\r\n```suggestion\r\ncontinues with `outbound` as its next VDAF message, moving to state WAITING.\r\n```",
              "createdAt": "2022-03-14T21:59:55Z",
              "updatedAt": "2022-03-14T22:12:21Z"
            },
            {
              "originalPosition": 260,
              "body": "nit\r\n```suggestion\r\nNext, it checks that the sequence of Transition messages corresponds to\r\n```",
              "createdAt": "2022-03-14T22:02:23Z",
              "updatedAt": "2022-03-14T22:12:21Z"
            },
            {
              "originalPosition": 262,
              "body": "I believe that \"or is missing\" conflicts with section 4.4.4.1, assuming a multi-round VDAF. That section says that the leader will filter out reports that correspond to a state of FAILED, in which case the next AggregateContinueReq would have fewer transitions/nonces/reports than the AggregateInitReq and AggregateResp that came before it.",
              "createdAt": "2022-03-14T22:11:21Z",
              "updatedAt": "2022-03-14T22:12:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42NoI1",
          "commit": {
            "abbreviatedOid": "35b1c8a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-15T00:44:18Z",
          "updatedAt": "2022-03-15T00:53:46Z",
          "comments": [
            {
              "originalPosition": 32,
              "body": "Yes, good catch.",
              "createdAt": "2022-03-15T00:44:18Z",
              "updatedAt": "2022-03-15T00:53:46Z"
            },
            {
              "originalPosition": 179,
              "body": "For the leader, \"WAITING\" means I'm waiting for the helper's Transition before either (1) I can compute my next VDAF message or (2) I commit to the output share I've just computed. Does that make sense or am I misunderstanding something?",
              "createdAt": "2022-03-15T00:49:02Z",
              "updatedAt": "2022-03-15T00:53:46Z"
            },
            {
              "originalPosition": 262,
              "body": "Hmm, maybe we just need to sharpen this a little. This paragraph pertains to the leader processing `AggregateResp`. The helper is not allowed to change the sequence of nonces relative to the previous `AggregateReq`. In particular, if a nonce is missing (relative to the previous request), then we abort.",
              "createdAt": "2022-03-15T00:53:18Z",
              "updatedAt": "2022-03-15T00:53:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42Nprm",
          "commit": {
            "abbreviatedOid": "10e8ef9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-15T00:57:59Z",
          "updatedAt": "2022-03-15T00:57:59Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "Hmm, interesting. We should to try to work this out now, if possible. I think what we want to say that, before a batch is collected, two reports with the same nonce MUST NOT be aggregated twice. Furthermore, the leader SHOULD report to the client if it attempted to upload a report with a previously observed nonce.",
              "createdAt": "2022-03-15T00:57:59Z",
              "updatedAt": "2022-03-15T00:57:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42RBpy",
          "commit": {
            "abbreviatedOid": "0d7fdcc"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-15T15:48:15Z",
          "updatedAt": "2022-03-15T15:48:16Z",
          "comments": [
            {
              "originalPosition": 179,
              "body": "Yes, that makes sense, thank you.",
              "createdAt": "2022-03-15T15:48:16Z",
              "updatedAt": "2022-03-15T15:48:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42RC65",
          "commit": {
            "abbreviatedOid": "35b1c8a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-15T15:51:25Z",
          "updatedAt": "2022-03-15T15:51:26Z",
          "comments": [
            {
              "originalPosition": 179,
              "body": "Yw, maybe we can be describing the state machine better...",
              "createdAt": "2022-03-15T15:51:26Z",
              "updatedAt": "2022-03-15T15:51:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42RKEe",
          "commit": {
            "abbreviatedOid": "10e8ef9"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-15T16:09:28Z",
          "updatedAt": "2022-03-15T16:09:28Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "Agreed, that behavior sounds reasonable.",
              "createdAt": "2022-03-15T16:09:28Z",
              "updatedAt": "2022-03-15T16:09:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42NeLA",
          "commit": {
            "abbreviatedOid": "0d7fdcc"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "While implementing upload today, I also noticed some inconsistencies between the handling of reports in the leader during the upload protocol and the helper ",
          "createdAt": "2022-03-14T23:20:43Z",
          "updatedAt": "2022-03-15T19:18:20Z",
          "comments": [
            {
              "originalPosition": 29,
              "body": "We should be constructing HTTP problem documents: https://datatracker.ietf.org/doc/html/rfc7807",
              "createdAt": "2022-03-14T23:20:43Z",
              "updatedAt": "2022-03-15T19:18:20Z"
            },
            {
              "originalPosition": 77,
              "body": "I think we need something equivalent to this in the leader's handling of the `upload` request. This gets awkward because now we have redundancy between the errors defined for the aggregate sub-protocol (`enum TransitionError`) and the errors defined in section 3.1 that can be encoded as HTTP problem documents. We should try to unify these.",
              "createdAt": "2022-03-15T19:18:15Z",
              "updatedAt": "2022-03-15T19:18:21Z"
            }
          ]
        }
      ]
    },
    {
      "number": 204,
      "id": "PR_kwDOFEJYQs40f5R3",
      "title": "Extend HPKE info strings",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/204",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "When constructing HPKE contexts, we now include both sender and\r\nrecipient roles, and the application label also includes the PPM\r\nversion. I used `ppm-02` here because we recently submitted draft-01 of\r\n`draft-gpew-priv-ppm` and so the next submitted draft, which would include\r\nthis change, will be `-02`. I also backported the definition of `struct\r\nHpkeCiphertext` from the interop PR because it makes everything tidier\r\nand should be non-controversial we can get a head-start on bringing those\r\nchanges into main.",
      "createdAt": "2022-03-15T23:02:11Z",
      "updatedAt": "2022-09-16T00:30:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f469f5ed1e24eafe00b6cf533158c0c97610bb88",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/info-strings",
      "headRefOid": "8f03d46e2922aff3f042c59f63d8bdb7ead224c8",
      "closedAt": "2022-03-16T18:01:06Z",
      "mergedAt": "2022-03-16T18:01:06Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "5379d3ba701d36dcdb3b31be84c935541b8fcfff"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> There's one more instance of `EncryptedInputShare.config_id` that needs to be changed to `HpkeCiphertext.config_id`. Otherwise, LGTM.\r\n\r\nFixed, thank you! I also rewrote the relevant paragraph to acknowledge that there's two encrypted input shares and that the leader should examine the HPKE config ID of the one intended for itself. Further, I recall @cjpatton pointing out to me that should this draft get adopted by the WG, we'll reset to something like `draft-authors-ppm-ppm-01`, so `ppm-01` is the more appropriate application label.",
          "createdAt": "2022-03-16T15:39:34Z",
          "updatedAt": "2022-03-16T15:39:34Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs42V4Xo",
          "commit": {
            "abbreviatedOid": "4b298e7"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "There's one more instance of `EncryptedInputShare.config_id` that needs to be changed to `HpkeCiphertext.config_id`. Otherwise, LGTM.",
          "createdAt": "2022-03-16T14:16:22Z",
          "updatedAt": "2022-03-16T14:16:22Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs42W5vD",
          "commit": {
            "abbreviatedOid": "f6e8758"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Just a few minor things.",
          "createdAt": "2022-03-16T16:52:43Z",
          "updatedAt": "2022-03-16T16:55:02Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "Assuming this gets adopted by the WG, the first draft will be \"draft-ietf-ppm-00\" (or something like that). If you take this suggestion, don't forget to apply it below.\r\n\r\n```suggestion\r\nenc, context = SetupBaseS(pk, Report.task_id || \"ppm-00 input share\" ||\r\n```",
              "createdAt": "2022-03-16T16:52:43Z",
              "updatedAt": "2022-03-16T16:55:02Z"
            },
            {
              "originalPosition": 73,
              "body": "I don't think so. I think we can drop this.",
              "createdAt": "2022-03-16T16:53:30Z",
              "updatedAt": "2022-03-16T16:55:02Z"
            },
            {
              "originalPosition": 149,
              "body": "Might be good to remind the reader here that \"0x01\" stands for the client.",
              "createdAt": "2022-03-16T16:54:19Z",
              "updatedAt": "2022-03-16T16:55:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42XL6E",
          "commit": {
            "abbreviatedOid": "5cbfabd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-16T17:49:58Z",
          "updatedAt": "2022-03-16T17:49:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 205,
      "id": "PR_kwDOFEJYQs40f_Mm",
      "title": "Define leader-to-helper abort procedure.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/205",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This solution optimizes for the size of the specification edit; if it\r\nworks out well, we should clean it up & attempt to unify this abort\r\nprocedure with the helper-to-leader abort procedure.",
      "createdAt": "2022-03-15T23:51:33Z",
      "updatedAt": "2023-10-26T15:44:24Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "f7017a203f4fe7f13a710f7cb6491923eed925c3",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "brandon/aggregate-abort",
      "headRefOid": "ac459acac1b4cf9140567c0f1dd0edc77c2b7dc6",
      "closedAt": "2022-09-16T00:31:26Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "So I had a thought about this this morning.  I'm questioning whether we actually need a mechanism for the Leader->Heper alert. IIUC the motivation for this signal that we discussed is that helper either (1) needs to tear down the HTTP session and all associated state or (2) \u201crevert\u201d any output shares it just accepted.\r\n\r\nFor (1) I wonder if a sufficient signal is that the HTTP session has been torn down, but the aggregation run isn\u2019t complete.\r\n\r\nWe\u2019re intentionally leaving (2) out of scope for now. Regardless, I think we would handle it similar to what we're doing here, but we don't need to do it now. What do y'all think?",
          "createdAt": "2022-03-16T15:05:42Z",
          "updatedAt": "2022-03-16T15:05:42Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm questioning whether we actually need a mechanism for the Leader->Heper alert.\r\n\r\nI think we can \"do nothing\" & ignore leader->helper alerts as long as we're OK with the helper never finding out a given aggregation job has been aborted. I don't think that causes problems with the spec & I don't think it will cause serious problems in implementations -- it seems like, at worst, we'd end up with the helper holding on to its aggregation data longer than needed.",
          "createdAt": "2022-03-16T16:26:19Z",
          "updatedAt": "2022-03-16T16:26:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "What do @tgeoghegan and @divergentdave think?",
          "createdAt": "2022-03-16T16:50:40Z",
          "updatedAt": "2022-03-16T16:50:40Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "What we discussed yesterday is that in the scope of the interop target, the only thing a helper would do with this alert is log it. I'm comfortable with not doing anything for now, on the premise that the aggregate share checksums will let us detect mismatches and we'll have a tight communication loop between aggregator operators.\r\n\r\nI do think we should keep thinking about whether we need  a Leader->Helper `AggregateCommit` message, but part of the point of the interop experiment is to find out how valuable that would be.\r\n\r\nIf you need my vote one way or the other: I say let's do nothing for interop.",
          "createdAt": "2022-03-16T17:50:23Z",
          "updatedAt": "2022-03-16T17:50:23Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Doing nothing is fine with me. Our underlying motivation is to resolve the issue that \"alert its peer\" is not implementable in the context of a leader processing an HTTP response from a helper, and I think not sending the error message to the helper at all would be a fine way to solve this for now. The only sensible action the helper could take is to delete some of its stored state. We can address deletion of saved prepare messages in the spec at a later point, when we address adding a \"time to live\" concept for reports, etc.",
          "createdAt": "2022-03-16T18:09:31Z",
          "updatedAt": "2022-03-16T18:09:31Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Doing nothing is fine for me too; retracting this PR, I'll leave it as a draft for a bit in case we want to pull something out of it in the future.",
          "createdAt": "2022-03-16T19:12:28Z",
          "updatedAt": "2022-03-16T19:12:28Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs42XK3p",
          "commit": {
            "abbreviatedOid": "ac459ac"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Implementation looks good.",
          "createdAt": "2022-03-16T17:46:10Z",
          "updatedAt": "2022-03-16T17:46:10Z",
          "comments": []
        }
      ]
    },
    {
      "number": 206,
      "id": "PR_kwDOFEJYQs40jMN4",
      "title": "Interop nits",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/206",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Minor changes/clarifications:\r\n\r\n* Clarify how to handle empty Transition sequences\r\n* Handle repeated nonces by aborting. While at it, this changes how the helper handles unrecognized nonces to be consistent with how the leader handles unrecognized nonces.\r\n* Move task/job ID to top of AggregateReq",
      "createdAt": "2022-03-16T17:44:42Z",
      "updatedAt": "2022-09-16T00:30:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "f7017a203f4fe7f13a710f7cb6491923eed925c3",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/interop-nits",
      "headRefOid": "f736eb913110c1725b6ef5757d7ad2ea8121288d",
      "closedAt": "2022-03-16T20:16:12Z",
      "mergedAt": "2022-03-16T20:16:12Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "6400e6bdc58eaf25afff95024de26ad1ee229cd7"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs42XOOi",
          "commit": {
            "abbreviatedOid": "423b962"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-16T17:58:24Z",
          "updatedAt": "2022-03-16T18:00:35Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "nit: the usages \"first\" and \"next\" are too prescriptive on how this should be done instead of stating what property is meant to be enforced. As written this suggests a helper has to iterate over `AggregateInitReq.seq` to check for duplicate nonces, and then again to compute initial preparation state, but an implementation could do both of these in a single pass through the `seq`, aborting and discarding any computed prep states if a duplicate nonce is found. Though I don't think we should wordsmith all that here; we can debate style and wording back on `main` in the months to come.",
              "createdAt": "2022-03-16T17:58:24Z",
              "updatedAt": "2022-03-16T18:00:35Z"
            },
            {
              "originalPosition": 82,
              "body": "```suggestion\r\ncorresponding state is FAILED and proceeds as described in the next section,\r\n```",
              "createdAt": "2022-03-16T17:58:49Z",
              "updatedAt": "2022-03-16T18:00:35Z"
            },
            {
              "originalPosition": 114,
              "body": "```suggestion\r\n{{aggregate-message-auth}}, the helper begins by scanning\r\n```",
              "createdAt": "2022-03-16T17:59:39Z",
              "updatedAt": "2022-03-16T18:00:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42Xo5j",
          "commit": {
            "abbreviatedOid": "423b962"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-16T19:36:20Z",
          "updatedAt": "2022-03-16T19:36:20Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Yeah this is a very important point, but one we want to address throughout. Happy to address it here, if you like. In any case let's keep this in mind.",
              "createdAt": "2022-03-16T19:36:20Z",
              "updatedAt": "2022-03-16T19:36:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42XsDH",
          "commit": {
            "abbreviatedOid": "fcacf78"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-03-16T19:48:44Z",
          "updatedAt": "2022-03-16T19:54:55Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "nit: we can remove the note below this struct now",
              "createdAt": "2022-03-16T19:48:45Z",
              "updatedAt": "2022-03-16T19:54:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42XuZ5",
          "commit": {
            "abbreviatedOid": "423b962"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-16T19:57:39Z",
          "updatedAt": "2022-03-16T19:57:39Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I don't think it's worth making changes of that nature to the text in the interop branch because it'll make the eventual merge harder. Let's just keep this in mind.",
              "createdAt": "2022-03-16T19:57:39Z",
              "updatedAt": "2022-03-16T19:57:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs42Xyy2",
          "commit": {
            "abbreviatedOid": "fcacf78"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-16T20:14:20Z",
          "updatedAt": "2022-03-16T20:14:20Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "Done",
              "createdAt": "2022-03-16T20:14:20Z",
              "updatedAt": "2022-03-16T20:14:20Z"
            }
          ]
        }
      ]
    },
    {
      "number": 207,
      "id": "PR_kwDOFEJYQs40x_RJ",
      "title": "Ietf113 slides",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/207",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "My slides, just FYI",
      "createdAt": "2022-03-22T02:10:31Z",
      "updatedAt": "2022-04-05T19:51:26Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5379d3ba701d36dcdb3b31be84c935541b8fcfff",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "ietf113-slides",
      "headRefOid": "85f0427aae0fabb90b0bf7e69bd2c30c8e083dc3",
      "closedAt": "2022-04-05T19:51:26Z",
      "mergedAt": "2022-04-05T19:51:26Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b1893aabd606f7dcecc1a8bece8027889821ec9e"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "How's about merging all of ours slides into one PDF? cc/ @chris-wood , @tgeoghegan ",
          "createdAt": "2022-03-22T02:48:42Z",
          "updatedAt": "2022-03-22T02:48:42Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I'd actually rather not because it makes it hard to make last minute changes. The Chairs are fine at handling multiple slides.",
          "createdAt": "2022-03-22T02:58:25Z",
          "updatedAt": "2022-03-22T02:58:25Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 208,
      "id": "PR_kwDOFEJYQs404luA",
      "title": "base64-encode TaskID in problem details JSON object",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/208",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Section 3.1 says to include the PPM task ID in a field on the problem details JSON object, but the task ID may be any 32 byte value, and may not be representable as a Unicode string. I propose that we base64-encode the task ID here to resolve this.",
      "createdAt": "2022-03-23T13:56:58Z",
      "updatedAt": "2022-04-01T15:58:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5379d3ba701d36dcdb3b31be84c935541b8fcfff",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "david/error-task-id-base64",
      "headRefOid": "078d474529ca83cf2c770b014521f38d93361ffa",
      "closedAt": "2022-04-01T15:58:48Z",
      "mergedAt": "2022-04-01T15:58:48Z",
      "mergedBy": "divergentdave",
      "mergeCommit": {
        "oid": "684f3604bd521878e79400de59c5a1705f7cfe10"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs43IwPr",
          "commit": {
            "abbreviatedOid": "00be804"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Good call! One clarifying comment.",
          "createdAt": "2022-03-29T18:11:35Z",
          "updatedAt": "2022-03-29T18:11:47Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I suppose we'd need to say whether we mean \"standard\" encoding, per https://datatracker.ietf.org/doc/html/rfc4648#section-4?",
              "createdAt": "2022-03-29T18:11:35Z",
              "updatedAt": "2022-03-29T18:11:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs43I1rw",
          "commit": {
            "abbreviatedOid": "00be804"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-29T18:31:44Z",
          "updatedAt": "2022-03-29T18:31:45Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I was leaning on \"This encoding may be referred to as 'base64'\" when I wrote this, but I'm open to other wording. \"encoded with base64 using the standard alphabet\"?",
              "createdAt": "2022-03-29T18:31:45Z",
              "updatedAt": "2022-03-29T18:31:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs43I3bA",
          "commit": {
            "abbreviatedOid": "00be804"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-03-29T18:38:17Z",
          "updatedAt": "2022-03-29T18:38:17Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I think the latter will be more clear in the long run.",
              "createdAt": "2022-03-29T18:38:17Z",
              "updatedAt": "2022-03-29T18:38:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs43YOha",
          "commit": {
            "abbreviatedOid": "078d474"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-01T15:26:16Z",
          "updatedAt": "2022-04-01T15:26:16Z",
          "comments": []
        }
      ]
    },
    {
      "number": 212,
      "id": "PR_kwDOFEJYQs41nUiq",
      "title": "add a `task_id` query parameter for `hpke_config`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/212",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "# This PR targets the interop target branch\r\n\r\nAll protocol request messages contain a `task_id` field, which allows\r\naggregators to service multiple tasks from a single set of endpoints.\r\nThe exception is `hpke_config`: since it is an HTTP GET request, there\r\nis no body, and thus no place for the client to indicate the desired\r\n`task_id` to the aggregator. We don't want to introduce a request body\r\nto `hpke_config`, because then it would have to be a POST instead of a\r\nGET, and that would make it harder for implementations to cache HPKE\r\nconfig values. So instead we have clients encode the task ID in RFC 4648\r\nBase16 (a.k.a. hexadecimal) and provide it as a query parameter.\r\n\r\nSee issue #146",
      "createdAt": "2022-04-04T21:22:54Z",
      "updatedAt": "2022-09-16T00:30:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "aebe375f3f7b37d1e2c240739e7f0308e01a656e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/interop-hpke-config-query-param",
      "headRefOid": "dddd1c4fa567cd720e85a1a564d357207bbd6a44",
      "closedAt": "2022-04-06T23:03:21Z",
      "mergedAt": "2022-04-06T23:03:21Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "b734ac2272e58b839b8c5d5511e518e42b324956"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "My goal here is to enable our aggregator implementations in the interop target to use one HPKE config per task. The alternative to this would be for us to use aggregator endpoints that somehow incorporate the task ID, as I described [here](https://github.com/divviup/janus/issues/48#issuecomment-1085209198), which is already permitted by the spec as written.",
          "createdAt": "2022-04-04T21:25:54Z",
          "updatedAt": "2022-04-04T21:25:54Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> SGTM, but why not base64 encode the taskID for consistency with #208?\r\n\r\nThe Base64 alphabet includes `/`, `+` and `=`, which means you have to URL encode (a.k.a. percent encode) Base64 strings before you can use them in a URL, which is a chore and yields less readable URLs (not that 64 character hex strings is especially user-friendly). An alternative would be to use RFC 4648's [`base64url` encoding](https://datatracker.ietf.org/doc/html/rfc4648#section-5), omitting the `=` padding characters as suggested in [RFC 4648 section 3.2](https://datatracker.ietf.org/doc/html/rfc4648#section-3.2) since task IDs are of a known length.",
          "createdAt": "2022-04-04T21:38:43Z",
          "updatedAt": "2022-04-04T21:40:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> > SGTM, but why not base64 encode the taskID for consistency with #208?\r\n> \r\n> The Base64 alphabet includes `/`, `+` and `=`, which means you have to URL encode (a.k.a. percent encode) Base64 strings before you can use them in a URL, which is a chore and yields less readable URLs (not that 64 character hex strings is especially user-friendly). An alternative would be to use RFC 4648's [`base64url` encoding](https://datatracker.ietf.org/doc/html/rfc4648#section-5).\r\n\r\nGotcha. I think the main thing is to be consistent. I would either:\r\n\r\n1. Change this PR to use base64url; or\r\n2. Tack on a commit that makes all encodings of the taskID (except those that appear in the body, of course) the same (i.e., hexadecimal).\r\n\r\nI think (2.) would be my preference.",
          "createdAt": "2022-04-04T22:04:50Z",
          "updatedAt": "2022-04-04T22:04:50Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with the consistency argument. I'm leaning towards using `base64url` everywhere because it's more compact than Base16/hex, and also I think implementations are less likely to be surprised by case requirements in base64url (i.e., it's obvious that base64 is case sensitive, but you have to read RFC 4648 attentively to notice that it mandates uppercase hex strings).",
          "createdAt": "2022-04-05T00:36:45Z",
          "updatedAt": "2022-04-05T00:36:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Base64Url sounds good to me. I took a quick look and it appears that most standard base64 libraries also support the URL encoding.",
          "createdAt": "2022-04-05T19:54:04Z",
          "updatedAt": "2022-04-05T19:54:04Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I will clean this up and merge it once I have implemented it in Janus and convinced myself it's sound.",
          "createdAt": "2022-04-05T20:22:01Z",
          "updatedAt": "2022-04-05T20:22:01Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs43fX2v",
          "commit": {
            "abbreviatedOid": "03e2114"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-04T21:26:04Z",
          "updatedAt": "2022-04-04T21:26:04Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs43fZ9S",
          "commit": {
            "abbreviatedOid": "03e2114"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "SGTM, but why not base64 encode the taskID for consistency with https://github.com/abetterinternet/ppm-specification/pull/208?",
          "createdAt": "2022-04-04T21:36:35Z",
          "updatedAt": "2022-04-04T21:36:35Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs43rtEp",
          "commit": {
            "abbreviatedOid": "92f79f3"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-06T22:13:37Z",
          "updatedAt": "2022-04-06T22:13:37Z",
          "comments": [
            {
              "originalPosition": 32,
              "body": "FYI, typo here\r\n```suggestion\r\nencoded in Base 64 with URL and filename safe alphabet with no padding, as\r\n```",
              "createdAt": "2022-04-06T22:13:37Z",
              "updatedAt": "2022-04-06T22:13:37Z"
            }
          ]
        }
      ]
    },
    {
      "number": 213,
      "id": "PR_kwDOFEJYQs411lPc",
      "title": "Ignore .DS_Store",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/213",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-04-07T21:56:20Z",
      "updatedAt": "2022-09-16T00:30:45Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b1893aabd606f7dcecc1a8bece8027889821ec9e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/ds-store",
      "headRefOid": "92519bc28d40955fece38f63258bb759498ea65a",
      "closedAt": "2022-04-07T22:18:57Z",
      "mergedAt": "2022-04-07T22:18:57Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "13baf6f2051b5ede1aa8edd82eb0614463e9c378"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs43xhaN",
          "commit": {
            "abbreviatedOid": "92519bc"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-07T22:13:35Z",
          "updatedAt": "2022-04-07T22:13:35Z",
          "comments": []
        }
      ]
    },
    {
      "number": 214,
      "id": "PR_kwDOFEJYQs411oW2",
      "title": "s/UploadReq/Report/",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/214",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-04-07T22:18:39Z",
      "updatedAt": "2022-09-16T00:30:45Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b1893aabd606f7dcecc1a8bece8027889821ec9e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/typo",
      "headRefOid": "aed9506bff0ff1359e7320e4ede259c02d035a72",
      "closedAt": "2022-04-07T22:52:59Z",
      "mergedAt": "2022-04-07T22:52:59Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "3fe22c7c3ab0e33d752a09736469c1ff52753a6b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs43xiRJ",
          "commit": {
            "abbreviatedOid": "aed9506"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-07T22:19:35Z",
          "updatedAt": "2022-04-07T22:19:35Z",
          "comments": []
        }
      ]
    },
    {
      "number": 219,
      "id": "PR_kwDOFEJYQs42XaHL",
      "title": "First cut of editorial changes in prep for aggregation flow",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/219",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Extracted and slightly modified from [chris-wood-cjpatton/agg-flow](https://github.com/abetterinternet/ppm-specification/tree/chris-wood-cjpatton/agg-flow).",
      "createdAt": "2022-04-18T14:51:50Z",
      "updatedAt": "2023-03-06T23:48:23Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3fe22c7c3ab0e33d752a09736469c1ff52753a6b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/agg-flow-1",
      "headRefOid": "6e2af65f5c2afff6e53ca17f6be32d3b306c90bd",
      "closedAt": "2022-04-20T14:09:17Z",
      "mergedAt": "2022-04-20T14:09:17Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "7e9e7a534cf14c945d68be3f092396dc9cb42227"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "(Build failure appears to be some GitHub API issue.)",
          "createdAt": "2022-04-18T14:57:22Z",
          "updatedAt": "2022-04-18T14:57:22Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah, this looks like we're hitting the patch for https://github.blog/2022-04-12-git-security-vulnerability-announced/. I'll check if the upstream template repository has a fix.",
          "createdAt": "2022-04-18T15:01:17Z",
          "updatedAt": "2022-04-18T15:01:17Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @divergentdave!",
          "createdAt": "2022-04-18T15:06:16Z",
          "updatedAt": "2022-04-18T15:06:16Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs44Ybid",
          "commit": {
            "abbreviatedOid": "28ff036"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-19T15:43:11Z",
          "updatedAt": "2022-04-19T15:43:11Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs44Y7bc",
          "commit": {
            "abbreviatedOid": "28ff036"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udea2 !",
          "createdAt": "2022-04-19T17:24:57Z",
          "updatedAt": "2022-04-19T17:24:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs44ZRPE",
          "commit": {
            "abbreviatedOid": "28ff036"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-19T18:44:42Z",
          "updatedAt": "2022-04-19T18:45:41Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "I think we need a bit more than this. Suppose that a PPM task executes [`Prio3Aes128Histogram`](https://github.com/divviup/libprio-rs/blob/1bd54185d01110ff4486b8d71ed8f17f1ea77b78/src/vdaf/prio3.rs#L123). Besides that label, everyone also needs to know what the bucket boundaries are. Perhaps the PPM text should refer to VDAF's description of a [`Measurement` type](https://github.com/cjpatton/vdaf/blob/6904d4a6295da82daf7ecd09c094326db4fb8ccd/draft-patton-cfrg-vdaf.md?plain=1#L404)?",
              "createdAt": "2022-04-19T18:44:43Z",
              "updatedAt": "2022-04-19T18:45:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44dZqa",
          "commit": {
            "abbreviatedOid": "28ff036"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-20T14:08:20Z",
          "updatedAt": "2022-04-20T14:08:20Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "```suggestion\r\n* A unique identifier for the VDAF instance used for the task, including the type of measurement associated with the task.\r\n```",
              "createdAt": "2022-04-20T14:08:20Z",
              "updatedAt": "2022-04-20T14:08:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44daBf",
          "commit": {
            "abbreviatedOid": "28ff036"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-20T14:09:04Z",
          "updatedAt": "2022-04-20T14:09:04Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "I think this needs to be \"something that fully defines the VDAF instance you're using, modulo the parameters generated from VDAF.setup() (since those change per invocation),\" so I elaborated on this a bit.",
              "createdAt": "2022-04-20T14:09:04Z",
              "updatedAt": "2022-04-20T14:09:04Z"
            }
          ]
        }
      ]
    },
    {
      "number": 220,
      "id": "PR_kwDOFEJYQs42g_6n",
      "title": "rename document",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/220",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The draft has been adopted by the WG and now needs a new name.\r\nAdditionally, this commit replaces references to the old repository on\r\n`abetterinternet` with the current GitHub organization.",
      "createdAt": "2022-04-20T20:13:03Z",
      "updatedAt": "2023-10-26T15:44:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "004020f88c38785af23adfa909e4d0c646434bb1",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/rename-draft",
      "headRefOid": "b230a040196ae63958ceef21418bed80d5035e89",
      "closedAt": "2022-05-03T17:43:09Z",
      "mergedAt": "2022-05-03T17:43:09Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "9884610ca4102df6e370f3061f9d6223b3244d9c"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> The protocol name should change too, right? Though that can happen later so no need to do it here and now.\r\n\r\nI'm happy to do it if you can help me find all the places it needs to be updated. Do you mean the `title` line in `draft-ietf-ppm-dap.md`, where \"Privacy Preserving Measurement\" should become \"Privacy Preserving Measurement via Distributed Aggregation Functions\"?",
          "createdAt": "2022-05-03T16:36:13Z",
          "updatedAt": "2022-05-03T16:36:13Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Well, I thought the outcome here was that the protocol name was \"Distributed Aggregation Protocol,\" so the title would be \"Distributed Aggregation Protocol for Privacy Preserving Measurement\" or whatever. I do not feel strongly about any of this. :) ",
          "createdAt": "2022-05-03T16:43:36Z",
          "updatedAt": "2022-05-03T16:43:36Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs44fUPU",
          "commit": {
            "abbreviatedOid": "eb97000"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-20T20:35:27Z",
          "updatedAt": "2022-04-20T20:35:27Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs44lB8N",
          "commit": {
            "abbreviatedOid": "eb97000"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-21T21:24:28Z",
          "updatedAt": "2022-04-21T21:24:28Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45Qnaf",
          "commit": {
            "abbreviatedOid": "28fb578"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "The protocol name should change too, right? Though that can happen later so no need to do it here and now.",
          "createdAt": "2022-05-03T16:33:34Z",
          "updatedAt": "2022-05-03T16:33:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45Qrhr",
          "commit": {
            "abbreviatedOid": "28fb578"
          },
          "author": "bemasc",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-03T16:44:11Z",
          "updatedAt": "2022-05-03T16:44:12Z",
          "comments": [
            {
              "originalPosition": 2,
              "body": "Please add \"Distributed Aggregation Protocol\" in some fashion.",
              "createdAt": "2022-05-03T16:44:11Z",
              "updatedAt": "2022-05-03T16:44:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45QzKd",
          "commit": {
            "abbreviatedOid": "41a44ab"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Ship it",
          "createdAt": "2022-05-03T17:08:11Z",
          "updatedAt": "2022-05-03T17:08:11Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45Q1-c",
          "commit": {
            "abbreviatedOid": "41a44ab"
          },
          "author": "bemasc",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-03T17:14:46Z",
          "updatedAt": "2022-05-03T17:14:46Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I believe you're also going to want\r\n```suggestion\r\ntitle: \"Distributed Aggregation Protocol for Privacy Preserving Measurement\"\r\nabbrev: DAP-PPM\r\n```\r\nOther possibilities: \"PPM-DAP\", \"Distributed Aggregation Protocol\".",
              "createdAt": "2022-05-03T17:14:46Z",
              "updatedAt": "2022-05-03T17:14:46Z"
            }
          ]
        }
      ]
    },
    {
      "number": 222,
      "id": "PR_kwDOFEJYQs42hX_j",
      "title": "define error codes for batch interval validation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/222",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "# This targets the interop branch\r\n\r\nAdds definitions of error codes to section 3.1 for validation of batch\r\nintervals in `CollectReq` or `AggregateShareReq`. Also adds an error\r\ncode for leader to use when rejecting a report from too far in the\r\nfuture.",
      "createdAt": "2022-04-20T22:24:17Z",
      "updatedAt": "2023-10-26T15:44:55Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "b734ac2272e58b839b8c5d5511e518e42b324956",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/error-codes",
      "headRefOid": "84c8321e9cdbbeecb1970ffa2cb0f91224253479",
      "closedAt": "2022-04-21T18:10:45Z",
      "mergedAt": "2022-04-21T18:10:44Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "87936b475e11402fa23bc924578f1a5b7c53fe28"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@BranLwyd I can't request review from you until you accept the invite to the ietf-wg-ppm organization but you should see this PR too!",
          "createdAt": "2022-04-21T17:58:18Z",
          "updatedAt": "2022-04-21T17:58:18Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs44gF0X",
          "commit": {
            "abbreviatedOid": "2ac6cca"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-04-21T01:05:55Z",
          "updatedAt": "2022-04-21T01:06:46Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "We don't want to overload the term \"privacy budget\", as this gets used in the context of DP.",
              "createdAt": "2022-04-21T01:05:55Z",
              "updatedAt": "2022-04-21T01:06:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44kOgh",
          "commit": {
            "abbreviatedOid": "84c8321"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-21T18:01:22Z",
          "updatedAt": "2022-04-21T18:01:22Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs44kQMZ",
          "commit": {
            "abbreviatedOid": "84c8321"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-21T18:05:06Z",
          "updatedAt": "2022-04-21T18:05:06Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs44kTp6",
          "commit": {
            "abbreviatedOid": "84c8321"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-21T18:10:34Z",
          "updatedAt": "2022-04-21T18:10:49Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "Optional: this is a really tiny section -- would this sentence fit somewhere else, allowing this section to be dropped? Perhaps in the description of the `/upload` flow.",
              "createdAt": "2022-04-21T18:10:34Z",
              "updatedAt": "2022-04-21T18:10:49Z"
            }
          ]
        }
      ]
    },
    {
      "number": 223,
      "id": "PR_kwDOFEJYQs42j4jb",
      "title": "Add VDAF mapping for aggregation flow",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/223",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This wires up PPM to drive the underlying VDAF state machine for individual reports. It splits the aggregate flow into three phases: initialization, progression (where the aggregators exchange messages to perform validation), and finalization. It also moves the process of fetching the aggregate share value for each aggregator to the collect flow, since that seems more closely aligned with the process of collection than it is about aggregation.\r\n\r\nOpen question (there may be more): Can we come up with a better name than `Process` for the message that conveys VDAF intermediate results?",
      "createdAt": "2022-04-21T13:47:34Z",
      "updatedAt": "2022-04-28T16:39:03Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7e9e7a534cf14c945d68be3f092396dc9cb42227",
      "headRepository": "chris-wood/ppm-specification",
      "headRefName": "caw/agg-flow-2",
      "headRefOid": "d6bcb63d504af9ce76e1f86ec5cce1ac8ac7964d",
      "closedAt": "2022-04-28T16:39:02Z",
      "mergedAt": "2022-04-28T16:39:02Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "a1db69a015476da0b423cbb2af6ff53a14dd8c3a"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs44jlDi",
          "commit": {
            "abbreviatedOid": "1cba7bb"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Technically this seems mostly OK. Editorially, I think it needs a lot of restructure to make it clear. At a higher level, it seems like the leader sends AggregateInitReq prior to engaging with the VDAF to compute the initial state? If so, I would restructure the text to show things in parallel. With that said, this is also a missed opportunity to save a round trip in some cases, I imagine\r\n\r\n\r\n\r\n",
          "createdAt": "2022-04-21T15:53:53Z",
          "updatedAt": "2022-04-21T16:34:53Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "```suggestion\r\nThe leader MUST buffer reports while waiting to aggregate them. The\r\n```",
              "createdAt": "2022-04-21T15:53:53Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 107,
              "body": "```suggestion\r\n  by the underlying VDAF instance until aggregation completes or an error occurs. These messages do not replay the shares.\r\n```\r\n\r\n",
              "createdAt": "2022-04-21T15:54:44Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 114,
              "body": "This text is a bit confusing. Is the idea here that there is a phase 2.1 where the helpers have assimilated the shares and so no more RTs are needed but you are not yet at the batch size?",
              "createdAt": "2022-04-21T15:55:57Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 126,
              "body": "```suggestion\r\n* It is an error to include a new report in a batch that has already been collected. If the report would have belonged to a batch that has been collected, but the leader has not yet aggregated the report, then it MUST be excluded.\r\n```",
              "createdAt": "2022-04-21T15:56:37Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 128,
              "body": "```suggestion\r\nAfter choosing the set of candidates, the leader begins aggregation by splitting each report into \"report\r\n```",
              "createdAt": "2022-04-21T15:57:24Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 141,
              "body": "Should this be later or earlier? It seems odd here.",
              "createdAt": "2022-04-21T15:57:47Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 171,
              "body": "I would restructure this so you have a separate section describing how to decrypt shares, as the leader and helper do much the same thing.",
              "createdAt": "2022-04-21T15:58:24Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 247,
              "body": "How does it do that? Show the API call.",
              "createdAt": "2022-04-21T16:11:04Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 223,
              "body": "Can we rename this to \"reports\"?",
              "createdAt": "2022-04-21T16:12:04Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 271,
              "body": "Can we just merge this with the uniquness check above?",
              "createdAt": "2022-04-21T16:12:30Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 297,
              "body": "See above about merging this.",
              "createdAt": "2022-04-21T16:14:00Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 387,
              "body": "This section has gotten very long.",
              "createdAt": "2022-04-21T16:15:59Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 409,
              "body": "Why are we both having order and the nonce? Seems like a great opportunity for things to go wrong.",
              "createdAt": "2022-04-21T16:17:04Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 420,
              "body": "Can we get a forward ref to how things ifnish.",
              "createdAt": "2022-04-21T16:17:34Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 426,
              "body": "Where is this defined?",
              "createdAt": "2022-04-21T16:18:27Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 445,
              "body": "If it's invalid does it have to tell the helper?",
              "createdAt": "2022-04-21T16:21:43Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 464,
              "body": "See comments above about ```seq```",
              "createdAt": "2022-04-21T16:22:04Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 487,
              "body": "Is this \"after I processed it\" or \"this is what the leader says\"",
              "createdAt": "2022-04-21T16:22:32Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 616,
              "body": "Why are we including the nonces and not the values?",
              "createdAt": "2022-04-21T16:28:39Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            },
            {
              "originalPosition": 640,
              "body": "Here too, let's break out the crypto.",
              "createdAt": "2022-04-21T16:33:17Z",
              "updatedAt": "2022-04-21T16:34:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44j3Gq",
          "commit": {
            "abbreviatedOid": "1cba7bb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T16:49:50Z",
          "updatedAt": "2022-04-21T17:04:45Z",
          "comments": [
            {
              "originalPosition": 223,
              "body": "Or perhaps \"report_shares\"?",
              "createdAt": "2022-04-21T16:49:50Z",
              "updatedAt": "2022-04-21T17:04:45Z"
            },
            {
              "originalPosition": 409,
              "body": "Basically the requirement is: don't change the order of the nonces. Otherwise you force your peer to do more computation than necessary.",
              "createdAt": "2022-04-21T16:51:14Z",
              "updatedAt": "2022-04-21T17:04:45Z"
            },
            {
              "originalPosition": 616,
              "body": "Could do that, though strictly speaking may not be necessary. I don't think we've quite figured out what we need from this yet. Note, however, that the computation of the checksum hasn't changed here.",
              "createdAt": "2022-04-21T16:53:07Z",
              "updatedAt": "2022-04-21T17:04:45Z"
            },
            {
              "originalPosition": 445,
              "body": "Our thinking here is that the leader would just skip reports that can't be processed further. The motivation for this is to keep the helper simple. It also has the advantage of avoiding communication overhead for reports that can't be aggregated. One potential downside is that the helper doesn't get to find out *why* processing failed --- VDAF evaluation may fail in different ways, for example --- whereas the leader does.",
              "createdAt": "2022-04-21T17:03:29Z",
              "updatedAt": "2022-04-21T17:04:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44j8CJ",
          "commit": {
            "abbreviatedOid": "1cba7bb"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T17:08:28Z",
          "updatedAt": "2022-04-21T17:08:29Z",
          "comments": [
            {
              "originalPosition": 445,
              "body": "I don't think I understand how this works. If you don't tell them, and they are incrementally aggregating, it seems like a lot of work.",
              "createdAt": "2022-04-21T17:08:28Z",
              "updatedAt": "2022-04-21T17:08:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44kVCU",
          "commit": {
            "abbreviatedOid": "1cba7bb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-04-21T18:15:06Z",
          "updatedAt": "2022-04-21T20:26:25Z",
          "comments": [
            {
              "originalPosition": 109,
              "body": "I don't think this happens until the helper gets an `AggregateShareReq` during the collect flow. The output of the aggregation flow is what VDAF calls \"output shares\", which are 1:1 to input shares.",
              "createdAt": "2022-04-21T18:15:06Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 126,
              "body": "Perhaps insert something into the sentence to explain that the likely reason this would happen is a late upload.",
              "createdAt": "2022-04-21T18:19:03Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 125,
              "body": "nit: wrap long lines please",
              "createdAt": "2022-04-21T18:20:44Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 125,
              "body": "I don't understand \"but the report does not pertain to a batch that has been collected\". The implication is that the leader is allowed to aggregate a report twice if it is included in a batch that has been collected?",
              "createdAt": "2022-04-21T18:24:07Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 193,
              "body": "Why do we have to dictate that the leader computes its first state transition before sending `AggregateInitReq` to helper?  The leader's first prepare message doesn't get transmitted to the helper until later, so a leader implementation could wait to do this until it gets the first `AggregateResp`. Is the idea to allow the leader to filter out those reports for which state initialization fails?",
              "createdAt": "2022-04-21T18:28:28Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 339,
              "body": "I think this phrasing makes it the PPM implementation's problem to know how many rounds the VDAF it is executing has, and thus what it should expect the output of `VDAF.prep_next` to consist of. [The definition of `prep_next` in VDAF](https://github.com/cjpatton/vdaf/blob/6904d4a6295da82daf7ecd09c094326db4fb8ccd/draft-patton-cfrg-vdaf.md?plain=1#L537) returns a union type to enable the VDAF to express to its caller whether preparation is complete or if there are more rounds, so I believe it should be possible for a PPM implementation to avoid any special knowledge of what \"should\" happen when it calls `prep_next`.\r\n\r\nI'd suggest something like:\r\n\r\n```\r\nIf this is the last round of the VDAF, then `out` is the aggregator's output share,\r\nin which case the aggregator finishes and stores its output share for further\r\nprocessing as described in {{agg-complete}}. Otherwise, `out` is the pair\r\n`(new_state, agg_msg)`, where `new_state` is its updated state and `agg_msg`\r\nis its next VDAF message. For the latter case, the helper sets `prep_state` to\r\n`new_state`.\r\n```",
              "createdAt": "2022-04-21T18:40:06Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 343,
              "body": "I think we should use \"prepare message\" or `prep_msg` to be consistent with the usage in the VDAF spec.",
              "createdAt": "2022-04-21T18:43:43Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 423,
              "body": "I'm a little concerned that we're using the verbs \"prepare\", \"aggregate\" and \"process\" kind of loosely. I think we should converge on one verb with a clear definition we can put in a glossary. VDAF already uses \"prepare\" heavily to describe the transformation of an input share into an output share. Then, after preparation, an output share can be accumulated into an aggregate.",
              "createdAt": "2022-04-21T18:48:06Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 523,
              "body": "```suggestion\r\n`new_state` is its updated preparation state and `agg_msg` is its next VDAF\r\n```",
              "createdAt": "2022-04-21T18:51:05Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 545,
              "body": "```suggestion\r\nis the AggregateContinueResp and media type is \"message/ppm-aggregate-continue-resp\". The helper\r\n```",
              "createdAt": "2022-04-21T18:52:00Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 559,
              "body": "```suggestion\r\nOnce processing of a report share is finished, each aggregator stores the\r\nrecovered output share until the batch to which it pertains is collected.\r\nTo aggregate the output shares, denoted `out_shares`, the aggregator runs\r\nthe aggregation algorithm specified by the VDAF:\r\n```\r\n\"output share\" should be singular to agree with \"report share\" on the line above, right?\r\n\r\nSeparately: I think there's a causality problem here: for Poplar1 or other VDAFs that have an aggregation parameter, you can't prepare/\"recover output shares\" (\"recover\" is another verb we casually use as a synonym for \"prepare\" and \"aggregate\") until the collector provides the agg_param in a CollectReq. So I think we need to be clear about what it means for a batch to be \"collected\". Does a batch become collected when the leader receives a `CollectReq`? Or when the collector successfully retrieves the aggregate shares produced in response to a `CollectReq`?\r\n\r\nI've argued before we should spell out the state machine for report shares and for batches. That shouldn't happen in this change, but since a lot of subtle stuff like anti-replay depends on the state of a report vs. the state of a batch, the document would benefit from more clarity about this.",
              "createdAt": "2022-04-21T19:51:48Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 572,
              "body": "This is kind of awkward. The implication is that batch parameters don't need to be validated when using a VDAF without an aggregation parameter. Also I don't understand how validation of batch parameters informs which aggregate is used?",
              "createdAt": "2022-04-21T20:17:41Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 594,
              "body": "This is identical to the definition of `checksum`, a couple paragraphs later. We should delete one.",
              "createdAt": "2022-04-21T20:19:05Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 666,
              "body": "```suggestion\r\nThe leader MAY make multiple aggregate-share requests for a given batch interval\r\nand aggregation parameter and MUST get the same result each time.\r\n```\r\nThe intent here is that the helper should use the same set of reports to service multiple aggregate-share requests, right? Could that be stated explicitly.",
              "createdAt": "2022-04-21T20:20:30Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            },
            {
              "originalPosition": 4,
              "body": "This error type doesn't seem to be used anywhere.",
              "createdAt": "2022-04-21T20:24:14Z",
              "updatedAt": "2022-04-21T20:26:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44k2Bj",
          "commit": {
            "abbreviatedOid": "1cba7bb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T20:30:14Z",
          "updatedAt": "2022-04-21T20:30:15Z",
          "comments": [
            {
              "originalPosition": 104,
              "body": "What is a \"public VDAF\"?",
              "createdAt": "2022-04-21T20:30:15Z",
              "updatedAt": "2022-04-21T20:30:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lDPi",
          "commit": {
            "abbreviatedOid": "ea9cb2e"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:31:45Z",
          "updatedAt": "2022-04-21T21:31:45Z",
          "comments": [
            {
              "originalPosition": 666,
              "body": "The intent here is that if the parameters of the aggregation request are the same then the output should be the same.",
              "createdAt": "2022-04-21T21:31:45Z",
              "updatedAt": "2022-04-21T21:31:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lD3c",
          "commit": {
            "abbreviatedOid": "2743ba7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:35:18Z",
          "updatedAt": "2022-04-21T21:35:19Z",
          "comments": [
            {
              "originalPosition": 559,
              "body": "> \"output share\" should be singular to agree with \"report share\" on the line above, right?\r\n\r\nYep!\r\n\r\n> I've argued before we should spell out the state machine for report shares and for batches. That shouldn't happen in this change, but since a lot of subtle stuff like anti-replay depends on the state of a report vs. the state of a batch, the document would benefit from more clarity about this.\r\n\r\nI don't think we have a clear set of requirements for batch validation yet (https://github.com/ietf-wg-ppm/ppm-specification/issues/195), but once we do, yes, we should definitely add more detail.",
              "createdAt": "2022-04-21T21:35:19Z",
              "updatedAt": "2022-04-21T21:35:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lD7k",
          "commit": {
            "abbreviatedOid": "2455e36"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:35:41Z",
          "updatedAt": "2022-04-21T21:35:41Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Leftover \ud83d\udc4d ",
              "createdAt": "2022-04-21T21:35:41Z",
              "updatedAt": "2022-04-21T21:35:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lEK7",
          "commit": {
            "abbreviatedOid": "2455e36"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:37:00Z",
          "updatedAt": "2022-04-21T21:37:00Z",
          "comments": [
            {
              "originalPosition": 104,
              "body": "Strange leftover. Deleting \"public\".",
              "createdAt": "2022-04-21T21:37:00Z",
              "updatedAt": "2022-04-21T21:37:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lEW3",
          "commit": {
            "abbreviatedOid": "2455e36"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:38:05Z",
          "updatedAt": "2022-04-21T21:38:06Z",
          "comments": [
            {
              "originalPosition": 109,
              "body": "Yep, good catch. This should be \"output shares.\"",
              "createdAt": "2022-04-21T21:38:05Z",
              "updatedAt": "2022-04-21T21:38:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lE51",
          "commit": {
            "abbreviatedOid": "bd95133"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:41:18Z",
          "updatedAt": "2022-04-21T21:41:18Z",
          "comments": [
            {
              "originalPosition": 193,
              "body": "That's right \ud83d\udc4d it would be pointless for the leader to kick off aggregation for any share that it would eventually throw away. ",
              "createdAt": "2022-04-21T21:41:18Z",
              "updatedAt": "2022-04-21T21:41:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lFXX",
          "commit": {
            "abbreviatedOid": "bd95133"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:43:54Z",
          "updatedAt": "2022-04-21T21:43:55Z",
          "comments": [
            {
              "originalPosition": 223,
              "body": "That's better!",
              "createdAt": "2022-04-21T21:43:54Z",
              "updatedAt": "2022-04-21T21:43:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lF1g",
          "commit": {
            "abbreviatedOid": "bd95133"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:46:35Z",
          "updatedAt": "2022-04-21T21:46:36Z",
          "comments": [
            {
              "originalPosition": 343,
              "body": "Yep, that's better!",
              "createdAt": "2022-04-21T21:46:36Z",
              "updatedAt": "2022-04-21T21:46:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lH4Z",
          "commit": {
            "abbreviatedOid": "7cbeda7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T21:58:14Z",
          "updatedAt": "2022-04-21T21:58:15Z",
          "comments": [
            {
              "originalPosition": 445,
              "body": "Right now, they shouldn't actually aggregate until the leader issues an AggregateShareReq. We can change how that's done if we want to optimize things, but let's do so separately.",
              "createdAt": "2022-04-21T21:58:14Z",
              "updatedAt": "2022-04-21T21:58:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lIYw",
          "commit": {
            "abbreviatedOid": "7cbeda7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:01:21Z",
          "updatedAt": "2022-04-21T22:01:21Z",
          "comments": [
            {
              "originalPosition": 487,
              "body": "It's \"do this check and then proceed accordingly to either continue processing, invalidate the report, or finish.\"",
              "createdAt": "2022-04-21T22:01:21Z",
              "updatedAt": "2022-04-21T22:01:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lJWi",
          "commit": {
            "abbreviatedOid": "e3973ec"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:07:50Z",
          "updatedAt": "2022-04-21T22:07:50Z",
          "comments": [
            {
              "originalPosition": 141,
              "body": "Yeah, I think you're right. I'll move it later.",
              "createdAt": "2022-04-21T22:07:50Z",
              "updatedAt": "2022-04-21T22:07:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lKUF",
          "commit": {
            "abbreviatedOid": "759edf3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:13:48Z",
          "updatedAt": "2022-04-21T22:13:48Z",
          "comments": [
            {
              "originalPosition": 423,
              "body": "Yeah, this is a good suggestion. Let's converge on \"prepare\" as the verb of choice here.",
              "createdAt": "2022-04-21T22:13:48Z",
              "updatedAt": "2022-04-21T22:13:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lLqW",
          "commit": {
            "abbreviatedOid": "759edf3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:22:56Z",
          "updatedAt": "2022-04-21T22:22:57Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I think we can remove it, since this is prematurely describing the collect flow. ",
              "createdAt": "2022-04-21T22:22:57Z",
              "updatedAt": "2022-04-21T22:22:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lM6v",
          "commit": {
            "abbreviatedOid": "4f3fb89"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:31:02Z",
          "updatedAt": "2022-04-21T22:31:03Z",
          "comments": [
            {
              "originalPosition": 247,
              "body": "This is shown below.",
              "createdAt": "2022-04-21T22:31:02Z",
              "updatedAt": "2022-04-21T22:31:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lOAT",
          "commit": {
            "abbreviatedOid": "4f3fb89"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:39:18Z",
          "updatedAt": "2022-04-21T22:39:19Z",
          "comments": [
            {
              "originalPosition": 409,
              "body": "I think we can probably relax this. Let's address it in a followup issue.",
              "createdAt": "2022-04-21T22:39:18Z",
              "updatedAt": "2022-04-21T22:39:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44lOCx",
          "commit": {
            "abbreviatedOid": "4f3fb89"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-21T22:39:29Z",
          "updatedAt": "2022-04-21T22:39:29Z",
          "comments": [
            {
              "originalPosition": 420,
              "body": "Yep, done.",
              "createdAt": "2022-04-21T22:39:29Z",
              "updatedAt": "2022-04-21T22:39:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44pXMu",
          "commit": {
            "abbreviatedOid": "792d100"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-22T17:09:15Z",
          "updatedAt": "2022-04-22T17:50:08Z",
          "comments": [
            {
              "originalPosition": 409,
              "body": "Followup issue: https://github.com/ietf-wg-ppm/ppm-specification/issues/217 (FWIW, I agree this can be relaxed, I included some potential considerations in the issue.)",
              "createdAt": "2022-04-22T17:09:16Z",
              "updatedAt": "2022-04-22T17:50:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44qq0r",
          "commit": {
            "abbreviatedOid": "792d100"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-22T23:29:33Z",
          "updatedAt": "2022-04-23T00:10:39Z",
          "comments": [
            {
              "originalPosition": 147,
              "body": "```suggestion\r\n1. For each valid report share, initialize the VDAF preparation process.\r\n```",
              "createdAt": "2022-04-22T23:29:33Z",
              "updatedAt": "2022-04-23T00:10:39Z"
            },
            {
              "originalPosition": 202,
              "body": "```suggestion\r\n1. Initialize VDAF preparation as described in {{input-share-prep}}.\r\n```",
              "createdAt": "2022-04-22T23:30:31Z",
              "updatedAt": "2022-04-23T00:10:39Z"
            },
            {
              "originalPosition": 207,
              "body": "```suggestion\r\nIf any step yields an invalid report share, the leader removes the report share from\r\n```",
              "createdAt": "2022-04-22T23:30:45Z",
              "updatedAt": "2022-04-23T00:10:39Z"
            },
            {
              "originalPosition": 244,
              "body": "```suggestion\r\nof candidate report shares obtained in aN `AggregateInitReq` message from the leader.\r\n```",
              "createdAt": "2022-04-22T23:32:14Z",
              "updatedAt": "2022-04-23T00:10:39Z"
            },
            {
              "originalPosition": 267,
              "body": "`PrepareResult` sounds like \"the result of the preparation process\", i.e., an output share. I think `PrepareStepType` would be more clear.",
              "createdAt": "2022-04-22T23:33:55Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 276,
              "body": "IIRC this is the TLS-syntax you want:\r\n```suggestion\r\n    case finished:  Empty;\r\n```",
              "createdAt": "2022-04-22T23:34:45Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 279,
              "body": "`PrepareShare` collides with terminology in the VDAF spec. (See \"prepare share\" in https://www.ietf.org/archive/id/draft-patton-cfrg-vdaf-01.html#name-preparation.) How about `PrepareStep`?",
              "createdAt": "2022-04-22T23:37:36Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 305,
              "body": "```suggestion\r\nas finished if the VDAF preparation process is finished for the report share.\r\n```",
              "createdAt": "2022-04-22T23:38:27Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 316,
              "body": "By \"bizarre\" I think you mean \"unspecified\" :) We discussed this a bit before but decided to punt. @BranLwyd did we file an issue to track this?",
              "createdAt": "2022-04-22T23:40:48Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 321,
              "body": "```suggestion\r\ninput share. Let `nonce`, `extensions`, and `encrypted_input_share` denote these\r\n```",
              "createdAt": "2022-04-22T23:41:21Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 322,
              "body": "```suggestion\r\nvalues, respectively. Given these values, an aggregator decrypts the input\r\n```",
              "createdAt": "2022-04-22T23:41:39Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 333,
              "body": "where `server_role` is `0x02` if the aggregator is the leader and `0x03` if the aggregator is the helper.\r\n```suggestion\r\n                     \"ppm-00 input share\" || 0x01 || server_role)\r\n```",
              "createdAt": "2022-04-22T23:46:35Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 132,
              "body": "This is now a bit redundant due to {{input-share-batch-validation}}. I wonder if, with a bit of word-smithing, we can replace this with a forward reference to that section?",
              "createdAt": "2022-04-22T23:49:21Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 386,
              "body": "Can we also add this language here? https://github.com/ietf-wg-ppm/ppm-specification/blob/87936b475e11402fa23bc924578f1a5b7c53fe28/draft-gpew-priv-ppm.md?plain=1#L865-L869",
              "createdAt": "2022-04-22T23:51:50Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 445,
              "body": "```suggestion\r\nThe leader begins each round of continuation for a report share based on its locally computed \r\nprepare message and the previous PrepareShare from the helper. If PrepareShare is of type \"failed\", \r\nthen the leader marks the report as failed and removes it from the candidate report set and does not\r\nprocess it further. If the type is \"finished\", then the leader aborts with \"unrecognizedMessage\".\r\n[[OPEN ISSUE: This behavior is not specified.]] If the type is \"continued\", then the leader proceeds as\r\nfollows.\r\n\r\nLet `leader_outbound` denote the leader's prepare message and `helper_outbound` denote the\r\nhelper's. The leader computes the next state transition as follows:\r\n```",
              "createdAt": "2022-04-23T00:05:40Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 536,
              "body": "```suggestion\r\nwhere `inbound` is the previous VDAF preapre message sent by the leader and `prep_state` is\r\n```",
              "createdAt": "2022-04-23T00:07:27Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            },
            {
              "originalPosition": 573,
              "body": "```suggestion\r\n[[OPEN ISSUE: consider relaxing this ordering constraint. See issue#217.]]\r\n```",
              "createdAt": "2022-04-23T00:08:22Z",
              "updatedAt": "2022-04-23T00:10:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44xpXH",
          "commit": {
            "abbreviatedOid": "792d100"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-25T23:09:55Z",
          "updatedAt": "2022-04-25T23:09:55Z",
          "comments": [
            {
              "originalPosition": 132,
              "body": "Agreed that it's redundant. I think we can omit this text and the forward reference entirely.",
              "createdAt": "2022-04-25T23:09:55Z",
              "updatedAt": "2022-04-25T23:09:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44xprt",
          "commit": {
            "abbreviatedOid": "5576090"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-25T23:12:27Z",
          "updatedAt": "2022-04-25T23:12:28Z",
          "comments": [
            {
              "originalPosition": 316,
              "body": "Indeed -- it's above.",
              "createdAt": "2022-04-25T23:12:28Z",
              "updatedAt": "2022-04-25T23:12:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44xqkN",
          "commit": {
            "abbreviatedOid": "64cda01"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-25T23:19:41Z",
          "updatedAt": "2022-04-25T23:19:41Z",
          "comments": [
            {
              "originalPosition": 267,
              "body": "How about `PrepareStepResult`? (We can bike shed the name later)",
              "createdAt": "2022-04-25T23:19:41Z",
              "updatedAt": "2022-04-25T23:19:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44xqqf",
          "commit": {
            "abbreviatedOid": "64cda01"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-25T23:20:27Z",
          "updatedAt": "2022-04-25T23:20:27Z",
          "comments": [
            {
              "originalPosition": 279,
              "body": "That works for me, and am happy to bike shed later.",
              "createdAt": "2022-04-25T23:20:27Z",
              "updatedAt": "2022-04-25T23:20:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44yWwu",
          "commit": {
            "abbreviatedOid": "2bedc58"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T04:08:14Z",
          "updatedAt": "2022-04-26T04:08:14Z",
          "comments": [
            {
              "originalPosition": 387,
              "body": "Headers included!",
              "createdAt": "2022-04-26T04:08:14Z",
              "updatedAt": "2022-04-26T04:08:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs442vC7",
          "commit": {
            "abbreviatedOid": "2bedc58"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Just a few editorial comments. Also, I'd like us to consider addressing https://github.com/ietf-wg-ppm/ppm-specification/pull/223/files#r856672481 here, but this can wait if you feel it needs discussion.",
          "createdAt": "2022-04-26T19:04:41Z",
          "updatedAt": "2022-04-26T19:20:14Z",
          "comments": [
            {
              "originalPosition": 248,
              "body": "```suggestion\r\nand eventually returns a response to the leader carrying a VDAF-specific message for each\r\n```",
              "createdAt": "2022-04-26T19:05:46Z",
              "updatedAt": "2022-04-26T19:20:14Z"
            },
            {
              "originalPosition": 386,
              "body": "Bump.",
              "createdAt": "2022-04-26T19:09:55Z",
              "updatedAt": "2022-04-26T19:20:14Z"
            },
            {
              "originalPosition": 595,
              "body": "nit: for consistency with similar abbreviation elsewhere\r\n```suggestion\r\n  PrepareStep prep_shares<1..2^16-1>;\r\n```",
              "createdAt": "2022-04-26T19:12:00Z",
              "updatedAt": "2022-04-26T19:20:14Z"
            },
            {
              "originalPosition": 767,
              "body": "I don't understand why this behaviour is useful. Is this new, or is this already in the spec?",
              "createdAt": "2022-04-26T19:15:26Z",
              "updatedAt": "2022-04-26T19:20:14Z"
            },
            {
              "originalPosition": 802,
              "body": "Replac\r\n```suggestion\r\n                              \"ppm-00 aggregate share\" || server_role || 0x00)\r\n```\r\nwhere `server_role` is `0x02` for the leader and `0x03` for a helper.",
              "createdAt": "2022-04-26T19:18:41Z",
              "updatedAt": "2022-04-26T19:20:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs442iI2",
          "commit": {
            "abbreviatedOid": "2bedc58"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-04-26T18:16:05Z",
          "updatedAt": "2022-04-26T19:39:36Z",
          "comments": [
            {
              "originalPosition": 274,
              "body": "```suggestion\r\n  PrepareStepResult prepare_step_result;\r\n  select (PrepareStep.prepare_step_result) {\r\n```\r\nUnless the plural connotes something I missed?",
              "createdAt": "2022-04-26T18:16:05Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 288,
              "body": "We renamed `AggregateInitReq.seq` to something more expressive. Could this be renamed too?\r\n```suggestion\r\n  PrepareStep prepare_steps<1..2^16-1>;\r\n```",
              "createdAt": "2022-04-26T18:17:03Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 316,
              "body": "#217 is about relaxing ordering requirements in the various `Aggregate` messages. The question here is what the leader should do when it detects a malformed message from the helper, given that the helper isn't awaiting a reply from the leader (the helper message is delivered in a response to the leader's request). I think that in this case, the leader should abort handling of this aggregate job, but shouldn't tell the helper about it. The helper simply won't get the next `AggregateContinueReq`. ",
              "createdAt": "2022-04-26T18:22:02Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 362,
              "body": "```suggestion\r\nEach report share has a corresponding task ID, nonce, list of extensions, and encrypted\r\ninput share. Let `task_id`, `nonce`, `extensions`, and `encrypted_input_share` denote these\r\nvalues, respectively. Given these values, an aggregator decrypts the input\r\n```\r\nSince `task_id` is used in the `SetupBaseR` call, below.",
              "createdAt": "2022-04-26T18:23:12Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 387,
              "body": "The definitions of `task_id`, `nonce` and `extensions here are redundant with the paragraph above starting with \"Each report share has...\"",
              "createdAt": "2022-04-26T18:24:11Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 394,
              "body": "```suggestion\r\n#### Input Share Validation {#input-share-validation}\r\n```\r\n\r\nThe phrasing here bothers me: it suggests that input shares arrive in batches, but what we're discussing here is validating an input share's timestamp against collection batches.",
              "createdAt": "2022-04-26T18:25:03Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 387,
              "body": "I appreciate the sub-sections. This will make it easier to put references to the spec into implementations.",
              "createdAt": "2022-04-26T18:26:02Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 389,
              "body": "This should spell out which `ReportShareError` variant to use (`hpke-unknown-config-id` or `hpke-decrypt-error` depending on the failure mode).",
              "createdAt": "2022-04-26T18:27:56Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 398,
              "body": "Specifically, `vdaf-prep-error`, right?",
              "createdAt": "2022-04-26T18:28:16Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 418,
              "body": "This last sentence (\"The helper also checks...\") is captured by bullet `1.`, isn't it?",
              "createdAt": "2022-04-26T18:29:21Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 413,
              "body": "Is this correct in the `poplar1` case, where we expect multiple queries against the same set of reports with varying aggregation parameters? This text should account for tasks where `max_batch_lifetime > 1` (or refer to the `anti-replay` section which I believe already does).",
              "createdAt": "2022-04-26T18:31:39Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 466,
              "body": "```suggestion\r\nOtherwise, the value `out` is interpreted as follows. If this is the last round of the VDAF,\r\nthen `out` is the aggregator's output share. Otherwise, `out` is the pair `(prep_state, prep_msg)`.\r\n```\r\nTo match the corresponding paragraph describing the leader's behavior on L952.",
              "createdAt": "2022-04-26T19:07:19Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 578,
              "body": "```suggestion\r\nthe helper's current preparation state. If this operation fails, then the helper fails\r\n```\r\n\"its\" is ambiguous since the sentence earlier refers to the leader.",
              "createdAt": "2022-04-26T19:09:07Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 604,
              "body": "```suggestion\r\nThe order of AggregateContinueResp.prepare_shares MUST match that of the PrepareStep values in\r\n`AggregateContinueReq.prepare_shares`. The helper's response to the leader is an HTTP 200 OK whose body\r\n```",
              "createdAt": "2022-04-26T19:10:09Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            },
            {
              "originalPosition": 761,
              "body": "I think this discussion of how to populate the fields of `encrypted_aggregate_share` would be more clear in the `aggregate-share-encrypt` section, below.",
              "createdAt": "2022-04-26T19:35:49Z",
              "updatedAt": "2022-04-26T19:39:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs44256c",
          "commit": {
            "abbreviatedOid": "2bedc58"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:44:19Z",
          "updatedAt": "2022-04-26T19:44:20Z",
          "comments": [
            {
              "originalPosition": 767,
              "body": "I'm not sure if this is what Chris W. had in mind but I agree with this text. Suppose we have a task with `max_batch_lifetime = 1`. Consider:\r\n\r\n1. Leader makes an `AggregateShareReq` for some batch interval.\r\n2. Helper services the request, and marks that batch interval as having been collected once.\r\n3. Helper attempts to transmit an `AggregateShareResp` to leader, but the message is truncated.\r\n\r\nNow, if the leader retries its `AggregateShareReq`, the helper will refuse the request because the batch interval's lifetime has been consumed. So, to allow the leader to retry this request, the helper has to be willing to resend previously computed aggregate shares. ",
              "createdAt": "2022-04-26T19:44:19Z",
              "updatedAt": "2022-04-26T19:44:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4426nu",
          "commit": {
            "abbreviatedOid": "b06c410"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:47:15Z",
          "updatedAt": "2022-04-26T19:47:15Z",
          "comments": [
            {
              "originalPosition": 387,
              "body": "Yep, that's right. I left it here to be specific. We can omit in the future if desired.",
              "createdAt": "2022-04-26T19:47:15Z",
              "updatedAt": "2022-04-26T19:47:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4427R8",
          "commit": {
            "abbreviatedOid": "b06c410"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:49:48Z",
          "updatedAt": "2022-04-26T19:49:49Z",
          "comments": [
            {
              "originalPosition": 413,
              "body": "Let's address this in subsequent changes. We _still_ don't have batch validation requirements nailed down, so this is going to likely change anyway.",
              "createdAt": "2022-04-26T19:49:49Z",
              "updatedAt": "2022-04-26T19:49:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4427aQ",
          "commit": {
            "abbreviatedOid": "b06c410"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:50:19Z",
          "updatedAt": "2022-04-26T19:50:19Z",
          "comments": [
            {
              "originalPosition": 418,
              "body": "Indeed, and it's also helper-specific, so I'll remove it.",
              "createdAt": "2022-04-26T19:50:19Z",
              "updatedAt": "2022-04-26T19:50:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4427yF",
          "commit": {
            "abbreviatedOid": "f5f79a9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:51:48Z",
          "updatedAt": "2022-04-26T19:51:49Z",
          "comments": [
            {
              "originalPosition": 386,
              "body": "Let's do this in a separate PR. We still don't have these requirements nailed down and the text is subject to change.",
              "createdAt": "2022-04-26T19:51:49Z",
              "updatedAt": "2022-04-26T19:51:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4428Eo",
          "commit": {
            "abbreviatedOid": "7d5d665"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:52:57Z",
          "updatedAt": "2022-04-26T19:52:58Z",
          "comments": [
            {
              "originalPosition": 595,
              "body": "Keeping this as-is for now. We can bash the name in a followup change.",
              "createdAt": "2022-04-26T19:52:57Z",
              "updatedAt": "2022-04-26T19:52:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4428te",
          "commit": {
            "abbreviatedOid": "fb27d3c"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-26T19:55:34Z",
          "updatedAt": "2022-04-26T19:55:34Z",
          "comments": [
            {
              "originalPosition": 767,
              "body": "Yanking this out to discuss in a separate issue: https://github.com/ietf-wg-ppm/ppm-specification/issues/226",
              "createdAt": "2022-04-26T19:55:34Z",
              "updatedAt": "2022-04-26T19:55:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs443gvG",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-26T22:35:15Z",
          "updatedAt": "2022-04-26T22:35:15Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4470tM",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T16:36:07Z",
          "updatedAt": "2022-04-27T16:36:08Z",
          "comments": [
            {
              "originalPosition": 728,
              "body": "Should we add this to the errors table above? (I see it says \"this list is not exhaustive\", but I'm not sure if that means other errors may be in the same URI namespace, or other errors would have entirely different URIs)",
              "createdAt": "2022-04-27T16:36:08Z",
              "updatedAt": "2022-04-27T16:36:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448DkG",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-04-27T17:15:15Z",
          "updatedAt": "2022-04-27T17:26:06Z",
          "comments": [
            {
              "originalPosition": 206,
              "body": "Does this have to be done serially? Could the leader instead send all of the shares and then discover later that one is bogus? Surely yes, because the proof could fail.",
              "createdAt": "2022-04-27T17:15:15Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 234,
              "body": "...to which the share is being sent...",
              "createdAt": "2022-04-27T17:16:05Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 249,
              "body": "I wonder if it would be better to factor out the common behaviors for all aggregators including the leader.",
              "createdAt": "2022-04-27T17:16:57Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 258,
              "body": "For instance, this is common to leader and helper.",
              "createdAt": "2022-04-27T17:17:24Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 269,
              "body": "Why is this called Prepare and above we call it Init?",
              "createdAt": "2022-04-27T17:17:51Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 418,
              "body": "Shouldn't the nonce check go here?",
              "createdAt": "2022-04-27T17:20:01Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 462,
              "body": "For the future, I think it would be better to make these clauses definition lists.",
              "createdAt": "2022-04-27T17:22:02Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 489,
              "body": "\"Prepare\" seems like an odd term here. How about \"Process\"",
              "createdAt": "2022-04-27T17:23:34Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 728,
              "body": ":+1:",
              "createdAt": "2022-04-27T17:24:36Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            },
            {
              "originalPosition": 741,
              "body": "Does the vdaf have a function like ```VDAF.update_out_shares(tmp_output, out_shares)```",
              "createdAt": "2022-04-27T17:25:26Z",
              "updatedAt": "2022-04-27T17:26:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448acu",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T18:23:49Z",
          "updatedAt": "2022-04-27T18:23:49Z",
          "comments": [
            {
              "originalPosition": 741,
              "body": "We're discussing adding a couple of methods to VDAF here: https://github.com/cjpatton/vdaf/issues/47",
              "createdAt": "2022-04-27T18:23:49Z",
              "updatedAt": "2022-04-27T18:23:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448sub",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:29:54Z",
          "updatedAt": "2022-04-27T19:29:54Z",
          "comments": [
            {
              "originalPosition": 728,
              "body": "Yep, done.",
              "createdAt": "2022-04-27T19:29:54Z",
              "updatedAt": "2022-04-27T19:29:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448tJA",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:31:29Z",
          "updatedAt": "2022-04-27T19:31:30Z",
          "comments": [
            {
              "originalPosition": 249,
              "body": "I think this done right now. The commonality is these steps, each of which have their own sections:\r\n\r\n```\r\n1. Decrypt the input share for each report share as described in {{input-share-decryption}}.\r\n1. Check that the resulting input share is valid as described in {{input-share-batch-validation}}.\r\n1. Initialize VDAF preparation and initial outputs as described in {{input-share-prep}}.\r\n```",
              "createdAt": "2022-04-27T19:31:30Z",
              "updatedAt": "2022-04-27T19:31:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448tOn",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:31:51Z",
          "updatedAt": "2022-04-27T19:31:52Z",
          "comments": [
            {
              "originalPosition": 258,
              "body": "Right -- the section referenced details the common behavior in an aggregator-agnostic way.",
              "createdAt": "2022-04-27T19:31:51Z",
              "updatedAt": "2022-04-27T19:31:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448tlS",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:33:08Z",
          "updatedAt": "2022-04-27T19:33:08Z",
          "comments": [
            {
              "originalPosition": 489,
              "body": "I think we had Process at some point in this PR, but that was confusing since it didn't align with the VDAF terminology. I suggest we punt this to the VDAF draft and then just use whatever term makes sense there.",
              "createdAt": "2022-04-27T19:33:08Z",
              "updatedAt": "2022-04-27T19:33:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448t0g",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:34:03Z",
          "updatedAt": "2022-04-27T19:34:04Z",
          "comments": [
            {
              "originalPosition": 489,
              "body": "https://github.com/cjpatton/vdaf/issues/48",
              "createdAt": "2022-04-27T19:34:03Z",
              "updatedAt": "2022-04-27T19:34:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448t30",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:34:17Z",
          "updatedAt": "2022-04-27T19:34:17Z",
          "comments": [
            {
              "originalPosition": 741,
              "body": "Cool! I'll resolve this then.",
              "createdAt": "2022-04-27T19:34:17Z",
              "updatedAt": "2022-04-27T19:34:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448t6u",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:34:30Z",
          "updatedAt": "2022-04-27T19:34:30Z",
          "comments": [
            {
              "originalPosition": 269,
              "body": "Only to match VDAF. See https://github.com/cjpatton/vdaf/issues/48.",
              "createdAt": "2022-04-27T19:34:30Z",
              "updatedAt": "2022-04-27T19:34:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448vJA",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:39:29Z",
          "updatedAt": "2022-04-27T19:39:29Z",
          "comments": [
            {
              "originalPosition": 206,
              "body": "Yep, it can. I added a note to that effect.",
              "createdAt": "2022-04-27T19:39:29Z",
              "updatedAt": "2022-04-27T19:39:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448wP0",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:43:56Z",
          "updatedAt": "2022-04-27T19:43:56Z",
          "comments": [
            {
              "originalPosition": 418,
              "body": "~~Sorry, what nonce check?~~\r\n\r\nAh, the helper nonce check. That's not here because this is a list of checks to apply on each report individually, whereas the nonce check applies to the AggregateInitReq set of reports as a whole. It seemed better to keep that behavior separate, but I'll flag this as a consideration.",
              "createdAt": "2022-04-27T19:43:56Z",
              "updatedAt": "2022-04-27T19:46:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs448wkF",
          "commit": {
            "abbreviatedOid": "cd67a25"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-27T19:45:12Z",
          "updatedAt": "2022-04-27T19:45:13Z",
          "comments": [
            {
              "originalPosition": 418,
              "body": "The check that the nonces aren't internally duplicated.",
              "createdAt": "2022-04-27T19:45:12Z",
              "updatedAt": "2022-04-27T19:45:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45BpCV",
          "commit": {
            "abbreviatedOid": "9a7e36d"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-28T16:27:31Z",
          "updatedAt": "2022-04-28T16:27:32Z",
          "comments": [
            {
              "originalPosition": 411,
              "body": "I think \"vdaf-prep-error\" here is incorrect. The two checks below each specify different ReportShareError codes. The next section, \"Input Share Preparation\", correctly says it may produce a \"vdaf-prep-error\" error.",
              "createdAt": "2022-04-28T16:27:32Z",
              "updatedAt": "2022-04-28T16:27:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45Bp8h",
          "commit": {
            "abbreviatedOid": "9a7e36d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-28T16:30:22Z",
          "updatedAt": "2022-04-28T16:30:23Z",
          "comments": [
            {
              "originalPosition": 411,
              "body": "```suggestion\r\nthe input share is marked as invalid with a corresponding ReportShareError error.\r\n```",
              "createdAt": "2022-04-28T16:30:22Z",
              "updatedAt": "2022-04-28T16:30:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45Bqc0",
          "commit": {
            "abbreviatedOid": "9a7e36d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-28T16:31:53Z",
          "updatedAt": "2022-04-28T16:31:53Z",
          "comments": [
            {
              "originalPosition": 411,
              "body": "Yep, resolved. Thanks!",
              "createdAt": "2022-04-28T16:31:53Z",
              "updatedAt": "2022-04-28T16:31:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45BrSj",
          "commit": {
            "abbreviatedOid": "9a7e36d"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-28T16:34:38Z",
          "updatedAt": "2022-04-28T16:34:38Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45Brni",
          "commit": {
            "abbreviatedOid": "a019919"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-28T16:35:44Z",
          "updatedAt": "2022-04-28T16:35:44Z",
          "comments": [
            {
              "originalPosition": 248,
              "body": "```suggestion\r\n~~~\r\n\r\n[[OPEN ISSUE: consider sending report shares separately (in parallel) to the aggregate instructions. RIght now, aggregation parameters and the corresponding report shares are sent at the same time, but this may not be strictly necessary. ]]\r\n```",
              "createdAt": "2022-04-28T16:35:44Z",
              "updatedAt": "2022-04-28T16:35:44Z"
            }
          ]
        }
      ]
    },
    {
      "number": 224,
      "id": "PR_kwDOFEJYQs42wQic",
      "title": "interop: Add aggregation parameter to AggregateShareReq",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/224",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Without this, the scope of the request is ambiguous.",
      "createdAt": "2022-04-25T20:55:06Z",
      "updatedAt": "2022-09-16T00:31:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "87936b475e11402fa23bc924578f1a5b7c53fe28",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/interop-agg-share-agg-param",
      "headRefOid": "3d3f05a111506ebe2552790fe51b15b57c71799f",
      "closedAt": "2022-04-25T21:22:42Z",
      "mergedAt": "2022-04-25T21:22:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b88f684b8845f33aafab1ece8583e8c9a7960c6e"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I had the same thought while implementing `/aggregate_share` but concluded that it's not needed. The scope of the `AggregateShareReq` is clear based on the `batch_interval`. The aggregation parameter gets provided to helper during `AggregateInitReq`. What do you need it for while handling `AggregateShareReq`, which should just be a matter of compiling the results of all the aggregation jobs described by `AggregateShareReq.batch_interval`?",
          "createdAt": "2022-04-25T20:59:42Z",
          "updatedAt": "2022-04-25T20:59:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Consider the case where the leader simultaneously runs two aggregation flows for the same batch interval but with different aggregation parameters.",
          "createdAt": "2022-04-25T21:03:51Z",
          "updatedAt": "2022-04-25T21:03:51Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs44xXoc",
          "commit": {
            "abbreviatedOid": "3d3f05a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-25T21:20:17Z",
          "updatedAt": "2022-04-25T21:20:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs44xYGw",
          "commit": {
            "abbreviatedOid": "3d3f05a"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-25T21:22:34Z",
          "updatedAt": "2022-04-25T21:22:34Z",
          "comments": []
        }
      ]
    },
    {
      "number": 225,
      "id": "PR_kwDOFEJYQs420gur",
      "title": "interop: Fix media types",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/225",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-04-26T18:16:36Z",
      "updatedAt": "2022-09-16T00:31:17Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "b88f684b8845f33aafab1ece8583e8c9a7960c6e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/interop-media-types",
      "headRefOid": "8389e9c216f4599a16fe6221a2bcf987ae6d65c4",
      "closedAt": "2022-04-26T18:37:13Z",
      "mergedAt": "2022-04-26T18:37:13Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "cedcb1a4cc3fd96bc6faa4c66fa54d73d1071f5f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs442mS1",
          "commit": {
            "abbreviatedOid": "8389e9c"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-26T18:31:35Z",
          "updatedAt": "2022-04-26T18:31:35Z",
          "comments": []
        }
      ]
    },
    {
      "number": 227,
      "id": "PR_kwDOFEJYQs42407w",
      "title": "Add problem type for batch checksum/report count",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/227",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "# This targets the interop branch\r\n\r\nAdds a problem document type for the case where the helper and leader\r\ncompute different checksums or report counts when constructing aggregate\r\nshares. The error is chosen to match what was added in #223.",
      "createdAt": "2022-04-27T16:30:56Z",
      "updatedAt": "2023-10-26T15:44:57Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton-tgeoghegan/draft-interop-target",
      "baseRefOid": "cedcb1a4cc3fd96bc6faa4c66fa54d73d1071f5f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/batch-mismatch-error",
      "headRefOid": "922c00786603e63be2ec3dbe9b850d7d7dfb1ae6",
      "closedAt": "2022-04-27T17:22:19Z",
      "mergedAt": "2022-04-27T17:22:19Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "79462d9f8703f908b6d4d21d9297cffb626b3ce2"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4472M0",
          "commit": {
            "abbreviatedOid": "922c007"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-27T16:41:05Z",
          "updatedAt": "2022-04-27T16:41:05Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4472YV",
          "commit": {
            "abbreviatedOid": "922c007"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-27T16:41:41Z",
          "updatedAt": "2022-04-27T16:41:41Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs448C1R",
          "commit": {
            "abbreviatedOid": "922c007"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Beat me to it!",
          "createdAt": "2022-04-27T17:13:26Z",
          "updatedAt": "2022-04-27T17:13:26Z",
          "comments": []
        }
      ]
    },
    {
      "number": 229,
      "id": "PR_kwDOFEJYQs4254JX",
      "title": "Update reference to VDAF draft",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/229",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #189.",
      "createdAt": "2022-04-27T21:52:51Z",
      "updatedAt": "2022-09-16T00:29:46Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7e9e7a534cf14c945d68be3f092396dc9cb42227",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/issue189",
      "headRefOid": "dfb7a0deff911485810e2c90e83cac7b9759a8bb",
      "closedAt": "2022-04-27T22:02:55Z",
      "mergedAt": "2022-04-27T22:02:55Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0b888c477c8b5edac305e458286fb39435151881"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs449P-S",
          "commit": {
            "abbreviatedOid": "f3c8823"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Maybe change each reference to `{{?VDAF=I-D.draft-irtf-cfrg-vdaf}}`? That way it'll render like `[VDAF]` in the text, which reads a bit nicer.",
          "createdAt": "2022-04-27T21:55:39Z",
          "updatedAt": "2022-04-27T21:55:39Z",
          "comments": []
        }
      ]
    },
    {
      "number": 231,
      "id": "PR_kwDOFEJYQs429f3l",
      "title": "Clarify encoding",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/231",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #228.",
      "createdAt": "2022-04-28T16:48:54Z",
      "updatedAt": "2023-10-26T15:44:57Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a1db69a015476da0b423cbb2af6ff53a14dd8c3a",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "clarify-encoding",
      "headRefOid": "19b4b987d31efb7acd150f715ada9801f0712495",
      "closedAt": "2022-04-28T17:45:54Z",
      "mergedAt": "2022-04-28T17:45:54Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "3aa0e86a4261cd749f5fa0b2569f5a44f482f042"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45B4F3",
          "commit": {
            "abbreviatedOid": "19b4b98"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-28T17:17:22Z",
          "updatedAt": "2022-04-28T17:17:22Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45CAty",
          "commit": {
            "abbreviatedOid": "19b4b98"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-04-28T17:39:33Z",
          "updatedAt": "2022-04-28T17:39:33Z",
          "comments": []
        }
      ]
    },
    {
      "number": 232,
      "id": "PR_kwDOFEJYQs42-8Al",
      "title": "Replace helper_state with index into per-aggregator storage",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/232",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Since aggregators already need to store nonce sets for the purpose of replay prevention, it doesn't make much sense to also offload part of their state to the leader. Instead, aggregators might benefit from a unique index into some local storage for keeping track of per-aggregation job state. This change drops the helper_state in favor of such an index.\r\n\r\nCloses #185.",
      "createdAt": "2022-04-28T23:07:12Z",
      "updatedAt": "2023-10-26T15:45:00Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9884610ca4102df6e370f3061f9d6223b3244d9c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "remove-helper-state",
      "headRefOid": "82c4bb91693852afc751d6148dc6c4771eb28e0d",
      "closedAt": "2022-05-10T14:21:24Z",
      "mergedAt": "2022-05-10T14:21:24Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "40fc7c55dfa4c7f699c6239ef5fe3734bd23a8a8"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "A few points:\r\n1. This seems strictly less flexible. The helpers could already store an index if they wanted, but now they are required to do so.\r\n2. Can't you store nonce sets in the helper state? \r\n",
          "createdAt": "2022-04-28T23:09:20Z",
          "updatedAt": "2022-04-28T23:09:20Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> This seems strictly less flexible. The helpers could already store an index if they wanted, but now they are required to do so.\r\n\r\nYeah, they could store an index in helper_state. However, helper_state is a \"per-aggregation\" storage mechanism, not a \"per-task\" mechanism, and (I think) the replay state needs to cover the entire task. So we could (a) change helper_state to be a per-task thing, in which case it's not clear 2^16 is enough space, or if we want to be paying that bandwidth for the duration of the task, or (b) go with the layer of indirection as in here. On balance, I think (b) is probably better?   ",
          "createdAt": "2022-04-28T23:18:24Z",
          "updatedAt": "2022-04-28T23:18:24Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n>     2. Can't you store nonce sets in the helper state?\r\n\r\nThis can get really hairy. Think of this way: Storing nonce sets is required for report replay protection, which is needed for privacy.  Depending on what state the helper has locally for managing the \"version\" of the state blob, it may be possible for the leader to \"replay\" an old state and bypass replay protection for reports.\r\n\r\nThere are other ways that helper state gets hairy. For example, we haven't been precise yet about how handling concurrent aggregation flows: If the helper state is \"per-task\", then concurrent flows would be a problem.\r\n\r\nOverall the goal of this change is to remove this complexity altogether. We can always add it back later if we need to.",
          "createdAt": "2022-04-28T23:26:53Z",
          "updatedAt": "2022-04-28T23:26:53Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't agree that this change reduces complexity. AFAICT, this is two changes:\r\n\r\n1. It has the leader assign the ID.\r\n2. It restricts the ID space to uint32.\r\n\r\nISTM that it's just as straightforward for the helper to assign the id at the start of the job, and then use any format it chooses, which the current system allows. I get that you think that certain ways of assigning and using that ID are complex, but that's not our problem because the helper can figure it out for themselves.\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2022-04-28T23:41:13Z",
          "updatedAt": "2022-04-28T23:41:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": ">     2. It restricts the ID space to uint32.\r\n\r\nYou mean 32-byte strings, right? uint32 is just 8 bytes, whereas the AggregationJobID is 32 bytes.\r\n\r\nAnther important thing it does is remove the state blob from the AggregateShareReq. Here's the bit I think you're missing: Right now we require the leader to send the same `helper_state` it received in the helper's previous response. This means that all aggregate requests, i.e., AggregateInitReq, AggregateContReq, and AggregateShareReq, need to be issued one after the other. In particular, it's not possible to run multiple aggregation flows concurrently. You're forced to do\r\n\r\n```\r\nAggregateInitReq(report_set_1, helper_state_0)\r\n<- helper_state_1\r\nAggregateContReq(report_set_1, helper_state_1)\r\n<- helper_state_2\r\n```\r\n\r\n*before* you do\r\n\r\n```\r\nAggregateInitReq(report_set_2, helper_state_2)\r\n<- helper_state_3\r\nAggregateContReq(report_set_2, heper_state_3)\r\n<- helper_state_4\r\n```\r\n\r\n(Note that the \"aggregation flow\" excludes the AggregateShareReq, which is used to combine the results across multiple aggregation flows.) It's going to be important for scalability that we can run multiple aggregation flows simultaneously. That means that we need, at a minimum, the following things:\r\n1. Don't carry state across aggregation flows\r\n2. Make sure there's a way for a helper to \"link\" aggregate requests pertaining to the same aggregation flow\r\n\r\nThis PR accomplishes both of these, but as you point out, is more strict than necessary. Perhaps we could keep the helper state blob as-is, but remove it from the AggregateShareReq and mandate only that the helper state is maintained for the duration of a single aggregation flow.",
          "createdAt": "2022-04-29T00:10:45Z",
          "updatedAt": "2022-04-29T00:10:45Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> Here's the bit I think you're missing: Right now we require the leader to send the same helper_state it received in the helper's previous response.\r\n\r\nI'm undecided about whether this is a good or bad change, but it should be decided on its own merits, not buried in a PR entitled \"Replace helper_state with index into per-aggregator storage\"\r\n\r\n\r\n\r\n",
          "createdAt": "2022-04-29T00:21:46Z",
          "updatedAt": "2022-04-29T00:21:46Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe this change does yield a significant reduction in complexity, because we no longer have to spell out how the helper needs to protect its state from the leader. Aggregation job IDs do not need to be kept secret from the leader, nor does the helper need to need to implement any anti-replay protections for them (because all the sensitive state is now assumed to be in the helper's database).",
          "createdAt": "2022-04-29T00:31:58Z",
          "updatedAt": "2022-04-29T00:31:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@ekr: The reasoning for this was originally discussed on issue #185. This issue didn't get much attention, so we didn't expect the change to be controversial. We certainly didn't intend to bury it. We tried implementing both and thought this approach solved several problems compared to the status quo. Would you prefer that we take this too the list?",
          "createdAt": "2022-04-29T00:35:20Z",
          "updatedAt": "2022-04-29T00:35:20Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's just move this discussion to the list \ud83d\ude03 ",
          "createdAt": "2022-04-29T00:36:54Z",
          "updatedAt": "2022-04-29T00:36:54Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> \r\n\r\nI don't understand these points. Modulo the parallelism points @cjpatton raises, nothing prevents the helper from simply issuing each task a fresh ID and sending that as the helper state. The change in this PR is that it prevents the helper from doing something fancier because the leader issues the ID. Yes, in the latter case, it might need some kind of crypto to protect the state, but those design choices are out of our purview.\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2022-04-29T00:36:58Z",
          "updatedAt": "2022-04-29T00:36:58Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "> @ekr: The reasoning for this was originally discussed on issue https://github.com/ietf-wg-ppm/ppm-specification/issues/185. This issue didn't get much attention, so we didn't expect the change to be controversial. We certainly didn't intend to bury it. We tried implementing both and thought this approach solved several problems compared to the status quo. Would you prefer that we take this too the list?\r\n\r\nI've just read that issue several times and I don't really see much discussion of issuing aggregation requests in parallel.\r\n\r\nThe question here is not primarily about the list versus not the list. It's about the process we are going to follow for landing changes. In general, substantive changes should be getting the consensus of the WG prior to being landed. \r\n\r\nAt this relatively early phase, IETF sometimes allows editors some latitude to make design decisions ahead of time that will then be reviewed by the group when the next draft is published. However, typically the way to do this would be to email the list with a clear explanation of what the changes were and a notification that you were going to land a set of PRs unless someone objected by (say) a week away.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
          "createdAt": "2022-04-29T00:44:49Z",
          "updatedAt": "2022-04-29T00:45:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood mind rebasing to fix conflicts?",
          "createdAt": "2022-05-04T16:53:12Z",
          "updatedAt": "2022-05-04T16:53:12Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton done.\r\n",
          "createdAt": "2022-05-04T17:58:11Z",
          "updatedAt": "2022-05-04T17:58:11Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45DS9g",
          "commit": {
            "abbreviatedOid": "5a4a07e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-04-29T00:32:43Z",
          "updatedAt": "2022-04-29T00:34:36Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "\"at most one\" suggests an aggregation job could be associated with zero PPM tasks.\r\n```suggestion\r\njob. Each aggregation job is associated with exactly one PPM task, and a PPM\r\n```",
              "createdAt": "2022-04-29T00:32:43Z",
              "updatedAt": "2022-04-29T00:34:36Z"
            },
            {
              "originalPosition": 52,
              "body": "I think the idea is that reports 11-20 would be job `M` where `M !=N`.",
              "createdAt": "2022-04-29T00:33:49Z",
              "updatedAt": "2022-04-29T00:34:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45Fkaz",
          "commit": {
            "abbreviatedOid": "d6b4e6f"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-29T13:43:31Z",
          "updatedAt": "2022-04-29T13:43:32Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "```suggestion\r\nAggregate request (Reports 11-20, Job = M) -------------->  \\\r\n<----------------------------- Aggregate response (Job = M) | Reports\r\nAggregate request (continued, Job = M) ------------------>  | 11-20\r\n<----------------------------- Aggregate response (Job = M) /\r\n```",
              "createdAt": "2022-04-29T13:43:31Z",
              "updatedAt": "2022-04-29T13:43:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45Gfgl",
          "commit": {
            "abbreviatedOid": "6655c0f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "The changes LGTM, but of course we should take this to the ppm list before merging.",
          "createdAt": "2022-04-29T16:53:13Z",
          "updatedAt": "2022-04-29T16:53:13Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45W3Ug",
          "commit": {
            "abbreviatedOid": "82c4bb9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-04T18:19:49Z",
          "updatedAt": "2022-05-04T18:19:49Z",
          "comments": []
        }
      ]
    },
    {
      "number": 233,
      "id": "PR_kwDOFEJYQs42_Pg8",
      "title": "HTTP problem document construction details",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/233",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Defines more error types to represent various failure modes, updates\r\nprotocol text to indicate when the new error types should be used, and\r\naffirms that the task ID should be encoded as base64url with no padding\r\n(since task ID has a fixed size).",
      "createdAt": "2022-04-29T00:45:28Z",
      "updatedAt": "2023-10-26T15:44:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9884610ca4102df6e370f3061f9d6223b3244d9c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/http-problem-docs",
      "headRefOid": "797117a45546a9f5fdd3a97888d1e1815ebd409b",
      "closedAt": "2022-05-10T17:15:45Z",
      "mergedAt": "2022-05-10T17:15:45Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "4c76096818c6bfc3635b58a7a922aa995cef559c"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45DUOw",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-29T00:48:46Z",
          "updatedAt": "2022-04-29T00:54:58Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I would probably call these `reportTooLate` and `reportTooEarly`",
              "createdAt": "2022-04-29T00:48:46Z",
              "updatedAt": "2022-04-29T00:54:58Z"
            },
            {
              "originalPosition": 7,
              "body": "Are all of these fatal?\r\n\r\n\r\n\r\n",
              "createdAt": "2022-04-29T00:51:49Z",
              "updatedAt": "2022-04-29T00:54:58Z"
            },
            {
              "originalPosition": 20,
              "body": "I would put the parenthetical after the word \"task ID\" rather than after the text about b64.",
              "createdAt": "2022-04-29T00:53:20Z",
              "updatedAt": "2022-04-29T00:54:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45GY3c",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-29T16:26:14Z",
          "updatedAt": "2022-04-29T16:26:14Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Yes, inasmuch as they represent conditions that the aggregators cannot recover from. `insufficientBatchSize` for instance means there aren't enough reports in `CollectReq.batch_interval` to satisfy the task's `min_batch_size`, which means the resulting aggregate would violate client privacy. The description text could be improved on this one.",
              "createdAt": "2022-04-29T16:26:14Z",
              "updatedAt": "2022-04-29T16:26:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45GgMw",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-29T16:56:11Z",
          "updatedAt": "2022-04-29T16:56:12Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Well, but could I aggregate some more into the batch and then ask for collect?",
              "createdAt": "2022-04-29T16:56:12Z",
              "updatedAt": "2022-04-29T16:56:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45GkyQ",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-29T17:15:53Z",
          "updatedAt": "2022-04-29T17:15:54Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Yes, absolutely. Getting this error is how a collector would know to either try again with a bigger `batch_interval` or wait for more reports to arrive.",
              "createdAt": "2022-04-29T17:15:53Z",
              "updatedAt": "2022-04-29T17:15:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45GlCn",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-04-29T17:16:58Z",
          "updatedAt": "2022-04-29T17:16:58Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "OK, so I think we're going to need to somehow note which errors are recoverable and which are not.",
              "createdAt": "2022-04-29T17:16:58Z",
              "updatedAt": "2022-04-29T17:16:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45R7dq",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-03T20:18:26Z",
          "updatedAt": "2022-05-03T20:18:26Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "That makes sense. And for this error, we should add some text to the section on collecting explaining what a collector could do if they encounter it.\r\n\r\n- [x] explain in `collect-flow` what collectors should do in the face of `insufficientBatchSize`",
              "createdAt": "2022-05-03T20:18:26Z",
              "updatedAt": "2022-05-10T17:14:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45SNt-",
          "commit": {
            "abbreviatedOid": "797117a"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-03T20:50:57Z",
          "updatedAt": "2022-05-03T20:50:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45WZ-L",
          "commit": {
            "abbreviatedOid": "797117a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks great!",
          "createdAt": "2022-05-04T16:44:34Z",
          "updatedAt": "2022-05-04T16:44:34Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45dT3P",
          "commit": {
            "abbreviatedOid": "797117a"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-05T21:40:47Z",
          "updatedAt": "2022-05-05T21:40:47Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45sLBJ",
          "commit": {
            "abbreviatedOid": "797117a"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM pending one suggestion, which we can definitely address separately if desired.",
          "createdAt": "2022-05-10T14:24:37Z",
          "updatedAt": "2022-05-10T14:25:24Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "```suggestion\r\nLeaders can buffer reports while waiting to aggregate them. The\r\n```\r\n\r\nI realize this is just relocated text, but I don't really see how we can have a MUST here without any further constraints (how long must they buffer them?), and in the end it's an implementation-specific decision, so I'd just remove the normative language here.",
              "createdAt": "2022-05-10T14:24:38Z",
              "updatedAt": "2022-05-10T14:25:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45sLhX",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T14:25:51Z",
          "updatedAt": "2022-05-10T14:25:52Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "@tgeoghegan can we file an issue to track this?",
              "createdAt": "2022-05-10T14:25:52Z",
              "updatedAt": "2022-05-10T14:25:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45tLN-",
          "commit": {
            "abbreviatedOid": "26541fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T17:14:29Z",
          "updatedAt": "2022-05-10T17:14:30Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Done: #239 ",
              "createdAt": "2022-05-10T17:14:30Z",
              "updatedAt": "2022-05-10T17:14:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45tMGo",
          "commit": {
            "abbreviatedOid": "797117a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T17:17:38Z",
          "updatedAt": "2022-05-10T17:17:39Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "Sorry, I mashed \"MERGE\" before I noticed this. I captured this as #240",
              "createdAt": "2022-05-10T17:17:38Z",
              "updatedAt": "2022-05-10T17:17:39Z"
            }
          ]
        }
      ]
    },
    {
      "number": 234,
      "id": "PR_kwDOFEJYQs43LR58",
      "title": "Remove trailing whitespace",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/234",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-05-02T15:59:03Z",
      "updatedAt": "2022-09-16T00:29:46Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3aa0e86a4261cd749f5fa0b2569f5a44f482f042",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/editorial",
      "headRefOid": "1556b310e3a53ca7e34d26dad704dcf0ee6ebfdc",
      "closedAt": "2022-05-02T16:32:46Z",
      "mergedAt": "2022-05-02T16:32:46Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "004020f88c38785af23adfa909e4d0c646434bb1"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "You can submit these right to main. No need for a PR, IMO.",
          "createdAt": "2022-05-02T15:59:51Z",
          "updatedAt": "2022-05-02T15:59:51Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45LQBx",
          "commit": {
            "abbreviatedOid": "1556b31"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-02T15:59:47Z",
          "updatedAt": "2022-05-02T15:59:47Z",
          "comments": []
        }
      ]
    },
    {
      "number": 235,
      "id": "PR_kwDOFEJYQs43LYs-",
      "title": "Update security considerations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/235",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #211.\r\n\r\nThis change is primarily editorial and includes the following changes:\r\n\r\n* Replace references to \"Prio\" with references to a generic VDAF. (This\r\n  section was written long ago when we had Prio in mind.)\r\n* Elaborate on known issues for collect requests.\r\n* Discuss Sybil attacks, including enumerating the different types\r\n  (#211).",
      "createdAt": "2022-05-02T16:28:54Z",
      "updatedAt": "2022-09-16T00:29:45Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9884610ca4102df6e370f3061f9d6223b3244d9c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatotn/editorial-sec-cons",
      "headRefOid": "edea17ef8f4902bf49564b0340405a05454a72b1",
      "closedAt": "2022-05-10T14:22:57Z",
      "mergedAt": "2022-05-10T14:22:56Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "92bb26e0f4b9b6dde37761496d7b39dba590b07d"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45SrI8",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-03T23:52:46Z",
          "updatedAt": "2022-05-03T23:58:05Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "```suggestion\r\nDAP assumes an active attacker that controls the network and has the ability to\r\n```\r\nSince #220 ",
              "createdAt": "2022-05-03T23:52:46Z",
              "updatedAt": "2022-05-03T23:58:05Z"
            },
            {
              "originalPosition": 34,
              "body": "```suggestion\r\nshares for aggregation or coerce an aggregator into diverting from the\r\n```",
              "createdAt": "2022-05-03T23:52:53Z",
              "updatedAt": "2022-05-03T23:58:05Z"
            },
            {
              "originalPosition": 79,
              "body": "```suggestion\r\n   such a mechanism beyond requiring server authentication for HTTPS sessions.\r\n```",
              "createdAt": "2022-05-03T23:55:47Z",
              "updatedAt": "2022-05-03T23:58:05Z"
            },
            {
              "originalPosition": 67,
              "body": "Does \"core\" mean a deployment that doesn't use report extensions or any kind of client authentication mechanism?",
              "createdAt": "2022-05-03T23:57:42Z",
              "updatedAt": "2022-05-03T23:58:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45TEZs",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T02:43:18Z",
          "updatedAt": "2022-05-04T02:48:22Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "Beyond the protocol extensions it is also the case that some VDAFs have Sybil protections (against privacy) built-in e.g. with noise addition / differential privacy. DAP _could_ enforce that the aggregation function provide this kind of privacy but we choose not to.",
              "createdAt": "2022-05-04T02:43:19Z",
              "updatedAt": "2022-05-04T02:50:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45VluG",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T14:24:03Z",
          "updatedAt": "2022-05-04T14:24:04Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "This change needs to be applied elsewhere, better to do it in a follow-up PR I think.",
              "createdAt": "2022-05-04T14:24:04Z",
              "updatedAt": "2022-05-04T14:33:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45VpLj",
          "commit": {
            "abbreviatedOid": "b78edc4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T14:32:35Z",
          "updatedAt": "2022-05-04T14:33:20Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "@tgeoghegan \"core\" refers to the bits of the spec that apply to all implementations. In particular, not every implementation will be required to interpret extensions the same way.\r\n\r\n@csharrison It's not clear to me that DP always provides adequate protection against Sybil attacks. I think it ultimately depends on how you define privacy. That said, requiring DP is totally on the table, but before doing this I think we should start by spelling out what this would look like as an optional feature of the protocol. I think the first question to ask is whether this mechanism lives here or in the underlying VDAF. I would love to have your help on this.",
              "createdAt": "2022-05-04T14:32:36Z",
              "updatedAt": "2022-05-04T14:33:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45VsdC",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T14:40:59Z",
          "updatedAt": "2022-05-04T14:40:59Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "> @csharrison It's not clear to me that DP always provides adequate protection against Sybil attacks. I think it ultimately depends on how you define privacy. That said, requiring DP is totally on the table, but before doing this I think we should start by spelling out what this would look like as an optional feature of the protocol. I think the first question to ask is whether this mechanism lives here or in the underlying VDAF. I would love to have your help on this.\r\n\r\nI think if you can prove that the system achieves user-level DP you should be able to show that even with worst-case Sybil attacks, a user's privacy is still protected to some degree. That being said, this is a property of the whole system, not just the VDAF.\r\n\r\nI am fairly sure that DAP / PPM does not want to require DP in its deployments. This brings on a lot of baggage, and for instances where these kinds of attacks are truly not a problem (e.g. with airtight client authentication) it might not be that useful. The main point here is that we might want to call out that the particular VDAF used may mitigate some of the risks of Sybil.",
              "createdAt": "2022-05-04T14:40:59Z",
              "updatedAt": "2022-05-04T14:41:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45WOaa",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T16:07:14Z",
          "updatedAt": "2022-05-04T16:07:15Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "If achievable, I think specifying an optional, generic DP mechanism here (perhaps for some class of VDAFs with a specific property) would have significant advantages. IIUC, tuning and enforcing the privacy budget will require knowledge of how measurements are generated by clients over time. This is something that will be dictated by the deployment, and I think it would be best to avoid bleeding deployment details into VDAF wherever possible.\r\n\r\nThat said, VDAF may need to at least say something about how/when to add noise into an input or aggregate share.",
              "createdAt": "2022-05-04T16:07:14Z",
              "updatedAt": "2022-05-04T16:07:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45Wfo-",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T17:01:43Z",
          "updatedAt": "2022-05-04T17:01:43Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "> If achievable, I think specifying an optional, generic DP mechanism here (perhaps for some class of VDAFs with a specific property) would have significant advantages\r\n\r\nI think this is possible. We can probably have some tunable, exposed property of a VDAF that is a necessary but not sufficient condition for achieving DP. We can describe the rest of the conditions needed in the DAP deployment to get the rest of the way there.\r\n\r\nThis will not be super easy to do, but I am happy to help think through what it will take (maybe in a separate issue).",
              "createdAt": "2022-05-04T17:01:43Z",
              "updatedAt": "2022-05-04T17:02:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45WtJE",
          "commit": {
            "abbreviatedOid": "9e8c636"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-05-04T17:43:05Z",
          "updatedAt": "2022-05-04T17:45:58Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "```suggestion\r\n1. Privacy. Clients trust that some aggregator is honest. That is, as long as at\r\n   least one aggregator executes the protocol faithfully, the parties learn nothing\r\n   beyond the aggregate result (i.e., the output of the aggregation function computed over\r\n   the honest measurements).\r\n1. Correctness. The collector trusts that the aggregators execute the protocol\r\n   correctly. That is, as long as the aggregators execute the protocol faithfully,\r\n   a malicious client can skew the aggregate result only by reporting\r\n   a false (untruthful) measurement. The result cannot be influenced in any\r\n   other way.\r\n```\r\nFor consistency across the two bullets",
              "createdAt": "2022-05-04T17:43:05Z",
              "updatedAt": "2022-05-04T17:45:58Z"
            },
            {
              "originalPosition": 80,
              "body": "I think we're currently debating this point and thus will need to revisit this text. Perhaps include a bracketed note referencing https://github.com/ietf-wg-ppm/ppm-specification/issues/155 or something else that captures the discussion about mutual auth?",
              "createdAt": "2022-05-04T17:45:53Z",
              "updatedAt": "2022-05-04T17:45:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45WuTy",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T17:47:24Z",
          "updatedAt": "2022-05-04T17:47:24Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "Kicking off an issue is probably a good way forward. I'm happy to help with the protocol design, but will need to lean on you to make sure we have all the plumbing we need to implement. A couple seed questions to start:\r\n1. Who adds noise and when? Do we want to support local DP, centralized DP, or both? Also would be helpful to define these (and point to references where needed).\r\n2. What syntactic changes are necessary for VDAF?\r\n\r\nOnce you create the issue, it would also be helpful to ping the list with a brief description of the issue and a link.\r\n",
              "createdAt": "2022-05-04T17:47:24Z",
              "updatedAt": "2022-05-04T17:47:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45W0y-",
          "commit": {
            "abbreviatedOid": "9e8c636"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-04T18:10:11Z",
          "updatedAt": "2022-05-04T18:10:11Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "Done",
              "createdAt": "2022-05-04T18:10:11Z",
              "updatedAt": "2022-05-04T18:10:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45W2G-",
          "commit": {
            "abbreviatedOid": "601699b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-04T18:15:13Z",
          "updatedAt": "2022-05-04T18:15:13Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45ajsw",
          "commit": {
            "abbreviatedOid": "601699b"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "The list of attacks is really a list of things we ought to fix in the protocol, so I would not even bother including them in the security considerations. If we want to use this as an opportunity to identify open issues that need to be addressed before we consider the protocol secure, I would simply list the open issues and keep discussion in GitHub. (That is, the text here seems redundant with the issue \ud83e\udd37 )",
          "createdAt": "2022-05-05T12:13:50Z",
          "updatedAt": "2022-05-05T12:15:02Z",
          "comments": [
            {
              "originalPosition": 51,
              "body": "```suggestion\r\nCurrently, the specification does not achieve these goals. In particular, there are several open\r\nissues that need to be addressed before these goals are met. Details for each issue are below.\r\n```",
              "createdAt": "2022-05-05T12:13:50Z",
              "updatedAt": "2022-05-05T12:15:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45cMdI",
          "commit": {
            "abbreviatedOid": "8fc3d2b"
          },
          "author": "csharrison",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-05T17:02:45Z",
          "updatedAt": "2022-05-05T17:02:46Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "commented on https://github.com/ietf-wg-ppm/ppm-specification/issues/19, I doesn't answer all the questions but it's a start :)",
              "createdAt": "2022-05-05T17:02:46Z",
              "updatedAt": "2022-05-05T17:02:46Z"
            }
          ]
        }
      ]
    },
    {
      "number": 236,
      "id": "PR_kwDOFEJYQs43P2Bq",
      "title": "include task ID in request to hpke_config",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/236",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "When obtaining an aggregator's HPKE configuration, clients now specify\r\nthe task ID they are interested in. Aggregators are not required to use\r\na distinct HPKE configuration for each task, but now it's possible for\r\nthem to do so. The endpoint is also renamed to `hpke_config` from\r\n`key_config` for clarity.",
      "createdAt": "2022-05-03T18:57:15Z",
      "updatedAt": "2023-10-26T15:45:00Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9884610ca4102df6e370f3061f9d6223b3244d9c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/hpke-config-endpoint",
      "headRefOid": "69bde38e65ba0fce31e5fd87550c76e5068eb15f",
      "closedAt": "2022-05-10T14:22:00Z",
      "mergedAt": "2022-05-10T14:22:00Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "46cfb1b4c79e9862046d3e91b24816f5f92f2f5f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45R8YB",
          "commit": {
            "abbreviatedOid": "69bde38"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-03T20:21:26Z",
          "updatedAt": "2022-05-03T20:21:26Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45WaXw",
          "commit": {
            "abbreviatedOid": "69bde38"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-04T16:45:55Z",
          "updatedAt": "2022-05-04T16:45:55Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45acN4",
          "commit": {
            "abbreviatedOid": "69bde38"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nice \ud83d\udc4d ",
          "createdAt": "2022-05-05T11:44:17Z",
          "updatedAt": "2022-05-05T11:44:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45dX4W",
          "commit": {
            "abbreviatedOid": "69bde38"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-05T22:02:57Z",
          "updatedAt": "2022-05-05T22:04:32Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "This referenced section is by and large particular to the client requesting an HPKE configuration from the leader, but it does include relevant structure definitions. How's this?\r\n\r\n```\r\n... The HPKE configuration of the collector (`HpkeConfig` defined in {{hpke-config}}).\r\n```",
              "createdAt": "2022-05-05T22:02:57Z",
              "updatedAt": "2022-05-05T22:04:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45dgMe",
          "commit": {
            "abbreviatedOid": "69bde38"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-05T23:02:30Z",
          "updatedAt": "2022-05-05T23:02:30Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "IMO we should fix this by moving the definition of `struct HpkeConfig` and its attendant types from `{{hpke-config}}` up to the common type definitions in \"Protocol Definition\" to eliminate this awkward forward reference. I'll do that in a separate change.",
              "createdAt": "2022-05-05T23:02:30Z",
              "updatedAt": "2022-05-05T23:02:30Z"
            }
          ]
        }
      ]
    },
    {
      "number": 238,
      "id": "PR_kwDOFEJYQs43iocw",
      "title": "Add bearer token for HTTP request authentication",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/238",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Defines an HTTP request authentication mechanism and requires its usage\r\nin all collector->leader and leader->helper requests. We intend for this\r\nmechanism to be replaced eventually by something more secure, e.g., TLS\r\nclient authentication.\r\n\r\nFor details see https://mailarchive.ietf.org/arch/msg/ppm/z65FK8kOU27Dt38WNhpI6apc2so/.\r\n\r\n~Closes issue 155.~ As pointed out in the comments, this change may not be sufficient.",
      "createdAt": "2022-05-10T00:50:26Z",
      "updatedAt": "2022-09-16T00:29:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "4c76096818c6bfc3635b58a7a922aa995cef559c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/agg-auth",
      "headRefOid": "9933d1dc6ebe36779b18406b90858a6c0cecf369",
      "closedAt": "2022-05-11T11:10:27Z",
      "mergedAt": "2022-05-11T11:10:27Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "477e8d5af823f0bfa93bc4a78c2361527e7693ad"
      },
      "comments": [
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "FWIW, I agree with @chris-wood: this isn't necessary for interop testing, where authentication doesn't matter that much (as an example, we did much of our TLS 1.3 interop testing with self-signed certs). Obviously, if people want to deploy a system with real data, then some authentication is required, but enabling people to do that does not need to be a priority for the WG at this point; they can just make their own arrangements. I'm OK to land something, but let's just do the minimum.",
          "createdAt": "2022-05-10T17:24:05Z",
          "updatedAt": "2022-05-10T17:26:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Alright, it sounds like after seeing an actual PR, folks are converging on leaving this unspecified for now. I'm OK with this result. Just one quick note here: Cloudflare/ISRG have both implemented the HMAC construction described in #179, so we will likely stick with that for the time being.\r\n\r\nI'll ping the list to make sure folks know where we've landed.",
          "createdAt": "2022-05-10T17:35:19Z",
          "updatedAt": "2022-05-10T17:35:19Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45sJXB",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I've no objections to this, but I don't think it needs to be codified in the spec. I think experimental deployments should be free to choose whatever header and bearer token content they want to make this work. \r\n\r\nAlso, are we sure we want to close #155 with this? I'd suggest we keep that issue open, as it tracks the \"real\" solution to this problem.",
          "createdAt": "2022-05-10T14:20:35Z",
          "updatedAt": "2022-05-10T15:21:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45st0p",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I agree with Chris W. that we should keep #155 open, especially since this change does not solve the problem of integrity or authenticity of collect parameters from the collector to the helper.",
          "createdAt": "2022-05-10T15:43:24Z",
          "updatedAt": "2022-05-10T15:48:29Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "```suggestion\r\nFor requests requiring authentication, the sender includes a \"DAP-Auth-Token\"\r\n```",
              "createdAt": "2022-05-10T15:43:25Z",
              "updatedAt": "2022-05-10T15:48:29Z"
            },
            {
              "originalPosition": 40,
              "body": "I think we should require servers to respond with HTTP 403 Forbidden here (our definition of \"abort with error ...\" allows servers to use any 4xx or 5xx status).",
              "createdAt": "2022-05-10T15:47:13Z",
              "updatedAt": "2022-05-10T15:48:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45s4VA",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-05-10T16:11:06Z",
          "updatedAt": "2022-05-10T16:22:17Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "```suggestion\r\nPrior to the start of the protocol, the sender and receiver arrange to share a\r\nsecret sender-specific API token, which MUST be suitable for representation in\r\nan HTTP header.\r\n```",
              "createdAt": "2022-05-10T16:11:06Z",
              "updatedAt": "2022-05-10T16:22:17Z"
            },
            {
              "originalPosition": 35,
              "body": "```suggestion\r\nheader in its HTTP request containing the API token.\r\n```",
              "createdAt": "2022-05-10T16:12:58Z",
              "updatedAt": "2022-05-10T16:22:17Z"
            },
            {
              "originalPosition": 40,
              "body": "```suggestion\r\nTo authenticate the request, the receiver looks up the token for the\r\nsender as determined by the task configuration. (See {{task-configuration}}.) If\r\nthe value of the \"DAP-Auth-Token\" header does not match the token, then\r\nthe receiver MUST abort with error \"unauthorizedRequest\".\r\n```",
              "createdAt": "2022-05-10T16:13:20Z",
              "updatedAt": "2022-05-10T17:00:20Z"
            },
            {
              "originalPosition": 40,
              "body": "This seems inconsistent with the guidance of BCP 56 S 8.",
              "createdAt": "2022-05-10T16:21:56Z",
              "updatedAt": "2022-05-10T16:22:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45tG-I",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T16:59:28Z",
          "updatedAt": "2022-05-10T17:00:34Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Why not be prescriptive?",
              "createdAt": "2022-05-10T16:59:28Z",
              "updatedAt": "2022-05-10T17:00:34Z"
            },
            {
              "originalPosition": 35,
              "body": "Same question as above.",
              "createdAt": "2022-05-10T16:59:48Z",
              "updatedAt": "2022-05-10T17:00:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45tKw_",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T17:12:56Z",
          "updatedAt": "2022-05-10T17:12:56Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "First, the actual guidance is not good:\r\n1. 32 bytes is far longer than needed\r\n2. Binary strings are hard to cut and paste, so people will want to actually have them in ASCII, which this prohibits, and even if it didn't then would then have to re-encode them per the rules below.\r\n3. It forbids structure (e.g., the account ID) which might be convenient.\r\n4. If people have existing systems for managing API tokens, this might conflict.\r\n\r\nSecond, it's unenforceable and not required for interoperability, so we it's not our job to tell people what to do.\r\n\r\n\r\n\r\n",
              "createdAt": "2022-05-10T17:12:56Z",
              "updatedAt": "2022-05-10T17:12:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45ta46",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T18:03:05Z",
          "updatedAt": "2022-05-10T18:03:05Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "Thanks for the reference, this is really useful! However I think this paragraph in [section 8](\r\nhttps://datatracker.ietf.org/doc/html/rfc3205#section-8) supports using an HTTP error code:\r\n\r\n>    A layered application should use appropriate HTTP error codes to\r\n>      report errors resulting from information in the HTTP request-line\r\n>      and *header fields associated with the request*.  This request\r\n>      information is part of the HTTP protocol and errors which are\r\n>      associated with that information should therefore be reported\r\n>      using HTTP protocol mechanisms.\r\n\r\nIn this specific case, the error results from information in the `PPM[/DAP]-Auth-Token` HTTP header and so indicating an error in HTTP seems appropriate. However this doesn't matter if we end up deciding not to dictate specific auth mechanisms at this stage or ever.",
              "createdAt": "2022-05-10T18:03:05Z",
              "updatedAt": "2022-05-10T18:03:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45tdY8",
          "commit": {
            "abbreviatedOid": "aad7312"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-10T18:12:16Z",
          "updatedAt": "2022-05-10T18:12:17Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "Agreed. Good point.",
              "createdAt": "2022-05-10T18:12:17Z",
              "updatedAt": "2022-05-10T18:12:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45tw1H",
          "commit": {
            "abbreviatedOid": "9d0cf25"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "After conferring offline, we decided to merge this PR afterall. The branch has been rebased.",
          "createdAt": "2022-05-10T19:19:01Z",
          "updatedAt": "2022-05-10T19:28:13Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Good points. ",
              "createdAt": "2022-05-10T19:19:01Z",
              "updatedAt": "2022-05-10T19:28:13Z"
            },
            {
              "originalPosition": 40,
              "body": "Oof, it would be useful to add an error code column to the error table. Will punt on this for now and simply add this to the text.",
              "createdAt": "2022-05-10T19:20:37Z",
              "updatedAt": "2022-05-10T19:28:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45t0yk",
          "commit": {
            "abbreviatedOid": "9933d1d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-10T19:32:29Z",
          "updatedAt": "2022-05-10T19:32:29Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45uX_K",
          "commit": {
            "abbreviatedOid": "9933d1d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-10T22:05:03Z",
          "updatedAt": "2022-05-10T22:05:03Z",
          "comments": []
        }
      ]
    },
    {
      "number": 242,
      "id": "PR_kwDOFEJYQs43qExV",
      "title": "Editorial",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/242",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-05-11T14:33:58Z",
      "updatedAt": "2022-09-16T00:29:43Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "477e8d5af823f0bfa93bc4a78c2361527e7693ad",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/edit-agg-structs",
      "headRefOid": "5eed0ec2b182df29880d4064fd30f4e33df0c8fa",
      "closedAt": "2022-05-11T14:57:49Z",
      "mergedAt": "2022-05-11T14:57:49Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "c6c70ec9b3dc795a39039c110ccf4e0470641e68"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45yT4r",
          "commit": {
            "abbreviatedOid": "97071bd"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-05-11T14:36:13Z",
          "updatedAt": "2022-05-11T14:36:21Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I realize this is a bike shed, but I think we should keep the previous name. Abbreviating for the sake of less characters (thanks, Unix) just makes the overall intent less clear.",
              "createdAt": "2022-05-11T14:36:13Z",
              "updatedAt": "2022-05-11T14:36:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yWqJ",
          "commit": {
            "abbreviatedOid": "97071bd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-11T14:42:30Z",
          "updatedAt": "2022-05-11T14:42:30Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Then I would suggest changing \"AggregateInit\" to \"AggregateInitialize\" for consistency. (My preference would be shorter names for both, but my main concern is being consistent.)",
              "createdAt": "2022-05-11T14:42:30Z",
              "updatedAt": "2022-05-11T14:42:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yXDR",
          "commit": {
            "abbreviatedOid": "e84af3e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "@chris-wood please have another look as I also replaced PPM with DAP in this change.",
          "createdAt": "2022-05-11T14:43:24Z",
          "updatedAt": "2022-05-11T14:43:24Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45yXlM",
          "commit": {
            "abbreviatedOid": "97071bd"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-11T14:44:26Z",
          "updatedAt": "2022-05-11T14:44:27Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Making them consistent seems good \ud83d\udc4d  ",
              "createdAt": "2022-05-11T14:44:26Z",
              "updatedAt": "2022-05-11T14:44:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yX6c",
          "commit": {
            "abbreviatedOid": "e84af3e"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "The s/PPM/DAP/g changes look good.",
          "createdAt": "2022-05-11T14:45:10Z",
          "updatedAt": "2022-05-11T14:45:10Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45yd-T",
          "commit": {
            "abbreviatedOid": "97071bd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-11T14:54:58Z",
          "updatedAt": "2022-05-11T14:54:59Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Done",
              "createdAt": "2022-05-11T14:54:59Z",
              "updatedAt": "2022-05-11T14:54:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yfVX",
          "commit": {
            "abbreviatedOid": "5eed0ec"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T14:57:30Z",
          "updatedAt": "2022-05-11T14:57:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 243,
      "id": "PR_kwDOFEJYQs43qM0q",
      "title": "Replace \"ppm\" with \"dap\" and align context strings with next draft",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/243",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This change is basically a search-and-replace for /ppm/dap/. However, I have also re-aligned the context strings to match the next draft that will be cut (-01).\r\n\r\nI think the change is mostly correct, my main concern is the \"URN Sub-namespace for PPM (urn:ietf:params:dap)\" section. \ud83d\udeb2 \ud83c\udfe0 \u23f0: Does it make sense for all drafts in the PPM WG to share the namespace, or do we want to define one specifically for DAP?",
      "createdAt": "2022-05-11T14:57:16Z",
      "updatedAt": "2022-09-16T00:29:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c6c70ec9b3dc795a39039c110ccf4e0470641e68",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/ppm-die-die-die",
      "headRefOid": "26dafdd1322a544f98e6872f4582365555f83c3e",
      "closedAt": "2022-05-12T15:20:58Z",
      "mergedAt": "2022-05-12T15:20:58Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "6c7c1b722f26ea626214e34420311fe269c0abb0"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> LGTM pending the conflict comment.\r\n\r\nBefore merging I would appreciate an answer on the question I raised above. I want to make sure the URN bits are correct.",
          "createdAt": "2022-05-11T15:03:38Z",
          "updatedAt": "2022-05-11T15:03:38Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Does it make sense for all drafts in the PPM WG to share the namespace, or do we want to define one specifically for DAP?\r\n\r\nSeparate namespaces seems better, especially if we're thinking of DAP and STAR. Names are cheap, after all.",
          "createdAt": "2022-05-11T15:05:03Z",
          "updatedAt": "2022-05-11T15:05:03Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Does it make sense for all drafts in the PPM WG to share the namespace, or do we want to define one specifically for DAP?\r\n\r\nSeparate namespaces, because otherwise I think we'd have to write a further document enumerating the error type URNs that both this document and other PPM drafts like STAR could reference. We should avoid spreading the specification across multiple documents more than necessary. Maybe including the WG in the URN path like `urn:ietf:params:ppm:dap:error` helps?",
          "createdAt": "2022-05-11T16:45:18Z",
          "updatedAt": "2022-05-11T16:45:18Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Here's the RFC defining `urn:ietf:params` https://www.rfc-editor.org/rfc/rfc3553.html. It's pretty loosely structured, and the existing sub-namespaces from published RFCs appear to just be named after protocols and formats, not working groups. https://www.iana.org/assignments/params/params.xhtml",
          "createdAt": "2022-05-11T16:54:56Z",
          "updatedAt": "2022-05-11T16:54:56Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I took @tgeoghegan's suggestion of prefixing the dap space with ppm.",
          "createdAt": "2022-05-11T16:56:04Z",
          "updatedAt": "2022-05-11T16:56:19Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45ygH8",
          "commit": {
            "abbreviatedOid": "4752c36"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM pending the conflict comment.",
          "createdAt": "2022-05-11T14:59:22Z",
          "updatedAt": "2022-05-11T14:59:34Z",
          "comments": [
            {
              "originalPosition": 115,
              "body": "I assume this will conflict now that it's AggregateInitializeReq/Resp?",
              "createdAt": "2022-05-11T14:59:23Z",
              "updatedAt": "2022-05-11T14:59:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yh3f",
          "commit": {
            "abbreviatedOid": "4752c36"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-11T15:03:10Z",
          "updatedAt": "2022-05-11T15:03:11Z",
          "comments": [
            {
              "originalPosition": 115,
              "body": "Fixed after rebase.",
              "createdAt": "2022-05-11T15:03:11Z",
              "updatedAt": "2022-05-11T15:03:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yiKg",
          "commit": {
            "abbreviatedOid": "7d219f0"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-11T15:03:56Z",
          "updatedAt": "2022-05-11T15:03:56Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "```suggestion\r\n```",
              "createdAt": "2022-05-11T15:03:56Z",
              "updatedAt": "2022-05-11T15:03:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45yioi",
          "commit": {
            "abbreviatedOid": "7d219f0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-11T15:05:09Z",
          "updatedAt": "2022-05-11T15:05:09Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Oops, that was sloppy. Fixed.",
              "createdAt": "2022-05-11T15:05:09Z",
              "updatedAt": "2022-05-11T15:05:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs45zFVa",
          "commit": {
            "abbreviatedOid": "4cdc2ca"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T16:43:07Z",
          "updatedAt": "2022-05-11T16:43:07Z",
          "comments": []
        }
      ]
    },
    {
      "number": 244,
      "id": "PR_kwDOFEJYQs43rGW9",
      "title": "remove unneeded MUST",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/244",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Per @chris-wood's [suggestion](https://github.com/ietf-wg-ppm/ppm-specification/pull/233#discussion_r869306691)\r\nResolves #240",
      "createdAt": "2022-05-11T19:03:21Z",
      "updatedAt": "2023-10-26T15:45:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c6c70ec9b3dc795a39039c110ccf4e0470641e68",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/leader-buffer-MUST",
      "headRefOid": "98e4ed5685a23c7b2d2818df9fb882a5417a8a9f",
      "closedAt": "2022-05-11T19:27:25Z",
      "mergedAt": "2022-05-11T19:27:25Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "dac00320cefce8193c37a3a305f25f7eb11f4c99"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45zwBS",
          "commit": {
            "abbreviatedOid": "98e4ed5"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T19:06:09Z",
          "updatedAt": "2022-05-11T19:06:09Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45zyOg",
          "commit": {
            "abbreviatedOid": "98e4ed5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T19:14:56Z",
          "updatedAt": "2022-05-11T19:14:56Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45zy-4",
          "commit": {
            "abbreviatedOid": "98e4ed5"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T19:17:58Z",
          "updatedAt": "2022-05-11T19:17:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 245,
      "id": "PR_kwDOFEJYQs43rNxT",
      "title": "Specify HTTP method for HPKE Configuration Request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/245",
      "state": "MERGED",
      "author": "jbr",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-05-11T19:37:52Z",
      "updatedAt": "2022-05-11T19:46:08Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "dac00320cefce8193c37a3a305f25f7eb11f4c99",
      "headRepository": "jbr/ppm-specification",
      "headRefName": "specify-http-verb-for-config-req",
      "headRefOid": "0d5724474631e95cb57d371e19d78225f343ddfa",
      "closedAt": "2022-05-11T19:46:08Z",
      "mergedAt": "2022-05-11T19:46:08Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "de6b2698ad671d7de0a37e183a732115b5f54fec"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45z41M",
          "commit": {
            "abbreviatedOid": "0d57244"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T19:40:14Z",
          "updatedAt": "2022-05-11T19:40:14Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45z5Md",
          "commit": {
            "abbreviatedOid": "0d57244"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-11T19:41:47Z",
          "updatedAt": "2022-05-11T19:41:47Z",
          "comments": []
        }
      ]
    },
    {
      "number": 246,
      "id": "PR_kwDOFEJYQs43ucJ1",
      "title": "Use consistent HTTP response status code phrasing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/246",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #192.",
      "createdAt": "2022-05-12T14:35:52Z",
      "updatedAt": "2023-10-26T15:45:02Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "6c7c1b722f26ea626214e34420311fe269c0abb0",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/consistent-status-codes",
      "headRefOid": "e56eeb3a23389fd39a46838ed22117b4dee4c1b8",
      "closedAt": "2022-05-12T20:28:52Z",
      "mergedAt": "2022-05-12T20:28:52Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "994ba9b5e45673b455d3a216944098f8090c311d"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs454KHS",
          "commit": {
            "abbreviatedOid": "29cfa46"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I didn't check if this change is complete, in the sense of making all the language in the doc consistent, but it's at least self-consistent. \ud83d\udea2 !",
          "createdAt": "2022-05-12T15:05:02Z",
          "updatedAt": "2022-05-12T15:05:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs454lPj",
          "commit": {
            "abbreviatedOid": "e56eeb3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-12T16:16:55Z",
          "updatedAt": "2022-05-12T16:16:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 247,
      "id": "PR_kwDOFEJYQs43ug64",
      "title": "HPKE configuration compliance suite",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/247",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #216.",
      "createdAt": "2022-05-12T14:49:44Z",
      "updatedAt": "2023-10-26T15:45:08Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3db13fa58ded8b263b6d04b83f7a7d07e92cfb04",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/hpke-complaince-and-refs",
      "headRefOid": "f013efad8e8712146e36b9360f4fe13e8a34a949",
      "closedAt": "2022-06-27T17:15:19Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Premature in what way?",
          "createdAt": "2022-05-12T15:16:06Z",
          "updatedAt": "2022-05-12T15:16:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Premature in what way?\r\n\r\nThis PR guides folks towards using a particular ciphersuite, which is simply not useful at this stage. I think people will read this and think \"I can't implement DAP because I need NIST curves (and only NIST curves)\".",
          "createdAt": "2022-05-12T15:17:35Z",
          "updatedAt": "2022-05-12T15:23:53Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "I think we need an MTI eventually and I would be pretty surprised if it didn't end up being the one indicated here. However, I could also defer writing it down if people feel strongly. With that said, I think one could address @cjpatton's concern by adding a MAY for a P256-based KEM.",
          "createdAt": "2022-05-12T15:24:34Z",
          "updatedAt": "2022-05-12T15:24:34Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> This PR guides folks towards using a particular ciphersuite, which is simply not useful at this stage.\r\n\r\nWell, this doesn't make much sense to me, since getting folks to use the same algorithm for interop _is_ useful.\r\n\r\n>  I think people will read this and think \"I can't implement DAP because I need NIST curves (and only NIST curves)\".\r\n\r\nDoesn't the text say \"if you want to do something else, you can\"? Explicitly saying one MAY implement another suite seems fine, too. I really don't feel strongly. ",
          "createdAt": "2022-05-12T15:26:55Z",
          "updatedAt": "2022-05-12T15:27:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, an explicit MAY would make me happy. I'll add a suggestion.\r\n\r\n",
          "createdAt": "2022-05-12T15:39:50Z",
          "updatedAt": "2022-05-12T15:39:50Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "As a process point, we clearly do need to get WG consensus before landing a MTI",
          "createdAt": "2022-05-12T16:25:09Z",
          "updatedAt": "2022-05-12T16:25:09Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "There's no conflict! Closing in favor of #280.",
          "createdAt": "2022-06-27T17:15:19Z",
          "updatedAt": "2022-06-27T17:15:19Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs454GNp",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "There's one bit of text I think needs to be loosened before merging. Otherwise, as an implementer I don't object to the choice of MTI suite, but I think specifying an MTI suite at this stage is premature. However, providing a bit of guidance on this interop issue may be useful.",
          "createdAt": "2022-05-12T14:56:36Z",
          "updatedAt": "2022-05-12T15:01:30Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "> Each task ID corresponds to exactly one HPKE configuration ...\r\n\r\nThis seems more restrictive than is useful/necessary. As a DAP aggregator, I may want to advertise a different ciphersuite depending on what I know about the client from information outside the scope of the DAP protocol. For instance I might use X25519 by default, but if the user agent is \"I-Only-Use-NIST-Stuff\" I would fallback to P-256.",
              "createdAt": "2022-05-12T14:56:36Z",
              "updatedAt": "2022-05-12T15:01:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454K8z",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:07:11Z",
          "updatedAt": "2022-05-12T15:07:12Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Well, I don't know why one would do that, but it's a fair point. Let's just drop this bit.",
              "createdAt": "2022-05-12T15:07:11Z",
              "updatedAt": "2022-05-12T15:07:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454LDY",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:07:26Z",
          "updatedAt": "2022-05-12T15:07:27Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "```suggestion\r\nconfiguration of each aggregator. See {{compliance}} for information on HPKE configuration algorithms.\r\n```",
              "createdAt": "2022-05-12T15:07:26Z",
              "updatedAt": "2022-05-12T15:07:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454Nxt",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "No objection, but I still think this is premature. We should make sure to get more eyes on this before merging.",
          "createdAt": "2022-05-12T15:14:31Z",
          "updatedAt": "2022-05-12T15:15:03Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "This make it sound like we only intend to support one suite. It may be worth adding a note here that we will consider adding new MTI suites as needed. ",
              "createdAt": "2022-05-12T15:14:31Z",
              "updatedAt": "2022-05-12T15:15:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454Sqe",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:26:07Z",
          "updatedAt": "2022-05-12T15:26:08Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "I don't think removing the text helps here, as it's implicit in the protocol. Isn't the fix here for the server to provide a list of HpkeConfigs?\r\n",
              "createdAt": "2022-05-12T15:26:07Z",
              "updatedAt": "2022-05-12T15:26:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454WO9",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:35:55Z",
          "updatedAt": "2022-05-12T15:35:55Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "They could do that, or just change what config they send out on a per-request basis. In any case, this is a separate issue, so let's address it in a separate PR.",
              "createdAt": "2022-05-12T15:35:55Z",
              "updatedAt": "2022-05-12T15:35:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454XKe",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:38:31Z",
          "updatedAt": "2022-05-12T15:38:31Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Won't that require the client to indicate its capabilities then?\r\n\r\nAnyway, I agree it's a separate PR. https://github.com/ietf-wg-ppm/ppm-specification/issues/248",
              "createdAt": "2022-05-12T15:38:31Z",
              "updatedAt": "2022-05-12T15:38:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454XlN",
          "commit": {
            "abbreviatedOid": "6594073"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:39:46Z",
          "updatedAt": "2022-05-12T15:39:46Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Yeah, that's true, it would need to do that.",
              "createdAt": "2022-05-12T15:39:46Z",
              "updatedAt": "2022-05-12T15:39:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454YnL",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:42:42Z",
          "updatedAt": "2022-05-12T15:42:50Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "```suggestion\r\nconfiguration of each aggregator.\r\n\r\n{{compliance}} specifies HPKE ciphersuites that DAP implementations MUST provide.\r\nIn addition, implementations MAY implement any ciphersuite compatible with {{!RFC9180}}.\r\n```",
              "createdAt": "2022-05-12T15:42:42Z",
              "updatedAt": "2022-05-12T15:42:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454Y9C",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:43:41Z",
          "updatedAt": "2022-05-12T15:43:41Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "Hmm, this doesn't seem to add anything new beyond \"In the absence of an application profile standard specifying otherwise,...\" What am I missing?",
              "createdAt": "2022-05-12T15:43:41Z",
              "updatedAt": "2022-05-12T15:43:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454cFO",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:51:41Z",
          "updatedAt": "2022-05-12T15:51:42Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "My worry is that the current text says that the only option is X25519. Here we just point to {{compliance}}, which says:\r\n\r\n> In the absence of an application profile standard specifying otherwise,\r\n> a compliant DAP application MUST implement the following ciphersuite:\r\n\r\nSo if I'm interested in implementing DAP, my choice is to either use X25519 or get the WG to adopt an alternative \"application profile standard\".",
              "createdAt": "2022-05-12T15:51:41Z",
              "updatedAt": "2022-05-12T15:51:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454cVO",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:52:25Z",
          "updatedAt": "2022-05-12T15:52:26Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "This might make sense in the end game, but at this early stage, I think this adds an unnecessary burden to implementors.",
              "createdAt": "2022-05-12T15:52:25Z",
              "updatedAt": "2022-05-12T15:52:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454dOb",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:54:52Z",
          "updatedAt": "2022-05-12T15:54:52Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "Ah, I understand the disconnect now. An application profile is not something a WG has to adopt or specify! On the contrary, it's just something a specific deployment of PPM can pick and then use as needed. ",
              "createdAt": "2022-05-12T15:54:52Z",
              "updatedAt": "2022-05-12T15:55:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454d_1",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:56:59Z",
          "updatedAt": "2022-05-12T15:56:59Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "Ohh I see. Perhaps adding some non-jargony language would help, in the {{compliance}} section.",
              "createdAt": "2022-05-12T15:56:59Z",
              "updatedAt": "2022-05-12T15:57:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454ead",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:58:10Z",
          "updatedAt": "2022-05-12T15:58:10Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "Maybe we could say \"application or deployment specific profile\"? I agree that \"application profile\" isn't the most clear term.",
              "createdAt": "2022-05-12T15:58:10Z",
              "updatedAt": "2022-05-12T15:58:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454eq_",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T15:58:54Z",
          "updatedAt": "2022-05-12T15:58:55Z",
          "comments": [
            {
              "originalPosition": 82,
              "body": "```suggestion\r\nIn the absence of an application or deployment-specific profile specifying otherwise,\r\n```",
              "createdAt": "2022-05-12T15:58:54Z",
              "updatedAt": "2022-05-12T15:58:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454fj5",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T16:01:31Z",
          "updatedAt": "2022-05-12T16:01:32Z",
          "comments": [
            {
              "originalPosition": 82,
              "body": "@cjpatton thoughts on this alternative phrasing?",
              "createdAt": "2022-05-12T16:01:31Z",
              "updatedAt": "2022-05-12T16:01:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454gEK",
          "commit": {
            "abbreviatedOid": "31814b3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-12T16:03:04Z",
          "updatedAt": "2022-05-12T16:03:05Z",
          "comments": [
            {
              "originalPosition": 82,
              "body": "Much better.",
              "createdAt": "2022-05-12T16:03:05Z",
              "updatedAt": "2022-05-12T16:03:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs454ocN",
          "commit": {
            "abbreviatedOid": "4c58007"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-12T16:26:49Z",
          "updatedAt": "2022-05-12T16:26:56Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "\"HPKE configuration algorithms\" is an awkward phrase. I think \"algorithm\" is meant to refer to the KDF, KEM, AEAD used in HPKE, but it suggests that DAP specifies an algorithm for configuring HPKE, which isn't what the \"compliance\" section describes.\r\n```suggestion\r\nconfiguration of each aggregator. See {{compliance}} for information on HPKE\r\nalgorithm choices.\r\n```",
              "createdAt": "2022-05-12T16:26:49Z",
              "updatedAt": "2022-05-12T16:26:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs48o8jT",
          "commit": {
            "abbreviatedOid": "f013efa"
          },
          "author": "tfpauly",
          "authorAssociation": "NONE",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-06-23T17:04:16Z",
          "updatedAt": "2022-06-23T17:04:16Z",
          "comments": []
        }
      ]
    },
    {
      "number": 249,
      "id": "PR_kwDOFEJYQs43v0B2",
      "title": "Revise usage of HPKE encryption.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/249",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Specifically:\r\n  * Specify use of the \"one-shot\" HPKE APIs defined in RFC9180,\r\n    replacing equivalent use of the \"multi-shot\" APIs. (This change is\r\n    intended to be a clarification, not a functional change.)\r\n  * Move the task_id parameter from the \"info\" parameter to the \"aad\"\r\n    parameter. This is intended to protect against\r\n    key-commitment-related attacks[1]. (This is a functional change.)\r\n  * Clarify that the HpkeCiphertext.enc field is an encapsulated key,\r\n    rather than an encryption context. (This is a clarification, not a\r\n    functional change.)\r\n\r\n[1] See discussion in https://github.com/ietf-wg-ppm/ppm-specification/issues/221.\r\n\r\nCloses #221.",
      "createdAt": "2022-05-12T21:21:35Z",
      "updatedAt": "2022-09-16T00:29:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "994ba9b5e45673b455d3a216944098f8090c311d",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "update-hpke-usage",
      "headRefOid": "1a452ba40db4acfee964af2b58c48d848935fa1b",
      "closedAt": "2022-05-13T16:23:02Z",
      "mergedAt": "2022-05-13T16:23:02Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "149c8c08a7ae7ad9f75ccbd14c815d38fd587bbe"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs455-tN",
          "commit": {
            "abbreviatedOid": "1a452ba"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-12T21:59:05Z",
          "updatedAt": "2022-05-12T21:59:42Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "Should we take the opportunity to bump this to `dap-02 input share` since we plan to submit `draft-ietf-ppm-dap-02` soon? Same goes for `dap-02 aggregate share` elsewhere in the doc.",
              "createdAt": "2022-05-12T21:59:05Z",
              "updatedAt": "2022-05-12T21:59:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs456e-K",
          "commit": {
            "abbreviatedOid": "1a452ba"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Perfect, great work!",
          "createdAt": "2022-05-13T02:16:37Z",
          "updatedAt": "2022-05-13T02:19:07Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "We're at -00 right now: https://datatracker.ietf.org/doc/draft-ietf-ppm-dap/",
              "createdAt": "2022-05-13T02:16:37Z",
              "updatedAt": "2022-05-13T02:19:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4590rl",
          "commit": {
            "abbreviatedOid": "1a452ba"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-13T16:21:28Z",
          "updatedAt": "2022-05-13T16:21:28Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "Leaving as `dap-01` for now since we are currently at `dap-00` and per Tim's comment we will soon be bumping versions.",
              "createdAt": "2022-05-13T16:21:28Z",
              "updatedAt": "2022-05-13T16:21:28Z"
            }
          ]
        }
      ]
    },
    {
      "number": 250,
      "id": "PR_kwDOFEJYQs43y-i3",
      "title": "Lock spec to draft-irtf-cfrg-vdaf-00",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/250",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The current draft is compatible with vdaf-00, and there will likely be\r\nAPI-breaking changes in vdaf-01. For planned changes see\r\nhttps://mailarchive.ietf.org/arch/msg/cfrg/Fvd_m64V9bC4VVJm2Zsuwr441mM/.",
      "createdAt": "2022-05-13T16:26:57Z",
      "updatedAt": "2022-09-16T00:29:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "149c8c08a7ae7ad9f75ccbd14c815d38fd587bbe",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/vdaf-draft",
      "headRefOid": "944454bc26cf6e29d821aeced8014181726b26c0",
      "closedAt": "2022-05-24T22:41:20Z",
      "mergedAt": "2022-05-24T22:41:20Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "68c2a0c98fcf7880fc1f3924d043f2ea77cc6e16"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@divergentdave, done (nice catch).",
          "createdAt": "2022-05-13T16:35:05Z",
          "updatedAt": "2022-05-13T16:35:05Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> No objections from me, but we could also just wait until VDAF-01 is out and then pin \ud83e\udd37\r\n\r\nThe big task remaining for VDAF-01 is finishing Poplar1, which likely won't happen for several weeks. I suppose we could cut VDAF-01 w/o Poplar1 and save it for VDAF-02? ",
          "createdAt": "2022-05-18T18:33:03Z",
          "updatedAt": "2022-05-18T18:33:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm going to merge this now, it'll be easy enough to point to the next draft once we're ready.",
          "createdAt": "2022-05-24T22:41:11Z",
          "updatedAt": "2022-05-24T22:41:11Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4593MR",
          "commit": {
            "abbreviatedOid": "c6e44bb"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks good, but there are two more references that should be updated under #task-configuration and #sec-considerations.",
          "createdAt": "2022-05-13T16:31:11Z",
          "updatedAt": "2022-05-13T16:31:11Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45-Kc9",
          "commit": {
            "abbreviatedOid": "944454b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-13T17:49:02Z",
          "updatedAt": "2022-05-13T17:49:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs46QmTg",
          "commit": {
            "abbreviatedOid": "944454b"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "No objections from me, but we could also just wait until VDAF-01 is out and then pin \ud83e\udd37 ",
          "createdAt": "2022-05-18T18:28:53Z",
          "updatedAt": "2022-05-18T18:28:53Z",
          "comments": []
        }
      ]
    },
    {
      "number": 251,
      "id": "PR_kwDOFEJYQs43zDTN",
      "title": "Revise usage of HPKE encryption (take two).",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/251",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Specifically:\r\n  * Specify use of the \"one-shot\" HPKE APIs defined in RFC9180,\r\n    replacing equivalent use of the \"multi-shot\" APIs. (This change is\r\n    intended to be a clarification, not a functional change.)\r\n  * Move the task_id parameter from the \"info\" parameter to the \"aad\"\r\n    parameter. This is intended to protect against\r\n    key-commitment-related attacks[1].) This is a functional change.\r\n  * Clarify that the HpkeCiphertext.enc field is an encapsulated key,\r\n    rather than an encryption context. (This is a clarification, not a\r\n    functional change.)\r\n\r\nThis is a second copy of a previous PR (#249) which was merged too\r\nhastily, and then reverted. There are no changes from that PR.\r\n\r\n[1] See discussion in #221.\r\n\r\nCloses #221.",
      "createdAt": "2022-05-13T16:53:01Z",
      "updatedAt": "2023-10-26T15:45:03Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0f507171aaad51c9eefccd1075df9874a1a8f7a8",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "update-hpke-usage-redux",
      "headRefOid": "fb89e349ba00c180d3de1f6fb59c3b38bc89bd14",
      "closedAt": "2022-05-21T10:57:14Z",
      "mergedAt": "2022-05-21T10:57:14Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "06ac0f17aafb1dc3f9f007dd30f042d64283abd8"
      },
      "comments": [
        {
          "author": "kraouf3",
          "authorAssociation": "NONE",
          "body": ". I will\r\nAppreciate any reviews or feedback from the group.\r\n\r\nTo summarize changes, one of the changes is an active change:\r\n* The value of DAP work ID that was already included in \"Data\r\nThere was a field of different HPKE encryption and deceptive operation\r\nInstead went to the \"Aad\" field. This change will make protocol\r\nVery interested for attackers trying to benefit from lack\r\nKey commitment in the main schemes of AEAD. See",
          "createdAt": "2023-05-01T15:43:53Z",
          "updatedAt": "2023-05-01T15:43:53Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs45-Koy",
          "commit": {
            "abbreviatedOid": "fb89e34"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-13T17:49:30Z",
          "updatedAt": "2022-05-13T17:49:30Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45-PW-",
          "commit": {
            "abbreviatedOid": "fb89e34"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-13T18:09:49Z",
          "updatedAt": "2022-05-13T18:09:49Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs45_vYn",
          "commit": {
            "abbreviatedOid": "fb89e34"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-14T12:10:10Z",
          "updatedAt": "2022-05-14T12:10:10Z",
          "comments": []
        }
      ]
    },
    {
      "number": 252,
      "id": "PR_kwDOFEJYQs43zQnV",
      "title": "Update repository reference",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/252",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-05-13T18:09:20Z",
      "updatedAt": "2022-09-16T00:29:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c9a07318214c3503aab30d1d1ea5afa5655f47c5",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/update-repo",
      "headRefOid": "f63a981d835ce4cb51d4dedb15effc2321435045",
      "closedAt": "2022-05-18T18:04:35Z",
      "mergedAt": "2022-05-18T18:04:35Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "a9d046da0b4a7b8b8b80658e68572dc8ecdf74c5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46EUXR",
          "commit": {
            "abbreviatedOid": "435b9ee"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-05-16T16:00:52Z",
          "updatedAt": "2022-05-16T16:00:52Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "```suggestion\r\nsecurity analysis.\r\n```",
              "createdAt": "2022-05-16T16:00:52Z",
              "updatedAt": "2022-05-16T16:00:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs46PjnC",
          "commit": {
            "abbreviatedOid": "f63a981"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-18T15:15:12Z",
          "updatedAt": "2022-05-18T15:15:12Z",
          "comments": []
        }
      ]
    },
    {
      "number": 253,
      "id": "PR_kwDOFEJYQs435BfY",
      "title": "Rename repo references",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/253",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-05-16T16:09:23Z",
      "updatedAt": "2023-10-26T15:45:04Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0f507171aaad51c9eefccd1075df9874a1a8f7a8",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/rename-repo",
      "headRefOid": "53c11f10fcfda4b517c9b239e696b05e10c5157d",
      "closedAt": "2022-05-16T16:10:12Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [],
      "reviews": []
    },
    {
      "number": 254,
      "id": "PR_kwDOFEJYQs435C_v",
      "title": "Re-run repo setup using new name",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/254",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-05-16T16:15:27Z",
      "updatedAt": "2023-10-26T15:45:05Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0f507171aaad51c9eefccd1075df9874a1a8f7a8",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/rename-repo-2",
      "headRefOid": "23858f01136dd332847b51e0a48908e3d4cf62e4",
      "closedAt": "2022-05-16T16:20:20Z",
      "mergedAt": "2022-05-16T16:20:20Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "85c42e767b3474c2aabfba01a03a9ac560b2bb55"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46EanN",
          "commit": {
            "abbreviatedOid": "23858f0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-16T16:19:58Z",
          "updatedAt": "2022-05-16T16:19:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 256,
      "id": "PR_kwDOFEJYQs44B41Y",
      "title": "Fix AggregateShareReq",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/256",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "AggregateShareReq should have an aggregation parameter field instead of\r\naggregation job ID.",
      "createdAt": "2022-05-18T14:02:45Z",
      "updatedAt": "2023-10-26T15:45:06Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "85c42e767b3474c2aabfba01a03a9ac560b2bb55",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/agg-share-req-agg-param",
      "headRefOid": "2d6a14d259c2e86f4de4155e56977f1ba8e77e8f",
      "closedAt": "2022-05-18T14:27:59Z",
      "mergedAt": "2022-05-18T14:27:59Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "01571a49c2ce5db69784724c5a0deb3f6d79e82d"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @simon-friedberger for spotting this!",
          "createdAt": "2022-05-18T14:04:40Z",
          "updatedAt": "2022-05-18T14:04:40Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46PDqg",
          "commit": {
            "abbreviatedOid": "2d6a14d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-18T14:03:37Z",
          "updatedAt": "2022-05-18T14:03:37Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs46PFzl",
          "commit": {
            "abbreviatedOid": "2d6a14d"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-18T14:07:14Z",
          "updatedAt": "2022-05-18T14:07:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 257,
      "id": "PR_kwDOFEJYQs44CD05",
      "title": "Remove references to helper state",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/257",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This is a follow-up to #232, removing a few dangling references to the helper state blob.",
      "createdAt": "2022-05-18T14:38:13Z",
      "updatedAt": "2022-05-18T15:09:23Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "01571a49c2ce5db69784724c5a0deb3f6d79e82d",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "helper-state-cleanup",
      "headRefOid": "17d5b4d09d4eba0f12359e26594ece92658d6644",
      "closedAt": "2022-05-18T15:02:31Z",
      "mergedAt": "2022-05-18T15:02:31Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "c9a07318214c3503aab30d1d1ea5afa5655f47c5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46Pdff",
          "commit": {
            "abbreviatedOid": "17d5b4d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-18T15:01:04Z",
          "updatedAt": "2022-05-18T15:01:04Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs46Pd5d",
          "commit": {
            "abbreviatedOid": "17d5b4d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-18T15:02:03Z",
          "updatedAt": "2022-05-18T15:02:03Z",
          "comments": []
        }
      ]
    },
    {
      "number": 258,
      "id": "PR_kwDOFEJYQs44DEmK",
      "title": "Fix section reference in Anti-replay section",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/258",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This fixes a reference that was pointing at the wrong section. The behavior relating to `ReportShareError.report-replayed` is under \"Input Share Validation\".",
      "createdAt": "2022-05-18T19:06:14Z",
      "updatedAt": "2022-05-19T01:27:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a9d046da0b4a7b8b8b80658e68572dc8ecdf74c5",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "patch-1",
      "headRefOid": "7cb1f0ca652374cc729da5f6f082179ad58d9fa1",
      "closedAt": "2022-05-19T01:27:11Z",
      "mergedAt": "2022-05-19T01:27:11Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "14ff0679a9781cd13a95486b5b199e9c8c508087"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46SFER",
          "commit": {
            "abbreviatedOid": "7cb1f0c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-19T01:27:02Z",
          "updatedAt": "2022-05-19T01:27:02Z",
          "comments": []
        }
      ]
    },
    {
      "number": 262,
      "id": "PR_kwDOFEJYQs44JhD8",
      "title": "Drop job_id from Aggregate{Initialize,Continue}Resp.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/262",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #261. (see that issue for justification of this change)",
      "createdAt": "2022-05-19T21:07:23Z",
      "updatedAt": "2023-11-27T23:10:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "14ff0679a9781cd13a95486b5b199e9c8c508087",
      "headRepository": null,
      "headRefName": "drop-job-id-in-agg-resps",
      "headRefOid": "0e2da079a0289218dd33cec4c5fbbfd12350c86b",
      "closedAt": "2022-06-23T16:26:05Z",
      "mergedAt": "2022-06-23T16:26:05Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "2cf835cef0e7ecad0080f17ccc7cfa5262be0327"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46oc_p",
          "commit": {
            "abbreviatedOid": "0e2da07"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-24T18:29:26Z",
          "updatedAt": "2022-05-24T18:29:26Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs48owpV",
          "commit": {
            "abbreviatedOid": "0e2da07"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Agreed that these are redundant with the current design, so I'm OK merging this as-is. Let's split out how we convey task and job parameters (via URL or application message or whatever) to a separate issue.",
          "createdAt": "2022-06-23T16:25:55Z",
          "updatedAt": "2022-06-23T16:25:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 263,
      "id": "PR_kwDOFEJYQs44JjgE",
      "title": "Include task_id in AggregateContinueReq.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/263",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "See https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/260 for the\r\nreasoning behind this change. In short, this keeps aggregation job IDs\r\nscoped to be unique-per-task rather than universally-unique.\r\n\r\nNote this isn't the only possible solution -- I'm opinionatedly sending this PR, if the discussion on #260 leads to a different conclusion, I'll update the PR accordingly.\r\n\r\nCloses #260.",
      "createdAt": "2022-05-19T21:16:43Z",
      "updatedAt": "2023-11-27T23:10:43Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "14ff0679a9781cd13a95486b5b199e9c8c508087",
      "headRepository": null,
      "headRefName": "agg-continue-include-task",
      "headRefOid": "6b3f8a0588bf67a2bfe428e6afc5285bc7d3120e",
      "closedAt": "2022-06-23T14:31:16Z",
      "mergedAt": "2022-06-23T14:31:16Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "eb1468488acdf896c1537ed43a17b400f0f97066"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46oc2m",
          "commit": {
            "abbreviatedOid": "6b3f8a0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-24T18:28:53Z",
          "updatedAt": "2022-05-24T18:28:53Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs48oAFS",
          "commit": {
            "abbreviatedOid": "6b3f8a0"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Indeed -- this was a bug. The task_id should have been included, just as it is in the initialization request. Good catch!",
          "createdAt": "2022-06-23T14:31:10Z",
          "updatedAt": "2022-06-23T14:31:10Z",
          "comments": []
        }
      ]
    },
    {
      "number": 265,
      "id": "PR_kwDOFEJYQs44Umtp",
      "title": "Switch all messages from the \"message\" to the \"application\" media type.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/265",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #264. (opinionatedly: please see discussion on that issue)",
      "createdAt": "2022-05-23T21:27:33Z",
      "updatedAt": "2023-11-27T23:10:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "06ac0f17aafb1dc3f9f007dd30f042d64283abd8",
      "headRepository": null,
      "headRefName": "media-type-application",
      "headRefOid": "6e971da0655d8333d3cbd978dcace5ba8997587c",
      "closedAt": "2022-06-23T14:21:05Z",
      "mergedAt": "2022-06-23T14:21:04Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "3db13fa58ded8b263b6d04b83f7a7d07e92cfb04"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs46odpD",
          "commit": {
            "abbreviatedOid": "6e971da"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Approving based on the text quoted by @BranLwyd in the linked issue. It would be great to get feedback from folks who have more experience working on HTTP-based applications.",
          "createdAt": "2022-05-24T18:31:45Z",
          "updatedAt": "2022-05-24T18:31:45Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs46omv3",
          "commit": {
            "abbreviatedOid": "6e971da"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-05-24T19:04:27Z",
          "updatedAt": "2022-05-24T19:04:27Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs48n7sG",
          "commit": {
            "abbreviatedOid": "6e971da"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "As a non-HTTP expert, I agree with the rationale. Thanks, @BranLwyd!",
          "createdAt": "2022-06-23T14:21:00Z",
          "updatedAt": "2022-06-23T14:21:00Z",
          "comments": []
        }
      ]
    },
    {
      "number": 268,
      "id": "PR_kwDOFEJYQs44zi2o",
      "title": "Venue stuff",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/268",
      "state": "MERGED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Remove some cruft",
      "createdAt": "2022-06-01T06:10:18Z",
      "updatedAt": "2022-06-01T20:10:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "68c2a0c98fcf7880fc1f3924d043f2ea77cc6e16",
      "headRepository": "martinthomson/ppm-dap",
      "headRefName": "venue-stuff",
      "headRefOid": "6a88651fc3d3ece58fede307e722877678da8420",
      "closedAt": "2022-06-01T20:10:19Z",
      "mergedAt": "2022-06-01T20:10:19Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "1225fa8f19288f4de093f341397d7c8028203b20"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs47ITea",
          "commit": {
            "abbreviatedOid": "6a88651"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\u2764\ufe0f \r\n",
          "createdAt": "2022-06-01T14:04:01Z",
          "updatedAt": "2022-06-01T14:04:01Z",
          "comments": []
        }
      ]
    },
    {
      "number": 269,
      "id": "PR_kwDOFEJYQs45XXUJ",
      "title": "Pretty pictures",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/269",
      "state": "OPEN",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Anyone running builds locally will need a few extra pieces of software to manage this, but the result is something like this:\r\n\r\n![image](https://user-images.githubusercontent.com/67641/172771593-c22d2278-d803-4391-9c95-5e6e60b9af29.png)\r\n\r\n...with the appropriate palette inversion for people using dark mode.",
      "createdAt": "2022-06-09T05:33:19Z",
      "updatedAt": "2023-06-21T00:29:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1225fa8f19288f4de093f341397d7c8028203b20",
      "headRepository": "martinthomson/ppm-dap",
      "headRefName": "aasvg",
      "headRefOid": "98b56b1e69cf0aa30874c74799bf74537013f331",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Oh, the software in question is 'svgcheck' (installed with pip3/python) and 'aasvg' (installed with npm/nodejs).",
          "createdAt": "2022-06-09T05:34:00Z",
          "updatedAt": "2022-06-09T05:34:00Z"
        },
        {
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "body": "Before we land this can we adjust the Makefiles to alert the user about this--and even better, fail gracefully. I found it kind of surprising when my drafts just stopped building.",
          "createdAt": "2022-06-09T14:33:03Z",
          "updatedAt": "2022-06-09T14:33:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm curious if there are any other drafts adopting this kind of thing? Happy to help blaze a trail if not!",
          "createdAt": "2022-09-15T19:26:51Z",
          "updatedAt": "2022-09-15T19:26:51Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "OHTTP uses this https://www.ietf.org/archive/id/draft-ietf-ohai-ohttp-05.html#section-2\r\n",
          "createdAt": "2022-10-11T00:30:16Z",
          "updatedAt": "2022-10-11T00:30:16Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "There is a small change I can make to have this work more cleanly.  Then you only need npm installed, not the specific package.\r\n\r\nAnd there is also [RFC 9113](https://httpwg.org/specs/rfc9113.html#StreamStates).",
          "createdAt": "2022-10-11T00:45:10Z",
          "updatedAt": "2022-10-11T00:47:29Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'd like to revive this PR, I'm willing to figure out the tooling.",
          "createdAt": "2023-06-20T16:47:53Z",
          "updatedAt": "2023-06-20T16:47:53Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'll let you rebase this PR then.  That's all that needs to happen, unless you want to tag a few other pictures and tweak them so that they look OK in SVG.",
          "createdAt": "2023-06-21T00:29:42Z",
          "updatedAt": "2023-06-21T00:29:42Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 275,
      "id": "PR_kwDOFEJYQs451uST",
      "title": "Clarify meaning of out-of-band.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/275",
      "state": "MERGED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Proposal for https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/271",
      "createdAt": "2022-06-17T08:53:37Z",
      "updatedAt": "2022-06-23T14:19:36Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1225fa8f19288f4de093f341397d7c8028203b20",
      "headRepository": "simon-friedberger/draft-ietf-ppm-dap",
      "headRefName": "oob",
      "headRefOid": "265534438fba1b0bad8837d750a6ab665486f22b",
      "closedAt": "2022-06-23T14:19:36Z",
      "mergedAt": "2022-06-23T14:19:36Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "2f98874be7a08b2cf1c048825839ea38032f22f5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs48n699",
          "commit": {
            "abbreviatedOid": "3231dd8"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-06-23T14:19:12Z",
          "updatedAt": "2022-06-23T14:19:17Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "```suggestion\r\nform. Each task is identified by a unique 32-byte ID which is used to\r\n```",
              "createdAt": "2022-06-23T14:19:12Z",
              "updatedAt": "2022-06-23T14:19:17Z"
            }
          ]
        }
      ]
    },
    {
      "number": 277,
      "id": "PR_kwDOFEJYQs46PxqB",
      "title": "Constrain collect requests to non-overlapping batch intervals",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/277",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Issue #195 discusses a problem with the current collect request validation requirements. We don't yet have a solution that permits safe query flexibility. This change fixes the privacy problem at the cost of flexibility, leaving an OPEN ISSUE for us to address this regression in a followup change. It does _not_ close the corresponding issue.",
      "createdAt": "2022-06-23T14:08:30Z",
      "updatedAt": "2023-10-26T15:45:07Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1225fa8f19288f4de093f341397d7c8028203b20",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/constrain-collect-requests",
      "headRefOid": "1709322af8834bbd0d92c7bfbda94f425684238a",
      "closedAt": "2022-07-11T20:11:29Z",
      "mergedAt": "2022-07-11T20:11:29Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "5c789f1b62ad8f8c1e0d766f9e71390f479405d9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs49raM1",
          "commit": {
            "abbreviatedOid": "1709322"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-11T18:47:58Z",
          "updatedAt": "2022-07-11T18:47:58Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49rj-b",
          "commit": {
            "abbreviatedOid": "1709322"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-11T19:25:53Z",
          "updatedAt": "2022-07-11T19:25:53Z",
          "comments": []
        }
      ]
    },
    {
      "number": 279,
      "id": "PR_kwDOFEJYQs46UXQa",
      "title": "Increase payload size in HpkeCiphertext to 2^32-1",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/279",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "See discussion here: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/272",
      "createdAt": "2022-06-24T14:49:36Z",
      "updatedAt": "2022-08-29T17:54:54Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5c789f1b62ad8f8c1e0d766f9e71390f479405d9",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "increase-ciphertext-size-limit",
      "headRefOid": "bfae37d1aaca6fea19abc1559bc55a3bed4c833e",
      "closedAt": "2022-08-02T12:42:03Z",
      "mergedAt": "2022-08-02T12:42:03Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "9639d4f7c6baa9efc9712d326b2a635edecadd4d"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Also, note that this will require bumping the length prefix in other fields as well.",
          "createdAt": "2022-06-27T16:17:23Z",
          "updatedAt": "2022-06-27T16:17:23Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton any pointer to where I should look at in particular? From what I can see only the definition of HpkeCiphertext specifies length of encrypted payload, the other fields like `Report.encrypted_input_shares` meant the number of HpkeCiphertext in an array, unless I misunderstood something.",
          "createdAt": "2022-07-04T15:18:34Z",
          "updatedAt": "2022-07-04T15:18:34Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "Per the [TLS presentation language](https://www.rfc-editor.org/rfc/rfc8446#section-3.4), and its definition of variable-length vectors, the numbers are in angle brackets, such as in `HpkeCiphertext encrypted_input_shares<1..2^16-1>` are numbers of bytes, not numbers of vector elements. Thus, we would need to recursively increase the maximum length of each vector that contains any `HpkeCiphertext` to realize benefits.",
          "createdAt": "2022-07-06T14:10:42Z",
          "updatedAt": "2022-07-06T14:10:42Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton I've updated the arrays that contain ciphertext to 2^32-1, if I understand correctly, this allows those arrays to contain up to 256 ciphertext with 2^16-1 payload size. However, the number of ReportShare in AggInit request would be smaller than 256 due to ReportShare itself contains more than just ciphertext.",
          "createdAt": "2022-07-11T14:37:20Z",
          "updatedAt": "2022-07-11T14:37:20Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> I've updated the arrays that contain ciphertext to 2^32-1, if I understand correctly, this allows those arrays to contain up to 256 ciphertext with 2^16-1 payload size.\r\n\r\nThe <1..2^32-1> syntax means that the size of the contents in the vector can be up to 2^32-1 bytes in length. It places no constraints on how those bytes are interpreted.",
          "createdAt": "2022-07-11T15:22:13Z",
          "updatedAt": "2022-07-11T15:22:13Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Oh, wait, don't we also need to increase the length for prep shares (for the VDAF) in `AggregateContinueReq` and the response?\r\n\r\nDo we need to? the `AggregateContinueReq` stores PrepareStep which has no direct relationship with Ciphertext. Are we saying because `AggregateInitializeReq` can have 2^32-1, to keep the index aligned `AggregateContinueReq` must also be 2^32-1?\r\n\r\n",
          "createdAt": "2022-07-12T14:51:05Z",
          "updatedAt": "2022-07-12T14:51:05Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@chris-wood @cjpatton I changed all types that contain PrepStep and ReportShare to 2^32-1, also taking @chris-wood 's advice to keep 2^32 everywhere by bumping ciphertext to 2^32 as well.",
          "createdAt": "2022-07-12T15:03:49Z",
          "updatedAt": "2022-07-12T15:03:49Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@chris-wood @cjpatton Something I'm slightly confused about TLS: the current commit has both payload and encrypted_input_shares at 2^32-1, this is not consistent because when payload is at 2^32-1, then the payload cannot possibly fit in to encrypted_input_shares, which also has a lower bound of 1. I'm not sure if I'm interpreting the TLS representation correctly.",
          "createdAt": "2022-07-15T18:27:24Z",
          "updatedAt": "2022-07-15T18:27:24Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Sorry for not replying, @wangshan. In general, we're pretty loose with these bounds. They're not exact. For example, the extensions slot holds up to 2^16 bytes, where each extension itself carries up to 2^16 bytes. Implementations typically just make sure things fit within the bounds of each field when creating and encoding messages. \r\n\r\nAll that said, this change looks good. Thanks!",
          "createdAt": "2022-08-02T12:41:48Z",
          "updatedAt": "2022-08-02T12:41:48Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs480se6",
          "commit": {
            "abbreviatedOid": "826fe94"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-27T16:16:26Z",
          "updatedAt": "2022-06-27T16:16:34Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I think a 4-byte length prefix is overkill, given that we're never going to have a 4GB `payload` here. Can we change this to a 3-byte prefix so that we don't have a dead byte on the wire?\r\n```suggestion\r\n  opaque payload<1..2^24-1>; // ciphertext\r\n```",
              "createdAt": "2022-06-27T16:16:26Z",
              "updatedAt": "2022-06-27T16:16:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49E4H-",
          "commit": {
            "abbreviatedOid": "826fe94"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T11:14:31Z",
          "updatedAt": "2022-06-30T11:14:31Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "3 bytes can only support up to 16MB, it's not impossible to need more than that in some use cases, especially if we go beyond histogram collecting. Having said that, I think that will be rare cases. ",
              "createdAt": "2022-06-30T11:14:31Z",
              "updatedAt": "2022-06-30T11:14:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49E9Zj",
          "commit": {
            "abbreviatedOid": "826fe94"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T11:33:46Z",
          "updatedAt": "2022-06-30T11:33:46Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "We're talking abut a single byte here, so I would just do 4 bytes. That would also keep implementations simpler (`uint32_t` is everywhere, but `uint24_t` is not really a thing).",
              "createdAt": "2022-06-30T11:33:46Z",
              "updatedAt": "2022-06-30T11:33:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49rXn_",
          "commit": {
            "abbreviatedOid": "6573b04"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-11T18:38:46Z",
          "updatedAt": "2022-07-11T18:38:46Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "```suggestion\r\n  opaque payload<1..2^32-1>; // ciphertext\r\n```",
              "createdAt": "2022-07-11T18:38:46Z",
              "updatedAt": "2022-07-11T18:38:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49rX4E",
          "commit": {
            "abbreviatedOid": "61f44e6"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-11T18:39:42Z",
          "updatedAt": "2022-07-11T18:39:42Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49rYS3",
          "commit": {
            "abbreviatedOid": "61f44e6"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Oh, wait, don't we also need to increase the length for prep shares (for the VDAF) in `AggregateContinueReq` and the response?",
          "createdAt": "2022-07-11T18:41:18Z",
          "updatedAt": "2022-07-11T18:41:18Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4_Gl1t",
          "commit": {
            "abbreviatedOid": "bfae37d"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-02T12:40:12Z",
          "updatedAt": "2022-08-02T12:40:12Z",
          "comments": []
        }
      ]
    },
    {
      "number": 280,
      "id": "PR_kwDOFEJYQs46bpkI",
      "title": "HPKE configuration compliance suite",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/280",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #216.\r\n\r\nGitHub seems to think there's a conflict on the other PR, but I don't see it on the branch, so this just cherry-picks all changes from that PR.",
      "createdAt": "2022-06-27T17:14:54Z",
      "updatedAt": "2023-10-26T15:45:09Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2cf835cef0e7ecad0080f17ccc7cfa5262be0327",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/hpke-compliance",
      "headRefOid": "0078ee6ec652ca80d74b6dd07261f3556a8ec37f",
      "closedAt": "2022-06-27T18:14:57Z",
      "mergedAt": "2022-06-27T18:14:57Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "93d3e3ab989129cee9ce1bbec4ec724c7734f0e5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs481Mwi",
          "commit": {
            "abbreviatedOid": "0078ee6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-06-27T17:58:40Z",
          "updatedAt": "2022-06-27T17:58:40Z",
          "comments": []
        }
      ]
    },
    {
      "number": 281,
      "id": "PR_kwDOFEJYQs46bvla",
      "title": "Make report timestamps more granular",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/281",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #274.\r\n\r\nWe might want to consider renaming `Nonce` to something based on what it's actually doing, which is helping with replay protection. Maybe `ReportID`?",
      "createdAt": "2022-06-27T17:38:11Z",
      "updatedAt": "2023-10-26T15:45:10Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "93d3e3ab989129cee9ce1bbec4ec724c7734f0e5",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/granular-nonces",
      "headRefOid": "8a642c83f404f4f5c74614d3fbc511a06e488461",
      "closedAt": "2022-07-07T12:21:41Z",
      "mergedAt": "2022-07-07T12:21:41Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "f016fc583bba8140191a69db1493504c975756d9"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> If we require clients to round Nonce.time down to the nearest min_batch_duration multiple, then that means the vast majority of Nonce values are now invalid and should be rejected by servers (either that or we'd have to specify what servers should do if Nonce.time isn't a multiple). \r\n\r\n~Wait, what? Replay is enforced on the basis of Nonce in its entirety, which currently consists of a (1) timestamp and (2) a random value.~\r\n\r\nChatted to clear this up in Slack \ud83d\udc4d ",
          "createdAt": "2022-06-30T15:50:17Z",
          "updatedAt": "2022-06-30T15:59:26Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs481K6p",
          "commit": {
            "abbreviatedOid": "520a612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-27T17:52:26Z",
          "updatedAt": "2022-06-27T17:52:33Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "What about moving `time` into `Report`? Then you could modify Report by changing the type of `nonce` to `uint8 [16]`.",
              "createdAt": "2022-06-27T17:52:26Z",
              "updatedAt": "2022-06-27T17:52:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs481O_o",
          "commit": {
            "abbreviatedOid": "520a612"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-27T18:06:46Z",
          "updatedAt": "2022-06-27T18:06:47Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "That would be another way to address the name issue, yeah. But we can do that separately.",
              "createdAt": "2022-06-27T18:06:46Z",
              "updatedAt": "2022-06-27T18:06:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4815ge",
          "commit": {
            "abbreviatedOid": "3e9f430"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-27T20:46:16Z",
          "updatedAt": "2022-06-27T20:46:37Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "Is there an RFC or other standard we can reference for \"UNIX time\", something that unambiguously spells out where the epoch starts?\r\n```suggestion\r\nto the number of seconds elapsed since the start of the UNIX epoch, rounded down to the nearest multiple of `min_batch_duration`.\r\n```",
              "createdAt": "2022-06-27T20:46:17Z",
              "updatedAt": "2022-06-27T20:46:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49F-by",
          "commit": {
            "abbreviatedOid": "3e9f430"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T14:27:53Z",
          "updatedAt": "2022-06-30T14:27:54Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "Hmm, I'm not sure, but if there is one we ought to use it.",
              "createdAt": "2022-06-30T14:27:53Z",
              "updatedAt": "2022-06-30T14:27:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49GfMR",
          "commit": {
            "abbreviatedOid": "8a642c8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "If we require clients to round `Nonce.time` down to the nearest `min_batch_duration` multiple, then that means the vast majority of `Nonce` values are now invalid and should be rejected by servers (either that or we'd have to specify what servers should do if `Nonce.time` isn't a multiple) (what I mean here is that for `min_batch_duration = 10`, `Nonce.time = 100` is OK, but `Nonce.time = 103` is illegal and meaningless).\r\n\r\nWhat if we replaced `Nonce.time` with something like `Nonce.batch_unit`, which is an integer representing how many `min_batch_duration`-length intervals have elapsed since the epoch? i.e., if `min_batch_duration = 10` and my report is generated at second 104, then instead of `Nonce.time = 104`, I do `Nonce.batch_unit = 10`?\r\n\r\nBesides making invalid values impossible to represent, this would extend the range of DAP nonces much farther into the future, or we could choose to save some bytes on the wire by making `Nonce.batch_unit` narrower.",
          "createdAt": "2022-06-30T15:48:48Z",
          "updatedAt": "2022-06-30T15:50:19Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49GlDC",
          "commit": {
            "abbreviatedOid": "8a642c8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Chris W. and I hashed this out [on the IETF Slack](https://ietf.slack.com/archives/C02P9620XT6/p1656604238052029). Chris convinced me that as written, the rounding of `Nonce.time` is optional for the client, and aggregators don't need to do anything special to handle unrounded nonces: if that happens aggregators can still use the unrounded nonce to correctly identify a bucket to put the report in, so this is something that a client should do in order to improve privacy, but nothing breaks if they fail to. With that in mind, I don't think we need new text to explain what aggregators should do with nonces that aren't rounded correctly, nor does the new text for the client need a MUST.",
          "createdAt": "2022-06-30T16:03:09Z",
          "updatedAt": "2022-06-30T16:03:09Z",
          "comments": []
        }
      ]
    },
    {
      "number": 282,
      "id": "PR_kwDOFEJYQs46janb",
      "title": "Clarify the handling of `agg_param` to close https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/267",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/282",
      "state": "MERGED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-06-29T09:51:25Z",
      "updatedAt": "2022-07-07T12:22:24Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "93d3e3ab989129cee9ce1bbec4ec724c7734f0e5",
      "headRepository": "simon-friedberger/draft-ietf-ppm-dap",
      "headRefName": "main",
      "headRefOid": "8e57dcc53115333869c3b83f4f09fefa2edca2dc",
      "closedAt": "2022-07-07T12:22:24Z",
      "mergedAt": "2022-07-07T12:22:24Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "8bb28ec01e97a273e6518f41cb34262d2944cf49"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs48-1dz",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Beyond editorial and implementation recommendation issues, I think this needs to be aligned with Poplar before we land it.",
          "createdAt": "2022-06-29T10:25:05Z",
          "updatedAt": "2022-06-29T10:29:16Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "This parameter changes in Poplar, so I don't think this definition is correct.",
              "createdAt": "2022-06-29T10:25:06Z",
              "updatedAt": "2022-06-29T10:29:16Z"
            },
            {
              "originalPosition": 18,
              "body": "I don't think we've introduced these types at this point yet, so this either needs a forward pointer or it needs to be moved down below.",
              "createdAt": "2022-06-29T10:25:40Z",
              "updatedAt": "2022-06-29T10:29:16Z"
            },
            {
              "originalPosition": 19,
              "body": "While tempting, I don't think this is good advice. The leader doesn't necessarily know that helper will know how to interpret `agg_param` in an `AggregateShareReq` as a reference. In any case, it seems like premature optimization to me, so I'd suggest removing it.",
              "createdAt": "2022-06-29T10:28:38Z",
              "updatedAt": "2022-06-29T10:29:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs48-9BT",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-29T10:51:08Z",
          "updatedAt": "2022-06-29T10:51:08Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "That is actually the point I wanted to make here. Previously it looked like the aggregation parameter (the thing that changes in Poplar) was part of the task definition. Since it changes, that's not possible. But of course other things that do not change can be given here.",
              "createdAt": "2022-06-29T10:51:08Z",
              "updatedAt": "2022-06-29T10:51:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs48-9lS",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-29T10:53:04Z",
          "updatedAt": "2022-06-29T10:53:04Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Yeah, the current language seems wrong too. Maybe we could say something like this?\r\n\r\n```\r\n* The aggregation function to compute (e.g., sum, mean, etc.) and a way to\r\n  produce optional parameters for the aggregation function.\r\n```",
              "createdAt": "2022-06-29T10:53:04Z",
              "updatedAt": "2022-06-29T10:53:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs48_OGX",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-29T11:49:41Z",
          "updatedAt": "2022-06-29T11:49:41Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "tbh I am not quite sure why we are sending it twice. Maybe elaborating on that here would be more useful. (Edit: If I understand @cjpatton correctly, we want to do the next round of Poplar with new prefixes without redoing the `AggregateInitializeReq` but I am somewhat confused about the flow here)",
              "createdAt": "2022-06-29T11:49:41Z",
              "updatedAt": "2022-06-29T11:58:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs48_Otx",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-29T11:51:22Z",
          "updatedAt": "2022-06-29T11:51:22Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Maybe we should just decomplicate things and remove it entirely and assume that this kind of thing is part of \"the function\".",
              "createdAt": "2022-06-29T11:51:22Z",
              "updatedAt": "2022-06-29T11:51:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49AVsM",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-29T14:39:36Z",
          "updatedAt": "2022-06-29T14:45:37Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I think removing it entirely is the right way to go.",
              "createdAt": "2022-06-29T14:39:36Z",
              "updatedAt": "2022-06-29T14:45:38Z"
            },
            {
              "originalPosition": 19,
              "body": "The agg param is needed here because this the first time the Helper learns its value. The AggregateShareReq message isn't sent until later, once the batch is ready to be collected.\r\n\r\nIf we wanted to save bandwidth, we could consider replacing AggregateShareReq.agg_param with something like SHA-256(agg_param). However I agree that this is premature and can wait until someone has implemented Poplar1.",
              "createdAt": "2022-06-29T14:45:21Z",
              "updatedAt": "2022-06-29T14:45:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49EEIF",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T08:28:31Z",
          "updatedAt": "2022-06-30T08:28:32Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "Fixed.",
              "createdAt": "2022-06-30T08:28:32Z",
              "updatedAt": "2022-06-30T08:28:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49EENs",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T08:28:45Z",
          "updatedAt": "2022-06-30T08:28:45Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Removed.",
              "createdAt": "2022-06-30T08:28:45Z",
              "updatedAt": "2022-06-30T08:28:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49EES1",
          "commit": {
            "abbreviatedOid": "28fc3ea"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T08:28:58Z",
          "updatedAt": "2022-06-30T08:28:58Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Removed that part.",
              "createdAt": "2022-06-30T08:28:58Z",
              "updatedAt": "2022-06-30T08:28:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49GPh-",
          "commit": {
            "abbreviatedOid": "304f176"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T15:09:06Z",
          "updatedAt": "2022-06-30T15:09:07Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "```suggestion\r\nThe `agg_param` field is an opaque, VDAF-specific aggregation parameter provided during a collection flow.\r\n```",
              "createdAt": "2022-06-30T15:09:06Z",
              "updatedAt": "2022-06-30T15:09:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49GPw_",
          "commit": {
            "abbreviatedOid": "304f176"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-06-30T15:09:42Z",
          "updatedAt": "2022-06-30T15:09:43Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "Can we move this note down to these sections? That is, when describing `CollectReq`, we can place a reference back to this section. Likewise for `AggregateShareReq`.",
              "createdAt": "2022-06-30T15:09:43Z",
              "updatedAt": "2022-06-30T15:09:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49GP4S",
          "commit": {
            "abbreviatedOid": "304f176"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I'd like to see some text moved down, but otherwise this is good pending one concrete suggestion.",
          "createdAt": "2022-06-30T15:10:00Z",
          "updatedAt": "2022-06-30T15:10:00Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49Oqpl",
          "commit": {
            "abbreviatedOid": "304f176"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-04T09:12:07Z",
          "updatedAt": "2022-07-04T09:12:07Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "Done.",
              "createdAt": "2022-07-04T09:12:07Z",
              "updatedAt": "2022-07-04T09:12:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49eyWS",
          "commit": {
            "abbreviatedOid": "8e57dcc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM -- thanks!",
          "createdAt": "2022-07-07T12:22:19Z",
          "updatedAt": "2022-07-07T12:22:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 284,
      "id": "PR_kwDOFEJYQs4639ua",
      "title": "report-replayed: allow varying aggregation parameter.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/284",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Previously, report-replayed would be returned if the same client report\r\nwas aggregated more than once. However, some VDAFs (e.g. poplar1) will\r\nrequire repeatedly aggregating a given client report with varying\r\naggregation parameters. This commit relaxes the condition on which\r\nreport-replayed will be returned to allow aggregating the same client\r\nreport multiple times, as long as each aggregation has a distinct\r\naggregation parameter.\r\n\r\nCloses #283.",
      "createdAt": "2022-07-05T18:57:32Z",
      "updatedAt": "2023-11-27T23:10:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "93d3e3ab989129cee9ce1bbec4ec724c7734f0e5",
      "headRepository": null,
      "headRefName": "report-replayed",
      "headRefOid": "a36dcc8c55aae94ab2ec91af774ae203b543d63e",
      "closedAt": "2022-07-11T18:38:02Z",
      "mergedAt": "2022-07-11T18:38:02Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "bb545267d847dcd00cde7fac5a4d916fa87d25e0"
      },
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "This is odd because the aggregation parameter is opaque and it's not clear if any change to it should allow aggregating a report again.\r\nFor example, if aggregation is allowed only once for privacy reasons but some kind of shared random number is needed as agg_param this will be an issue.\r\nI think checking if another aggregation is allowed has to be done depending on the VDAF and the task.",
          "createdAt": "2022-07-06T09:13:33Z",
          "updatedAt": "2022-07-06T09:13:33Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "There is a separate check, under the section \"Validating Batch Parameters\", that each report is used in at most `max_batch_lifetime` aggregate shares. This should be set to 1 for Prio3 and VDAFs like you describe. If `max_batch_lifetime` for a task is 1, then the present change will make no difference, but if `max_batch_lifetime` for a task is greater than one, we need this change in order to allow that second batch.",
          "createdAt": "2022-07-06T15:38:13Z",
          "updatedAt": "2022-07-06T15:38:13Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs49VqOq",
          "commit": {
            "abbreviatedOid": "a36dcc8"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-05T18:59:49Z",
          "updatedAt": "2022-07-05T18:59:49Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Note: the reference to an \"aggregation parameter\" in this section is new. However, I think it's OK: this section is referenced from the Leader Initialization & the Helper Initialization sections. The Leader Initialization section already assumes an aggregation parameter, and the Helper Initialization section explicitly receives an aggregation parameter from the leader's request. So I think this reference is well-formed.",
              "createdAt": "2022-07-05T18:59:49Z",
              "updatedAt": "2022-07-05T18:59:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49eyw_",
          "commit": {
            "abbreviatedOid": "a36dcc8"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks @BranLwyd -- this change LGTM.",
          "createdAt": "2022-07-07T12:23:40Z",
          "updatedAt": "2022-07-07T12:23:40Z",
          "comments": []
        }
      ]
    },
    {
      "number": 285,
      "id": "PR_kwDOFEJYQs464Ppr",
      "title": "Add \"unrecognizedAggregationJob\" error type.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/285",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This is intended for use if an aggregator receives a message with an unknown aggregation job ID.\r\n\r\nCloses #270.",
      "createdAt": "2022-07-05T20:39:23Z",
      "updatedAt": "2023-11-27T23:10:40Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "93d3e3ab989129cee9ce1bbec4ec724c7734f0e5",
      "headRepository": null,
      "headRefName": "unrecognized-aggregation-job",
      "headRefOid": "6c76aa82b611ff5b5f1cb258139fb19e8d333a73",
      "closedAt": "2022-07-07T12:28:18Z",
      "mergedAt": "2022-07-07T12:28:18Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "c1d08f00993c49a6d809ede1f7a4a9204cd54125"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs49WHQx",
          "commit": {
            "abbreviatedOid": "6c76aa8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-05T21:03:53Z",
          "updatedAt": "2022-07-05T21:03:53Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49e0Q-",
          "commit": {
            "abbreviatedOid": "6c76aa8"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-07T12:28:14Z",
          "updatedAt": "2022-07-07T12:28:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 286,
      "id": "PR_kwDOFEJYQs47HpgT",
      "title": "Align draft with draft-irtf-cfrg-vdaf-01",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/286",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #266.",
      "createdAt": "2022-07-08T16:43:33Z",
      "updatedAt": "2022-09-16T00:29:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c1d08f00993c49a6d809ede1f7a4a9204cd54125",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/266",
      "headRefOid": "5a98afccafc35b8987ad83e57cbebeab064b97c0",
      "closedAt": "2022-07-11T18:37:37Z",
      "mergedAt": "2022-07-11T18:37:37Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "b9172ae4ef4f61d09fd3e6ceb6604dada463d7ad"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs49lWHT",
          "commit": {
            "abbreviatedOid": "4dea8bb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-07-08T17:28:05Z",
          "updatedAt": "2022-07-08T17:45:23Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "We haven't yet worked out how aggregators negotiate a verify ~parameter~ key so I think the note about it being computed before the start of the protocol and the issue reference are still relevant.",
              "createdAt": "2022-07-08T17:28:05Z",
              "updatedAt": "2022-07-08T17:45:23Z"
            },
            {
              "originalPosition": 55,
              "body": "I think the `- 0x02` is a bit misleading here since we explicitly spell out that it's `0x00` and `0x01` for leader and helper, respectively, and don't make it clear that `server_role` is a value from the `Role` enum.\r\n\r\nWhy does this differ from the role values used elsewhere at all? The description of [`Vdaf.prep_init`](https://github.com/cfrg/draft-irtf-cfrg-vdaf/blob/main/draft-irtf-cfrg-vdaf.md#preparation-sec-vdaf-prepare) only seems to care that the `agg_id` be unique, not that it start at 0.",
              "createdAt": "2022-07-08T17:30:53Z",
              "updatedAt": "2022-07-08T17:45:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49ljxA",
          "commit": {
            "abbreviatedOid": "4dea8bb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T18:27:34Z",
          "updatedAt": "2022-07-08T18:27:35Z",
          "comments": [
            {
              "originalPosition": 45,
              "body": "Good call, done.",
              "createdAt": "2022-07-08T18:27:34Z",
              "updatedAt": "2022-07-08T18:27:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49lkF7",
          "commit": {
            "abbreviatedOid": "4dea8bb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T18:29:08Z",
          "updatedAt": "2022-07-08T18:29:08Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "hmmm yeah, I was trying to avoid introducing `agg_id` here, as doing so seemed overly complex. Ideally we would not have to subtract `0x02`, as noted in the OPEN ISSUE below.",
              "createdAt": "2022-07-08T18:29:08Z",
              "updatedAt": "2022-07-08T18:31:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49lpI8",
          "commit": {
            "abbreviatedOid": "97d4016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T18:52:36Z",
          "updatedAt": "2022-07-08T18:52:37Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "Overall I think we should just use the values `0x02` and `0x03` here, but so long as there's an OPEN ISSUE about this I'm happy. For now my vote would be to delete the ` - 0x02`, since I think the description of `server_role` below makes this explicit, and right now it could be read as requiring `0x00 - 0x02` for the leader and `0x01 - 0x02` for the helper.",
              "createdAt": "2022-07-08T18:52:36Z",
              "updatedAt": "2022-07-08T18:52:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49lpbd",
          "commit": {
            "abbreviatedOid": "4dea8bb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T18:53:59Z",
          "updatedAt": "2022-07-08T18:53:59Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "That would make it incompatible with VDAF-01, unfortunately. Indexing needs to start at 0.",
              "createdAt": "2022-07-08T18:53:59Z",
              "updatedAt": "2022-07-08T18:53:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49l_14",
          "commit": {
            "abbreviatedOid": "97d4016"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T20:49:12Z",
          "updatedAt": "2022-07-08T20:49:13Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "As currently written, this PR does suggest that this parameter take `0x00 - 0x02 = -0x02` for leader and `0x01 - 0x02 = -0x01` for helper (assuming signed math in both cases). I think we need to either get rid of the subtraction of `0x02` in the highlighted line of the code block, or modify the text in the following paragraph to suggest using `0x02` for leader and `0x03` for helper, in order to ultimately map this parameter to either `0x00` or `0x01` as intended.\r\n\r\n(FWIW, I support modifying the DAP spec to make leader & helper `0x00` & `0x01` directly, but that's probably a change larger than the scope of this PR.)",
              "createdAt": "2022-07-08T20:49:12Z",
              "updatedAt": "2022-07-08T21:15:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49mJJ6",
          "commit": {
            "abbreviatedOid": "4dea8bb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T21:52:01Z",
          "updatedAt": "2022-07-08T21:52:01Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "For some historical context: the reason that the DAP `Role` enum has `Leader = 0x02, Helper = 0x03` is in order to allow for additional helpers whose `Role` would naturally be `0x04`, `0x05`, etc. If we go to `Leader = 0x00, Helper = 0x01`, then we are kinda-sorta committing DAP to never having more than two aggregators, or at least to having this peculiar discontinuity in aggregator role values.\r\n\r\nAnyway, for this PR, let's strike the `- 0x02` for clarity.",
              "createdAt": "2022-07-08T21:52:01Z",
              "updatedAt": "2022-07-08T21:52:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49mNT6",
          "commit": {
            "abbreviatedOid": "4dea8bb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-08T22:30:52Z",
          "updatedAt": "2022-07-08T22:30:52Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "Ohhh yeah, sorry. The parenthetical was wrong, which is what is misleading. I've replaced `server_role - 0x02` with `agg_id`, which should at least be unambiguous. It would be nice to change `Role` to match, but we can do that later.",
              "createdAt": "2022-07-08T22:30:52Z",
              "updatedAt": "2022-07-08T22:30:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs49mRFp",
          "commit": {
            "abbreviatedOid": "5a98afc"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-08T23:16:48Z",
          "updatedAt": "2022-07-08T23:16:48Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49q46v",
          "commit": {
            "abbreviatedOid": "5a98afc"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-11T16:51:22Z",
          "updatedAt": "2022-07-11T16:51:22Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs49rXTg",
          "commit": {
            "abbreviatedOid": "5a98afc"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-11T18:37:32Z",
          "updatedAt": "2022-07-11T18:37:32Z",
          "comments": []
        }
      ]
    },
    {
      "number": 287,
      "id": "PR_kwDOFEJYQs47q-sI",
      "title": "Add `report_count` to `CollectResp`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/287",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "This adds a field to collect responses with the number of reports that contributed to the aggregation. Collectors will likely want this count to aid in the interpretation of aggregate results. (see #163) There is also a change in draft-irtf-cfrg-vdaf-02 that opens the door for VDAFs that would require the report count during unsharding. (see cfrg/draft-irtf-cfrg-vdaf#95)\r\n\r\nNote that while this field is not part of the adjacent `HpkeCiphertext`s, it will be protected by to the confidentiality of HTTPS and the leader-collector bearer token authentication.",
      "createdAt": "2022-07-19T18:42:30Z",
      "updatedAt": "2022-08-29T17:56:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9639d4f7c6baa9efc9712d326b2a635edecadd4d",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "collectresp-report-count",
      "headRefOid": "d17632d4009eb97a1c22b70a2658322a16622860",
      "closedAt": "2022-08-03T18:51:25Z",
      "mergedAt": "2022-08-03T18:51:25Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "2a7b8b5e5ab90d5641da8be0d0919df24b52023f"
      },
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm open to pushing it into the ciphertext, but my thinking was that a tampered report_count would only affect robustness, not privacy, and thus we only need to arrive at the correct report_count when all aggregators follow the protocol.",
          "createdAt": "2022-07-19T19:17:46Z",
          "updatedAt": "2022-07-19T19:17:46Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@divergentdave can you please rebase? Sorry!",
          "createdAt": "2022-08-02T12:45:32Z",
          "updatedAt": "2022-08-02T12:45:55Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4-OXJW",
          "commit": {
            "abbreviatedOid": "3c86ffe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I think we do want the `report_count` to be included inside each encrypted aggregate share, because otherwise the leader could lie to the collector about how many reports were included in an aggregate. I think this would require defining a structure like\r\n```\r\nstruct {\r\n    opaque agg_share;\r\n    uint64 report_count;\r\n} AggregateShare\r\n```\r\nWhere `agg_share = VDAF.out_shares_to_agg_share(agg_param, out_shares)`. Then, `AggregateShareResp.encrypted_aggregate_share` is the encrypted `struct AggregateShare`, and `CollectResp.encrypted_agg_shares` is a vector of encrypted `struct AggregateShare`.",
          "createdAt": "2022-07-19T19:08:35Z",
          "updatedAt": "2022-07-19T19:08:35Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4-O_Ht",
          "commit": {
            "abbreviatedOid": "3c86ffe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I agree with David, a binding between `report_count` and the ciphertext is not essential. If we wanted this for defense-in-depth, an alternative to sticking it in the ciphertext is to stick it in the AAD. This seems slightly easier to implement to me.",
          "createdAt": "2022-07-19T21:44:48Z",
          "updatedAt": "2022-07-19T21:44:48Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4_GnVQ",
          "commit": {
            "abbreviatedOid": "3c86ffe"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-02T12:44:37Z",
          "updatedAt": "2022-08-02T12:44:37Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4_Oxyw",
          "commit": {
            "abbreviatedOid": "d17632d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-03T18:43:23Z",
          "updatedAt": "2022-08-03T18:43:23Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4_OzAK",
          "commit": {
            "abbreviatedOid": "d17632d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "OK, seems reasonable to me.",
          "createdAt": "2022-08-03T18:47:50Z",
          "updatedAt": "2022-08-03T18:47:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 292,
      "id": "PR_kwDOFEJYQs48IxDg",
      "title": "Allow leader to remove collect results without an explicit DELETE.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/292",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "The previous wording required the leader to store collect results until\r\nthe collector sent an explicit DELETE request to the collect job's URI.\r\nNow, the relevant text is reworded to allow the leader to remove collect\r\nresults for other reasons (for example, a leader can now garbage-collect\r\nold results after a suitable period of time).\r\n\r\nThis fell out of discussion on #288.",
      "createdAt": "2022-07-26T20:23:50Z",
      "updatedAt": "2022-08-29T17:56:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "22364f6d76421f6f4edcbe81c03cdce3682aad0a",
      "headRepository": null,
      "headRefName": "bran/allow-deletion-of-collect-results",
      "headRefOid": "86ebbbdff01eb6c27010f708a3eaee3534c5ac69",
      "closedAt": "2022-08-02T12:38:03Z",
      "mergedAt": "2022-08-02T12:38:03Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "b4ab93142240e7a2df43be7e4a827ec5a6cf40c4"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "OK -- I left the motivating example out as I didn't want to be too prescriptive to implementations. I would also be interested in comments from folks more experienced with RFC writing; I've added the motivating example for now.",
          "createdAt": "2022-07-27T16:58:30Z",
          "updatedAt": "2022-07-27T16:58:30Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4-rgg7",
          "commit": {
            "abbreviatedOid": "3c34b7a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-07-26T20:45:25Z",
          "updatedAt": "2022-07-26T20:45:43Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "I think this MUST should be rewritten to describe externally observable behavior. For instance:\r\n\r\n```suggestion\r\nThe collector may send an HTTP DELETE request to the collect job URI, to which\r\nthe leader MUST respond with HTTP status 204 No Content. The leader MUST\r\nrespond to subsequent requests to the collect job URI with HTTP status 204 No\r\nContent.\r\n```\r\n\r\nNote that this also specifies how the leader should respond to the DELETE, which is missing in the existing text. My intuition is that there's no way for other protocol participants to know whether or not the leader's storage still has some aggregate share. The only thing they can see is how the leader responds to a request.\r\n\r\nBesides that: this text makes it clear what a leader MUST do if it gets an explicit DELETE, but I think we also need a MAY sentence making it clear that the leader can start respond to the collect URI with HTTP 204 _before_ a collector sends DELETE.",
              "createdAt": "2022-07-26T20:45:25Z",
              "updatedAt": "2022-07-26T20:45:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4-rt9L",
          "commit": {
            "abbreviatedOid": "3c34b7a"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-26T21:41:38Z",
          "updatedAt": "2022-07-26T21:41:39Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "OK -- I had left the `MAY` off since it's IMO implied by the updated text, but explicit is usually better than implicit.",
              "createdAt": "2022-07-26T21:41:39Z",
              "updatedAt": "2022-07-26T21:41:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4-tdYu",
          "commit": {
            "abbreviatedOid": "bc5cfae"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-27T08:33:36Z",
          "updatedAt": "2022-07-27T08:33:37Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "I think it would be nice to add a little \"(e.g. because the data has been deleted due to age or lack of space)\". It's a little confusing if you don't have that on your mind.",
              "createdAt": "2022-07-27T08:33:36Z",
              "updatedAt": "2022-07-27T08:33:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4-wEVC",
          "commit": {
            "abbreviatedOid": "bc5cfae"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Approved as this change achieves the intended mechanical purpose, though I agree with Simon's point about clarity, and others more experienced in the RFC writing style might comment further.",
          "createdAt": "2022-07-27T16:18:07Z",
          "updatedAt": "2022-07-27T16:18:07Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4-53Mz",
          "commit": {
            "abbreviatedOid": "6c281b1"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-07-29T10:52:56Z",
          "updatedAt": "2022-07-29T10:52:56Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4-53XH",
          "commit": {
            "abbreviatedOid": "6c281b1"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-07-29T10:53:40Z",
          "updatedAt": "2022-07-29T10:53:40Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "```suggestion\r\nThe collector can send an HTTP DELETE request to the collect job URI, to which\r\n```",
              "createdAt": "2022-07-29T10:53:40Z",
              "updatedAt": "2022-07-29T10:53:40Z"
            }
          ]
        }
      ]
    },
    {
      "number": 297,
      "id": "PR_kwDOFEJYQs48WYDT",
      "title": "Add support for fixed-size tasks",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Closes #273.\r\n\r\nGeneralizes \"batch_interval\" to \"query\" and enumerate the different query\r\ntypes and constraints that apply to each. The existing collect request\r\nsemantics is subsumed by the `time-interval` query type.\r\n\r\nAdds a query type, `fixed-size`, that is designed to support use cases\r\nin which the sample size needs to be tightly controlled. The main\r\nfeature of this query type is that the Aggregators enforce limits on the\r\nsize of each batch. We given them a little wiggle room in order to make\r\ncoordination of aggregation jobs a bit easier.\r\n\r\nOne notable feature of this change is that each tasks supports exactly\r\none query type. This allows us to punt on the coomplexity of having to\r\ncomposing different query types per task. For example: If a report can\r\nbe batched into a chunk or a batch interval, then it's not clear that\r\nit's possible for Aggregators to aggregate reports ahead of a collect\r\nrequest.",
      "createdAt": "2022-07-30T02:28:18Z",
      "updatedAt": "2022-09-16T00:29:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2a7b8b5e5ab90d5641da8be0d0919df24b52023f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/273/1",
      "headRefOid": "7891e4b93170e5bb7d2dc3db680cb76edbb051b9",
      "closedAt": "2022-08-19T21:56:18Z",
      "mergedAt": "2022-08-19T21:56:18Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "8d845c030f9ef3929b00484cdd9ae4e591dd7108"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @tgeoghegan for the comments! Based on your feedback and feedback in the PPM slack, I will definitely be adding the timestamp back into the report struct for fixed-chunk tasks.",
          "createdAt": "2022-08-06T01:14:38Z",
          "updatedAt": "2022-08-06T01:14:38Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I would definitely like if we could find a way to somehow unify the different query types (lots of places in the spec now basically bifurcate struct content/validation logic/etc), but IIUC this is a somewhat exploratory change to discover the different query types that need to be supported, so merging this LGTM.\r\n\r\nI think we should aim to merge what we intend to implement. I agree that that it would be great to make the Helper as agnostic as possible to to the query type. I think we've done about as good a job as we can here, *if we want to reach for property (3.)*. But I'm open to alternative ideas, or reasons to drop (3.). Let's keep discussing!",
          "createdAt": "2022-08-11T20:14:18Z",
          "updatedAt": "2022-08-11T20:17:39Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "My current thinking (pretty high-level, may be sketchy, review appreciated):\r\n\r\n* I think something like property (3) is very desirable to have. We've been discussing achieving this property by requiring that distinct batches be non-overlapping in time. I think one way to make it feasible to implement that idea is to change how client reports, aggregation jobs, and batches are bound together:\r\n  * Currently, we bind a client report to a batch when the aggregation job is created via an `AggregateInitializeReq`. But there's no reason this conceptually needs to be true: we could instead choose to bind client reports to batches at or after the end of aggregation. We also require that all reports in the same aggregation job be bound to the same batch, but again, there is no reason this conceptually needs to be true: we could instead allow different client reports in the same aggregation job to be bound to different batches. We could implement these conceptual changes by adding an additional step to the aggregate flow, or perhaps even better by having the `AggregateShareReq` include enough information to identify the aggregated reports that are bound to the batch. I think the latter option would be better since, if we commit to batches having non-overlapping time intervals in the `fixed-size` task type, a batch would be identified by a batch interval in both the `time-interval` and `fixed-size` task types.\r\n  * These changes would make it much easier to group reports into batches while still meeting the `{min,max}_batch_size` & non-overlapping-in-time batch constraints for a few reasons. First, since the binding happens after aggregation concludes, reports that fail the aggregation flow can no longer unexpectedly \"remove\" reports from a batch -- we only add successfully-aggregated reports to batches. Even more importantly, by allowing different reports in the same aggregation job to be bound to different batches, we can effectively split (successfully aggregated) reports in an aggregation job to more than one batch -- so an aggregation job that is \"too big\" to fit in the currently-open batch can still provide enough reports to finish the currently-open batch, with the remaining reports going into a following batch. (In fact, I think with these changes we could probably always hit `min_batch_size` exactly -- the leader would just select an interval end that selects the correct number of reports. We probably want some way to break timestamp ties -- details likely a bit messy.)\r\n  * This doesn't _uniquely_ determine batches if we allow leeway in the batch size. But it's close, and like I said, I think we actually don't need the leeway anymore in which case the batches are uniquely determined.\r\n  * `BatchId` no longer needs to exist as a concept, since batches are now always identified by an interval.\r\n\r\n* Once we have (3) for the `fixed-size` case, I think we can unify both the `fixed-size` and the `time-interval` cases to nearly the same logic -- they are both effectively aggregating reports over intervals of time, the largest difference is how the intervals of time are decided.\r\n  * Specifically, the `time-interval` case would determine the batches as `[0, T)`, `[T, 2T)`, ... just as today (where `T` is the `min_batch_duration`). The `fixed-size` case would generate intervals based on the rate of client reports arriving -- the interval boundaries would be selected to create batches of the appropriate size.\r\n  * We still need the ability added in this PR for the collector to request the \"next\" batch ID. It's a nice-to-have for the `time-interval` case, but a requirement for the `fixed-size` case since in this case the collector has no insight into what the batch intervals might be.\r\n  * The ability for the collector to collect multiple contiguous batches is still required for the `time-interval` case, since we might need to do so to hit `min_batch_size`. We could implement it for `fixed-size` if we wished, by mandating that the generated batches be contiguous (i.e. no gaps between batches).\r\n  * The expected collector behavior for `time-interval` wouldn't change: occasionally use the collect flow to collect the next interval of interest. (With these changes, the collector could safely request an interval that is not yet complete; or, if it wanted to, ask to be *told* what the \"next\" batch interval is, though I don't think this would be required.)\r\n  * The expected collector behavior for `fixed-size` would be very similar to what is suggested in this PR: occasionally use the collect flow to collect the \"next\" batch interval, using the \"tell-me-the-next-batch\" functionality.\r\n  * I think we'd want to keep the change in this PR to allow reports to arrive after a collect request arrives (but not after the collection completes), since otherwise the \"tell-me-the-next-batch\" functionality could not be used without potentially cutting a batch short.\r\n\r\nLike I said, this is pretty sketchy. I'm probably missing some things, review would be appreciated.",
          "createdAt": "2022-08-12T03:23:10Z",
          "updatedAt": "2022-08-12T04:10:04Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> * Currently, we bind a client report to a batch when the aggregation job is created via an `AggregateInitializeReq`. But there's no reason this conceptually needs to be true: we could instead choose to bind client reports to batches at or after the end of aggregation. We also require that all reports in the same aggregation job be bound to the same batch, but again, there is no reason this conceptually needs to be true: we could instead allow different client reports in the same aggregation job to be bound to different batches. We could implement these conceptual changes by adding an additional step to the aggregate flow, or perhaps even better by having the `AggregateShareReq` include enough information to identify the aggregated reports that are bound to the batch. I think the latter option would be better since, if we commit to batches having non-overlapping time intervals in the `fixed-size` task type, a batch would be identified by a batch interval in both the `time-interval` and `fixed-size` task types.\r\n\r\nIIUC you're saying we can do the report validation first and only assign reports to batches for the actual aggregation. I completely agree with that. If that's not what you're saying, could you please elaborate?\r\n\r\n> we can effectively split (successfully aggregated) reports in an aggregation job to more than one batch \r\n\r\nI think your wording here is a bit confusing. Once the jobs are aggregated they are already in a batch because that's what we're aggregating over. I think I know what you mean but I'm not sure.",
          "createdAt": "2022-08-12T09:03:18Z",
          "updatedAt": "2022-08-12T18:05:52Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Meta: Can we move this discussion to an issue? The order gets confusing and parts are lost in PRs.",
          "createdAt": "2022-08-12T11:20:55Z",
          "updatedAt": "2022-08-12T11:20:55Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> > ```\r\n> >   * Currently, we bind a client report to a batch when the aggregation job is created via an `AggregateInitializeReq`. But there's no reason this conceptually needs to be true: we could instead choose to bind client reports to batches at or after the end of aggregation. We also require that all reports in the same aggregation job be bound to the same batch, but again, there is no reason this conceptually needs to be true: we could instead allow different client reports in the same aggregation job to be bound to different batches. We could implement these conceptual changes by adding an additional step to the aggregate flow, or perhaps even better by having the `AggregateShareReq` include enough information to identify the aggregated reports that are bound to the batch. I think the latter option would be better since, if we commit to batches having non-overlapping time intervals in the `fixed-size` task type, a batch would be identified by a batch interval in both the `time-interval` and `fixed-size` task types.\r\n> > ```\r\n> \r\n> IIUC you're saying we can do the report validation first and only assign reports to batches for the actual aggregation. I completely agree with that. If that's not what you're saying, could you please elaborate?\r\n\r\nYes, that's correct. With how the PR is currently worded, in the `fixed-size` case reports are assigned to batches when aggregation begins; my suggestion is to change that to assign reports to batches implicitly based on their timestamp.\r\n\r\n> > ```\r\n> >   we can effectively split (successfully aggregated) reports in an aggregation job to more than one batch \r\n> > ```\r\n> \r\n> I think your wording here is a bit confusing. Once the jobs are aggregated they are already in a batch because that's what we're aggregating over. I think I know what you mean but I'm not sure.\r\n\r\nThe aggregation flow is done via a grouping of reports called an \"aggregation job\". Collection is done via a (different!) grouping of reports called a \"batch\", I suppose -- this \"batch\" concept is explicit for the `fixed-size` case which introduces an explicit batch ID, and was I believe implicit for the existing `time-interval` case (which identified batches by \"batch interval\").\r\n\r\nWith how this PR is currently worded, all of the reports in a given aggregation job must fall into the same batch in the `fixed-size` case, since reports are bound to a batch during `AggregateInitializeReq`. My suggestion is to change this to bind reports to batches implicitly based on their timestamp, and identify batches by a batch interval in all cases, rather than using a \"batch ID\" in the `fixed-size` case.\r\n\r\n> Meta: Can we move this discussion to an issue? The order gets confusing and parts are lost in PRs.\r\n\r\nI'd agree with this, is #273 a good place for further discussion?",
          "createdAt": "2022-08-12T16:26:22Z",
          "updatedAt": "2022-08-12T16:26:22Z"
        },
        {
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think unifying the types in `AggregateShareReq` regardless of task query type is great, but whether we use interval or list of aggregation job IDs in `AggregateShareReq` is inconvenient for one or other task query type: I \r\n- If we use interval in `AggregateShareReq`: This can already support interval-based query type today, but this is hard to support fixed-size batch query, especially with coarse-grained timestamps. Regarding @branlwyd's suggestions to \"assign reports to batches implicitly based on their timestamp\", if the clients choose to round down their timestamps to a `n`-hour window (i.e `min_batch_duration = n hours`), we could possibly have multiple batches with minimum batch size in each n-hour interval. Then we cannot guarantee that the interval in `AggregateShareReq` can uniquely identify a batch. We either need to distinguish batches within an interval (but DAP prevents overlapping intervals in many places), or just throw away reports that belong to an interval after the minimum batch size has been met in that interval, which is a waste of client measurements IMO.\r\n- If we use list of aggregation job IDs in `AggregateShareReq` like @tgeoghegan suggested: this is relatively easy for the fixed-size batch query, because leader just needs to create aggregation jobs with the appropriate size based on the number of reports in the open batch/chunk. However, this is hard for the interval-based query, for leader particularly. Leader needs to assign reports, with the same unit batch interval start time, to the same aggregation job. However, in today's DAP, leader and helper pre-aggregate output shares implicitly based on their timestamps, so there is no restriction on how leader assigns reports to aggregation jobs.\r\n- A separate note is that we can use the batch selector (interval or batch ID) to uniquely identify a batch, so that when leader wants to retry an `AggregateShareReq` in case of any temporary network error, the batch selector serves as the key to retrieve the aggregate share from helper side. Whereas using something like list of aggregation job IDs can be ambiguous, whether leader wants to retry a previous request, or wants to collect the same list of aggregation job IDs again.\r\n\r\nMy take is we can see if future task query types need to be supported, and evaluate again whether unifying the types in `AggregateShareReq` is worth doing? Other than that, I think this PR looks good to me.\r\n",
          "createdAt": "2022-08-15T04:46:27Z",
          "updatedAt": "2022-08-15T04:47:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "FYI, I have squashed this branch into a single commit.",
          "createdAt": "2022-08-15T16:07:02Z",
          "updatedAt": "2022-08-15T16:07:02Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> > Only 1 blocking comment on the constraint that Collectors can't opportunistically query for batch IDs.\r\n>\r\n> I think this point needs a bit more discussion. Can we do so in a new issue?\r\n\r\nI don't really see how this introduces edge cases. If the batch ID doesn't exist yet, the leader can just discard it. The behavior is pretty straightforward.\r\n\r\nI don't see where this has been discussed on slack. Please drop an OPEN ISSUE so we can track relaxing this constraint. ",
          "createdAt": "2022-08-18T16:29:55Z",
          "updatedAt": "2022-08-18T16:29:55Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "\r\n> I don't see where this has been discussed on slack. Please drop an OPEN ISSUE so we can track relaxing this constraint.\r\n\r\nDone. Filed https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/301.",
          "createdAt": "2022-08-18T17:27:25Z",
          "updatedAt": "2022-08-18T17:27:25Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed and updated commit message.",
          "createdAt": "2022-08-19T21:55:37Z",
          "updatedAt": "2022-08-19T21:55:37Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4_PWnr",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-03T20:45:30Z",
          "updatedAt": "2022-08-03T20:45:30Z",
          "comments": [
            {
              "originalPosition": 669,
              "body": "Maybe \"boundary\" is the wrong word? For time-series tasks, these are  the batch windows. For fixed-chunk tasks, these are the chunks.",
              "createdAt": "2022-08-03T20:45:30Z",
              "updatedAt": "2022-08-03T20:45:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_PZNE",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-03T20:55:43Z",
          "updatedAt": "2022-08-03T20:55:52Z",
          "comments": [
            {
              "originalPosition": 147,
              "body": "TODO: Deal with the case where a Collector queries an unknown chunk.",
              "createdAt": "2022-08-03T20:55:43Z",
              "updatedAt": "2022-08-06T00:48:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_PYpS",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "First pass for spelling and a couple editorial gripes, still reviewing for content",
          "createdAt": "2022-08-03T20:53:36Z",
          "updatedAt": "2022-08-03T21:34:07Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "```suggestion\r\nincluded in a batch for aggregation. However, since reports are uploaded to the\r\n```",
              "createdAt": "2022-08-03T20:53:47Z",
              "updatedAt": "2022-08-03T21:34:07Z"
            },
            {
              "originalPosition": 34,
              "body": "It's unclear which entity \"it\" refers to in this sentence \r\n```suggestion\r\nLeader, the Collector cannot choose the batch directly. Instead, it issues a \"query\" used\r\n```\r\n",
              "createdAt": "2022-08-03T20:54:27Z",
              "updatedAt": "2022-08-05T23:01:36Z"
            },
            {
              "originalPosition": 42,
              "body": "nit/bikeshedding: [\"time series\"](https://en.wikipedia.org/wiki/Time_series) already has a particular meaning, especially among metrics and telemetry people who are likely to be comparing DAP to systems like Prometheus, which discusses time series extensively. I'm not sure that this usage of \"time series\" is wrong, but how about \"time range\" or even \"interval\" to eliminate the ambiguity?",
              "createdAt": "2022-08-03T20:56:18Z",
              "updatedAt": "2022-08-03T21:34:07Z"
            },
            {
              "originalPosition": 89,
              "body": "```suggestion\r\n} QueryConfig;\r\n```",
              "createdAt": "2022-08-03T20:57:14Z",
              "updatedAt": "2022-08-03T21:34:07Z"
            },
            {
              "originalPosition": 112,
              "body": "```suggestion\r\nThe Collector is free to choose any sequence of batch intervals it wishes, subject\r\n```",
              "createdAt": "2022-08-03T20:58:30Z",
              "updatedAt": "2022-08-03T21:34:07Z"
            },
            {
              "originalPosition": 113,
              "body": "```suggestion\r\nto the restrictions prescribed in {{batch-validation}}. However, it is\r\n```",
              "createdAt": "2022-08-03T20:58:41Z",
              "updatedAt": "2022-08-03T21:34:07Z"
            },
            {
              "originalPosition": 119,
              "body": "```suggestion\r\ndenotes the duration.) There are exceptions to this standard operating\r\n```",
              "createdAt": "2022-08-03T20:59:18Z",
              "updatedAt": "2022-08-03T21:34:07Z"
            },
            {
              "originalPosition": 122,
              "body": "```suggestion\r\n- The Collector may need to extend a batch interval forward in time if the\r\n```",
              "createdAt": "2022-08-03T20:59:25Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 123,
              "body": "```suggestion\r\n  interval has elapsed and an insufficient number of measurements have been\r\n```",
              "createdAt": "2022-08-03T20:59:35Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 124,
              "body": "```suggestion\r\n  uploaded. For example, instead of `(1659546000, 1000)`, the Collector might\r\n```",
              "createdAt": "2022-08-03T20:59:43Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 139,
              "body": "```suggestion\r\nFor this query type, the Aggregators batch measurements into arbitrary \"chunks\"\r\n```",
              "createdAt": "2022-08-03T21:00:06Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 146,
              "body": "```suggestion\r\n(see {{collect-flow}}) includes a \"chunk ID\", which can be used to query the\r\n```",
              "createdAt": "2022-08-03T21:00:30Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 250,
              "body": "```suggestion\r\nas specified by the VDAF. It then encrypts each input share as follows:\r\n```",
              "createdAt": "2022-08-03T21:03:14Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 400,
              "body": "```suggestion\r\n   also choose to mark an input share as invalid with the error\r\n```",
              "createdAt": "2022-08-03T21:31:21Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 407,
              "body": "```suggestion\r\n      \"batch-saturated\". Note that this behavior is not strictly enforced here\r\n```",
              "createdAt": "2022-08-03T21:31:39Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 423,
              "body": "```suggestion\r\nthen locally combines them to yield a single aggregate result. In particular,\r\n```",
              "createdAt": "2022-08-03T21:31:57Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            },
            {
              "originalPosition": 669,
              "body": "```suggestion\r\nFirst the aggregator checks that the request respects any batch \"boundaries\"\r\n```",
              "createdAt": "2022-08-03T21:33:04Z",
              "updatedAt": "2022-08-03T21:34:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_Pj6L",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I think this change accomplishes what it intends. However, besides specific comments left inline, I have an orthogonal, meta-thought here: can we design this so that the helper is agnostic to what kind of query the task uses?\r\n\r\nBefore this PR, we only supported batch interval queries. The leader's job was to assign reports to aggregation jobs and then communicate that mapping to helpers. The helper would then have to keep track of:\r\n\r\n - which reports belong to which aggregation jobs;\r\n - which reports belong to which batch intervals.\r\n\r\nWith the new fixed chunk queries, the helper has to keep track of:\r\n\r\n - which reports belong to which aggregation jobs (as before);\r\n - which reports belong to which chunks OR which aggregation jobs belong to which chunks.\r\n\r\nThe second mapping in both cases is new code and maybe new database tables that the helper has to deal with. Maybe we can spell this out so that for new query types, only the leader and collector need to know about them.\r\n\r\nUltimately, the only thing a helper is enforcing is that the number of reports included in an aggregate share fall within a range. `min_batch_size <= count < infinity` in the batch interval case, or `min_batch_size < count < max_batch_size` in the fixed chunk case.\r\n\r\nSo instead of:\r\n\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  BatchSelector batch_selector;\r\n  opaque agg_param<0..2^16-1>;\r\n  uint64 report_count;\r\n  opaque checksum[32];\r\n} AggregateShareReq;\r\n```\r\n\r\n...what if we had:\r\n\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  AggregationJobId job_ids<0..2^?-1>;\r\n  opaque agg_param<0..2^16-1>;\r\n  uint64 report_count;\r\n  opaque checksum[32];\r\n} AggregateShareReq;\r\n```\r\n\r\nTo service that request, the helper looks up the set of reports associated with each `AggregationJobId` (recall that the helper already has to keep track of this mapping), then verifies that the total number of reports described satisfies the range requirement, and releases the aggregate share if so. There are no options based on query flavour, and the helper has a single codepath for handling aggregate share requests.\r\n\r\nSo long as new query types' results can ultimately be described by the leader in terms of a list of aggregation job IDs, then helpers should be able to support them without code changes.\r\n\r\nThe downsides or questions w.r.t. this idea are:\r\n\r\nP1 - The current design of the batch interval query case allows aggregators to \"eagerly\" aggregate into `min_batch_duration`-sized buckets ahead of a collect request providing the actual `batch_interval`. If we make everything aggregation job ID based, then the helper can't eagerly aggregate across aggregation jobs. The leader could choose to construct aggregation jobs in some pathological manner that makes the helper extremely inefficient but I think that might already have been the case.\r\n\r\nP2 - In the batch interval query case, the helper can no longer enforce `min_batch_duration`. But was that ever needed for robustness or privacy, or was that just to enable to aggregators to eagerly accumulate report shares ahead of a collect request? If an aggregate share contains enough reports to satisfy the task's minimum, does it matter if they all arrived in the same instant?\r\n\r\nP3 - The helper can no longer verify that parameters in an `AggregateShareReq` have anything to do with an authentic collector `CollectReq`. If `AggregateShareReq` is written in terms of a list of `AggregationJobId`, then the leader gets total control over what reports are included. This was already the case in the fixed chunk case, but it does seem like a new power in the batch interval case.\r\n\r\nThat's a pitch for expressing both batch interval and fixed chunk queries to the helper in terms of a list of aggregation job IDs. But I think we could also express fixed chunk queries in terms of batch intervals. Supposing that report nonces contained time regardless of query type, then when the leader constructs an `AggregateShareReq`, it could construct a batch interval from the chunk's lowest and highest timestamps and then send that to the helper.",
          "createdAt": "2022-08-03T21:41:01Z",
          "updatedAt": "2022-08-03T23:26:45Z",
          "comments": [
            {
              "originalPosition": 115,
              "body": "```suggestion\r\nhave the same duration, are continuous, and increases monotonically. For example,\r\n```",
              "createdAt": "2022-08-03T21:45:39Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 114,
              "body": "Why recommend this? I see how this is generally what we expect collectors to do, but does it matter to the aggregators or to any protocol participant if the collector doesn't do this? The way I see it, if a collector should issue collect requests with non-contiguous batch intervals, then some number of prepared reports will be orphaned, and the aggregators will eventually throw them away based on their respective retention policies.",
              "createdAt": "2022-08-03T21:47:34Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 142,
              "body": "```suggestion\r\nthe minimum and maximum number of measurements per chunk.\r\n```",
              "createdAt": "2022-08-03T21:51:22Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 165,
              "body": "```suggestion\r\n  determines the query type for batch selection and the properties that all\r\n```",
              "createdAt": "2022-08-03T21:52:41Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 436,
              "body": "In the batch interval query model, the helper evaluates the batch interval against the report nonces to validate that a collect request is OK. The helper can trust that the report nonces came from the client (they're in the AEAD) and that the batch interval came from the collector (or at least we want that to be the case; #155). So it's possible for the leader to remove reports from an aggregate share (by never telling the helper about them) but it can't trick the helper into including a report into a batch interval that it shouldn't.\r\n\r\nIn the fixed chunk model, however, we grant the leader total power to assign reports to chunks (modulo the max and min sizes the task defines), so we should carefully analyze what powers that gives the leader. For instance, suppose the leader can see the uploading client's IP and do geolocation. Maybe it constructs one chunk that only contains reports from clients in country A and another chunk that only contains reports from country B in an effort to skew either chunk's aggregation somehow?\r\n\r\nMy intuition on how this feature is used is that you want to enable the leader to create a chunk every `min_chunk_size` reports. So maybe we want to enable the client to verify that the chunks defined by the leader are made up of non-overlapping intervals? That would require putting the `Time` component back in a report `Nonce`, but this would use code the helper needs to have anyway to enforce that batch interval queries don't overlap.",
              "createdAt": "2022-08-03T22:07:08Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 62,
              "body": "Is type `Id` defined anywhere?",
              "createdAt": "2022-08-03T22:09:37Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 140,
              "body": "editorial nit: clarify which dimension of \"size\" we are interested in here\r\n```suggestion\r\nsuch that each chunk has roughly the same number of reports. The configuration includes\r\n```",
              "createdAt": "2022-08-03T22:12:21Z",
              "updatedAt": "2022-08-03T23:26:45Z"
            },
            {
              "originalPosition": 152,
              "body": "Can you elaborate on what's hard about this, here in the PR thread if not necessarily in the protocol text? You're arguing that if I have multiple running leaders, they can race with each other to define chunks? Or is it that they race with clients uploading reports?",
              "createdAt": "2022-08-03T22:15:22Z",
              "updatedAt": "2022-08-03T23:26:46Z"
            },
            {
              "originalPosition": 724,
              "body": "```suggestion\r\nignores it or aborts the upload sub-protocol as described in {{upload-flow}}. A\r\n```",
              "createdAt": "2022-08-03T22:25:43Z",
              "updatedAt": "2022-08-03T23:26:46Z"
            },
            {
              "originalPosition": 555,
              "body": "```suggestion\r\n    * For fixed-chunk tasks, the request specifies the chunk ID.\r\n```",
              "createdAt": "2022-08-03T23:21:38Z",
              "updatedAt": "2022-08-03T23:26:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_bsQP",
          "commit": {
            "abbreviatedOid": "0462f37"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-05T23:07:56Z",
          "updatedAt": "2022-08-06T01:02:44Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "I sort of think of this query type as being the \"DAP analogue\" of time-series data in Prometheus, but maybe this is wrong? From my (probably misinformed!) point of view, \"time-series DAP\" provides all of the functionality of \"time-series Prometheus\", except that lose the ability to see individual measurements and only see aggregates. \r\n\r\nIf I'm missing something conceptually important, we should definitely avoid overloading the term.",
              "createdAt": "2022-08-05T23:07:56Z",
              "updatedAt": "2022-08-06T01:02:44Z"
            },
            {
              "originalPosition": 62,
              "body": "Nope, this supposed to be an `opaque [32]`, i.e., the same type as other IDs in the protocol. Fixed.",
              "createdAt": "2022-08-05T23:11:13Z",
              "updatedAt": "2022-08-06T01:02:44Z"
            },
            {
              "originalPosition": 114,
              "body": "That all sounds right. I think it's good for us to spell out how we expect Collectors to behave, but \"RECOMMENDED\" may be stronger than needed. We certainly don't want to give Collectors the impression that, if they don't follow this recommendation, they're doing something wrong. (This is certainly not the case, there are lots of reasons to diverge here.)\r\n\r\nA better approach might be to a give a bit of guidance, then spell out optional Aggregator behavior elsewhere.\r\n> Typically the Collector issues queries for which the batch intervals continuous, monotonically increasing, have the same duration. For example ... However, there are cases in which Collector may need to issue queries out-of-order. For example, a previous batch might need to be queried again with a different aggregation parameter (e.g, for Poplar1). In addition, the Collector may need to vary the duration to adjust to changing measurement upload rates.\r\n\r\nThen in the relevant collect sub-protocol section(s), we would have something like:\r\n> The Leader (resp. Helper) MAY abort a CollectReq (resp. AggregateShareReq) with error \"XXX\" as needed. For example, the batch of reports to which the request pertains might be so old that the Aggregator has removed them from its long-term storage.\r\n\r\nIncidentally, this came up this week with @BranLwyd while working on Daphne/Janus interop.\r\ncc/ @nakatsuka-y since we're discussing this question in https://github.com/cloudflare/daphne/issues/45.",
              "createdAt": "2022-08-05T23:29:38Z",
              "updatedAt": "2022-08-06T01:02:44Z"
            },
            {
              "originalPosition": 152,
              "body": "Imagine you're streaming reports to a bunch of independent Leader \"workers\", all of which are trying to fill the same chunk. Let's say each worker has the following procedure for kicking off an agg job:\r\n1. Wait until min(100, N) reports have arrived, where N is the number of reports left to go for the chunk. (I.e., you have aggregated min_batch_size - N reports for the batch.)\r\n2. Run an aggregation job. Tell your peers how many reports you aggregated. (This may be less than the number of reports processed, due to rejections.)\r\n\r\nThe \"hard\" part is that the value of N at the start of step 1 depends on the output of each worker at step 2. Computing this correctly requires coordination. I can imagine a few ways of doing this, but the \"best\" option may depend on the scale and other deployment-specific considerations. It seems like a good idea to give some leeway.",
              "createdAt": "2022-08-05T23:54:17Z",
              "updatedAt": "2022-08-06T01:02:44Z"
            },
            {
              "originalPosition": 436,
              "body": "> In the fixed chunk model, however, we grant the leader total power to assign reports to chunks (modulo the max and min sizes the task defines), so we should carefully analyze what powers that gives the leader.\r\n\r\nNice catch. There should already be an OPEN ISSUE somewhere in this PR about this.\r\n\r\n> For instance, suppose the leader can see the uploading client's IP and do geolocation. Maybe it constructs one chunk that only contains reports from clients in country A and another chunk that only contains reports from country B in an effort to skew either chunk's aggregation somehow?\r\n\r\nAgreed, this is a risk worth considering. I imagine there are other things the Leader can do like this already, but you're right that the intention of including the timestamp in the report was to allow the Client to control (at least in part), which batch its report goes in.\r\n\r\nIt's worth pointing out that replay protection requirements haven't changed: Each Aggregator still needs to ensure that a single report doesn't get replayed in multiple chunks.\r\n\r\n> My intuition on how this feature is used is that you want to enable the leader to create a chunk every `min_chunk_size` reports. So maybe we want to enable the client to verify that the chunks defined by the leader are made up of non-overlapping intervals? That would require putting the `Time` component back in a report `Nonce`, but this would use code the helper needs to have anyway to enforce that batch interval queries don't overlap.\r\n\r\nOhhhh that's an interesting idea! Your intuition is right I think. It's a little stricter than what this PR does now. (And thus harder to implement?). The Leader can still mount a similar \"attack\" by only processing reports from the Client population it's interested in. However there is a cost: It'll end up losing the data for all other Clients.\r\n\r\nFeedback on feasibility of the implementation would be useful.",
              "createdAt": "2022-08-06T01:02:28Z",
              "updatedAt": "2022-08-06T01:02:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_csta",
          "commit": {
            "abbreviatedOid": "42f9cf8"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T00:08:25Z",
          "updatedAt": "2022-08-08T18:34:48Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "nit: `QueryConfig` isn't ever transmitted on-the-wire; I think it does not need to be expressed as a struct, and could instead be expressed as a set of properties (similarly to task configurations)",
              "createdAt": "2022-08-08T00:08:26Z",
              "updatedAt": "2022-08-08T18:34:48Z"
            },
            {
              "originalPosition": 145,
              "body": "```suggestion\r\nTo get the aggregate of the next chunk, the Collector issues a \"chunk query\" of\r\n```\r\n\r\n(typo)",
              "createdAt": "2022-08-08T00:37:55Z",
              "updatedAt": "2022-08-08T18:34:48Z"
            },
            {
              "originalPosition": 58,
              "body": "Q: what is the conceptual difference between a \"chunk\" and a \"batch\"? That is, why isn't this type called `BatchId` (and other names made in terms of \"batches\" instead of \"chunks\"?)\r\n\r\nThis is most confusing in interacting with `{min,max}_batch_size`, which determine the acceptable size of a chunk (rather than a batch). I think `max_batch_duration` would also apply to a chunk, rather than a batch.",
              "createdAt": "2022-08-08T02:11:03Z",
              "updatedAt": "2022-08-08T18:34:48Z"
            },
            {
              "originalPosition": 436,
              "body": "> So maybe we want to enable the client to verify that the chunks defined by the leader are made up of non-overlapping intervals?\r\n\r\nI think enforcing non-overlapping timestamps in distinct chunks would introduce a few challenges:\r\n\r\n1. Reports will not arrive ordered by timestamp. Given that, we might receive reports after we have already created a chunk for a given interval; I suppose we'd have no choice but to toss reports like this if aggregating them would size the chunk over `max_batch_size`.\r\n\r\n1. More seriously, we don't generally know if we're done with a given chunk until all of the aggregation jobs in that chunk have concluded: enough reports might be rejected from outstanding aggregation jobs that we no longer meet the `min_batch_size` parameter. If we have already created another chunk to aggregate into when we realize this has happened, I think we are stuck -- we can't finish aggregating the chunk because it doesn't contain enough reports, we can't add additional reports to this chunk since doing so would extend this chunk into the new chunk. OTOH, if we don't allow another chunk to be created until we have finished aggregating reports into the current chunk, we have limited the amount of parallel aggregations we can do: near the end of a chunk, we might have to stop creating new aggregation jobs until we have completed enough oustanding aggregation jobs to confirm that we are done with the current chunk before we can create another chunk & start aggregating reports into it.\r\n\r\n(I do think it would be valuable to somehow limit the leader's power here for the reasons argued upthread; but I think we need to think through these issues.)",
              "createdAt": "2022-08-08T02:47:22Z",
              "updatedAt": "2022-08-08T18:34:49Z"
            },
            {
              "originalPosition": 200,
              "body": "nit: this is the only place that `ReportHeader` is used; inline `ReportHeader` into `Report`?",
              "createdAt": "2022-08-08T02:52:02Z",
              "updatedAt": "2022-08-08T18:34:48Z"
            },
            {
              "originalPosition": 347,
              "body": "I think L1008-L1010 are now redundant with the parameter description that precedes it, remove?",
              "createdAt": "2022-08-08T03:14:18Z",
              "updatedAt": "2022-08-08T18:34:48Z"
            },
            {
              "originalPosition": 526,
              "body": "```suggestion\r\n    fixed-chunk: ChunkId chunk_id;\r\n```\r\n\r\n(typo)",
              "createdAt": "2022-08-08T03:26:51Z",
              "updatedAt": "2022-08-08T18:34:48Z"
            },
            {
              "originalPosition": 152,
              "body": "I think creating an aggregation job and attaching it to a given chunk requires knowing [an overestimate of] the sum of the number of reports successfully aggregated + the number of \"outstanding\" reports in in-progress aggregation jobs. Otherwise, creating an aggregation job may cause the chunk to grow past `max_batch_size`, effectively losing the chunk.\r\n\r\nIf you can update this sum \"transactionally\" with creation of the aggregation job, you can always hit `min_batch_size` exactly.\r\n\r\nIf you can't, I think avoiding chunk overflow requires having enough \"leeway\" (in `max_batch_size - min_batch_size`) such that every worker creating a max-size job at once will not move a chunk that has not yet aggregated `min_batch_size` reports to have aggregated more than `max_batch_size` reports, since every worker might simultaneously see the same information and independently decide to schedule an aggregation job. Another way of putting it is: I think increasing the \"precision\" of the size of your chunks would require reducing parallelism in the number of workers simultaneously aggregating into the chunk; equivalently, increasing worker parallelism to aggregate a task more quickly will lead to decreased \"precision\" in the size of the chunks that are generated.\r\n\r\nIs that sort of what you're thinking (especially in the analysis in the previous paragraph)? I'm also curious if hitting `min_batch_size` exactly provides advantages over just knowing the number of reports in the chunk -- it would be relatively easy to communicate the report count to the collector, and if doing so would let us stop worrying about `max_batch_size` that might be the easier path.",
              "createdAt": "2022-08-08T18:22:58Z",
              "updatedAt": "2022-08-08T18:34:49Z"
            },
            {
              "originalPosition": 114,
              "body": "One thing useful to call out (but probably not add to the spec text) is that collectors may want to send a collect request before reports for the \"query\" have finished arriving. For example, in a time-series task, the collector may want to send a collect request for a batch interval that has not yet expired, with the expectation that the created collect job will not be resolved until the interval has completed.\r\n\r\nChris, you pointed out that this is nice in the sense that it means the collector & the aggregators don't need to agree on the clock. Makes sense to me.\r\n\r\nI think this is even more important for fixed-chunk tasks: the next chunk being \"ready for collection\" is based on the rate of report arrival, which the collector can't see. So I think we _need_ to support the \"aggressive collector\" use-case to support fixed-chunk tasks.\r\n\r\nAll that said, I'm agnostic on whether this kind of detail should make it into the spec text. :) I think mentioning that the the collector can choose whatever intervals it wishes is probably a good idea; I'm less sure of the value of `RECOMMEND`ing a particular sequence of intervals.",
              "createdAt": "2022-08-08T18:34:43Z",
              "updatedAt": "2022-08-08T18:45:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_hJjV",
          "commit": {
            "abbreviatedOid": "42f9cf8"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T19:03:29Z",
          "updatedAt": "2022-08-08T19:03:32Z",
          "comments": [
            {
              "originalPosition": 270,
              "body": "I think L800 (a few lines down from here) needs to change. It currently reads:\r\n\r\n> The leader MUST ignore any report whose nonce contains a timestamp that falls in\r\na batch interval for which it has received at least one collect request from the\r\ncollector.\r\n\r\nNonces no longer have a timestamp. Depending on the task configuration, reports might no longer have a timestamp at all.\r\n\r\nMore importantly, this would block an \"aggressive collector\" which wants to send collect requests before reports have finished arriving for a given batch, with the expectation that the collect job will stay live until the batch is \"over\". Supporting the aggressive collector is a nice-to-have for time-series tasks (since it breaks the need for a collector to have a clock that matches the aggregators', and removes a case where a collector can trigger data loss accidentally), and I think a requirement for fixed-chunk tasks (since batches are determined by the rate of report arrival, and the collector does not have any insight into this, so it can't know when to make another collect request).",
              "createdAt": "2022-08-08T19:03:29Z",
              "updatedAt": "2022-08-08T19:03:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_hd8h",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T20:15:25Z",
          "updatedAt": "2022-08-08T20:15:26Z",
          "comments": [
            {
              "originalPosition": 436,
              "body": "I think this concern is valid, but making sure each chunk has non-overlapping intervals can cause us to lose some late-arriving reports. For example, assuming max_batch_lifetime is 1 for simplicity, and I have a task that can quickly fulfill min chunk size in a short period of time (e.g. 1 minute). If leader has constructed a chunk that meets minimum chunk size, the min and max time of all the reports in this chunk defines the interval for this chunk. Now for the next chunk, leader is only allowed to collect reports that have timestamps later than the max timestamp of the previous chunk. If there are late arriving reports, leader is forced to drop all of them even though they are valid, so the availability of the leader service may take a hit in this case.\r\n\r\nEDIT: just saw @BranLwyd's comments, I think my point mostly echoes your bullet point 1.",
              "createdAt": "2022-08-08T20:15:26Z",
              "updatedAt": "2022-08-08T20:19:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_hVLL",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T19:41:07Z",
          "updatedAt": "2022-08-08T20:17:57Z",
          "comments": [
            {
              "originalPosition": 58,
              "body": "Hmmm yeah. I was trying not to overload the term \"batch\", but (1) I don't think this does and (2) a new term is confusing. I'll update \"chunk\" to \"batch\".",
              "createdAt": "2022-08-08T19:41:07Z",
              "updatedAt": "2022-08-08T20:17:57Z"
            },
            {
              "originalPosition": 80,
              "body": "Agreed, but what's the downside of defining a wire format? ",
              "createdAt": "2022-08-08T19:43:09Z",
              "updatedAt": "2022-08-08T20:17:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_hK4z",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T19:08:38Z",
          "updatedAt": "2022-08-08T20:35:16Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "I see what you mean. I think my only remaining gripe is that calling this query variant `time-series` implies that the other one isn't. Wikipedia's headline definition of time series is \"a series of data points indexed (or listed or graphed) in time order.\" Now that we've put the timestamp back into fixed-chunk reports, those are also time series.\r\n\r\nMore generally, a \"time series\" is a body of data points. This enum is `QueryType`, which should describe how we query some body of data. I think that's why a query type of `interval` feels more appropriate. Basically: a time series is a noun, but this enumeration should list verbs that you do to that noun.",
              "createdAt": "2022-08-08T19:08:38Z",
              "updatedAt": "2022-08-08T20:35:16Z"
            },
            {
              "originalPosition": 114,
              "body": "I think downgrading to lowercase guidance here is appropriate. My reading of [RFC 2119's description of SHOULD/RECOMMENDED](https://datatracker.ietf.org/doc/html/rfc2119#section-3) is that implementations should be very careful before they ignore a RECOMMENDation, but I don't think anything particularly bad happens if a collector doesn't do what is prescribed here.",
              "createdAt": "2022-08-08T19:59:54Z",
              "updatedAt": "2022-08-08T20:35:16Z"
            },
            {
              "originalPosition": 152,
              "body": "My bias is for simplicity in the protocol text, which I think argues for requiring fixed size chunks, and my intuition is that this should be achievable. My intuition (I'm using \"intuition\" as a weasel word here so that I don't actually have to support my arguments) is that the coordination you describe should be achievable in a datastore with [serializability](https://en.wikipedia.org/wiki/Serializability), which I think DAP implementations require anyway in order to do things like anti-replay.\r\n\r\nHowever I think I want to get more implementation experience with the fixed chunk mode before I argue this point further, so let's go ahead with what's here now.\r\n\r\nFinally, we already have #163 to wire up the report count to the collector, orthogonally to what's going on in this PR.",
              "createdAt": "2022-08-08T20:02:12Z",
              "updatedAt": "2022-08-08T20:35:17Z"
            },
            {
              "originalPosition": 97,
              "body": "```suggestion\r\nspecifies a \"batch interval\" that determines the time range for measurements\r\n```",
              "createdAt": "2022-08-08T20:04:53Z",
              "updatedAt": "2022-08-08T20:35:16Z"
            },
            {
              "originalPosition": 230,
              "body": "This wasn't a MUST previously, and we discussed why at the time in [#281](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/281#pullrequestreview-1025110801).",
              "createdAt": "2022-08-08T20:08:24Z",
              "updatedAt": "2022-08-08T20:35:16Z"
            },
            {
              "originalPosition": 436,
              "body": "re: Brandon's points:\r\n\r\n(1): This was already a case we had to handle in the batch interval query, and I think we would handle it the same way here: if a report arrives after the collect job for its chunk has been serviced, it gets ignored.\r\n\r\n(2): Ooh, this is interesting. I think this can be handled, though. Let's say a collector comes along and asks for the result of aggregating chunk ID 4, and then immediately asks for chunk ID 5. The leader will generate a couple of collect job URIs and send those back to the collector -- at this point there's no aggregator commitment to which reports go in which chunk. The leader might then identify a set of candidate reports for chunk 4 and another set for chunk 5. It then carves each set of reports into some number of aggregation jobs and executes them with the helper.\r\n\r\nNow, suppose enough reports are rejected from the candidate set assigned to chunk 4 that `min_batch_size` isn't satisfied. The leader will want to then steal some aggregation jobs from chunk 5 to get chunk 4 over the minimum.\r\n\r\nOK, but what if the aggregation jobs for chunk 5 finished first and the aggregate result was delivered to the collector? In that case, yes, we are stuck and can never satisfy the collect request for chunk 4. But we could assert that the leader can't deliver the results for the collect job for chunk 5 until it has delivered the results for chunk 4. Note that the constraint here is not on when or in what order the aggregators can do the work of preparation. The constraint is on when the results can be delivered, because it's only at that point that the aggregators commit to assigning a set of reports to a chunk.\r\n\r\nSo I think this means that aggregators could still be running all the aggregate jobs in parallel. They just have to require that before you can deliver the results for collect job _n_, the collect jobs for the previous _n-1_ chunks must have been completed.\r\n\r\nThe problem with this is that we want to allow collectors to skip ahead in the stream of chunks. I suppose this could be accomplished by having the collector send DELETE requests to collect job URIs for those chunks it wants to skip?",
              "createdAt": "2022-08-08T20:34:41Z",
              "updatedAt": "2022-08-08T20:35:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_hwQR",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T21:28:03Z",
          "updatedAt": "2022-08-08T21:31:43Z",
          "comments": [
            {
              "originalPosition": 319,
              "body": "VDAF-specifict -> VDAF-specific",
              "createdAt": "2022-08-08T21:28:03Z",
              "updatedAt": "2022-08-08T21:31:43Z"
            },
            {
              "originalPosition": 360,
              "body": "report info should be nonce, time, and extensions?",
              "createdAt": "2022-08-08T21:29:04Z",
              "updatedAt": "2022-08-08T21:31:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_h2sS",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T21:55:31Z",
          "updatedAt": "2022-08-08T21:55:32Z",
          "comments": [
            {
              "originalPosition": 436,
              "body": "@tgeoghegan Regarding bullet point 1, what if each report rounds down its timestamp to the nearest n-hour window (a coarse-grained timestamp to protect client privacy, let's say 4 hours for example)? Leader can create a chunk with just enough reports that have timestamp 00:00, doesn't that mean leader will be forced to drop all future reports that mark its timestamp to be 00:00? and must wait until reports that have timestamp 04:00 to create a new chunk?",
              "createdAt": "2022-08-08T21:55:31Z",
              "updatedAt": "2022-08-08T21:56:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_hvoR",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-08T21:25:14Z",
          "updatedAt": "2022-08-08T22:16:16Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "It's a minor thing -- it makes the standard slightly harder to follow for human readers (if I read an RFC that defines a wire-format for some structure, I'd expect that structure to end up serialized at some point, and might become confused or think I'm missing something if it's not).",
              "createdAt": "2022-08-08T21:25:14Z",
              "updatedAt": "2022-08-08T22:16:16Z"
            },
            {
              "originalPosition": 436,
              "body": "(1) I think this is different than the previous case: previously, the collector could control how long it waited before issuing a collect request (this provided a bound on the maximum _negative_ clock skew/effective latency permissible in a client). All clients with clock skew smaller than this would expect to have their reports aggregated.\r\n\r\nThis new issue will arise any time a chunk fills up, and will risk dropping data by design, with no actor in the system positioned to control for/mitigate the data loss. (Increasing the batch sizes will decrease how often chunks fill up & therefore reduce how frequently we risk data loss, but the data loss risk is still there any time we cut a new batch/chunk.)\r\n\r\n\r\n(2)\r\n\r\n> The constraint is on when the results can be delivered, because it's only at that point that the aggregators commit to assigning a set of reports to a chunk.\r\n\r\nWith how this PR is currently written, aggregators commit to assigning a set of reports to a chunk when they send an `AggregateInitializeReq`, since that request includes the batch ID. Given that, in VDAFs that support it, aggregation will be happening incrementally regardless of the receipt of any collect requests, I think the issue is not fixed by asserting that collect request results must be delivered in-order.\r\n\r\nMaybe that should change? I don't think there's anything that conceptually requires the agg-job-to-batch binding to happen on creation of the aggregation job; OTOH, if we don't do it then, currently none of the aggregation or collection messages would provide the binding, so we'd need to spell out how the binding would be communicated.\r\n\r\nNote that even if agg-job-to-batch binding happened at e.g. time of collect request, this issue might still be tricky: depending on how we previously allocated reports to aggregation jobs, it might not be easy/possible to select a set of aggregation jobs to include in the batch that (a) meets the `{min,max}_batch_size` requirements and (b) does not end up throwing away any reports from other aggregation jobs due to overlapping intervals.\r\n\r\n\r\n(If permissible, it might be simpler to drop `max_batch_size`, since that would make both of these issues much easier to deal with.)",
              "createdAt": "2022-08-08T21:48:37Z",
              "updatedAt": "2022-08-08T22:16:16Z"
            },
            {
              "originalPosition": 230,
              "body": "I think rounding should be suggested (to whatever degree is appropriate based on discussion in the other comment on this line) for both time-series and fixed-chunk tasks, since both task types now contain a timestamp & I suppose the client privacy concern is the same either way.\r\n\r\nThe wrinkle is that fixed-chunk tasks don't have a `min_batch_duration` to round to. Maybe a client-only parameter could be introduced? It'd be nice if a cleverer solution was apparent that didn't add another parameter to the system, but nothing comes to mind.",
              "createdAt": "2022-08-08T21:56:46Z",
              "updatedAt": "2022-08-08T22:16:16Z"
            },
            {
              "originalPosition": 327,
              "body": "```suggestion\r\n    * For fixed-chunks tasks, the Leader specifies a \"batch ID\" that determines\r\n```\r\n\r\n(edit: also, perhaps \"fixed-batch tasks\" rather than \"fixed-chunk tasks\"? this is less obviously correct, but I think \"fixed-chunk\" is the only place the \"chunk\" terminology is still used)",
              "createdAt": "2022-08-08T21:59:03Z",
              "updatedAt": "2022-08-08T22:16:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_m_kp",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T18:20:01Z",
          "updatedAt": "2022-08-09T18:20:01Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "TODO: Ready for text",
              "createdAt": "2022-08-09T18:20:01Z",
              "updatedAt": "2022-08-09T18:20:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_nElY",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T18:28:17Z",
          "updatedAt": "2022-08-09T18:28:18Z",
          "comments": [
            {
              "originalPosition": 152,
              "body": "@BranLwyd as discussed at IETF, the main reason to target a given batch size is that in some statistical applications you need to be able to control for the sample size. This would be important, for example, when using local differential privacy.\r\n\r\nI'd like to start resolving threads where possible. The question left open here is whether requiring `min_batch_size == max_batch_size` is feasible. @tgeoghegan are you alright with resolving this with an OPEN ISSUE in the text?",
              "createdAt": "2022-08-09T18:28:18Z",
              "updatedAt": "2022-08-09T18:28:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_nEvx",
          "commit": {
            "abbreviatedOid": "42f9cf8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T18:28:52Z",
          "updatedAt": "2022-08-09T18:28:52Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "Ack.",
              "createdAt": "2022-08-09T18:28:52Z",
              "updatedAt": "2022-08-09T18:28:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_nG1j",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T18:36:24Z",
          "updatedAt": "2022-08-09T18:36:25Z",
          "comments": [
            {
              "originalPosition": 152,
              "body": "Yes, I'm comfortable revisiting this after we get some experience implementing the fixed chunk mode. However I would urge you to highlight this particular question in the thread you started on the ppm-ietf list just in case someone there has an opinion.",
              "createdAt": "2022-08-09T18:36:24Z",
              "updatedAt": "2022-08-09T18:36:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_npXT",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T20:42:15Z",
          "updatedAt": "2022-08-09T20:42:15Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Dropped the RECOMMENDATION. I'm not sure how to spell the Aggregator behavior, so I left as an open issue. WE can deal with this later.",
              "createdAt": "2022-08-09T20:42:15Z",
              "updatedAt": "2022-08-09T20:42:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_np44",
          "commit": {
            "abbreviatedOid": "59f982b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T20:44:18Z",
          "updatedAt": "2022-08-09T20:44:18Z",
          "comments": [
            {
              "originalPosition": 152,
              "body": "Done.",
              "createdAt": "2022-08-09T20:44:18Z",
              "updatedAt": "2022-08-09T20:44:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_nrFu",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T20:48:59Z",
          "updatedAt": "2022-08-09T20:48:59Z",
          "comments": [
            {
              "originalPosition": 360,
              "body": "Haha yup good catch! ",
              "createdAt": "2022-08-09T20:48:59Z",
              "updatedAt": "2022-08-09T20:48:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_nzJP",
          "commit": {
            "abbreviatedOid": "42f9cf8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T21:20:10Z",
          "updatedAt": "2022-08-09T21:20:11Z",
          "comments": [
            {
              "originalPosition": 270,
              "body": "Good catch. I think the following solves both issues. This also matches the Helper's behavior in {{input-share-batch-validation}}.\r\n\r\n> The Leader MUST ignore any report pertaining to a batch that has already been collected.\r\n\r\nFor time-interval tasks the intended interpretation is \"the Leader rejects the report if the timestamp falls in a batch interval that has been collected\". For fixed-size tasks the intended interpretation is \"The Leader MUST NOT include the report in a previously collected batch.\"",
              "createdAt": "2022-08-09T21:20:11Z",
              "updatedAt": "2022-08-09T21:20:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_nzcH",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T21:21:26Z",
          "updatedAt": "2022-08-09T21:21:27Z",
          "comments": [
            {
              "originalPosition": 230,
              "body": "Oh good, I thought the intention was MUST. How about SHOULD?",
              "createdAt": "2022-08-09T21:21:27Z",
              "updatedAt": "2022-08-09T21:21:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_n0Xe",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T21:25:39Z",
          "updatedAt": "2022-08-09T21:25:39Z",
          "comments": [
            {
              "originalPosition": 230,
              "body": "Right, I don't know what to do about this awkwardness. One way to resolve this is to find a reason not to include the timestamp for fixed-size tasks. (The argument would be that preventing overlapping batch intervals for this task type has marginal benefit or is not feasible.) There is an OPEN ISSUE in the text in case we want to punt.",
              "createdAt": "2022-08-09T21:25:39Z",
              "updatedAt": "2022-08-09T21:25:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_n_oi",
          "commit": {
            "abbreviatedOid": "7b03795"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T22:24:03Z",
          "updatedAt": "2022-08-09T22:24:03Z",
          "comments": [
            {
              "originalPosition": 436,
              "body": "Thanks, everyone, for taking time to help with this. I think we're now at a point where we need to take a step back and think through requirements.\r\n\r\nFirst, in response to feedback from @tgeoghegan and @BranLwyd, not that we have renamed \"time-series\" to \"time-interval\" and \"fixed-chunk\" to \"fized-size\". Instead of \"chunk\" we'll just say \"batch\".\r\n\r\nA quick summary to see if we're on the same page. @tgeoghegan points out a desirable property of time-interval tasks: Given the sequence of queries, a Clients know exactly *which* batch their report would be included in, since we have the following properties:\r\n- (1.) a report can belong to exactly one batch\r\n- (2.) batches are non-overlapping\r\n- (3.) a query determines a unique batch \r\n\r\nFor \"fixed-size\" tasks, we currently guarantee something strictly weaker: a Client is know their report is included in at most one batch, but it does not know which one. This is because we have (1.) and (2.) but not (3.).\r\n\r\nWe don't seem to have a workable solution to this yet:\r\n- (a.) We could have the Client include the batch ID in its report. This is the direct analogue of including the timestamp in the report. However this is a non-starter because it shifts coordination of the batch selection to Clients.\r\n- (b.) We could require that batches don't overlap in time, just as we do for time-interval tasks. @BranLwyd and @junyechen1996 have pointed out various reasons why this is tricky.\r\n\r\nThese difficulties beg the following essential question: **How important is property (3.)?** Without it, the Leader is capable of sorting reports into batches as it pleases. For example:\r\n* The Leader can batch reports by User-Agent, location, and so on. This is is still possible with (3.), but this would at least force the Leader to \"throw away\" reports from clients that are not in the target group, but pertain to the query.\r\n* The Leader can do a kind of weak stuffing attack. Stuffing attacks ares still possible with (3.), but again, it would at least force the Leader to throw away any reports that pertain to the query.",
              "createdAt": "2022-08-09T22:24:03Z",
              "updatedAt": "2022-08-09T22:24:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_oAXN",
          "commit": {
            "abbreviatedOid": "7b03795"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-09T22:28:40Z",
          "updatedAt": "2022-08-09T22:28:41Z",
          "comments": [
            {
              "originalPosition": 436,
              "body": "My take: This threat falls into a big bucket of Sybil-like attacks that I would like to see a more systematic solution for. (Yes Sybil attacks are hard, but they're at least tractable.) Thus I don't think it's worth expending too much effort trying to get (3.). My vote would be to not require it for every query type (and fixed-size queries in particular), but make sure whatever we spell doesn't prevent it.",
              "createdAt": "2022-08-09T22:28:40Z",
              "updatedAt": "2022-08-09T22:28:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_tk0V",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-10T20:46:46Z",
          "updatedAt": "2022-08-10T20:46:46Z",
          "comments": [
            {
              "originalPosition": 230,
              "body": "Thinking about this a little more, I suppose the task doesn't need a new explicit parameter -- clients can freely round their submitted timestamps to any desired resolution already.\r\n\r\nI suppose this will be left as an OPEN ISSUE right now, but one way to resolve would be to say that clients can round their timestamps, and additionally mandate (or perhaps suggest) that clients of a time-series task must round to at most the `min_batch_duration` to keep their reports in the correct batch.",
              "createdAt": "2022-08-10T20:46:46Z",
              "updatedAt": "2022-08-10T20:47:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_toSh",
          "commit": {
            "abbreviatedOid": "7b03795"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks for your patience. :) This LGTM.\r\n\r\nI would definitely like if we could find a way to somehow unify the different query types (lots of places in the spec now basically bifurcate struct content/validation logic/etc), but IIUC this is a somewhat exploratory change to discover the different query types that need to be supported, so merging this LGTM.",
          "createdAt": "2022-08-10T21:00:31Z",
          "updatedAt": "2022-08-10T21:23:44Z",
          "comments": [
            {
              "originalPosition": 143,
              "body": "```suggestion\r\n`min_batch_size` measurements per batch. Doing so, however, may be challenging\r\n```\r\n\r\n(typo)",
              "createdAt": "2022-08-10T21:00:31Z",
              "updatedAt": "2022-08-10T21:23:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_6OtM",
          "commit": {
            "abbreviatedOid": "8e01916"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-15T04:46:54Z",
          "updatedAt": "2022-08-15T04:46:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4_8LQA",
          "commit": {
            "abbreviatedOid": "8e01916"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T14:10:50Z",
          "updatedAt": "2022-08-15T15:29:49Z",
          "comments": [
            {
              "originalPosition": 226,
              "body": "I think coarse grained timestamp should be recommended for all query types, for fixed-size tasks, the exact precision can be determined by the task creator.",
              "createdAt": "2022-08-15T14:10:50Z",
              "updatedAt": "2022-08-15T15:29:49Z"
            },
            {
              "originalPosition": 189,
              "body": "How about naming this ReportMetadata? in my opinion info doesn't quite convey the purpose ",
              "createdAt": "2022-08-15T14:13:01Z",
              "updatedAt": "2022-08-15T15:29:49Z"
            },
            {
              "originalPosition": 436,
              "body": "I don't think property (3) is critical for this feature, since leader can do similar things no matter what collection type is used, with pretty trivial effort. If the leader/collector also authors clients, then it can design a task that apply a malicious filter from clients, I don't think this is the type of threat DAP can address on its own.\r\n\r\n",
              "createdAt": "2022-08-15T15:08:23Z",
              "updatedAt": "2022-08-15T15:29:49Z"
            },
            {
              "originalPosition": 341,
              "body": "Is it worth mentioning that the binding of batch_id and agg_job_ids is permanent? so collect query can be idempotent: query of the same batch_id should return the same result, if max_batch_lifetime > 0. ",
              "createdAt": "2022-08-15T15:14:11Z",
              "updatedAt": "2022-08-15T15:29:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_8q2P",
          "commit": {
            "abbreviatedOid": "8e01916"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T15:51:22Z",
          "updatedAt": "2022-08-15T15:51:23Z",
          "comments": [
            {
              "originalPosition": 226,
              "body": "Done.",
              "createdAt": "2022-08-15T15:51:23Z",
              "updatedAt": "2022-08-15T15:51:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_8rkS",
          "commit": {
            "abbreviatedOid": "8e01916"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T15:53:50Z",
          "updatedAt": "2022-08-15T15:53:50Z",
          "comments": [
            {
              "originalPosition": 189,
              "body": "Done.",
              "createdAt": "2022-08-15T15:53:50Z",
              "updatedAt": "2022-08-15T15:53:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_8tTk",
          "commit": {
            "abbreviatedOid": "8e01916"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T15:59:44Z",
          "updatedAt": "2022-08-15T15:59:44Z",
          "comments": [
            {
              "originalPosition": 341,
              "body": "Maybe, but not here. This would go in the collect sub-protocol I'd think. ",
              "createdAt": "2022-08-15T15:59:44Z",
              "updatedAt": "2022-08-15T15:59:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_8uFJ",
          "commit": {
            "abbreviatedOid": "12d84c0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T16:02:27Z",
          "updatedAt": "2022-08-15T16:02:27Z",
          "comments": [
            {
              "originalPosition": 230,
              "body": "@wangshan made the same suggestion. What I decided to do was replace `min_batch_duration` with `time_precision`, which is used by the Clients to truncate their report and, in the case time-series tasks, to define the min batch duration.",
              "createdAt": "2022-08-15T16:02:27Z",
              "updatedAt": "2022-08-15T16:02:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_9Ce6",
          "commit": {
            "abbreviatedOid": "af23122"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T17:18:56Z",
          "updatedAt": "2022-08-15T17:18:56Z",
          "comments": [
            {
              "originalPosition": 139,
              "body": "Do we want to describe the behaviour when collector calls with `next-batch` before the last `batch-id` has been returned? I'm not sure there is a use case for collecting fixed sized batches in parallel. One of the reasons to have fixed size collection is to close a batch asap so any iterative tasks can move on to the next iteration, if there are two opening `batch-id`s for one task, then I imagine the aggregators would distribute reports to these two batches and make the collection slower for one batch? @cjpatton ",
              "createdAt": "2022-08-15T17:18:56Z",
              "updatedAt": "2022-08-15T17:18:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_9Dwf",
          "commit": {
            "abbreviatedOid": "af23122"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T17:24:00Z",
          "updatedAt": "2022-08-15T17:24:00Z",
          "comments": [
            {
              "originalPosition": 704,
              "body": "nit: `max_batch_size` is the only upper bound that's exclusive, should we make it `<=` ? @cjpatton ",
              "createdAt": "2022-08-15T17:24:00Z",
              "updatedAt": "2022-08-15T17:24:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_9H5z",
          "commit": {
            "abbreviatedOid": "af23122"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T17:40:16Z",
          "updatedAt": "2022-08-15T17:40:16Z",
          "comments": [
            {
              "originalPosition": 139,
              "body": "\r\n> One of the reasons to have fixed size collection is to close a batch asap so any iterative tasks can move on to the next iteration, ...\r\n\r\nIs there anything in the spec that precludes this? \r\n\r\n> if there are two opening `batch-id`s for one task, then I imagine the aggregators would distribute reports to these two batches and make the collection slower for one batch?\r\n\r\nIt seems like this is a matter for the implementation. I agree this behavior is permitted, but the Leader could also fill the first batch first, then the second.\r\n\r\n",
              "createdAt": "2022-08-15T17:40:16Z",
              "updatedAt": "2022-08-15T17:40:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_9IRx",
          "commit": {
            "abbreviatedOid": "af23122"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-15T17:41:44Z",
          "updatedAt": "2022-08-15T17:41:44Z",
          "comments": [
            {
              "originalPosition": 704,
              "body": "Done.",
              "createdAt": "2022-08-15T17:41:44Z",
              "updatedAt": "2022-08-15T17:41:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs4_9LGp",
          "commit": {
            "abbreviatedOid": "72f8d9c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "Looks good to me. I have some opinions about the proposal by @branlwyd and @tgeoghegan , I'll continue in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/273. This PR looks ready to be merged for me.",
          "createdAt": "2022-08-15T17:52:49Z",
          "updatedAt": "2022-08-15T17:52:49Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs4_9UdO",
          "commit": {
            "abbreviatedOid": "72f8d9c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "A couple more minor changes, but given the open questions we have agreed to keep discussing as we implement this, I think this is OK to merge.",
          "createdAt": "2022-08-15T18:30:21Z",
          "updatedAt": "2022-08-15T18:34:33Z",
          "comments": [
            {
              "originalPosition": 152,
              "body": "```suggestion\r\n[OPEN ISSUE: It may be feasible to require a fixed batch size, i.e.,\r\n```",
              "createdAt": "2022-08-15T18:30:21Z",
              "updatedAt": "2022-08-15T18:34:15Z"
            },
            {
              "originalPosition": 80,
              "body": "+1 to not defining this as a struct, for the reasons Bran lays out but also to be consistent with how we discuss task parameters.",
              "createdAt": "2022-08-15T18:31:01Z",
              "updatedAt": "2022-08-15T18:34:15Z"
            },
            {
              "originalPosition": 226,
              "body": "```suggestion\r\n  the start of the UNIX epoch. The client SHOULD round this value down to the nearest\r\n```\r\nThe existing text had \"rounded down\", so let's be consistent with that.",
              "createdAt": "2022-08-15T18:32:10Z",
              "updatedAt": "2022-08-15T18:34:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AHJAL",
          "commit": {
            "abbreviatedOid": "06b4c24"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "This is good. I have some requests for editorial changes, as well as some protocol questions, but it's pretty close to being ready.",
          "createdAt": "2022-08-17T12:14:31Z",
          "updatedAt": "2022-08-17T12:48:09Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "```suggestion\r\nAggregated results are computed based on sets of report, called batches. The Collector\r\ninfluences which reports are used in a batch via a \"query.\" The Aggregators use this query\r\nto carry out the aggregation flow and produce aggregate shares encrypted to the Collector.\r\n```",
              "createdAt": "2022-08-17T12:14:31Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 48,
              "body": "```suggestion\r\nFuture specifications can introduce new query types as needed. A query includes parameters\r\n```",
              "createdAt": "2022-08-17T12:14:56Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 49,
              "body": "```suggestion\r\nused by the Aggregators to select a batch of reports specific to the\r\n```",
              "createdAt": "2022-08-17T12:15:42Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 56,
              "body": "Why not split these out into two separate query types, rather than a single query type with different parameters? ",
              "createdAt": "2022-08-17T12:16:36Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 91,
              "body": "We need to define what all these fields mean. For example, what is the `time_precision` field?",
              "createdAt": "2022-08-17T12:18:01Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 97,
              "body": "```suggestion\r\nwhich measurements are collected over a long period of time. The Collector\r\n```",
              "createdAt": "2022-08-17T12:18:25Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 98,
              "body": "```suggestion\r\nspecifies a \"batch interval\" that determines the time range for reports\r\n```",
              "createdAt": "2022-08-17T12:18:35Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 98,
              "body": "s/measurement/report generally everywhere we are actually operating on reports. ",
              "createdAt": "2022-08-17T12:18:56Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 111,
              "body": "```suggestion\r\n- `min_batch_interval` - The duration of the smallest permitted batch interval.\r\n```",
              "createdAt": "2022-08-17T12:19:52Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 111,
              "body": "Since `time_precision` is confusing to me.",
              "createdAt": "2022-08-17T12:20:04Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 134,
              "body": "```suggestion\r\nFor this query type, the Aggregators group reports into arbitrary batches\r\n```",
              "createdAt": "2022-08-17T12:20:58Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 142,
              "body": "What's the use case for querying by-batch-id again in the future? Poplar?",
              "createdAt": "2022-08-17T12:24:56Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 105,
              "body": "Why is this a query-specific parameter and not something that applies to _all_ types of queries?",
              "createdAt": "2022-08-17T12:26:09Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 219,
              "body": "If we're going to refer to the header, why not actually introduce a header struct?\r\n\r\n```\r\nstruct {\r\n    TaskID task_id;\r\n    Time time;\r\n    Nonce nonce;\r\n    Extension extensions<0..2^16-1>;\r\n} ReportHeader;\r\n```",
              "createdAt": "2022-08-17T12:27:10Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 219,
              "body": "Also, this change seems unrelated to the query type change. Is it possible to apply it in a separate PR?",
              "createdAt": "2022-08-17T12:27:42Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 436,
              "body": "Let's use the open issue to track resolution to this. I'm not convinced that we want to abandon (3) just yet.",
              "createdAt": "2022-08-17T12:40:38Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 681,
              "body": "```suggestion\r\n* For fixed-size tasks, the batch is associated with a \"batch ID\" selected by\r\n```",
              "createdAt": "2022-08-17T12:41:50Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 687,
              "body": "Why would the Collector not be able to opportunistically query by batch ID? What if the IDs were monotonically increasing and the Collector just wanted to iterate over the batches?",
              "createdAt": "2022-08-17T12:45:35Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 640,
              "body": "Can we refactor this such that there are subsections for each type of query? I know that there's some shared logic, but the mixed time/fixed-size validation steps is a bit hard to parse, especially if I'm only implementing one of them.",
              "createdAt": "2022-08-17T12:46:57Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            },
            {
              "originalPosition": 730,
              "body": "Definitely. Why is this even an open issue?",
              "createdAt": "2022-08-17T12:47:25Z",
              "updatedAt": "2022-08-17T12:48:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AJhO2",
          "commit": {
            "abbreviatedOid": "017c549"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T18:45:47Z",
          "updatedAt": "2022-08-17T20:19:40Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "I was trying to avoid the word \"report\", which is not defined until later. But, meh, we can work this out in the future.",
              "createdAt": "2022-08-17T18:45:48Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 56,
              "body": "What this enum is meant to express is that there are two \"subtypes\" for the fixed-size query type: One for getting the \"next batch\" and another for querying a previous batch.\r\n\r\nI suppose we could pull these \"subtypes\" up into the `QueryType` enum, but then you have the problem that there is a many-to-one relationship between query types and query configs. I worry this would create a corner cases we'd have to address in text. (E.g., \"if the query type doesn't match the query config for the task, then abort.\")\r\n\r\nPerhaps there is another alternative?",
              "createdAt": "2022-08-17T18:49:03Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 98,
              "body": "Reports and measurements have a 1:1 correspondence, so I figured this would be clear. Note that we don't define \"report\" at this point in the text.",
              "createdAt": "2022-08-17T18:51:42Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 111,
              "body": "I wanted a term that applies to all query types, in particular because it's used to truncate the report timestamp in all query types. (For time-interval queries, it's also used as the minimum batch duration.) ",
              "createdAt": "2022-08-17T18:56:12Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 219,
              "body": "The header would be\r\n```\r\nstruct {\r\n  TaskID task_id;\r\n  ReportMetadata metadata;\r\n} ReportHeader;\r\n```\r\n`metadata` needs to be defined because it's sent with each `ReportShare` in the agg flow. Given the small delta between header and metadata (just the task ID), I figured it wouldn't be worth it. \r\n\r\nHmm, you're right this change is technically separable. It wasn't in an earlier version of this PR in which the structure of the metadata depended on the query type. (Previously we had included the timestamp only for time-interval queries.) I can split this out if you insist, but it would require a bit of work. If it's all the same to you, I would prefer to keep as-is. ",
              "createdAt": "2022-08-17T19:07:00Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 730,
              "body": "I realized while working on this PR that we should spell this out, if we haven't already. I figured this could happen in a future PR. Maybe I should just file an issue rather than leave this here?",
              "createdAt": "2022-08-17T19:08:23Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 91,
              "body": "I've added a line to punt to the relevant subsection. Will this suffice?\r\n> The parameters pertaining to each query configuration are described in the relevant the subsection below.",
              "createdAt": "2022-08-17T20:10:54Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 142,
              "body": "Yup!",
              "createdAt": "2022-08-17T20:11:10Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 687,
              "body": "We kicked this around a bit. It seems like this could get tricky. For example, there would need to be a mechanism to ensure that that queries are issued in order: Imagine the collector picks batch id = 10,000, then batch id = 1.\r\n\r\nOn balance, explicit batch IDs picked by the leader seemed simpler.",
              "createdAt": "2022-08-17T20:18:21Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            },
            {
              "originalPosition": 640,
              "body": "Good idea, I will work on this now.",
              "createdAt": "2022-08-17T20:19:37Z",
              "updatedAt": "2022-08-17T20:19:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKE--",
          "commit": {
            "abbreviatedOid": "06b4c24"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T20:22:20Z",
          "updatedAt": "2022-08-17T20:22:20Z",
          "comments": [
            {
              "originalPosition": 105,
              "body": "It does, I just didn't spell it that way. I'll take another crack.",
              "createdAt": "2022-08-17T20:22:20Z",
              "updatedAt": "2022-08-17T20:22:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKMyC",
          "commit": {
            "abbreviatedOid": "1241355"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T20:52:31Z",
          "updatedAt": "2022-08-17T20:52:31Z",
          "comments": [
            {
              "originalPosition": 730,
              "body": "Yup, this appears to be redundant: see https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/180. Thanks @tgeoghegan!",
              "createdAt": "2022-08-17T20:52:31Z",
              "updatedAt": "2022-08-17T20:52:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKK6D",
          "commit": {
            "abbreviatedOid": "b416f78"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T20:45:17Z",
          "updatedAt": "2022-08-17T21:33:46Z",
          "comments": [
            {
              "originalPosition": 436,
              "body": "I've retracted #300 since it induces a delay in collection up to the timestamp rounding factor, and the timestamp rounding factor will be on the order of hours in practice.\r\n\r\nI think something like Tim's suggestion to bind aggregation jobs to batches at time of collect request (rather than aggregate initialization) may work to avoid the issues around needing to stop generating aggregation jobs at batch boundaries until we've confirmed the current batch has enough reports, though even with this there are cases where mapping aggregation jobs to batches (while hitting our `{min,max}_batch_size` requirements & without allowing batches to overlap in time) is quite tricky.",
              "createdAt": "2022-08-17T20:56:23Z",
              "updatedAt": "2022-08-17T21:33:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKZxS",
          "commit": {
            "abbreviatedOid": "b416f78"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T21:51:36Z",
          "updatedAt": "2022-08-17T21:51:36Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "What is the idempotency story for `next-batch` requests? I think achieving idempotency requires the interaction to look something like this:\r\n\r\n1. Collector issues `next-batch` collect request to Leader, receives collect job URI. Collector notes that the collect job URI needs to be collected to its own durable storage.\r\n2. Collector polls collect job URI until the Leader gives a completed collect response, records the aggregation value & that the collect job is complete to its own durable storage.\r\n\r\nIf that's accurate, I think that we should spell out that `next-batch` returns the same collect job until the Collector actually collects the job--I think this isn't spelled out currently, and if we don't have those semantics, repeated `next-batch` requests might cause batches to be dropped if recording the collect job URI to the Collector's durable storage fails.\r\n\r\nAlso, if it's acceptable to allow batch IDs to be enumerable (e.g. a counter, as suggested by others), I think a simpler idempotent design would be to have the Collector track which batch ID is next itself, and change `next-batch` to something like `max-batch` to allow Collectors to \"start up\" without knowing what the next batch is. This approach would simplify the Collector's state updates.",
              "createdAt": "2022-08-17T21:51:36Z",
              "updatedAt": "2022-08-17T21:51:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKabS",
          "commit": {
            "abbreviatedOid": "06b4c24"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T21:55:08Z",
          "updatedAt": "2022-08-17T21:55:08Z",
          "comments": [
            {
              "originalPosition": 640,
              "body": "Done, please have a look.",
              "createdAt": "2022-08-17T21:55:08Z",
              "updatedAt": "2022-08-17T21:55:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKpQj",
          "commit": {
            "abbreviatedOid": "06b4c24"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T23:34:41Z",
          "updatedAt": "2022-08-17T23:34:41Z",
          "comments": [
            {
              "originalPosition": 105,
              "body": "I've lifted the parameters that apply to all query types to supersection.",
              "createdAt": "2022-08-17T23:34:41Z",
              "updatedAt": "2022-08-17T23:34:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AKp2t",
          "commit": {
            "abbreviatedOid": "b416f78"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T23:39:38Z",
          "updatedAt": "2022-08-17T23:39:38Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "> What is the idempotency story for `next-batch` requests? I think achieving idempotency requires the interaction to look something like this:\r\n> \r\n>     1. Collector issues `next-batch` collect request to Leader, receives collect job URI. Collector notes that the collect job URI needs to be collected to its own durable storage.\r\n> \r\n>     2. Collector polls collect job URI until the Leader gives a completed collect response, records the aggregation value & that the collect job is complete to its own durable storage.\r\n\r\nThis is accurate.\r\n\r\n> If that's accurate, I think that we should spell out that `next-batch` returns the same collect job until the Collector actually collects the job--I think this isn't spelled out currently, and if we don't have those semantics, repeated `next-batch` requests might cause batches to be dropped if recording the collect job URI to the Collector's durable storage fails.\r\n\r\nEach `next-batch` query would correspond to a different batch, and thus a different collect job. If the Collector queues up a bunch of collect jobs, then each job would still correspond to a different batch, right?\r\n\r\n\r\n> Also, if it's acceptable to allow batch IDs to be enumerable (e.g. a counter, as suggested by others), I think a simpler idempotent design would be to have the Collector track which batch ID is next itself, and change `next-batch` to something like `max-batch` to allow Collectors to \"start up\" without knowing what the next batch is. This approach would simplify the Collector's state updates.\r\n\r\n",
              "createdAt": "2022-08-17T23:39:38Z",
              "updatedAt": "2022-08-17T23:39:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANnXE",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:04:41Z",
          "updatedAt": "2022-08-18T13:04:42Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I agree that, in the current formulation, the subtypes make sense. However, I don't really see the need for the by-batch-id use case. Why would a collector want to repeatedly query the same batch? Poplar?",
              "createdAt": "2022-08-18T13:04:41Z",
              "updatedAt": "2022-08-18T13:04:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANnyV",
          "commit": {
            "abbreviatedOid": "42f9cf8"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:05:49Z",
          "updatedAt": "2022-08-18T13:05:49Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "I agree with Brandon here. Can we describe a configuration the same way we do a task configuration?",
              "createdAt": "2022-08-18T13:05:49Z",
              "updatedAt": "2022-08-18T13:05:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANn29",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:06:01Z",
          "updatedAt": "2022-08-18T13:06:01Z",
          "comments": [
            {
              "originalPosition": 91,
              "body": "That's fine, yeah.",
              "createdAt": "2022-08-18T13:06:01Z",
              "updatedAt": "2022-08-18T13:06:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANogC",
          "commit": {
            "abbreviatedOid": "06b4c24"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:07:43Z",
          "updatedAt": "2022-08-18T13:07:44Z",
          "comments": [
            {
              "originalPosition": 142,
              "body": "OK, well, if we didn't have Poplar, would we still need this? I'm growing less and less convinced that Poplar will be actually usable, so I'm not sure we want to add complexity for the sake of something that will not get used.",
              "createdAt": "2022-08-18T13:07:43Z",
              "updatedAt": "2022-08-18T13:07:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANomU",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:08:00Z",
          "updatedAt": "2022-08-18T13:08:00Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "(Saw the response below -- we can resolve this)",
              "createdAt": "2022-08-18T13:08:00Z",
              "updatedAt": "2022-08-18T13:08:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANpfe",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:10:20Z",
          "updatedAt": "2022-08-18T13:10:20Z",
          "comments": [
            {
              "originalPosition": 219,
              "body": "That's a fine way to spell the header, but isn't the task ID also included in each report? In any case, we refer to this thing called a \"header\" but then don't actually define what it is. Let's either drop the word \"header\", or clarify that header == task ID + metadata. \r\n\r\n(And in the future, we need to stop landing unrelated changes in PRs like this!)",
              "createdAt": "2022-08-18T13:10:20Z",
              "updatedAt": "2022-08-18T13:10:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANp2z",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:11:16Z",
          "updatedAt": "2022-08-18T13:11:16Z",
          "comments": [
            {
              "originalPosition": 227,
              "body": "Here let's say that header consists of (1) task ID and (2) metadata, and then list what the metadata consists of. That way the text is consistent with the struct.",
              "createdAt": "2022-08-18T13:11:16Z",
              "updatedAt": "2022-08-18T13:11:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANqU2",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:12:31Z",
          "updatedAt": "2022-08-18T13:12:32Z",
          "comments": [
            {
              "originalPosition": 640,
              "body": "Looks good!",
              "createdAt": "2022-08-18T13:12:31Z",
              "updatedAt": "2022-08-18T13:12:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANqpV",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-18T13:13:25Z",
          "updatedAt": "2022-08-18T13:13:26Z",
          "comments": [
            {
              "originalPosition": 735,
              "body": "I don't think this requirement is necessary. The Collector should be able to query for batches by ID opportunistically, I think.",
              "createdAt": "2022-08-18T13:13:25Z",
              "updatedAt": "2022-08-18T13:13:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ANq0X",
          "commit": {
            "abbreviatedOid": "6f40954"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Only 1 blocking comment on the constraint that Collectors can't opportunistically query for batch IDs. ",
          "createdAt": "2022-08-18T13:13:54Z",
          "updatedAt": "2022-08-18T13:13:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5AO2qq",
          "commit": {
            "abbreviatedOid": "6c56f18"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "> Only 1 blocking comment on the constraint that Collectors can't opportunistically query for batch IDs.\r\n\r\nI think this point needs a bit more discussion. Can we do so in a new issue?",
          "createdAt": "2022-08-18T16:05:30Z",
          "updatedAt": "2022-08-18T16:25:02Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "Done.",
              "createdAt": "2022-08-18T16:05:30Z",
              "updatedAt": "2022-08-18T16:25:02Z"
            },
            {
              "originalPosition": 219,
              "body": "The task ID is included in each `Report`, but is not sent in each `ReportShare` sent during the agg flow. OTOH the metadata is unique to each `ReportShare`, which is why I've split metadata and task ID.\r\n\r\nAgreed on minimizing PRs. Like I said, it's an artifact of a previous version of this PR. Still, I think it's a useful change. I'm happy to pull it out into a separate PR.",
              "createdAt": "2022-08-18T16:07:59Z",
              "updatedAt": "2022-08-18T16:25:02Z"
            },
            {
              "originalPosition": 227,
              "body": "Good idea. Done!",
              "createdAt": "2022-08-18T16:10:17Z",
              "updatedAt": "2022-08-18T16:25:02Z"
            },
            {
              "originalPosition": 436,
              "body": "I think that approach would drop (3.) for time-interval tasks, which at this point I prefer not to do :) Perhaps there is a version of this that'll work.",
              "createdAt": "2022-08-18T16:12:18Z",
              "updatedAt": "2022-08-18T16:25:02Z"
            },
            {
              "originalPosition": 735,
              "body": "I don't see a way to enable this without creating lots of corner cases. I think we need two things in order to pursue this:\r\n1. What's the motivation? Is there a reason why the current scheme is not workable for Collectors? What's the advantage of knowing the batch ID in advance, beyond not having to store it?\r\n2. A way to address corner cases like: What does the Collector do if the Leader requests batch 9999, then batch 1? If the Leader is aggregating opportunistically (like it may do for Prio), it might have to wait to starting filling batch 9999 until 1, ..., 9998 have been filled.\r\n\r\nThis point has already been discussed at some length on the slack. I'm happy to keep discussing, but could we do so in an issue so we have a chance to level set?",
              "createdAt": "2022-08-18T16:20:32Z",
              "updatedAt": "2022-08-18T16:25:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5APS9M",
          "commit": {
            "abbreviatedOid": "38f1c43"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nice work! Ship it!",
          "createdAt": "2022-08-18T17:36:19Z",
          "updatedAt": "2022-08-18T17:36:19Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5AVnMP",
          "commit": {
            "abbreviatedOid": "b416f78"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-19T22:48:58Z",
          "updatedAt": "2022-08-19T22:48:58Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "(sorry, this comment is a little late to the party)\r\n\r\n> Each next-batch query would correspond to a different batch\r\n\r\nI see -- in that case, I think there is potential for data-loss if the Collector crashes at an inopportune time. Consider the following:\r\n\r\n1. The Leader has a batch ready to be retrieved, call it `X`.\r\n2. The Collector issues a `next-batch` request, receives a collect job corresponding to `X`.\r\n3. Before storing the collect job ID to durable storage or polling for completion, the Collector (or some relevant subcomponent) crashes & loses its state.\r\n\r\nIn this case, I think the Collector has lost batch `X` permanently: it no longer knows the relevant collect job ID, and there is no way to interact with the Leader to retrieve it since further `next-batch` requests will result in different collect jobs being returned.\r\n\r\nI think the best way to fix this might be based on the idea of making the batch IDs enumerable as suggested by #301. I'll add a comment to that issue.",
              "createdAt": "2022-08-19T22:48:58Z",
              "updatedAt": "2022-08-19T22:53:36Z"
            }
          ]
        }
      ]
    },
    {
      "number": 298,
      "id": "PR_kwDOFEJYQs48hQnA",
      "title": "Fix copy-paste error in dap-aggregate-initialize-req media type.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/298",
      "state": "MERGED",
      "author": "MichaelScaria",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "",
      "createdAt": "2022-08-02T16:50:54Z",
      "updatedAt": "2022-08-29T17:56:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9639d4f7c6baa9efc9712d326b2a635edecadd4d",
      "headRepository": "MichaelScaria/draft-ietf-ppm-dap",
      "headRefName": "main",
      "headRefOid": "5061a2f95f23cfd53aedd498f90a9c58547b7114",
      "closedAt": "2022-08-02T17:25:55Z",
      "mergedAt": "2022-08-02T17:25:54Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "5440547227aa58c8b1c799a2a34cc052b2031e6a"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs4_IS1y",
          "commit": {
            "abbreviatedOid": "5061a2f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-02T17:25:48Z",
          "updatedAt": "2022-08-02T17:25:48Z",
          "comments": []
        }
      ]
    },
    {
      "number": 300,
      "id": "PR_kwDOFEJYQs49Sehq",
      "title": "[retracted] Merge \"fixed-interval\" & \"fixed-size\" task types (mostly).",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/300",
      "state": "CLOSED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "# PR is retracted, please don't review.\r\n\r\n([Reasoning for retracting PR](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/300#issuecomment-1218456857))\r\n\r\nAlmost all functionality & message wire formats are the same for the two\r\ntask types again. The largest point of divergence is in the batch\r\nvalidation implemented by the leader & helper: this validation is used\r\nto enforce the desired properties on each batch (having a fixed interval\r\nof time or including a fixed number of reports, respectively).\r\n\r\nSee [motivating discussion](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/273#issuecomment-1213552532) in #273.",
      "createdAt": "2022-08-17T02:42:04Z",
      "updatedAt": "2023-11-27T23:10:40Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton/273/1",
      "baseRefOid": "7891e4b93170e5bb7d2dc3db680cb76edbb051b9",
      "headRepository": null,
      "headRefName": "bran/273/batch-intervals-for-everyone",
      "headRefOid": "39f4d101443f5ba832b12d04eb640dae0a790740",
      "closedAt": "2022-08-19T21:57:17Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Based on https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/273#issuecomment-1218133868 & the goal to have `fixed-size` batches be collected as soon as they are available, I am retracting this PR: it introduces a delay in collection which may be as long as the timestamp rounding factor, which per the linked comment may be on the order of hours.\r\n\r\nBringing this back to draft for now in case some of the text is useful, but please hold off on reviewing.",
          "createdAt": "2022-08-17T20:24:41Z",
          "updatedAt": "2022-08-17T20:24:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for working on this Brandon, it was useful to see this idea spelled out. A good high level goal should be to minimize the differences between various DAP \"modes\", so we should revisit in the future.",
          "createdAt": "2022-08-19T21:59:41Z",
          "updatedAt": "2022-08-19T21:59:41Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5AJZF5",
          "commit": {
            "abbreviatedOid": "39f4d10"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-17T18:17:05Z",
          "updatedAt": "2022-08-17T18:38:35Z",
          "comments": [
            {
              "originalPosition": 173,
              "body": "Too informal for an RFC\r\n```suggestion\r\n  of reports are generated and validated by the leader and helper for this task. It\r\n```",
              "createdAt": "2022-08-17T18:17:06Z",
              "updatedAt": "2022-08-17T18:38:35Z"
            },
            {
              "originalPosition": 341,
              "body": "```suggestion\r\nhaving a zero `time` and all-zero `nonce`, the Leader will choose a batch\r\n```",
              "createdAt": "2022-08-17T18:19:04Z",
              "updatedAt": "2022-08-17T18:38:35Z"
            },
            {
              "originalPosition": 525,
              "body": "Do you mean time_precision here, or something else?\r\n```suggestion\r\ndivisible by `time_precision`, and that\r\n```",
              "createdAt": "2022-08-17T18:20:59Z",
              "updatedAt": "2022-08-17T18:38:35Z"
            },
            {
              "originalPosition": 543,
              "body": "I'm fairly sure we'll still need a bit of flexibility.\r\n```suggestion\r\nthat `N <= max_batch_size`. If the batch size check fails, then the Aggregator\r\n```",
              "createdAt": "2022-08-17T18:25:25Z",
              "updatedAt": "2022-08-17T18:38:35Z"
            },
            {
              "originalPosition": 28,
              "body": "(Just to check that I understand:) A batch `batch1` is \"valid\" if:\r\n1. (Total ordering) `batch1.start.time < batch1.end.time` or `batch1.start.time == batch1.end.time` and `batch1.start.nonce < batch1.end.nonce`\r\n2. (Non-overlapping) There is no prior batch `batch0` that intersects with `batch1`, i.e., there is at least one nonce contained in both batches\r\n3. (Boundary aligned) `batch1.start.time` and `batch1.start.end` are both equal to `0 (mod time_precision)`\r\n\r\nAnything I'm missing?",
              "createdAt": "2022-08-17T18:38:01Z",
              "updatedAt": "2022-08-17T18:38:35Z"
            }
          ]
        }
      ]
    },
    {
      "number": 303,
      "id": "PR_kwDOFEJYQs49tC-w",
      "title": "Make \"task_id\" optional in \"/hpke_config\" endpoint",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/303",
      "state": "MERGED",
      "author": "junyechen1996",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Closes #289.",
      "createdAt": "2022-08-24T09:34:13Z",
      "updatedAt": "2022-09-01T19:54:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8d845c030f9ef3929b00484cdd9ae4e591dd7108",
      "headRepository": "junyechen1996/draft-ietf-ppm-dap",
      "headRefName": "junyec/hpke_config_change",
      "headRefOid": "cb7aa6aeea5823909b8382fb6f80e75cb15035db",
      "closedAt": "2022-09-01T19:54:38Z",
      "mergedAt": "2022-09-01T19:54:38Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "1ee91724360fc7bf7b2477c23e4657f15f0a0f2c"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "looks good, @chris-wood is the size of hpke_configs <1..2^16-1> needs expanding, given that it can contain more than one configs?",
          "createdAt": "2022-08-24T12:42:03Z",
          "updatedAt": "2022-08-24T12:42:03Z"
        },
        {
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Looking good. One high level question I have: How does a client decide whether to add a query parameter or not? Perhaps we should try to spell out this discovery process:\r\n> \r\n> * The Client first sends GET /hpke_config.\r\n> * The Aggregator MAY abort with \"missingTaskID\", in which case the client retries with `task_id`.\r\n> * The Client is of course free to skip the first request if it knows it's going to need to provide a `task_id`.\r\n\r\nDo you mean I should have some texts specifying the behavior when a client doesn't know if a deployment enforces different HPKE configurations for different tasks? Some thoughts I have here:\r\n- As a first step, in the existing protocol text, after this sentence: \"then the aggregator responds with HTTP status code 400 Bad Request and an error of type `missingTaskID`\", I can add: \"The client can then retry the request by specifying a well-formed Task ID as the `task_id` parameter\".\r\n- With the above sentence, I think it's fairly clear that clients should retry the request with a task ID if the problem type is `missingTaskID`. I can try to spell out the discovery process as well, but I don't know if it makes the protocol text unnecessarily long.\r\n- If we need to spell this process out, regarding the \"MAY\" in your second bullet point, should I avoid using it, because I think it is a deployment-specific behavior? I think I can just say \"If the aggregator aborts with `missingTaskID`, client can retry the request by specifying `task_id`.",
          "createdAt": "2022-08-24T23:51:00Z",
          "updatedAt": "2022-08-24T23:52:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Do you mean I should have some texts specifying the behavior when a client doesn't know if a deployment enforces  different HPKE configurations for different tasks? Some thoughts I have here:\r\n\r\nYes :)\r\n\r\n> - As a first step, in the existing protocol text, after this sentence: \"then the aggregator responds with HTTP status code 400 Bad Request and an error of type `missingTaskID`\", I can add: \"The client can then retry the request by specifying a well-formed Task ID as the `task_id` parameter\".\r\n> - With the above sentence, I think it's fairly clear that clients should retry the request with a task ID if the problem type is `missingTaskID`. I can try to spell out the discovery process as well, but I don't know if it makes the protocol text unnecessarily long.\r\n> - If we need to spell this process out, regarding the \"MAY\" in your second bullet point, should I avoid using it, because I think it is a deployment-specific behavior? I think I can just say \"If the aggregator aborts with `missingTaskID`, client can retry the request by specifying `task_id`.\r\n\r\nI think spelling it out is a good idea. It shouldn't be too much text, but it may require a bit of refactoring. \"MAY\" is intended to capture deployment-specific behavior, so it's appropriate here.",
          "createdAt": "2022-08-24T23:56:28Z",
          "updatedAt": "2022-08-24T23:56:28Z"
        },
        {
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> I think spelling it out is a good idea. It shouldn't be too much text, but it may require a bit of refactoring. \"MAY\" is intended to capture deployment-specific behavior, so it's appropriate here.\r\n\r\nSounds good. I split the texts into three paragraphs: the first paragraph introduces the `/hpke_config` endpoint. The second paragraph talks about a deployment MAY enforce different HPKE configurations for different tasks, and different cases of aggregators' aborting. The third paragraph spells out client's discovery process of whether `task_id` is required. Let me know if it makes sense to you.",
          "createdAt": "2022-08-25T04:44:36Z",
          "updatedAt": "2022-08-25T04:44:36Z"
        },
        {
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> LGTM! One minor question for reviewers, which we can punt on.\r\n\r\nThanks! I just squashed commits into one.",
          "createdAt": "2022-08-25T20:45:22Z",
          "updatedAt": "2022-08-25T20:45:22Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "These changes were approved about a week ago and there have been no comments since, so I am merging.",
          "createdAt": "2022-09-01T19:54:31Z",
          "updatedAt": "2022-09-01T19:54:31Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5AoYxs",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-08-24T18:33:50Z",
          "updatedAt": "2022-08-24T18:35:44Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "`unrecognizedMessage` and HTTP 400 seem inappropriate. The problem isn't an incorrectly encoded message, it's that this particular aggregator chooses not to implement this functionality. If a client does `GET /hpke_config` and the server doesn't support it, it should get some indication that it should now try `GET /hpke_config?task_id=[task_id]`.\r\n\r\n[HTTP 422](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422) seems pretty appropriate for this case (\"the server understands the content type of the request entity, and the syntax of the request entity is correct, but it was unable to process the contained instructions\"). I think we should use that status code and define a new problem document type (in section 3.1 errors) for this specific case.",
              "createdAt": "2022-08-24T18:33:50Z",
              "updatedAt": "2022-08-24T18:35:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AoaqF",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-08-24T18:38:23Z",
          "updatedAt": "2022-08-24T18:49:11Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "```suggestion\r\nparameters. A deployment may choose to use different HPKE configurations\r\nfor different tasks, so clients MAY specify a query parameter `task_id`\r\n```",
              "createdAt": "2022-08-24T18:38:23Z",
              "updatedAt": "2022-08-24T18:49:11Z"
            },
            {
              "originalPosition": 20,
              "body": "+1 to new abort type (e.g., \"missingTaskId\"), but @tgeoghegan, we currently we return the same status code for all abort cases (400). It might be a good idea to let the status code differ for different abort types, but we should leave that out of this change I think.",
              "createdAt": "2022-08-24T18:43:11Z",
              "updatedAt": "2022-08-24T18:49:11Z"
            },
            {
              "originalPosition": 29,
              "body": "I don't think this adequately addresses #248:\r\n1. You have defined a new struct for a sequence of HPKE configs, but you have not changed the return type (see line 670 above).\r\n2. This hasn't been discussed in the issue yet, but merely allowing the server to return multiple HPKE configs doesn't totally solve the problem. What is needed, I think, is a mechanism for allowing the client to indicate which cipher suites its supports.\r\n\r\nIn any case, I think we should push this change to a future PR. Best practices for spec review (as well as code review!) is to keep changes minimal.",
              "createdAt": "2022-08-24T18:49:06Z",
              "updatedAt": "2022-08-24T18:49:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Aog8o",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T18:57:59Z",
          "updatedAt": "2022-08-24T18:57:59Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "That's fair enough. The problem document type will make this explicit enough, and [RFC 9205](https://httpwg.org/specs/rfc9205.html#using-http-status-codes) recommends using the most general possible HTTP status anyway (in this case, 400).",
              "createdAt": "2022-08-24T18:57:59Z",
              "updatedAt": "2022-08-24T18:57:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ApcXM",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:12:00Z",
          "updatedAt": "2022-08-24T23:12:01Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "Sounds good, I added a new `missingTaskId` problem type under {{Errors}}.",
              "createdAt": "2022-08-24T23:12:00Z",
              "updatedAt": "2022-08-24T23:12:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ApcbM",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:12:28Z",
          "updatedAt": "2022-08-24T23:12:29Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "Fixed now.",
              "createdAt": "2022-08-24T23:12:28Z",
              "updatedAt": "2022-08-24T23:12:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Apc-Y",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:16:32Z",
          "updatedAt": "2022-08-24T23:16:32Z",
          "comments": [
            {
              "originalPosition": 29,
              "body": "Sounds good, I reverted the changes on returning a vector of `HpkeConfig`. Previously I changed line 670 to indicate the endpoint would return a vector of `HpkeConfig` values, maybe I didn't explicitly say the return type would change to `HpkeConfigResp`.\r\n\r\nMy initial thinking was the endpoint would return all `HpkeConfig` that it supports, and clients can use any of them with a cipher suite that it recognizes. But I'm fine with addressing this in a separate PR if that's clearer. ",
              "createdAt": "2022-08-24T23:16:32Z",
              "updatedAt": "2022-08-24T23:16:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Apdus",
          "commit": {
            "abbreviatedOid": "499d9d0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:22:14Z",
          "updatedAt": "2022-08-24T23:22:14Z",
          "comments": [
            {
              "originalPosition": 29,
              "body": "In practice that might decrease the likelihood of failing to negotiate an HPKE suite, but I think we can do a bit better. In any case, this can/should be handled separately.",
              "createdAt": "2022-08-24T23:22:14Z",
              "updatedAt": "2022-08-24T23:22:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Apd9i",
          "commit": {
            "abbreviatedOid": "159d4d2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Looking good. One high level question I have: How does a client decide whether to add a query parameter or not? Perhaps we should try to spell out this discovery process:\r\n* The Client first sends GET /hpke_config.\r\n* The Aggregator MAY abort with \"missingTaskID\", in which case the client retries with `task_id`.\r\n* The Client is of course free to skip the first request if it knows it's going to need to provide a `task_id`.",
          "createdAt": "2022-08-24T23:23:52Z",
          "updatedAt": "2022-08-24T23:30:05Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "While precise, this description of error is hard to parse without knowing the protocol context. What's going on here, intuitively?",
              "createdAt": "2022-08-24T23:23:52Z",
              "updatedAt": "2022-08-24T23:30:05Z"
            },
            {
              "originalPosition": 4,
              "body": "Also, @tgeoghegan: would you prefer `missingTaskId` or `missingTaskID`? I think the latter might be more consistent.",
              "createdAt": "2022-08-24T23:24:31Z",
              "updatedAt": "2022-08-24T23:30:05Z"
            },
            {
              "originalPosition": 26,
              "body": "```suggestion\r\nHTTP GET request, then the aggregator MUST respond with HTTP status code 400 Bad Request\r\n```",
              "createdAt": "2022-08-24T23:25:17Z",
              "updatedAt": "2022-08-24T23:30:05Z"
            },
            {
              "originalPosition": 26,
              "body": "This could be stated a bit simpler as:\r\n> then the Aggregator MUST abort with error `missingTaskId`.",
              "createdAt": "2022-08-24T23:25:58Z",
              "updatedAt": "2022-08-24T23:30:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ApgiL",
          "commit": {
            "abbreviatedOid": "159d4d2"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:41:12Z",
          "updatedAt": "2022-08-24T23:41:12Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "> While precise, this description of error is hard to parse without knowing the protocol context. What's going on here, intuitively?\r\n\r\nShould I add on to the above text with \"if a deployment enforces different HPKE configurations for different tasks\"?\r\n\r\nRegarding TaskId vs TaskID, I think the type definition is `TaskId` at line 608, but there are struct fields where `TaskID` is used, so it seems like there is some inconsistency here. I was trying to follow the original type definition.",
              "createdAt": "2022-08-24T23:41:12Z",
              "updatedAt": "2022-08-24T23:41:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AphRb",
          "commit": {
            "abbreviatedOid": "159d4d2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:47:31Z",
          "updatedAt": "2022-08-24T23:47:31Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "> Should I add on to the above text with \"if a deployment enforces different HPKE configurations for different tasks\"?\r\n\r\nI would simplify it. How about:\r\n```\r\nHPKE config requested without specifying the task ID\r\n```\r\n\r\n> Regarding TaskId vs TaskID, I think the type definition is `TaskId` at line 608, but there are struct fields where `TaskID` is used, so it seems like there is some inconsistency here. I was trying to follow the original type definition.\r\n\r\nAh OK, nice catch. Let's correct the inconsistency here. It seems like the preference is likely to be \"ID\".\r\n\r\n",
              "createdAt": "2022-08-24T23:47:31Z",
              "updatedAt": "2022-08-24T23:47:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AqFoO",
          "commit": {
            "abbreviatedOid": "159d4d2"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T04:42:16Z",
          "updatedAt": "2022-08-25T04:42:16Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "I added the MUST keyword, but still kept the HTTP status code, to be more explicit.",
              "createdAt": "2022-08-25T04:42:16Z",
              "updatedAt": "2022-08-25T04:42:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Aua_m",
          "commit": {
            "abbreviatedOid": "159d4d2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T18:23:43Z",
          "updatedAt": "2022-08-25T18:23:43Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "[RFC 7807](https://www.rfc-editor.org/rfc/rfc7807.html#section-4) doesn't have any obvious guidance I can see, but if we look at [ACME aka RFC 8555](https://www.rfc-editor.org/rfc/rfc8555.html#section-6.7)'s errors, it has types like `badCSR`, `caa` and `dns`, which I think would lead us to using `missingTaskId`, and that would be consistent with the struct definitions elsewhere. But I'm happy with either; the ambiguous case problems weren't introduced by this change and can always be cleaned up later in the standardization process.",
              "createdAt": "2022-08-25T18:23:43Z",
              "updatedAt": "2022-08-25T18:23:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AufC6",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-08-25T18:38:00Z",
          "updatedAt": "2022-08-25T18:44:59Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "I think MUST isn't right in this case, because it's not a violation of the protocol for the client to not send a `task_id` query parameter when the aggregator lacks a \"global\" HPKE config. Same goes for the one one line 669, because an aggregator is always free to respond to a request without the `task_id` query parameter.",
              "createdAt": "2022-08-25T18:38:01Z",
              "updatedAt": "2022-08-25T18:44:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AutT2",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T19:15:13Z",
          "updatedAt": "2022-08-25T19:15:14Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "I think this paragraph tries to characterize the behaviors of an aggregator, when the deployment needs to enforce different HPKE configurations for different tasks. In this case, clients are required to send `task_id`, and aggregators are required to abort if no `task_id` is included in the request. Should I be clearer about the assumptions in this paragraph, or should I relax the \"MUST\" keyword, perhaps to \"SHOULD\"?",
              "createdAt": "2022-08-25T19:15:13Z",
              "updatedAt": "2022-08-25T19:15:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Auv5m",
          "commit": {
            "abbreviatedOid": "159d4d2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T19:23:10Z",
          "updatedAt": "2022-08-25T19:23:11Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Works for me",
              "createdAt": "2022-08-25T19:23:10Z",
              "updatedAt": "2022-08-25T19:23:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AuwKD",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T19:24:08Z",
          "updatedAt": "2022-08-25T19:25:37Z",
          "comments": [
            {
              "originalPosition": 37,
              "body": "nit: Wrap lines at 80 characters",
              "createdAt": "2022-08-25T19:24:08Z",
              "updatedAt": "2022-08-25T19:25:37Z"
            },
            {
              "originalPosition": 26,
              "body": "That's fine, but if later on we decide to change the status code, we'll have to update here as well. That's why it's better to specify something once.",
              "createdAt": "2022-08-25T19:25:37Z",
              "updatedAt": "2022-08-25T19:25:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AuztA",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T19:37:55Z",
          "updatedAt": "2022-08-25T19:38:35Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "I agree with Tim here. The problem is that the Client may not know a priori that the Aggregator enforces HPKE key separation per task. I think a different tack is needed.\r\n\r\nIn the paragraph above where you specify the GET request, you should also add: \"The Client MAY include the task ID with the query parameters. [Spell out what the path looks like.]\"\r\n\r\nIn this paragraph, you should start off with: \"The Aggregator is free to use different HPKE configurations for each task with which it's configured. If the task ID is missing from the Client's request, the Aggregator MAY abort with error \"missingTaskID\", in which case the Client SHOULD retry the request with the task ID included.\"\r\n\r\nThis accomplishes two things:\r\n* It provides a fail-safe way to obtain an HPKE config, regardless of how the Aggregator is configured.\r\n* It permits the optional, deployment-specific beahvior.",
              "createdAt": "2022-08-25T19:37:55Z",
              "updatedAt": "2022-08-25T19:39:29Z"
            },
            {
              "originalPosition": 35,
              "body": "I would fold this into the previous paragraph, as described above.",
              "createdAt": "2022-08-25T19:38:26Z",
              "updatedAt": "2022-08-25T19:38:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Au-3K",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T20:20:14Z",
          "updatedAt": "2022-08-25T20:20:14Z",
          "comments": [
            {
              "originalPosition": 37,
              "body": "Fixed now.",
              "createdAt": "2022-08-25T20:20:14Z",
              "updatedAt": "2022-08-25T20:20:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Au_UD",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "junyechen1996",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T20:22:05Z",
          "updatedAt": "2022-08-25T20:22:05Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "@tgeoghegan @cjpatton Please take a look again at the revised wording based on Chris's suggestions.",
              "createdAt": "2022-08-25T20:22:05Z",
              "updatedAt": "2022-08-25T20:22:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AvE-I",
          "commit": {
            "abbreviatedOid": "40008b7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM! One minor question for reviewers, which we can punt on.",
          "createdAt": "2022-08-25T20:37:55Z",
          "updatedAt": "2022-08-25T20:38:43Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "(Orthogonal to this PR, so we can clean up later if needed.) @tgeoghegan is 404 correct here you think?",
              "createdAt": "2022-08-25T20:37:55Z",
              "updatedAt": "2022-08-25T20:38:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AvFlT",
          "commit": {
            "abbreviatedOid": "d1a7ec2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T20:40:17Z",
          "updatedAt": "2022-08-25T20:40:18Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Looks good to me.",
              "createdAt": "2022-08-25T20:40:17Z",
              "updatedAt": "2022-08-25T20:40:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AvMWK",
          "commit": {
            "abbreviatedOid": "40008b7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T21:05:20Z",
          "updatedAt": "2022-08-25T21:05:20Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "My reading of [RFC 9205's discussion of HTTP statuses](https://www.rfc-editor.org/rfc/rfc9205#name-using-http-status-codes) is that we should lean toward HTTP 400 and communicate more specific issues with the problem document, but [I made a note in #278](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/278#issuecomment-1226237695) that we should take a look at all this, so I don't think we need to iterate on that in this PR.",
              "createdAt": "2022-08-25T21:05:20Z",
              "updatedAt": "2022-08-25T21:05:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AvPmh",
          "commit": {
            "abbreviatedOid": "cb7aa6a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This works, thank you for iterating on it with us!",
          "createdAt": "2022-08-25T21:20:16Z",
          "updatedAt": "2022-08-25T21:20:16Z",
          "comments": []
        }
      ]
    },
    {
      "number": 304,
      "id": "PR_kwDOFEJYQs49t8Dp",
      "title": "Adding max_task_lifetime to task parameters",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/304",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Closes #291.",
      "createdAt": "2022-08-24T13:01:23Z",
      "updatedAt": "2022-09-15T18:19:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a464af7f5d5aadfffb62605842b3ba9759974438",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "max-task-lifetime",
      "headRefOid": "4fc7b3e75feae29ad6c9eb9069b6e15670fe595a",
      "closedAt": "2022-09-15T18:19:32Z",
      "mergedAt": "2022-09-15T18:19:32Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "2f72dd9b534cca0f96508f62bdde30f0afc35dea"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton I have made the definition more precise, in the process changing the meaning of this field a bit, please have another pass.",
          "createdAt": "2022-08-26T09:15:22Z",
          "updatedAt": "2022-08-26T09:15:22Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> In particular, do we need to spell out how the Leader/Helper rejects reports with time stamps that exceed the max batch lifetime?\r\n\r\ndo you mean we should specify whether expired reports should be result in upload error or be dropped silently?",
          "createdAt": "2022-08-31T23:33:42Z",
          "updatedAt": "2022-08-31T23:33:42Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> Suppose the Helper receives an aggregation job for a task that has not yet \"expired\" but contains at least one report whose timestamp is passed the expiration date. What should it do?\r\n> \r\n> I think it would be appropriate to add a `ReportShareError` variant for this case.\r\n\r\ngood point, let me add that",
          "createdAt": "2022-09-07T11:36:45Z",
          "updatedAt": "2022-09-07T11:36:45Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@wangshan please squash this branch and fix the build issue so that it's ready to merge :)",
          "createdAt": "2022-09-08T15:58:36Z",
          "updatedAt": "2022-09-08T15:58:36Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton squashed and ready to merge",
          "createdAt": "2022-09-08T17:18:11Z",
          "updatedAt": "2022-09-08T17:18:11Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Aof1b",
          "commit": {
            "abbreviatedOid": "11a2418"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "We will need to be more precise about client/aggregator/collector behavior. Specifically, the text needs to prescribe optional, or even mandatory behavior. For example:  \"Aggregators MAY (or MUST?) reject reports whose timestamps exceed `max_task_lifetime`.\"",
          "createdAt": "2022-08-24T18:54:00Z",
          "updatedAt": "2022-08-24T18:55:54Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "\"maximum timestamp of a task\" is a bit ill-defined. Can you be more precise about what this means? Is it that reports with timestamps exceeding the max_task_lifetime are rejected? Or is it that requests are aborted if they arrive after max_task_lifetime?",
              "createdAt": "2022-08-24T18:54:00Z",
              "updatedAt": "2022-08-24T18:55:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AzKO-",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "This is looking pretty good to me. One thing we might be missing is report rejection during the aggregation flow. In particular, do we need to spell out how the Leader/Helper rejects reports with time stamps that exceed the max batch lifetime?",
          "createdAt": "2022-08-26T15:40:57Z",
          "updatedAt": "2022-08-26T16:06:04Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "```suggestion\r\n  to this task. The task is considered completed after this timestamp. Aggregators\r\n  MAY reject reports that have timestamps later than `max_task_lifetime`.\r\n```",
              "createdAt": "2022-08-26T15:40:57Z",
              "updatedAt": "2022-08-26T16:06:04Z"
            },
            {
              "originalPosition": 27,
              "body": "I think this can be merged with the paragraph on line 785 above. Basically there are now two reasons a report can be late:\r\n* its batch has been collected (time-series tasks)\r\n* the max task lifetime has elapsed.",
              "createdAt": "2022-08-26T15:43:28Z",
              "updatedAt": "2022-08-26T16:06:04Z"
            },
            {
              "originalPosition": 37,
              "body": "This paragraph isn't normative.\r\n```suggestion\r\nBefore an Aggregator responds to a CollectReq or AggregateShareReq, it must\r\n```",
              "createdAt": "2022-08-26T15:48:18Z",
              "updatedAt": "2022-08-26T16:06:04Z"
            },
            {
              "originalPosition": 44,
              "body": "I don't think this needs to be specified.\r\n1. The behavior is optional, so MUST is not appropriate.\r\n2. If an Aggregator has \"retired\" a task via max_task_lifetime, then it will do this anyway.\r\n\r\nI suggest reverting this change.",
              "createdAt": "2022-08-26T15:54:19Z",
              "updatedAt": "2022-08-26T16:06:04Z"
            },
            {
              "originalPosition": 56,
              "body": "nit: Wrap lines at 80 characters.",
              "createdAt": "2022-08-26T16:03:02Z",
              "updatedAt": "2022-08-26T16:06:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A0nHQ",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "There's an ambiguity here: is `task.max_batch_lifetime` an instant in time, or a timestamp (i.e., an encoding of an instant in time that might appear in a protocol message)?\r\n\r\n",
          "createdAt": "2022-08-26T21:23:46Z",
          "updatedAt": "2022-08-26T21:51:30Z",
          "comments": [
            {
              "originalPosition": 68,
              "body": "We already define type `Time` in terms of the Unix epoch, so no need to refer to that here.\r\n```suggestion\r\nFurthermore, the aggregators must store data related to a task as long as the current time\r\nhas not passed this task's `max_task_lifetime`. Aggregator MAY delete the task and all\r\ndata pertaining to this task after `max_task_lifetime`. Implementors SHOULD provide for some\r\nleeway so collector can collect the batch after some delay.\r\n```",
              "createdAt": "2022-08-26T21:23:47Z",
              "updatedAt": "2022-08-26T21:51:30Z"
            },
            {
              "originalPosition": 15,
              "body": "Let's not add new language referencing HTTP error codes more specific than 400 or 500 since [RFC 9205 cautions against that](https://www.rfc-editor.org/rfc/rfc9205#name-using-http-status-codes). In any case, I don't think we should require that aggregators stop advertising HPKE config for some task ID at any moment in time. The reason is that HPKE configs are likely to be cached for a long time (elsewhere the document explicitly suggests caching them for days), meaning it'd be difficult for implementations to meaningfully guarantee that they'll stop serving a task's HPKE config at the precise instant of task expiration. I don't see any harm in an aggregator continuing to serve expired HPKE configs because the subsequent upload requests to an expired task will reliably fail anyway.",
              "createdAt": "2022-08-26T21:26:43Z",
              "updatedAt": "2022-08-26T21:51:30Z"
            },
            {
              "originalPosition": 27,
              "body": "Should the leader make this determination based on the timestamp in the report, or based on the leader's perception of the current time when it receives the report?\r\n\r\nPut another way: suppose a task has `max_batch_lifetime` = `t_1`. The leader receives a report with timestamp `t_0 < t_1`. The leader checks its own clock and determines that the current time is `t_2 > t_1`. Should the report be rejected because it was too late by the time the leader saw it, or accepted because its timestamp indicates the measurement was taken before the end of the task?\r\n\r\nI think it's got to be the former (because otherwise clients could send back-dated reports long after a task has expired), in which case this should be rewritten to discuss time of report reception rather than the timestamp.",
              "createdAt": "2022-08-26T21:33:59Z",
              "updatedAt": "2022-08-26T21:51:30Z"
            },
            {
              "originalPosition": 4,
              "body": "I think defining this as a timestamp (i.e., the encoding of an instant in time that might appear in a message) is wrong. The `max_task_lifetime` is an instant in time.\r\n\r\n`lifetime` also seems like the wrong word. For one thing, it collides awkwardly with `max_batch_lifetime` (which refers to a totally different quantity). For another, \"lifetime\" implies to me a span of time, or a duration, but this is a single instant. Would `task_expiration` be better?",
              "createdAt": "2022-08-26T21:41:50Z",
              "updatedAt": "2022-08-26T21:51:30Z"
            },
            {
              "originalPosition": 44,
              "body": "I don't think we need this paragraph, but for a different reason: it should be OK to do aggregations on a task after the aggregators have stopped accepting client reports for it. Let's say a task has `max_task_lifetime` `t` and it takes `d` seconds to run the collect protocol. So far as clients know, it is OK for them to continue submitting reports right up until instant `t`. As written, the collector must initiate the collect protocol at time `t - d`. So that means any reports that arrive during the interval `(t - d, t)` won't be included in the aggregation.\r\n\r\nThe other problem is that there's no good way in the protocol for the collector to figure out what `d` is and thus figure out how late it can issue a collect request without risking losing an aggregation because the task expired.\r\n\r\nIn fact, that seems to be implied by the text added in \"Reducing storage requirements\".",
              "createdAt": "2022-08-26T21:50:21Z",
              "updatedAt": "2022-08-26T21:51:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BH4nK",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T22:39:23Z",
          "updatedAt": "2022-08-31T22:39:23Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I agree `task_expiration` is a better name if it's defined as a timestamp.\r\n\r\nWhat's the main concern of encoding an instant in time in the protocol message (sorry if this is  obvious). Defining it as a duration will make it very hard for client to check this, since a client doesn't know the starting time of a task. ",
              "createdAt": "2022-08-31T22:39:23Z",
              "updatedAt": "2022-08-31T22:39:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BH4_k",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T22:41:57Z",
          "updatedAt": "2022-08-31T22:41:58Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "will do but the current doc doesn't seem to follow that strictly :)",
              "createdAt": "2022-08-31T22:41:57Z",
              "updatedAt": "2022-08-31T22:41:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BH8Rh",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T23:05:06Z",
          "updatedAt": "2022-08-31T23:05:06Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "I'll revert the change.\r\n\r\n@tgeoghegan The current description indeed doesn't say explicitly how soon a task should be retired after expiry, IMO it should be left to the deployment. Collector should know `t` and try to collect before `t` expires.",
              "createdAt": "2022-08-31T23:05:06Z",
              "updatedAt": "2022-08-31T23:05:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BH92r",
          "commit": {
            "abbreviatedOid": "cbb9bbf"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T23:16:53Z",
          "updatedAt": "2022-08-31T23:16:53Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "@tgeoghegan I think it should be based on a timestamp in the report. Assuming we want to give client a chance to check this, it can't be a duration since client may \"wake up\" at very different times.\r\nEven for aggregators, if we determine by arrival time, then we could lose reports due to any latency caused on route to leader. The threat you mentioned is real, but attacker would have to hijack a large number of clients to make this attack effective.",
              "createdAt": "2022-08-31T23:16:53Z",
              "updatedAt": "2022-08-31T23:16:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BIAqc",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T23:39:26Z",
          "updatedAt": "2022-08-31T23:39:26Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "having said that, I realised that given the `time_precision` is used to blur client timestamp, if we really want client to check this expiry time, then we should round (up?) `task_expiry` with `time_precision` too, otherwise if time_expiry is close to the rounded timestamp, people could figure out a more accurate timestamp of client.",
              "createdAt": "2022-08-31T23:39:26Z",
              "updatedAt": "2022-08-31T23:39:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BMvPA",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-01T18:12:35Z",
          "updatedAt": "2022-09-01T18:12:36Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I think the distinction is that `task_expiration` should be defined as an abstract instant in time, not a timestamp representing that instant. Since the expiration is going to be shared out of bounds, and won't appear in encoded form anywhere in the protocol, it would be cleanest to define it as a point in time, and not define a timestamp encoding for it that the protocol doesn't use.",
              "createdAt": "2022-09-01T18:12:36Z",
              "updatedAt": "2022-09-01T18:12:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BM7G2",
          "commit": {
            "abbreviatedOid": "35663d3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-01T18:55:17Z",
          "updatedAt": "2022-09-01T19:09:48Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I'm being fussy about the distinction between an instant in time and a timestamp which is a representation of an instant in time. I'm not suggesting that the type of this value be a duration.\r\n```suggestion\r\n* `task_expiration`: The time up to which clients are allowed to upload\r\n  to this task. The task is considered completed after this time. Aggregators\r\n```",
              "createdAt": "2022-09-01T18:55:17Z",
              "updatedAt": "2022-09-01T19:09:48Z"
            },
            {
              "originalPosition": 21,
              "body": "```suggestion\r\nprivacy violation. (Note that the Helpers enforce this as well.) The Leader MAY\r\nignore any reports whose timestamp is past the task's `task_expiration`.\r\nIn addition, the leader SHOULD abort the upload protocol\r\nand alert the client with error \"reportTooLate\". Client MAY choose to opt out\r\nof the task if its own clock has passed `task_expiration`.\r\n```",
              "createdAt": "2022-09-01T18:56:09Z",
              "updatedAt": "2022-09-01T19:09:48Z"
            },
            {
              "originalPosition": 30,
              "body": "Looks like a copy-paste error. Should these lines be deleted?",
              "createdAt": "2022-09-01T18:56:45Z",
              "updatedAt": "2022-09-01T19:09:48Z"
            },
            {
              "originalPosition": 27,
              "body": "I think you're right that it makes sense to trust the timestamp in the report.\r\n\r\nCan you expand on why evaluating `task_expiration` could leak information about the report? Which actor (the client, the aggregator, a network observer) could learn something about the true time a measurement was taken based on what observable behavior of which actor?",
              "createdAt": "2022-09-01T19:09:48Z",
              "updatedAt": "2022-09-01T19:09:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BM_6R",
          "commit": {
            "abbreviatedOid": "35663d3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I think the text in its current iteration accomplishes what it should, which is that clients may upload reports right up until `task_expiration`, and collectors can initiate the collect flow _after_ `task_expiration`. I left a few more comments about word choices and some typos but I think this is OK to go forward.",
          "createdAt": "2022-09-01T19:11:55Z",
          "updatedAt": "2022-09-01T19:11:55Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BV_Ux",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-05T11:11:46Z",
          "updatedAt": "2022-09-05T11:11:47Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "@tgeoghegan for e.g. if the precision is in 1hr, and \r\n1. one client has rounded it's true timestamp to 14:00:00\r\n2. the aggregators know the `task_expiration` is effectively 14:00:01 UTC\r\n3. the aggregators know clients will check their timestamp < `task_expiration` before sending.\r\n\r\nThen the aggregator could know that the client has a true timestamp between 14:00:00 and 14:00:01. ",
              "createdAt": "2022-09-05T11:11:47Z",
              "updatedAt": "2022-09-05T11:11:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bbo3U",
          "commit": {
            "abbreviatedOid": "cc9defd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Suppose the Helper receives an aggregation job for a task that has not yet \"expired\" but contains at least one report whose timestamp is passed the expiration date. What should it do?\r\n\r\nI think it would be appropriate to add a `ReportShareError` variant for this case.",
          "createdAt": "2022-09-06T15:04:06Z",
          "updatedAt": "2022-09-06T15:09:26Z",
          "comments": [
            {
              "originalPosition": 39,
              "body": "```suggestion\r\nthe reports themselves. For schemes like Prio3 {{!VDAF}} in which the input-validation\r\n```",
              "createdAt": "2022-09-06T15:04:06Z",
              "updatedAt": "2022-09-06T15:09:26Z"
            },
            {
              "originalPosition": 49,
              "body": "```suggestion\r\nImplementors SHOULD provide for some leeway so the collector can collect the batch\r\n```",
              "createdAt": "2022-09-06T15:05:50Z",
              "updatedAt": "2022-09-06T15:09:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BiWDU",
          "commit": {
            "abbreviatedOid": "700e72b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-07T16:55:21Z",
          "updatedAt": "2022-09-07T16:55:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Bjxvi",
          "commit": {
            "abbreviatedOid": "700e72b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "A couple editorial nits, otherwise I'm good landing this.",
          "createdAt": "2022-09-07T22:40:38Z",
          "updatedAt": "2022-09-07T22:42:32Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "```suggestion\r\n* `task_expiration`: The time up to which clients are expected to upload to this\r\n```",
              "createdAt": "2022-09-07T22:40:38Z",
              "updatedAt": "2022-09-07T22:42:32Z"
            },
            {
              "originalPosition": 19,
              "body": "```suggestion\r\nignore any reports whose timestamp is past the task's `task_expiration`. When it does so, \r\nthe leader SHOULD abort the upload protocol and alert the client with\r\n```",
              "createdAt": "2022-09-07T22:41:30Z",
              "updatedAt": "2022-09-07T22:42:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bvq_U",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-10T00:37:02Z",
          "updatedAt": "2022-09-10T00:37:02Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Can we resolve this convo?",
              "createdAt": "2022-09-10T00:37:02Z",
              "updatedAt": "2022-09-10T00:37:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BvrCi",
          "commit": {
            "abbreviatedOid": "35663d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-10T00:38:31Z",
          "updatedAt": "2022-09-10T00:38:31Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "Can we resolve this?",
              "createdAt": "2022-09-10T00:38:31Z",
              "updatedAt": "2022-09-10T00:38:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bvrxr",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-10T00:59:18Z",
          "updatedAt": "2022-09-10T00:59:18Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "I see what you mean @wangshan. I think requiring that `task_expiration` be aligned with `time_precision` would help. Would you please file an issue about that so we can deal with it in another PR?",
              "createdAt": "2022-09-10T00:59:18Z",
              "updatedAt": "2022-09-10T00:59:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0PQJ",
          "commit": {
            "abbreviatedOid": "51f357c"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:09:10Z",
          "updatedAt": "2022-09-12T14:09:10Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "will do @tgeoghegan, let me first resolve this conversation and rebase this PR for merging",
              "createdAt": "2022-09-12T14:09:10Z",
              "updatedAt": "2022-09-12T14:09:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CJAzr",
          "commit": {
            "abbreviatedOid": "4fc7b3e"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-15T18:19:24Z",
          "updatedAt": "2022-09-15T18:19:24Z",
          "comments": []
        }
      ]
    },
    {
      "number": 305,
      "id": "PR_kwDOFEJYQs49wL4e",
      "title": "Revisit request authentication",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/305",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Remove the requirement that implementations use `DAP-Auth-Token` with a\r\nfixed, pre-negotiated secret and instead allow any client auth scheme\r\nthat is composable with HTTP (which includes `DAP-Auth-Token`).\r\n\r\nRelevant to #293",
      "createdAt": "2022-08-24T22:18:59Z",
      "updatedAt": "2023-10-26T15:45:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1ee91724360fc7bf7b2477c23e4657f15f0a0f2c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/request-auth",
      "headRefOid": "3a2e91463acde28b81563749b52ad2071f7366e8",
      "closedAt": "2022-09-01T20:07:14Z",
      "mergedAt": "2022-09-01T20:07:14Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "60178323ac998e89ccd85235f9c9f7fd08a15613"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I had to do a rebase to fix a trivial conflict with #303. These changes were approved about a week ago and there's been no further comment, so I am merging.",
          "createdAt": "2022-09-01T20:07:10Z",
          "updatedAt": "2022-09-01T20:07:10Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ApUlg",
          "commit": {
            "abbreviatedOid": "d975b33"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T22:20:32Z",
          "updatedAt": "2022-08-24T22:22:28Z",
          "comments": [
            {
              "originalPosition": 173,
              "body": "I deleted this because it's implicit by virtue of using HTTPS: you won't even be able to establish a connection if you can't verify server identity, so we don't need more protocol/application layer language to repeat features offered by the transport.",
              "createdAt": "2022-08-24T22:20:32Z",
              "updatedAt": "2022-08-24T22:22:28Z"
            },
            {
              "originalPosition": 10,
              "body": "I think there's good arguments against relying on a transport-level concept of identity to authenticate protocol-level concepts, but [this is good enough for ACME](https://www.rfc-editor.org/rfc/rfc8555.html#section-6) so perhaps we can lean on that precedent.",
              "createdAt": "2022-08-24T22:22:24Z",
              "updatedAt": "2022-08-24T22:22:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ApbV1",
          "commit": {
            "abbreviatedOid": "1d5b2cd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:04:31Z",
          "updatedAt": "2022-08-24T23:20:13Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Ultimately this question would be answered by a proof of (in)security in a formal model. I'm perfectly happy to make this a requirement for now.\r\n\r\nOne nit: Does RFC2818 cover HTTP/3? I'm wondering if @martinthomson can help us spell this requirement :)",
              "createdAt": "2022-08-24T23:04:31Z",
              "updatedAt": "2022-08-24T23:20:14Z"
            },
            {
              "originalPosition": 12,
              "body": "?\r\n\r\n```suggestion\r\n## HTTP Request Authentication {#request-authentication}\r\n```",
              "createdAt": "2022-08-24T23:06:03Z",
              "updatedAt": "2022-08-24T23:20:13Z"
            },
            {
              "originalPosition": 22,
              "body": "We should try to avoid overloading \"Client\" wherever possible.\r\n\r\n```suggestion\r\nIn other cases, DAP requires sender authentication. Any authentication scheme\r\n```",
              "createdAt": "2022-08-24T23:10:10Z",
              "updatedAt": "2022-08-24T23:20:13Z"
            },
            {
              "originalPosition": 99,
              "body": "```suggestion\r\nclient and the aggregator it is being sent to. In addition, clients must be able to\r\n```",
              "createdAt": "2022-08-24T23:11:47Z",
              "updatedAt": "2022-08-24T23:20:13Z"
            },
            {
              "originalPosition": 119,
              "body": "As it stands, this problem is unsolved. I don't think it's fair to nudge implementors until we have a better sense of what alternatives are possible. It might be worth pointing to an issue regarding Sybil attacks (we have at least one somewhere).\r\n\r\n```suggestion\r\nto any service), so client authentication is not mandatory in DAP.\r\n```",
              "createdAt": "2022-08-24T23:15:47Z",
              "updatedAt": "2022-08-24T23:20:14Z"
            },
            {
              "originalPosition": 222,
              "body": "Point to security considerations here.",
              "createdAt": "2022-08-24T23:19:18Z",
              "updatedAt": "2022-08-24T23:20:14Z"
            },
            {
              "originalPosition": 227,
              "body": "VDAF does already. Does DAP not as well?",
              "createdAt": "2022-08-24T23:19:52Z",
              "updatedAt": "2022-08-24T23:20:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Aphqm",
          "commit": {
            "abbreviatedOid": "1d5b2cd"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-24T23:50:55Z",
          "updatedAt": "2022-08-24T23:55:16Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "```suggestion\r\nexisting well-known HTTP authentication mechanisms that they already support.\r\n```\r\n\r\n(typo)",
              "createdAt": "2022-08-24T23:50:55Z",
              "updatedAt": "2022-08-24T23:55:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ApmIN",
          "commit": {
            "abbreviatedOid": "d975b33"
          },
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T00:30:04Z",
          "updatedAt": "2022-08-25T00:30:05Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "You can start to refer to RFC 9110 for that stuff now.  That definitely covers HTTP/3.\r\n\r\n(Technically, RFC2818 did cover HTTP/3, but only circuitously.  Also, RFC 2818 is a little old in a few important ways.)",
              "createdAt": "2022-08-25T00:30:04Z",
              "updatedAt": "2022-08-25T00:30:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Auieu",
          "commit": {
            "abbreviatedOid": "1d5b2cd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T18:48:38Z",
          "updatedAt": "2022-08-25T19:10:49Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Indeed, RFC2818 is explicitly marked as obsoleted by 9110. I've updated this reference.",
              "createdAt": "2022-08-25T18:48:38Z",
              "updatedAt": "2022-08-25T19:10:49Z"
            },
            {
              "originalPosition": 12,
              "body": "Sure, HTTP**S** even.",
              "createdAt": "2022-08-25T18:49:04Z",
              "updatedAt": "2022-08-25T19:10:49Z"
            },
            {
              "originalPosition": 22,
              "body": "Hmm, that's a good point, but then \"client\" also has a clear meaning in the context of HTTP(S) and TLS that I think we want to explicitly reference. I made this \"HTTPS client authentication\" in an effort to distinguish from a DAP client.",
              "createdAt": "2022-08-25T18:53:46Z",
              "updatedAt": "2022-08-25T19:10:49Z"
            },
            {
              "originalPosition": 119,
              "body": "#89 is what we point to elsewhere in the doc that touches on Sybil attacks, so I threw in an OPEN ISSUE and deleted the sentence as you suggest.",
              "createdAt": "2022-08-25T18:56:30Z",
              "updatedAt": "2022-08-25T19:10:49Z"
            },
            {
              "originalPosition": 227,
              "body": "Oh, whoops, I was searching for \"soundness\" in VDAF, but it discusses \"robustness\". And in security considerations we discuss \"privacy\" and \"correctness\", but the doc has references to \"soundness\" and \"robustness\", both in protocol text and the odd OPEN ISSUE block. I filed #306 to clean up this language in DAP and align it with VDAF.",
              "createdAt": "2022-08-25T19:02:26Z",
              "updatedAt": "2022-08-25T19:10:49Z"
            },
            {
              "originalPosition": 222,
              "body": "I put in a reference to {{sec-considerations}} and use the word \"correctness\", which is currently the term that Security Considerations uses. That's wrong, but it'll get cleaned up in #306.\r\n\r\nIt also occurs to me that maybe everything in these \"{Upload, Aggregate, Collect} Message Security\" sections that isn't a MUST or a MAY should be punted to security considerations, since it's discussion, context and justification rather than prescription.",
              "createdAt": "2022-08-25T19:06:55Z",
              "updatedAt": "2022-08-25T19:10:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Au1Ah",
          "commit": {
            "abbreviatedOid": "1d5b2cd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T19:43:10Z",
          "updatedAt": "2022-08-25T19:43:10Z",
          "comments": [
            {
              "originalPosition": 222,
              "body": "I think it's OK to point to security considerations as needed in the normative text, especially when it clarifies certain design choices to the reader. That said, mnimizing where possible is a good idea. In particular I'm not sure I would call these sections \".* Message Security\".",
              "createdAt": "2022-08-25T19:43:10Z",
              "updatedAt": "2022-08-25T19:43:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Au1_3",
          "commit": {
            "abbreviatedOid": "f9aad2f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-25T19:47:06Z",
          "updatedAt": "2022-08-25T19:55:32Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "Bump: We're not using HPKE for authenticated encryption here. The properties we need are confidentiality and non-malleability.\r\n```suggestion\r\nanother protocol participant, DAP mandates the use of public-key encryption\r\n```",
              "createdAt": "2022-08-25T19:47:06Z",
              "updatedAt": "2022-08-25T19:55:32Z"
            },
            {
              "originalPosition": 28,
              "body": "It's worth mentioning mutual TLS here as a viable option.",
              "createdAt": "2022-08-25T19:47:59Z",
              "updatedAt": "2022-08-25T19:55:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5AvGHY",
          "commit": {
            "abbreviatedOid": "dcd1a3d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I think it would be useful to minimizing security considerations language in the Message Security sections, but I don't have a clear idea how. I think this is good enough for now, hence the approval, but let's see if we can get more feedback before merging.",
          "createdAt": "2022-08-25T20:42:27Z",
          "updatedAt": "2022-08-25T20:42:27Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5A7xtU",
          "commit": {
            "abbreviatedOid": "445c526"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-29T22:12:45Z",
          "updatedAt": "2022-08-29T22:12:45Z",
          "comments": []
        }
      ]
    },
    {
      "number": 307,
      "id": "PR_kwDOFEJYQs490sXj",
      "title": "Move draft-irtf-cfrg-vdaf-01 to 03",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/307",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Closes #296.\r\n\r\nAlign with the latest VDAF draft. This requires two minor protocol\r\nchanges:\r\n\r\n* 02 added a value called the \"public_share\" to the VDAF syntax. This is\r\n  to support Poplar1 (and likely other schemes that need a similar\r\n  extractability property). To accommodate this, the public_share field\r\n  has been added to the Report and ReportShare structures and appended to\r\n  the AAD for input share encryption. (This may end up being useful for\r\n  privacy.)\r\n\r\n* 02 added the report count to the aggregate result computation. This is\r\n  already passed to the Collector in the CollectResp, so we just need to\r\n  update the aggregate result computation in the draft to match.",
      "createdAt": "2022-08-25T20:33:24Z",
      "updatedAt": "2022-09-16T00:29:29Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0ea091f3e7d36528c79fe2e7a8b795e08b7bc69d",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/296",
      "headRefOid": "416f052e8d63614074336a2318cdc9c1ca85ac3c",
      "closedAt": "2022-09-06T16:28:11Z",
      "mergedAt": "2022-09-06T16:28:11Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "50053bc92912cc937b599b47028efbe403584a9e"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and pushed a minor update: The public share in Report and ReportStore now both have the same length prefix.",
          "createdAt": "2022-09-02T00:22:22Z",
          "updatedAt": "2022-09-02T00:22:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased.",
          "createdAt": "2022-09-06T16:27:25Z",
          "updatedAt": "2022-09-06T16:27:25Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Avats",
          "commit": {
            "abbreviatedOid": "5b71fe7"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-25T22:20:03Z",
          "updatedAt": "2022-08-26T17:20:56Z",
          "comments": [
            {
              "originalPosition": 116,
              "body": "nit: typo\r\n```suggestion\r\n`report_count` denote the report count sent by the Leader, and let `agg_param`\r\n```",
              "createdAt": "2022-08-25T22:20:03Z",
              "updatedAt": "2022-08-26T17:20:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A0X36",
          "commit": {
            "abbreviatedOid": "c3e039b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-26T20:01:56Z",
          "updatedAt": "2022-08-26T20:02:36Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "AFAICT, `public_share` always accompanies `ReportMetadata`, so it'd be nice to tuck the former into the latter, because then the public share would get included in message AAD and appear in all the places it needs to \"for free\". My only reservation is that calling the public share \"metadata\" feels like a misnomer. ",
              "createdAt": "2022-08-26T20:01:56Z",
              "updatedAt": "2022-08-26T20:02:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A7OMd",
          "commit": {
            "abbreviatedOid": "c3e039b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-08-29T19:44:53Z",
          "updatedAt": "2022-08-29T19:45:16Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "While we're in here touching these lines: I think VDAF qualifies as a normative reference rather than an informative one based on [the IETF's criteria](https://www.ietf.org/about/groups/iesg/statements/normative-informative-references/). This and all the other VDAF references should be updated to use `!`.\r\n```suggestion\r\n   {{!VDAF=I-D.draft-irtf-cfrg-vdaf-03}} sharding algorithm. This algorithm is run by\r\n```",
              "createdAt": "2022-08-29T19:44:53Z",
              "updatedAt": "2022-08-29T19:45:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A7y1h",
          "commit": {
            "abbreviatedOid": "c3e039b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-29T22:19:17Z",
          "updatedAt": "2022-08-29T22:19:17Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "Yes, you're right. We could rename to \"header\" maybe, but @chris-wood rejected this name in an earlier PR. Maybe it makes sense to rename if we include public share?",
              "createdAt": "2022-08-29T22:19:17Z",
              "updatedAt": "2022-08-29T22:19:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A7zCS",
          "commit": {
            "abbreviatedOid": "c3e039b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-29T22:20:33Z",
          "updatedAt": "2022-08-29T22:20:34Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Done!",
              "createdAt": "2022-08-29T22:20:34Z",
              "updatedAt": "2022-08-29T22:20:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BHUkq",
          "commit": {
            "abbreviatedOid": "ddc2a3c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-31T19:58:08Z",
          "updatedAt": "2022-08-31T19:58:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 308,
      "id": "PR_kwDOFEJYQs495TvC",
      "title": "Fixed-size tasks: remove `next-batch` functionality.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "This PR does not specify how the Collector knows what batch IDs to query for.\r\nThis will be addressed in a follow-on PR.",
      "createdAt": "2022-08-26T23:03:13Z",
      "updatedAt": "2023-11-27T23:10:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ec4968f251ff0936797455bfa3fd5597b1d2cc7a",
      "headRepository": null,
      "headRefName": "bran/predictable-batch-ids",
      "headRefOid": "605d0a4e16be69e98637ff77a0b07e96bad0a97d",
      "closedAt": "2022-09-15T19:05:34Z",
      "mergedAt": "2022-09-15T19:05:34Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "5d2e6dc44bc68bece7249495d256e0539af7cf5a"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> LGTM. One potential downside is that the Leader can no longer choose batch IDs at random, since it needs to fill batch IDs consecutively. I wonder if this could potentially create a processing bottleneck for the Leader. (I don't know the answer so I'm hoping other reviewers will take the time to think this through!)\r\n\r\nI'm not _too_ worried about this over-and-above the current design: I think there is already effectively a \"serialization point\" required when the Leader assigns the next aggregation job to a batch -- at that point, it needs to know the assignment of existing aggregation jobs to batches, and how many reports are in those aggregation jobs, in order to decide the batch ID to use for its next aggregation job. I think it needs to know this since, if this knowledge is unavailable or imprecise, it might assign too many reports to a single batch, leading to those reports being uncollectable.\r\n\r\nOnce you have a serialization point at point-of-generation-of-aggregation-job, choosing a new batch ID would be something like the pseudo-SQL `SELECT MAX(batch_id) + 1 FROM batches WHERE batch_full = true`.\r\n\r\nThat said -- maybe the existing serialization point will be too much of a performance hit, with or without generating a consecutive batch ID? I guess implementation experience will tell. I'm not too worried since this serialization point wouldn't affect the aggregation protocol, which is likely to be the costliest part of the overall protocol, but it's hard to be certain without actually implementing.\r\n\r\n(also, did I miss something with the above? maybe there's some kinds of imprecision in knowledge of assignment of client reports/aggregation jobs to batches that would allow a serialization point to be safely skipped?)",
          "createdAt": "2022-08-29T18:34:53Z",
          "updatedAt": "2022-08-29T18:40:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> (also, did I miss something with the above? maybe there's some kinds of imprecision in knowledge of assignment of client reports/aggregation jobs to batches that would allow a serialization point to be safely skipped?)\r\n\r\nI'm not sure myself, I haven't fully thought this through. I'm also happy to wait for implementation experience to tell us if we need to change course here.",
          "createdAt": "2022-08-29T22:23:47Z",
          "updatedAt": "2022-08-29T22:23:47Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "In #301:\r\n\r\n> With the current next-batch semantics, there is the possibility that a Collector crashing at an inopportune time might lead to a batch being lost permanently.\r\n\r\n\r\nWould this happen when the Leader commits that it sent the Collector the next batch and the Collector crashes before it commits that it received the batch?\r\n\r\n",
          "createdAt": "2022-08-31T16:59:18Z",
          "updatedAt": "2022-08-31T16:59:18Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'm a little concerned with requiring that the Leader assigns batch ids consecutively. If there are multiple Leaders instances running (for scale purposes), it would require that we share some type of counter across the instances. Additionally, it's not clear to me what happens if we lose this state.\r\n\r\nI'm also just catching up on this thread, so apologies in advance if I'm missing any details. Looking forward to hearing back!",
          "createdAt": "2022-08-31T17:01:29Z",
          "updatedAt": "2022-08-31T17:01:29Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Today we already require collector to store some states, for e.g. the task ID, and the \"collect job URI\" returned by leader, the (random) batchID is just an addition to that.\r\nI guess I don't see the added value of having consecutive numbers. ",
          "createdAt": "2022-08-31T18:03:58Z",
          "updatedAt": "2022-08-31T18:03:58Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "For this PR, having the Collector to indicate the desired batch ID (as proposed), but not requiring the Leader to consecutively order the batches? That allows implementations to have the flexibility to:\r\n\r\n\u2028\u2028a) Make the Leader generate random batch ids and explicitly hand it off to the collector.\r\n\u2028\u2028\r\n\u2028\u2028or\r\n\u2028\u2028\r\n\u2028\u2028\u2028b) Make the Leader consecutively generate batch ids and allow the collector to predict the batch id.\r\n\u2028\u2028\u2028\r\n\u2028\u2028\u2028\u2028\u2028In a separate issue/PR we can discuss how a Leader could implement the \u201cnext batch\u201d functionality. Currently, it isn\u2019t clear how the Leader and Collector reconcile the batch id if state is lost for whatever reason. An analogy I have in my mind would be how Kafka maintains offsets for consumer groups. Similarly, the Leader could maintain the \u201cbatch offset\u201d the collector last committed receiving.",
          "createdAt": "2022-08-31T19:44:41Z",
          "updatedAt": "2022-08-31T19:44:41Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> In #301:\r\n> \r\n> > With the current next-batch semantics, there is the possibility that a Collector crashing at an inopportune time might lead to a batch being lost permanently.\r\n> \r\n> Would this happen when the Leader commits that it sent the Collector the next batch and the Collector crashes before it commits that it received the batch?\r\n\r\nCorrect, that is the situation I was concerned with. To be fully explicit, the problematic scenario with the old `next-batch` semantics was:\r\n\r\n1. The Collector issues a `next-batch` request to the Leader. The Leader returns the next batch ID and updates its durable storage to note that this batch has been returned to the Collector.\r\n2. Before the Collector stores the batch ID back to its own durable storage, it crashes.\r\n\r\nAt this point, there would be no way for the Collector to interact with the Leader to retrieve this batch ID again, so the batch would effectively be lost.\r\n\r\nPutting it another way, I think the core of this problem is that the old `next-batch` semantics required an (inherently-fallible) durable state update from both the Leader & the Collector; these updates were not transactional, so the Leader's update could succeed while the Collector's update could fail; and if this did occur, there was no way for the Collector to interact with the Leader to retrieve the same batch ID again.",
          "createdAt": "2022-08-31T19:46:37Z",
          "updatedAt": "2022-08-31T19:46:37Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @MichaelScaria, I'm with you on the requirements. However I just noticed a potential problem here: If the Leader doesn't get to choose the batch ID, then it doesn't get to proactively aggregate reports as they come in: It has to wait until the batch ID is assigned by the Collector.",
          "createdAt": "2022-08-31T19:47:43Z",
          "updatedAt": "2022-08-31T19:47:43Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm a little concerned with requiring that the Leader assigns batch ids consecutively. If there are multiple Leaders instances running (for scale purposes), it would require that we share some type of counter across the instances.\r\n\r\nIn this scenario, do the multiple Leader instances share state or not? (\"state\" in this case being received client reports, aggregation jobs, etc)\r\n\r\nIf the Leader instances do share state, I think the argument in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308#issuecomment-1230705299 applies: assigning batch IDs without risking adding too many client reports to a given batch already implies serialization of creation of aggregation jobs. Once that serialization is accepted, the counter will not significantly affect performance over-and-above the existing performance cost of serializing the rest of the aggregation-job-creation operation. (That said, I'd be interested in review of the linked argument; I may well be missing some clever implementation strategy.)\r\n\r\nIf the Leader instances don't share state, I think the two Leaders are effectively separate DAP deployments and are free to use a per-Leader counter.\r\n\r\n\r\n> Additionally, it's not clear to me what happens if we lose this state.\r\n\r\nThis is a good point -- I don't think the DAP specification covers data-loss-recovery scenarios currently. For this case in particular, if only the counter is lost, out-of-band communication would allow recovery. I think that is probably acceptable for an initial pass. (If more data is lost, e.g. client reports/aggregation jobs, recovery may be much more difficult--but that's not new with this PR & IMO would best be covered in separate work.)",
          "createdAt": "2022-08-31T20:02:44Z",
          "updatedAt": "2022-08-31T20:52:10Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> 1. The Collector issues a next-batch request to the Leader. The Leader returns the next batch ID and updates its durable storage to note that this batch has been returned to the Collector.\r\n> 2. Before the Collector stores the batch ID back to its own durable storage, it crashes.\r\n\r\nCan we not ask the collector to re-send `next-batch` request in this case?  \r\n\r\n> Upon receipt of a CollectReq, the leader begins by checking that the request meets the requirements of the batch parameters using the procedure in {{batch-validation}}. If so, it immediately sends the collector a response with HTTP status 303 See Other and a Location header containing a URI identifying the collect job that can be polled by the collector, called the \"collect job URI\". \r\n\r\nShould we include the batchID in the \"collect job URI\", so if the collector crashes before saving the batchID, it will have to re-send CollectReq with `next-batch` since the collect job URI will be lost too? @branlwyd @cjpatton @MichaelScaria \r\n",
          "createdAt": "2022-08-31T20:35:44Z",
          "updatedAt": "2022-08-31T20:36:20Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Today we already require collector to store some states, for e.g. the task ID, and the \"collect job URI\" returned by leader, the (random) batchID is just an addition to that. I guess I don't see the added value of having consecutive numbers.\r\n\r\nThe reason for this change are:\r\n* This allows the Collector to opportunistically query for batches. (the discussion around this point starts at https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297#discussion_r949117056)\r\n* The semantics of `next-batch` could lead to data loss by design (see https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308#issuecomment-1233352095 for a full explanation). Allowing the Collector to predict batch IDs avoids this problem. (but it is not the only possible solution to this problem)",
          "createdAt": "2022-08-31T20:51:17Z",
          "updatedAt": "2022-08-31T20:51:17Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> > 1. The Collector issues a next-batch request to the Leader. The Leader returns the next batch ID and updates its durable storage to note that this batch has been returned to the Collector.\r\n> > 2. Before the Collector stores the batch ID back to its own durable storage, it crashes.\r\n> \r\n> Can we not ask the collector to re-send `next-batch` request in this case?\r\n\r\nThe semantics of `next-batch` were defined such that sending another `next-batch` request would receive a different batch. There was no way for the Leader to know that it should return the same batch twice for two different `next-batch` requests. See https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/297#discussion_r948460390 for original discussion around these semantics.\r\n\r\n\r\n> > Upon receipt of a CollectReq, the leader begins by checking that the request meets the requirements of the batch parameters using the procedure in {{batch-validation}}. If so, it immediately sends the collector a response with HTTP status 303 See Other and a Location header containing a URI identifying the collect job that can be polled by the collector, called the \"collect job URI\".\r\n> \r\n> Should we include the batchID in the \"collect job URI\", so if the collector crashes before saving the batchID, it will have to re-send CollectReq with `next-batch` since the collect job URI will be lost too? @branlwyd @cjpatton @MichaelScaria\r\n\r\nI think in the medium-term, including batch ID in the collect job URI is likely a good idea. I think this work would be part of (or perhaps an eventual extension of) https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/278.\r\n\r\nYou're correct that the data-loss scenario under discussion would lose the collect job URI, as well as the batch ID. However, I'm not certain how including the batch ID in the collect job URI helps:\r\n\r\n* When using `next-batch` (i.e. the approach before this PR), a second `next-batch` request will receive a different batch, so including the batch ID in the collect job URI does not help to avoid data loss.\r\n* When using the approach in this PR, if the Collector loses the collect job URI, it can issue a second collect request with the same batch ID to receive it again.",
          "createdAt": "2022-08-31T21:44:05Z",
          "updatedAt": "2022-08-31T21:44:05Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> For this PR, having the Collector to indicate the desired batch ID (as proposed), but not requiring the Leader to consecutively order the batches?\r\n\r\nThat would work, but we'd need to specify how the Collector \"discovers\" batches. That is, I think we'd need to specify the equivalent of \"next-batch\" functionality in this PR or at least before DAP-02 is published.\r\n\r\n\r\n> Similarly, the Leader could maintain the \u201cbatch offset\u201d the collector last committed receiving.\r\n\r\nA commit-based approach would definitely work; my only complaint is that it may be more complex than is needed.",
          "createdAt": "2022-08-31T21:51:58Z",
          "updatedAt": "2022-08-31T21:51:58Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd thank you for pointing out previous comments, definitely helped explaining the ideas. But I'm curious why the \"current-batch\" solution was removed, it feels like the right solution to this problem: the \"current\" batch is maintained by leader, if collector restarts without the batchID, it can ask leader with a \"current-batch\" CollectReq. \r\n\r\nAlternatively, we can let collector specify a randomised batchID (what @MichaelScaria said), in this case the \"current\" batch is maintained by collector, there's no need for batchID discovery, collector could persist the batchID before sending CollectReq. But as @cjpatton pointed out, since the batchID is used in AggregateInitializeReq, the aggregators will have to wait for a CollectReq before aggregating. Unless we change AggregateInitializeReq to use a list of agg job IDs instead of a batchID and associate the list with batchID when CollectReq is received.\r\n",
          "createdAt": "2022-08-31T22:08:51Z",
          "updatedAt": "2022-08-31T22:08:51Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> That is, I think we'd need to specify the equivalent of \"next-batch\" functionality in this PR or at least before DAP-02 is published.\r\n\r\nAny reason we need \"next-batch\" functionality before draft 02? If we remove that feature and assume that the Leader and Collector have a way to share the batchID (for example, encoding it in the collect URI), then fixed size query isn't regressing from a data loss perspective.\r\n\r\nJust to clear up any confusion, I wrote earlier: \r\n\r\n> For this PR, having the Collector to indicate the desired batch ID (as proposed), but not requiring the Leader to consecutively order the batches? \r\n\r\nTo clarify, Collector would set the batchID on CollectReq that the Leader had previously generated.",
          "createdAt": "2022-08-31T23:54:54Z",
          "updatedAt": "2022-08-31T23:54:54Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm curious why the \"current-batch\" solution was removed, it feels like the right solution to this problem: the \"current\" batch is maintained by leader, if collector restarts without the batchID, it can ask leader with a \"current-batch\" CollectReq.\r\n\r\nThe only reason it was removed was we believed we didn't need it in an initial implementation (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308#discussion_r956491618).\r\n\r\nI'd be OK restoring it if desirable -- @cjpatton any thoughts?",
          "createdAt": "2022-08-31T23:55:52Z",
          "updatedAt": "2022-08-31T23:55:52Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> > I'm curious why the \"current-batch\" solution was removed, it feels like the right solution to this problem: the \"current\" batch is maintained by leader, if collector restarts without the batchID, it can ask leader with a \"current-batch\" CollectReq.\r\n> \r\n> The only reason it was removed was we believed we didn't need it in an initial implementation ([#308 (comment)](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308#discussion_r956491618)).\r\n> \r\n> I'd be OK restoring it if desirable -- @cjpatton any thoughts?\r\n\r\nI'm not sure restoring solves the problem at hand. It might be helpful to take a step back and organize our requirements for this change. I believe these are as follows (let me know if I'm missing anything):\r\n1. Our main goal is to prevent data loss in case the Collector forgets the batch ID (due to @branlwyd).\r\n2. Restrictions on ordering of batch IDs is untenable (due to @MichaelScaria). In particular, it would be useful if the Leader could choose batch IDs at random.\r\n\r\nThe decision tree is as follows:\r\n* If folks generally agree on these requirements, then our goal is to extend this PR so that it solves for (2.), without regressing on (1.).\r\n   * Someone observed previously that forgetting the collect URI also results in data loss. Thus one way we can solve for these requirements without *increasing* the risk of data loss is to require the collect URI to encode the batch ID. This appears to be the best suggestion right now.\r\n   * Perhaps we can de-couple the batch ID used by the Aggregators for aggregation from the collect flow. In other words, the Leader maintains in its head a map from batch IDs use during aggregation and \"batch indices\" queried by the collector. For example, when the Collector queries \"batch index == 0\", the Leader looks for the batch that's closest to being finished and associated the corresponding batch ID with that index. This would allow the Collector to \"predict\" batches without storing them (thereby solving (1.)) without constraining the Leader.\r\n   * Any other ideas I've missed?\r\n* If folks don't agree on these requirements, then we should move the discussion back to the issue and figure out what we need.",
          "createdAt": "2022-09-01T01:25:57Z",
          "updatedAt": "2022-09-01T01:29:15Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think 1. is the main requirement, 2. is only an issue if we choose to go the predictable ID route. @cjpatton \r\n\r\nI think the decision tree should be following:\r\n\r\nIf we want to maintain the \"current batch id\" in leader, then a clean solution could be using an idempotent `current-batch` query, and add batch ID to collect job URI. Collector only sends CollectReq with `current-batch`\r\n* if it saves the returned collect job URI successfully, then it also knows the batchID, and from that moment on it can pull from collect job URI. \r\n* If it failed to save the returned collect job URI, it can send the same `current-batch` again.\r\n\r\nHaving a predicable ID is another solution but I would imagine the complexity of maintaining such count in leader is no less than maintaining the current batch ID.\r\n\r\nIf we want to maintain the \"current batch id\" in collector, then we just need the collector to come up with an(random) ID and always use that in CollectReq, we can still add batchID to collect job URI so collector only need to maintain one state. \r\n* If we want to go this route, we must resolve what batchID to send in `AggregateInitailiseReq`, we could, for e.g. not send any batchID in `AggregateInitailiseReq`, but send the list of agg job IDs in `AggregateShareReq`, but that has the problem of how long the list of agg job ID can become, pointed out by someone from the mailing list when the issue was open.\r\n",
          "createdAt": "2022-09-01T14:53:36Z",
          "updatedAt": "2022-09-01T14:53:36Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Ok, it sounds like we might be converging on the following changes to this PR:\r\n1. Let the Leader pick the batch ID\r\n2. (Re-)introduce `current-batch` query, in response to which the Leader returns the batch ID for the latest batch being processed.\r\n3. Require the collect URI to encode the batch ID. (Probably we would just spell out the structure of the URI explicitly.)\r\n\r\nPlease \ud83d\udc4d this comment if you're happy with this :)",
          "createdAt": "2022-09-01T16:11:48Z",
          "updatedAt": "2022-09-01T17:29:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'll just note that, for the third point, we might be able to remove the batch ID from the CollectResp.",
          "createdAt": "2022-09-01T16:13:53Z",
          "updatedAt": "2022-09-01T16:13:53Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's please keep the semantics of how batch IDs are chosen outside of this PR, and instead do the simple thing ((3) in @cjpatton's last comment) here. There are lots of ways in which implementations might choose to pick the batch IDs. I don't think we need to be prescriptive here.",
          "createdAt": "2022-09-01T16:17:10Z",
          "updatedAt": "2022-09-01T16:17:10Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@chris-wood (3) is desirable, I think we should do it regardless of how batchID is chosen, but as @branlwyd has pointed out above: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308#issuecomment-1233450503, it won't solve the requirement 1, if we keep current `next-batch` query.\r\n\r\n",
          "createdAt": "2022-09-01T16:33:53Z",
          "updatedAt": "2022-09-01T16:33:53Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@wangshan that's fine, but this is a _separable change_. I am suggesting we make this change separately. This PR is doing too much.",
          "createdAt": "2022-09-01T16:35:07Z",
          "updatedAt": "2022-09-01T16:35:07Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd sorry :( I made this conflict. Could you please resolve.",
          "createdAt": "2022-09-15T18:21:29Z",
          "updatedAt": "2022-09-15T18:21:29Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> @branlwyd sorry :( I made this conflict. Could you please resolve.\r\n\r\nNo worries -- rebased again, GH reports no conflicts. This should be ready for review again.",
          "createdAt": "2022-09-15T18:46:04Z",
          "updatedAt": "2022-09-15T18:46:04Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5A0yyD",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-26T23:07:10Z",
          "updatedAt": "2022-08-26T23:07:54Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "The `current-batch` functionality may not be ideal:\r\n\r\n* It assumes that a Collector might want to start up against a task that has already collected some batches, and will want to start from \"recent\" data. If this is not the case (for example, if we assume that a Collector can always safely start from batch ID 0, or are OK saying that communicating a \"recent\" batch ID can be done out-of-band), we could drop this functionality entirely.\r\n* Even if we keep this functionality, it would be nicer to separate \"give me a recent batch ID\" from \"start collecting a batch ID\". One way to do this would be to expose an endpoint that forwards to a recent batch ID; this would be easier to specify with #278, at which point the batch ID might be specified as part of the URL.",
              "createdAt": "2022-08-26T23:07:11Z",
              "updatedAt": "2022-08-26T23:07:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A0zov",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-26T23:18:29Z",
          "updatedAt": "2022-08-26T23:25:20Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Since other \"IDs\" have type `opaque [32]`, I wonder if it makes sense to give this a different name. What about `BatchNum`?",
              "createdAt": "2022-08-26T23:18:30Z",
              "updatedAt": "2022-08-26T23:25:20Z"
            },
            {
              "originalPosition": 96,
              "body": "```suggestion\r\n  MAY abort with error \"XXX\" if the batch ID is considered out of range (too small or too large).\r\n```",
              "createdAt": "2022-08-26T23:21:04Z",
              "updatedAt": "2022-08-26T23:25:20Z"
            },
            {
              "originalPosition": 102,
              "body": "```suggestion\r\n  AggregateInitializeReq for the task. The Helper MAY abort with error \"XXX\" if the batch ID is\r\n```",
              "createdAt": "2022-08-26T23:21:43Z",
              "updatedAt": "2022-08-26T23:25:20Z"
            },
            {
              "originalPosition": 59,
              "body": "Yeah, this is a bit hairy. The protocol seems like it would be a lot simpler if all the Collector could do is specify which batch ID it wants.\r\n\r\nDo you thin this functionality is needed right now? If not, maybe better to punt on and keep DAP-02 simple.",
              "createdAt": "2022-08-26T23:25:11Z",
              "updatedAt": "2022-08-26T23:25:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A038g",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:02:25Z",
          "updatedAt": "2022-08-27T00:02:25Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "I think an eventual \"final\"/\"production-quality\" design would definitely want some provision to allow Collectors to discover a recent batch ID from the Aggregators.\r\n\r\nHowever, you're probably right that this isn't needed for initial testing: I suspect interop-test-level deployments can get away with the Collector always starting at batch ID 0, and if that doesn't work out, out-of-band communications would allow configuration of the Collector to start from the correct batch ID.\r\n\r\nI went ahead and tore out the `current-batch` functionality for now.",
              "createdAt": "2022-08-27T00:02:25Z",
              "updatedAt": "2022-08-27T00:02:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A040R",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:19:47Z",
          "updatedAt": "2022-08-27T00:19:48Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "I like calling it an \"ID\" because that is an accurate description of this type's semantics -- it's a primary identifier for a batch. The difference in types doesn't bother me so much; IMO, it's fairly natural for IDs for different kinds of entities to have different types, depending on the needs for the ID.\r\n\r\nOTOH, we might rename some messages such that a \"batch ID\" is either an interval (for `time-interval` tasks) or an opaque numeric value (for `fixed-size` tasks), i.e.:\r\n\r\n```\r\nstruct BatchID {\r\n  select (query_type) {\r\n    case time-interval: Interval batch_interval;\r\n    case fixed-size: BatchID batch_id;\r\n  }\r\n}\r\n```\r\n\r\nThis would be \"cleaner\" (IMO) -- this new BatchID struct would effectively replace the Query -- but I'm not sure it's worth taking the time to do right now.",
              "createdAt": "2022-08-27T00:19:48Z",
              "updatedAt": "2022-08-27T00:19:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A045b",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:21:41Z",
          "updatedAt": "2022-08-27T00:21:42Z",
          "comments": [
            {
              "originalPosition": 96,
              "body": "In the Batch Validation section, these boundary conditions are specified as causing a failure of type \"batchInvalid\". I reworded this from \"failing\" to \"checking\", since this section should be describing a set of checks.",
              "createdAt": "2022-08-27T00:21:42Z",
              "updatedAt": "2022-08-27T00:21:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A0452",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:21:51Z",
          "updatedAt": "2022-08-27T00:21:51Z",
          "comments": [
            {
              "originalPosition": 102,
              "body": "Same as above: in the Batch Validation section, these boundary conditions are specified as causing a failure of type \"batchInvalid\". I reworded this from \"failing\" to \"checking\", since this section should be describing a set of checks.",
              "createdAt": "2022-08-27T00:21:51Z",
              "updatedAt": "2022-08-27T00:21:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A05HW",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:26:38Z",
          "updatedAt": "2022-08-27T00:26:38Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Good point about my suggestion. The second idea I like quite a lot as well, though I think we'd have to rename the inner \"batch_id\" :) If the change is not overly invasive, I say go for it and let's see what it looks like! Just be ready to revert if needed.",
              "createdAt": "2022-08-27T00:26:38Z",
              "updatedAt": "2022-08-27T00:26:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A05Mx",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:28:19Z",
          "updatedAt": "2022-08-27T00:28:20Z",
          "comments": [
            {
              "originalPosition": 96,
              "body": "Yup, you're right. I was a bit careless there. Text looks good after revision.",
              "createdAt": "2022-08-27T00:28:19Z",
              "updatedAt": "2022-08-27T00:28:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A05PB",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:29:16Z",
          "updatedAt": "2022-08-27T00:29:17Z",
          "comments": [
            {
              "originalPosition": 59,
              "body": "Sounds good. Most likely we'll be able to spell this feature when we actually need it.",
              "createdAt": "2022-08-27T00:29:16Z",
              "updatedAt": "2022-08-27T00:29:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A05S8",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-27T00:30:43Z",
          "updatedAt": "2022-08-27T00:30:44Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Actually, thinking about it more, it sounds like what you want is to rename \"Query\" to \"BatchID\"? ",
              "createdAt": "2022-08-27T00:30:44Z",
              "updatedAt": "2022-08-27T00:30:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A05Zb",
          "commit": {
            "abbreviatedOid": "f97e991"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM. One potential downside is that the Leader can no longer choose batch IDs at random, since it needs to fill batch IDs consecutively. I wonder if this could potentially create a processing bottleneck for the Leader. (I don't know the answer so I'm hoping other reviewers will take the time to think this through!)",
          "createdAt": "2022-08-27T00:33:20Z",
          "updatedAt": "2022-08-27T00:33:48Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5A6f8N",
          "commit": {
            "abbreviatedOid": "f97e991"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-29T17:17:22Z",
          "updatedAt": "2022-08-29T17:21:41Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "```suggestion\r\nIn addition to the minimum batch size common to all query types, the\r\n```",
              "createdAt": "2022-08-29T17:17:22Z",
              "updatedAt": "2022-08-29T17:21:42Z"
            },
            {
              "originalPosition": 60,
              "body": "Should we say \"monotonic\" to allow discontinuities in batch IDs? Does anything bad happen if there are discontinuities in batch IDs? I assume we already have text instructing aggregators what to do if they don't recognize a batch ID.",
              "createdAt": "2022-08-29T17:19:31Z",
              "updatedAt": "2022-08-29T17:21:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A61qR",
          "commit": {
            "abbreviatedOid": "f97e991"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-29T18:18:30Z",
          "updatedAt": "2022-08-29T18:18:31Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "I think we don't want discontinuities: the idea of this approach is that the Collector can send requests for batch 0, followed by batch 1, followed by batch 2, and so on -- i.e. that the Collector can predict the sequence of batch identifiers. While a failed aggregation or technical issues might lead to an uncollectable batch, which (hopefully) is communicated well enough for the Collector to realize it won't be able to collect that batch, allowing the Leader to skip arbitrary batch IDs would currently be painful[1]. For example, if the Leader decided to skip `10^6` batch IDs, I suppose the Collector would have to walk through `10^6` collect requests getting an \"unknown batch\" before finding the next \"good\" batch.\r\n\r\n\r\n[1] I think the best way to reduce this pain, if we decide we need to, would be to have some way for the Collector to ask the Leader which batch IDs are usable/acceptable. That's what I was trying to get at with the now-removed `current-batch` functionality, to allow Collectors to startup against Aggregators that have been running for awhile without the need for out-of-band communication. Like with that functionality, IMO we should see whether we need to implement something more complicated before we specify it.",
              "createdAt": "2022-08-29T18:18:30Z",
              "updatedAt": "2022-08-29T18:18:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A7Cb7",
          "commit": {
            "abbreviatedOid": "f97e991"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-29T19:02:59Z",
          "updatedAt": "2022-08-29T19:02:59Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "OK, then do we need a MUST describing how the leader should assign batch IDs?",
              "createdAt": "2022-08-29T19:02:59Z",
              "updatedAt": "2022-08-29T19:02:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A7XEB",
          "commit": {
            "abbreviatedOid": "f97e991"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-29T20:16:10Z",
          "updatedAt": "2022-08-29T20:16:10Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "Sure -- the existing text in the \"Fixed-size Queries\" section defined them this way, but it's probably valuable to reiterate this point in the \"Leader Initialization\" section since that section defines the specific protocol steps. I did so in terms of a `MUST`.",
              "createdAt": "2022-08-29T20:16:10Z",
              "updatedAt": "2022-08-29T20:16:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A7axi",
          "commit": {
            "abbreviatedOid": "de354d7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-29T20:30:32Z",
          "updatedAt": "2022-08-29T20:30:33Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Thinking about this some more, I think I don't want to rename Query to BatchID: while this would work well for `fixed-size`, I think the interval included in `time-interval` isn't quite an ID: specifically, `time-interval` tasks allow queries that span multiple `min_batch_duration` intervals, so for `time-interval` the query is actually describing a range of batches rather than a single batch. IMO, a batch ID would identify a single `min_batch_duration` interval for `time-interval`.\r\n\r\n(of course, I could keep Query and separately define BatchID, but at that point the suggested change is no longer a simplification...)",
              "createdAt": "2022-08-29T20:30:32Z",
              "updatedAt": "2022-08-29T20:30:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BGuOi",
          "commit": {
            "abbreviatedOid": "ceff8f3"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T17:49:40Z",
          "updatedAt": "2022-08-31T17:49:40Z",
          "comments": [
            {
              "originalPosition": 108,
              "body": "this contradicts with previous text: \"it is expected that the batch ID to start\r\n from will be configured out-of-band from the DAP protocol.\" at line #571",
              "createdAt": "2022-08-31T17:49:40Z",
              "updatedAt": "2022-08-31T17:49:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BHZsB",
          "commit": {
            "abbreviatedOid": "ceff8f3"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-08-31T20:15:50Z",
          "updatedAt": "2022-08-31T20:15:50Z",
          "comments": [
            {
              "originalPosition": 108,
              "body": "Ah, good point. The intent of the text you reference on line 571 is that a Collector that starts after the Aggregators have been running may not be able to start from 0 -- while the Aggregators generated batch IDs starting from 0, by the time the Collector starts, batch ID 0 will have already been garbage collected & no longer be available for collection. Given that, the Collector needs some way to know where to start from.\r\n\r\nThat said, this text is confusing & likely unnecessary at this point. I deleted the relevant text around line 571.",
              "createdAt": "2022-08-31T20:15:50Z",
              "updatedAt": "2022-08-31T20:26:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BNQzN",
          "commit": {
            "abbreviatedOid": "f856e42"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Unapproving, pending changes discussed in slack.",
          "createdAt": "2022-09-01T20:09:26Z",
          "updatedAt": "2022-09-01T20:09:26Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BNrqo",
          "commit": {
            "abbreviatedOid": "483c8af"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "OK, this is ready for review w/ the changes suggested from Slack discussion.",
          "createdAt": "2022-09-01T21:53:02Z",
          "updatedAt": "2022-09-01T21:53:55Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "I'm not totally sure \"The Leader MAY allocate batch IDs in any order.\" is necessary -- we could remove this sentence and leave this implicit. Thoughts?",
              "createdAt": "2022-09-01T21:53:02Z",
              "updatedAt": "2022-09-01T21:53:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BOEtt",
          "commit": {
            "abbreviatedOid": "483c8af"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T01:14:44Z",
          "updatedAt": "2022-09-02T01:17:34Z",
          "comments": [
            {
              "originalPosition": 34,
              "body": "Consider making the BatchID at least 16 bytes in order to allow implementations to choose this at random.",
              "createdAt": "2022-09-02T01:14:44Z",
              "updatedAt": "2022-09-02T01:17:34Z"
            },
            {
              "originalPosition": 84,
              "body": "Let's make sure the follow-on PR actually resolves this!",
              "createdAt": "2022-09-02T01:15:39Z",
              "updatedAt": "2022-09-02T01:17:34Z"
            },
            {
              "originalPosition": 103,
              "body": "I agree it's unnecessary.",
              "createdAt": "2022-09-02T01:16:06Z",
              "updatedAt": "2022-09-02T01:17:34Z"
            },
            {
              "originalPosition": 143,
              "body": "Is this required? For this PR we're assuming the mechanism by which batch IDs are assigned is implementation-specific. (This will be addressed in the follow-up PR.)",
              "createdAt": "2022-09-02T01:17:29Z",
              "updatedAt": "2022-09-02T01:17:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSBdb",
          "commit": {
            "abbreviatedOid": "483c8af"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T18:58:52Z",
          "updatedAt": "2022-09-02T18:58:53Z",
          "comments": [
            {
              "originalPosition": 34,
              "body": "\ud83d\udc4d\ud83c\udffb I restored it to 32 bytes as originally written; I suppose this matches the size of other \"opaque identifiers for which we want to enable random generation with negligible collision chance\". (I'd advocate for changing all of these identifiers to 16 bytes rather than 32, but I also think that's a separable change, and so for now I think maintaining consistency is more valuable.)",
              "createdAt": "2022-09-02T18:58:53Z",
              "updatedAt": "2022-09-02T18:58:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSCK0",
          "commit": {
            "abbreviatedOid": "483c8af"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T19:02:25Z",
          "updatedAt": "2022-09-02T19:02:26Z",
          "comments": [
            {
              "originalPosition": 34,
              "body": "I'd be all for 16 bytes, but yes this change is separable :) One thing we need to keep in mind is whether we need IDs to be the hash of things, in which case 32 bytes might be better.",
              "createdAt": "2022-09-02T19:02:26Z",
              "updatedAt": "2022-09-02T19:02:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSDX_",
          "commit": {
            "abbreviatedOid": "483c8af"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T19:08:35Z",
          "updatedAt": "2022-09-02T19:08:35Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "Removed.",
              "createdAt": "2022-09-02T19:08:35Z",
              "updatedAt": "2022-09-02T19:08:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSDxx",
          "commit": {
            "abbreviatedOid": "483c8af"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T19:10:43Z",
          "updatedAt": "2022-09-02T19:10:43Z",
          "comments": [
            {
              "originalPosition": 143,
              "body": "Good point, I think it's not required in this PR. Removed.",
              "createdAt": "2022-09-02T19:10:43Z",
              "updatedAt": "2022-09-02T19:10:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSf2K",
          "commit": {
            "abbreviatedOid": "4acb70f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-02T21:29:27Z",
          "updatedAt": "2022-09-02T21:29:27Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BXKsC",
          "commit": {
            "abbreviatedOid": "4acb70f"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-05T15:13:41Z",
          "updatedAt": "2022-09-05T15:13:41Z",
          "comments": []
        }
      ]
    },
    {
      "number": 309,
      "id": "PR_kwDOFEJYQs49_19A",
      "title": "Rely on VDAF's definitions of privacy, robustness",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/309",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Resolves #306",
      "createdAt": "2022-08-29T20:05:00Z",
      "updatedAt": "2023-10-26T15:45:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "50053bc92912cc937b599b47028efbe403584a9e",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/robust-private-vdaf-defs",
      "headRefOid": "f1c8131bdba7f1299896205b4cabb5d739fc0aa9",
      "closedAt": "2022-09-06T16:51:58Z",
      "mergedAt": "2022-09-06T16:51:58Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "b999a149faefb573283e137cc7cbaa1112c0fa21"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan let's squash-then-merge this change.",
          "createdAt": "2022-09-06T16:23:10Z",
          "updatedAt": "2022-09-06T16:23:10Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5A7g00",
          "commit": {
            "abbreviatedOid": "89dacb9"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-29T20:53:33Z",
          "updatedAt": "2022-08-29T20:53:33Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5A7hel",
          "commit": {
            "abbreviatedOid": "89dacb9"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-29T20:56:04Z",
          "updatedAt": "2022-08-29T20:56:04Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5A7yjf",
          "commit": {
            "abbreviatedOid": "89dacb9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-08-29T22:17:44Z",
          "updatedAt": "2022-08-29T22:17:52Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "Sadly I don't think italics get rendered in txt the way you want https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap\r\n```suggestion\r\nIn the presence of this adversary, DAP aims to achieve the \"privacy\" and\r\n\"robustness\" security goals described in {{!VDAF, Section 9}}.\r\n```",
              "createdAt": "2022-08-29T22:17:44Z",
              "updatedAt": "2022-08-29T22:17:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5A8DL9",
          "commit": {
            "abbreviatedOid": "a053edb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-08-30T00:08:28Z",
          "updatedAt": "2022-08-30T00:08:33Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "The section number is subject to change (if, for example, we add a new VDAF). Maybe refer to security considerations, just to make our lives later in the future.",
              "createdAt": "2022-08-30T00:08:28Z",
              "updatedAt": "2022-08-30T00:08:33Z"
            }
          ]
        }
      ]
    },
    {
      "number": 312,
      "id": "PR_kwDOFEJYQs4-Ok5G",
      "title": "Update HPKE info strings for dap-02",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/312",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Fixes #310",
      "createdAt": "2022-09-01T17:09:06Z",
      "updatedAt": "2023-10-26T15:45:10Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8d845c030f9ef3929b00484cdd9ae4e591dd7108",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/hpke-info-dap-02",
      "headRefOid": "c60e8a21d47b72ad557e5a04fe4f867645bc8a28",
      "closedAt": "2022-09-06T16:23:28Z",
      "mergedAt": "2022-09-06T16:23:28Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0ea091f3e7d36528c79fe2e7a8b795e08b7bc69d"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BMdmG",
          "commit": {
            "abbreviatedOid": "c60e8a2"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-01T17:19:37Z",
          "updatedAt": "2022-09-01T17:19:37Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BNMHw",
          "commit": {
            "abbreviatedOid": "c60e8a2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-01T19:54:58Z",
          "updatedAt": "2022-09-01T19:54:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 313,
      "id": "PR_kwDOFEJYQs4-P8m8",
      "title": "Fixed-size tasks: specify `current-batch` to allow Collector to discover batch IDs.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/313",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "This is implemented via a new `current-batch` query type for fixed-size\r\ntasks, which allows the Leader to select a batch to return to the\r\nCollector.\r\n\r\nThis is stacked on https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/308. (Unfortunately, I don't know how to use that PR as a base without merging into\r\na branch on my personal fork of the DAP specification repository, which is\r\nprobably not desirable.)",
      "createdAt": "2022-09-02T00:03:57Z",
      "updatedAt": "2023-11-27T23:10:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b849420524188648fce90845cfaa03449e9984c5",
      "headRepository": null,
      "headRefName": "bran/current-batch",
      "headRefOid": "4a58a0f73bb943351489cb691f65ffcc06e230cb",
      "closedAt": "2022-10-25T16:11:31Z",
      "mergedAt": "2022-10-25T16:11:30Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "cc7fe02c8210349a52c959c930e51057ed852f27"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "@MichaelScaria & @wangshan: GitHub's UI will not allow me to add you as reviewers, but I would appreciate your review.",
          "createdAt": "2022-09-02T00:07:02Z",
          "updatedAt": "2022-09-02T00:07:02Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(I also picked up changes based on review comments for #308)",
          "createdAt": "2022-09-02T20:00:04Z",
          "updatedAt": "2022-09-02T20:00:04Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks all for reviews and thanks to @branlwyd for creating the PR. Based on conversations I've had, it doesn't seem like we quite have consensus on this. Unless it's a blocker for anyone, I would like to suggest we punt to DAP-03.\r\n\r\nThe main problem this PR is intended to solve is interop between the Leader and Collector: As of #308, the communicating batch IDs from Leader to Collector happens out-of-band, i.e., is deployment specific: The Leader could choose predictable batch IDs, or it could choose them at random and communicate them, somehow, to the Collector. It's also possible to implement the main idea of this PR by exposing an HTTP endpoint for the \"current batch\".\r\n\r\nTL;DR: My suggestion is to wait for deployment experience to decide if we need to spell anything out here. Any objections?",
          "createdAt": "2022-09-15T19:10:19Z",
          "updatedAt": "2022-09-15T19:10:19Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": " I think this PR looks fine on its own. My main question/comment is it looks like the main difference between existing `next-batch` query and this PR is that the former requires collector to call next-batch first, then `by-batch-id`, where this PR says there is `current-batch` and `by-batch-id` but how they are used by collector is unspecified. If the main purpose for the PR is to define inter-op between leader and collector, I don't think it quite achieved that goal.\r\n\r\nSo I support punting this to DAP-03.",
          "createdAt": "2022-09-15T20:59:39Z",
          "updatedAt": "2022-09-15T20:59:39Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> My main question/comment is it looks like the main difference between existing `next-batch` query and this PR is that the former requires collector to call next-batch first, then `by-batch-id`, where this PR says there is `current-batch` and `by-batch-id` but how they are used by collector is unspecified.\r\n\r\nFor what it's worth/for clarity: the intended use of `current-batch` was effectively the same as `next-batch` -- the Collector would call `current-batch` (to discover a batch), then `by-batch-id` (if necessary).\r\n\r\nThe reason `current-batch` is preferable to `next-batch` is `next-batch` requires durable state updates from both the Leader & Collector. These updates were not transactional, so it was possible for one of the updates to succeed while the other update failed. In the case that the Leader's update succeeded but the Collector's update failed, the returned batch would effectively be lost -- there would be no way for the Collector to interact with the Leader to re-retrieve the batch. That is, `current-batch` is intended as a bugfix on `next-batch`, but the intended usage is otherwise the same.",
          "createdAt": "2022-09-15T21:30:59Z",
          "updatedAt": "2022-09-15T21:39:04Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I've filed https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/342. Let's consider landing this in DAP-03. @branlwyd if you want you can rebase this now, or we can close this PR and re-open when we're ready to revisit.",
          "createdAt": "2022-09-15T21:34:12Z",
          "updatedAt": "2022-09-15T21:34:12Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I've rebased -- I will rebase again if/when folks are ready to consider this PR again, or on request.\r\n\r\n@wangshan I'd like to alleviate the confusion over the expected usage of this approach; I'd like your opinion. I think the most relevant text in the PR is:\r\n\r\n> To get the aggregate of a batch, the Collector issues a query specifying the\r\nbatch ID of interest (see {{query}}). The Collector may not know which batch ID\r\nit is interested in; in this case, it can also issue a query of type\r\n`current-batch`, which allows the Leader to select a recent batch to aggregate.\r\n\r\nIs there anything you would suggest to make the intended usage clearer? Would it be helpful if `current-batch` was mentioned before mentioning that the Collector can query by batch ID? (reviewing this, I think I should mention also `by-batch-id` by name)",
          "createdAt": "2022-09-16T00:10:55Z",
          "updatedAt": "2022-09-16T00:10:55Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd sorry for the (very) late reply, I think my main question/comment is it looks like the main difference between previous `next-batch` query and this PR is that the former requires collector to call `next-batch` first, then `by-batch-id`, where this PR says there is `current-batch` and `by-batch-id` but how they are used by collector is unspecified on purpose. \r\n\r\nIf leader decides the batch-id, how can collector know the batch-id without calling `current-batch` first? Would it depends on some deployment specific rules so it can deduce the batch-id on its own? Based on the previous discussion of this topic, I think we should specify these situations, or at least say it's unspecified and left to deployment.",
          "createdAt": "2022-10-17T15:59:50Z",
          "updatedAt": "2022-10-17T15:59:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> If leader decides the batch-id, how can collector know the batch-id without calling current-batch first?\r\n\r\nIt doesn't: The CollrectResp will carry the batch ID, which is how the Collector learns this for the first time.\r\n\r\n",
          "createdAt": "2022-10-17T22:36:14Z",
          "updatedAt": "2022-10-17T22:36:14Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I have rebased this PR on current main; it should be ready for review once again.",
          "createdAt": "2022-10-17T23:16:00Z",
          "updatedAt": "2022-10-17T23:16:00Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton @branlwyd I see, so there are two interactions if batch-id is chosen by leader:\r\n1A. collector sends CollectReq with `current-batch` or 1B. collector sends CollectReq with `by-batch-id`\r\n2. leader responses with a 303 + \"collect job URI\"\r\n3. collector pulls \"collect job URI\", leader responds 200 + CollectResp which includes batch-id\r\n\r\nif crash happens between 2 and 3, collector will go to step 1A, instead of relying on collect job URI; if crash happens after 3, collector will go to step 1B. I think it might worth calling out collect job URI is transactional and should not be relied on as an identifier, but I guess this doesn't affect the inter-op around batch-id and should be addressed separately. \r\n\r\nI'm happy to approve this as long as the enum value format is corrected.",
          "createdAt": "2022-10-18T16:00:47Z",
          "updatedAt": "2022-10-18T16:00:47Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> @cjpatton @branlwyd I see, so there are two interactions if batch-id is chosen by leader: 1A. collector sends CollectReq with `current-batch` or 1B. collector sends CollectReq with `by-batch-id` 2. leader responses with a 303 + \"collect job URI\" 3. collector pulls \"collect job URI\", leader responds 200 + CollectResp which includes batch-id\r\n> \r\n> if crash happens between 2 and 3, collector will go to step 1A, instead of relying on collect job URI; if crash happens after 3, collector will go to step 1B. I think it might worth calling out collect job URI is transactional and should not be relied on as an identifier, but I guess this doesn't affect the inter-op around batch-id and should be addressed separately.\r\n\r\nCorrect, except that the crash happening before the collector stores the collect job URI would cause it to go back to 1A; the crash happening after the collector stores the collect job URI would cause it to continue polling. (`by-batch-id` is used for collecting the same batch a second time, for VDAFs where that is relevant such as `poplar1`)",
          "createdAt": "2022-10-18T22:51:35Z",
          "updatedAt": "2022-10-18T22:54:46Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> However, I think that `current-batch` obscures what we're really specifying here, which is an ordered list of batches with a cursor maintained by the leader. My suspicion, which I'm exploring in #367, is that we want the leader to expose a resource for a list of known batches that the collector can traverse. But I don't want to complicate this PR, which I think is well-focused and achieves a meaningful improvement, with that conversation, so to re-iterate, I think we should take this change.\r\n\r\nOnly thing I'd add here is that there's no requirement for ordering -- the aggregator is free to choose nearly any strategy to choose a batch, there is no need for a linear order. Agreed to discuss the rest on #367.",
          "createdAt": "2022-10-18T22:54:24Z",
          "updatedAt": "2022-10-18T22:54:24Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Could the collector miss out on batches if there isn't a requirement to order?",
          "createdAt": "2022-10-19T14:15:06Z",
          "updatedAt": "2022-10-19T14:15:06Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Could the collector miss out on batches if there isn't a requirement to order?\r\n\r\nThat depends on the implementation's strategy to return batches (as well as how quickly uncollected batches are GC'ed & how aggressively the Collector polls for new batches to collect).\r\n\r\n(Strawman example showing how batches might end up missed: consider an aggregator which implements `current-batch` by choosing a random uncollected batch each time. If the Collector happens to only request a `current-batch` when there are at least two batches ready for collection, simple bad luck could lead the Aggregator to never randomly select a given batch to be returned to the Collector. This unlucky batch would eventually be GC'ed & therefore lost.)\r\n\r\nSince this is avoidable by implementations, I think it's OK -- that is, I'd view such an issue as an implementation bug and/or operational configuration problem, rather than a problem with the specification. I'd also note that every example I can think of here requires the Collector to collect batches at a rate slower than they are being generated -- otherwise, any given batch will eventually be returned to the Collector because it will eventually be the only available batch to return. [I'm curious if anyone can think of a scenario that doesn't require a slow Collector.] But I think we should expect to lose batches if the Collector is collecting them slower than they are being generated, since IMO it's fairly unreasonable to expect the Aggregator to build up an arbitrarily-sized backlog of batches ready for collection -- that is, I think it is important for Aggregators to be able to eventually GC batches even if they have not yet been collected.\r\n\r\nThis leads me to wonder if we might eventually have some hint to the Collector that it should collect more frequently or risk losing some batches, but I think that's beyond the scope of this PR. It might help deployment scenarios where the rate of batch generation is unpredictably variable. Likely we shouldn't think too hard about this unless/until implementation experience shows that it might be helpful.",
          "createdAt": "2022-10-19T20:40:48Z",
          "updatedAt": "2022-10-19T21:14:38Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "What are the semantics of `current-batch` for VDAFs where the aggregation parameter varies? Concretely:\r\n\r\nLet's suppose a leader is running a chunky DAP task. It's continuously receiving reports, and assigning them to batch IDs as it goes. At some point in time, it has accumulated enough reports to fill three batches, which it assigns batch IDs 1, 2 and 3.\r\n\r\nA collector comes along and makes a collect req with some aggregation parameter `a_1` and a `current-batch` query. I think it makes sense that the leader would run aggregation jobs for batch ID 1 with agg param `a_1`. Next, the collector sends another collect req with the same aggregation parameter and `current-batch`. Having already served up results for batch ID 1, the leader decides that batch ID 2 is now current, and runs aggregation jobs over it with `a_1`.\r\n\r\nNow, suppose the collector sends a collect req with `current-batch` and aggregation parameter `a_2`. What batch does the leader prepare?\r\n\r\nDoes it prepare batch 3, on the premise that it's now current? Or should the change in aggregation parameter reset the cursor position back to batch 1? Or do we expect the leader to maintain a different cursor in the batch stream for each aggregation parameter?\r\n\r\nI understand that `by-batch-id` is the mechanism by which collectors are intended to query a batch multiple times, but it's still possible for a collector to do nothing but `current-batch` with varying agg params. I suppose any collector that does this is accepting that it's at the mercy of whatever leader implementation it's talking to?",
          "createdAt": "2022-10-20T20:10:05Z",
          "updatedAt": "2022-10-20T20:10:05Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> What are the semantics of `current-batch` for VDAFs where the aggregation parameter varies? Concretely:\r\n> \r\n> Let's suppose a leader is running a chunky DAP task. It's continuously receiving reports, and assigning them to batch IDs as it goes. At some point in time, it has accumulated enough reports to fill three batches, which it assigns batch IDs 1, 2 and 3.\r\n> \r\n> A collector comes along and makes a collect req with some aggregation parameter `a_1` and a `current-batch` query. I think it makes sense that the leader would run aggregation jobs for batch ID 1 with agg param `a_1`. Next, the collector sends another collect req with the same aggregation parameter and `current-batch`. Having already served up results for batch ID 1, the leader decides that batch ID 2 is now current, and runs aggregation jobs over it with `a_1`.\r\n> \r\n> Now, suppose the collector sends a collect req with `current-batch` and aggregation parameter `a_2`. What batch does the leader prepare?\r\n> \r\n> Does it prepare batch 3, on the premise that it's now current? Or should the change in aggregation parameter reset the cursor position back to batch 1? Or do we expect the leader to maintain a different cursor in the batch stream for each aggregation parameter?\r\n> \r\n> I understand that `by-batch-id` is the mechanism by which collectors are intended to query a batch multiple times, but it's still possible for a collector to do nothing but `current-batch` with varying agg params. I suppose any collector that does this is accepting that it's at the mercy of whatever leader implementation it's talking to?\r\n\r\nThe semantics are the same: any given batch can be returned by `current-batch` only until collection of that batch is attempted for the first time. The particular aggregation parameter is not relevant to these semantics. In the example that you give, (assuming batches 1, 2, and 3 are _all_ of the batches, i.e. there is no batch 4) the leader _must_ aggregate batch 3 since that is the only batch ready for collection that has not yet started collection. [I'm assuming in your example that the Collector is beginning collection on the batches that are returned before issuing the next `current-batch` request. If not, it's free to aggregate any of batches 1, 2, or 3.]\r\n\r\nThe Collector is free to vary the aggregation parameter on `current-batch` requests if it wishes, but I can't think of a plausible/practical reason it would want to do so. I think DAP currently bakes in an assumption that the initial aggregation parameter can be decided by the Collector without knowledge of the particular batch being collected -- IIUC that's true of `poplar1`, and unless the opaque batch ID or time interval of the batch are used to determine the initial aggregation parameter, I think that's an assumption of DAP itself.",
          "createdAt": "2022-10-20T20:45:33Z",
          "updatedAt": "2022-10-20T20:45:33Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "\ud83d\udc4d\ud83c\udffb That all makes sense to me. I think what I struggle with is that `current-batch`, a mechanism for the collector to discover (read) what batches exist and can be acted upon, occurs within `CollectReq`, a mechanism for the collector act upon batches and mutate them (by consuming queries). I don't see a way to do anything different without making the collect flow for time-interval and chunk DAP radically different from each other, though.",
          "createdAt": "2022-10-20T20:56:38Z",
          "updatedAt": "2022-10-20T21:03:43Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BRfrY",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T16:23:48Z",
          "updatedAt": "2022-09-02T16:23:49Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "Won't the line above this need to be `BatchID`?",
              "createdAt": "2022-09-02T16:23:49Z",
              "updatedAt": "2022-09-02T16:23:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BRf_e",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T16:25:03Z",
          "updatedAt": "2022-09-02T16:25:03Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "Do we need to also make an update to `AggregateContinueReq`?",
              "createdAt": "2022-09-02T16:25:03Z",
              "updatedAt": "2022-09-02T16:25:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSKW9",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T19:46:17Z",
          "updatedAt": "2022-09-02T19:46:18Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "Indeed -- updated.",
              "createdAt": "2022-09-02T19:46:17Z",
              "updatedAt": "2022-09-02T19:46:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSLW4",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T19:51:57Z",
          "updatedAt": "2022-09-02T19:51:57Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "What update are you thinking of?\r\n\r\nOn the assumption it is something along the lines of \"include the batch ID in the `AggregateContinueReq` as well\", I think we don't need an update to `AggregateContinueReq`: once the mapping of aggregation job to batch ID is determined by the `AggregateInitializeReq`, it is sufficient to specify only the aggregation job ID in further communications.\r\n\r\n(apologies if I'm misinterpreting your comment!)",
              "createdAt": "2022-09-02T19:51:57Z",
              "updatedAt": "2022-09-02T19:51:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BShJc",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T21:37:42Z",
          "updatedAt": "2022-09-02T21:37:42Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "Nope, you assumed right! Would the Helper need to have some sort of reverse map to recover the batch ID from the job ID?",
              "createdAt": "2022-09-02T21:37:42Z",
              "updatedAt": "2022-09-02T21:37:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BShRP",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-02T21:38:38Z",
          "updatedAt": "2022-09-02T21:38:39Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "Thinking aloud here - \r\n\r\nOn init, Helper gets batch and job IDs. \r\nOn cont, Helper gets job ID.\r\nOn collect, Helper gets batchID.\r\n\r\nSo we would need jobID -> batchID (for cont) and batchID -> jobID (for collect) right?",
              "createdAt": "2022-09-02T21:38:39Z",
              "updatedAt": "2022-09-02T21:39:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BSusx",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-03T00:05:55Z",
          "updatedAt": "2022-09-03T00:05:55Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "> Nope, you assumed right! Would the Helper need to have some sort of reverse map to recover the batch ID from the job ID?\r\n\r\nIndeed: the Helper will need to track this as an additional piece of state. Presumably this reverse map would be an index (or equivalent) in a Helper implementation.\r\n\r\nThinking about whether this state is needed, I think it is: even if we decided that `AggregateContinueReq` should include a `BatchID` as well, I think we would want to enforce that the `BatchID` matches for all `Aggregate{Initialize,Continue}Req` messages for the same aggregation job ID, which would imply the helper storing the state there as well. (maybe there is a larger change possible?)\r\n\r\n\r\n>Thinking aloud here -\r\n>\r\n>On init, Helper gets batch and job IDs.\r\n>On cont, Helper gets job ID.\r\n>On collect, Helper gets batchID.\r\n>\r\n>So we would need jobID -> batchID (for cont) and batchID -> jobID (for collect) right?\r\n\r\nThat's correct. Effectively, a batch is made up of one or more aggregation jobs. Informally, aggregation jobs are the unit of batching between the Aggregators for the majority of the aggregation process (i.e. the Aggregators evaluating the VDAF up to the point of recovering their respective output shares). Batches are the unit of collection by the Collector.\r\n\r\nOne reason that the concept of \"batch\" is separate from \"aggregation job\" is that it may be practically infeasible to group together enough reports at a time to meet the batch requirements (e.g. `min_batch_size`). Also, in many scenarios Aggregators can begin the aggregation process before the collection process can start; this is aided by allowing aggregation jobs to be created even before e.g. enough reports have arrived to fill a batch. That is, the concept of aggregation jobs allows aggregating already-received reports for a batch concurrently with additional reports arriving.",
              "createdAt": "2022-09-03T00:05:55Z",
              "updatedAt": "2022-09-03T00:07:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BXODn",
          "commit": {
            "abbreviatedOid": "9b7dcef"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-05T15:27:52Z",
          "updatedAt": "2022-09-05T15:27:52Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "why do we need a reverse map of jobID-> batchID?",
              "createdAt": "2022-09-05T15:27:52Z",
              "updatedAt": "2022-09-05T15:27:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BXPYX",
          "commit": {
            "abbreviatedOid": "9b7dcef"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-05T15:33:42Z",
          "updatedAt": "2022-09-05T15:33:42Z",
          "comments": [
            {
              "originalPosition": 63,
              "body": "If the batchID is allocated by Leader in an arbitrary fashion, then wouldn't the collect HAVE TO issue a current-batch query first? Or are we deliberately not being specific about the way current-batch is used, so some OOB agreement can be used to \"predict\" the batchID?",
              "createdAt": "2022-09-05T15:33:42Z",
              "updatedAt": "2022-09-05T15:33:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BXPfy",
          "commit": {
            "abbreviatedOid": "9b7dcef"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-05T15:34:15Z",
          "updatedAt": "2022-09-05T15:34:15Z",
          "comments": [
            {
              "originalPosition": 134,
              "body": "nit: \"in its\" is redundant",
              "createdAt": "2022-09-05T15:34:15Z",
              "updatedAt": "2022-09-05T15:34:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BcGb_",
          "commit": {
            "abbreviatedOid": "9b7dcef"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-06T16:08:44Z",
          "updatedAt": "2022-09-06T16:08:44Z",
          "comments": [
            {
              "originalPosition": 134,
              "body": "Removed, thanks.",
              "createdAt": "2022-09-06T16:08:44Z",
              "updatedAt": "2022-09-06T16:08:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BcHvS",
          "commit": {
            "abbreviatedOid": "9b7dcef"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-06T16:13:11Z",
          "updatedAt": "2022-09-06T16:13:11Z",
          "comments": [
            {
              "originalPosition": 63,
              "body": "Typically yes, I expect that the Collector would use `current-batch` first (outside of OOB agreement you mention), and then possibly use one or more `by-batch-id` queries for that batch, depending on the VDAF.\r\n\r\nI was writing this text as an explanation of available functionality/expectations for the `fixed-size` query type, rather than a step-by-step guide to how the query types are expected to be used. I can change if folks like?",
              "createdAt": "2022-09-06T16:13:11Z",
              "updatedAt": "2022-09-06T16:13:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BcQWX",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-06T16:38:47Z",
          "updatedAt": "2022-09-06T16:38:47Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "I think the main reason we need a map from aggregation job IDs to collect batches is for enforcing collect-time privacy requirements (the aggregators will need to examine all collect batches that each aggregation job was previously a part of, given the candidate list of aggregation jobs for the current collect batch).\r\n\r\nThe reverse map may also be used during handling of AggregateContinueRequest, to look up state relating to the collect batch and incrementally incorporate output shares into an aggregate share, but that's an implementation decision. Aggregators could just as well store output shares indexed by aggregation job, and then only fetch output shares and combine them all at once when handling a CollectRequest or AggregateShareRequest.",
              "createdAt": "2022-09-06T16:38:47Z",
              "updatedAt": "2022-09-06T16:38:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bh5q5",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-07T15:33:41Z",
          "updatedAt": "2022-09-07T15:33:41Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "@branlwyd Wouldn't the simplest thing be to send both batchID and jobID on init and continue? On continue, if those two parameters don't uniquely resolve to a set of transitions, then we send back some error. On aggregate share, the Leader only sends the batchID, then the Helper finds all jobs for that batchID and can enforce the collect-time privacy requirements on the output shares for those jobs.\r\n\r\nFrom an implementation perspective, you could use any distributed key value store that additionally supports a prefix query (S3 and Redis should be good candidates for this).\r\n\r\nIdeally we can find a solution that uses a single data structure.",
              "createdAt": "2022-09-07T15:33:41Z",
              "updatedAt": "2022-09-07T15:33:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BjXM4",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-07T20:42:18Z",
          "updatedAt": "2022-09-07T20:42:19Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "IMO, your suggestion is actually more complicated, in the sense that it sends more data over the wire & creates an additional possible error case (i.e. aggregate continue req contains a mismatched aggregation job ID/batch ID) for implementations to potentially mishandle. (Resending the data does ensure that the two aggregators don't have different conceptions of the aggregation job<->batch mapping; but this would be a pretty nasty bug, and would be caught at collection time even without resending the data.)\r\n\r\nFor that reason, I'd prefer to keep the current text as written.\r\n\r\n===\r\n\r\nre: single data structure\r\n\r\nI'd be amenable to a single data structure somehow merging the concept of an aggregation job & a batch; however, I don't believe the suggestion to send both batch ID & aggregation job ID moves us towards that goal. More generally, I think we'd need to somehow overcome/sidestep the practical issues I mention upthread from this comment:\r\n\r\n> One reason that the concept of \"batch\" is separate from \"aggregation job\" is that it may be practically infeasible to group together enough reports at a time to meet the batch requirements (e.g. min_batch_size). Also, in many scenarios Aggregators can begin the aggregation process before the collection process can start; this is aided by allowing aggregation jobs to be created even before e.g. enough reports have arrived to fill a batch. That is, the concept of aggregation jobs allows aggregating already-received reports for a batch concurrently with additional reports arriving.\r\n\r\n(To add a few more words to this: I like the ability to \"unhitch\" the privacy parameters, like `min_batch_size`, from the technical parameters, such as \"how many client reports will we batch into a single unit of aggregation\" -- I wouldn't be too happy if we ended up with a design which required trading off privacy to reach desired performance/reliability properties. And I consider the ability to commonly begin the aggregation process before the batch is full to be an important performance property of the current design.)\r\n\r\nFWIW, I think implementations may practically already be in the world where only one data structure is required: it is possible to arrange an implementation such that \"aggregation jobs\" are a real/reified data structure, and \"batches\" exist only as a collection of aggregation jobs -- the mapping being stored with the aggregation job data structure. (In this implementation strategy, collection information is also attached to aggregation jobs, which may be surprising but also assists in ensuring that the same aggregation job is not aggregated/collected too many times.) Since in this strategy batches are represented entirely as an indexed identifier associated with an aggregation job, in this implementation strategy batches are implicit.",
              "createdAt": "2022-09-07T20:42:19Z",
              "updatedAt": "2022-09-07T20:42:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BtkHA",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T14:52:37Z",
          "updatedAt": "2022-09-09T14:52:37Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "Sure, the tradeoff seems to be either we send an extra field (batchID) and maintain one mapping `(batchID, jobID) -> state` on the Helper, or we only send jobID in `AggregateContinueReq` and maintain two mappings `batchID -> jobID` and `jobID -> state`. \r\n\r\nI was wrong earlier when I thought we would need a reverse map, so I no longer feel as strongly in either direction.\r\n\r\nThat all said, this conversation is probably out of scope for this PR and I don't want to hold this up. Happy to continue the discussion elsewhere if needed.\r\n\r\nAs for what I meant by single data structure, I was talking about the number of mappings. But I \ud83d\udcaf agree with the last paragraph, we were actually implementing it that way while the fixed-size proposal was iterated on. Now that Init requires the batchID to be generated, we can no longer do that I think. @branlwyd ",
              "createdAt": "2022-09-09T14:52:37Z",
              "updatedAt": "2022-09-09T14:52:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BvjOC",
          "commit": {
            "abbreviatedOid": "2e7de42"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T22:54:43Z",
          "updatedAt": "2022-09-09T22:54:44Z",
          "comments": [
            {
              "originalPosition": 88,
              "body": "OK -- I'm happy to make a change if there's consensus (or if additional discussion provides even stronger justification for making this change), but will leave the text as-is for now.\r\n\r\n===\r\n\r\nOne question (this might be moot if we are OK going with the current direction, but might influence how we iterate on this approach in the future):\r\n\r\n> But I 100 agree with the last paragraph, we were actually implementing it that way while the fixed-size proposal was iterated on. Now that Init requires the batchID to be generated, we can no longer do that I think.\r\n\r\nCould you elaborate on this point? I'm not sure how associating batch ID at time-of-aggregate-job-initialization blocks this implementation strategy. Also, I'm not sure what the suggested alternative is: even with the text as written at `HEAD`, i.e. before PRs #308 & #313, the text was written such aggregation jobs were associated with a batch at time of aggregation job initialization, and that batch IDs would be generated as-needed by the leader at this time. (to my memory, #297 was always written that way--although I may be misremembering, the PR branch was squashed before merging so I've lost some of the history)\r\n\r\nI'd like to enable implementations which represent batches \"implicitly\", so if I'm missing something here I'd be interested in fixing things up. (but as long as you're OK with not holding up this PR, I do think it might be valuable to continue discussion elsewhere)",
              "createdAt": "2022-09-09T22:54:43Z",
              "updatedAt": "2022-09-09T23:25:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bvq4m",
          "commit": {
            "abbreviatedOid": "3230ea6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Very well done (though I'm sure reviewers played no small part). I'm happy to land this and consider the issue closed.",
          "createdAt": "2022-09-10T00:34:02Z",
          "updatedAt": "2022-09-10T00:34:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5ET1zY",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Revoking approval while until I can do another pass.",
          "createdAt": "2022-10-18T15:22:43Z",
          "updatedAt": "2022-10-18T15:22:43Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5ET697",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T15:34:36Z",
          "updatedAt": "2022-10-18T15:34:36Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "the current draft unified enum cases using underscore.",
              "createdAt": "2022-10-18T15:34:36Z",
              "updatedAt": "2022-10-18T15:34:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EUG8s",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T16:04:06Z",
          "updatedAt": "2022-10-18T16:04:06Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "(here and below)",
              "createdAt": "2022-10-18T16:04:06Z",
              "updatedAt": "2022-10-18T16:04:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EUHgn",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "> [Shan] if crash happens between 2 and 3, collector will go to step 1A, instead of relying on collect job URI; if crash happens after 3, collector will go to step 1B. I think it might worth calling out collect job URI is transactional and should not be relied on as an identifier, but I guess this doesn't affect the inter-op around batch-id and should be addressed separately.\r\n\r\n+1 here. Might as well address it in this PR, I think.",
          "createdAt": "2022-10-18T16:05:37Z",
          "updatedAt": "2022-10-18T16:08:25Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "We might add a SHOULD here to guide the Leader towards returning the oldest, not-yet-collected batch.",
              "createdAt": "2022-10-18T16:05:37Z",
              "updatedAt": "2022-10-18T16:08:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EVHRx",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I think this change works, in that it enables a collector that has no idea what batch IDs exist for a task to bootstrap itself into collecting a sequence of aggregate results. We should take this change because without it, fixed-batch DAP doesn't work without some out of band arrangement between leader and collector on where batch IDs start. I'm guessing that DAP-02 implementations will have no choice but to implement this or something that is functionally equivalent, so we might as well pull it into the protocol text.\r\n\r\nHowever, I think that `current-batch` obscures what we're really specifying here, which is an ordered list of batches with a cursor maintained by the leader. My suspicion, which I'm exploring in #367, is that we want the leader to expose a resource for a list of known batches that the collector can traverse. But I don't want to complicate this PR, which I think is well-focused and achieves a meaningful improvement, with that conversation, so to re-iterate, I think we should take this change.",
          "createdAt": "2022-10-18T19:17:21Z",
          "updatedAt": "2022-10-18T19:17:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5EVLNw",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T19:31:02Z",
          "updatedAt": "2022-10-18T19:31:02Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "I'm not sure this notion can be meaningful. The leader necessarily keeps track of the query count against a batch for privacy reasons, but unfortunately I don't think that means it can correctly maintain the position of the `current-batch` cursor. Suppose the leader services a collect request and delivers an aggregate result to the collector, but the collector crashes while receiving the message. The leader now might consider the batch in question collected and thus advance the cursor to the next batch. But when the collector restarts and queries `current-batch`, it really should get the batch that the leader just advanced past. Without an explicit ack from the collector, the leader has to assume the result has not been delivered.\r\n\r\nThe more I think about it, the more I think that this is the semantics of a message queue like AWS SQS or GCP PubSub, both of which will redeliver messages to consumers until a message is explicitly acked.\r\n\r\nAnyway, the best the leader can do given what's in this PR is to advance the cursor regardless of the message being received by the collector. This means that some batches could be lost, but this proposal is still workable in the common case where the collector doesn't crash.",
              "createdAt": "2022-10-18T19:31:02Z",
              "updatedAt": "2022-10-18T19:31:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EV7lA",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T22:33:34Z",
          "updatedAt": "2022-10-18T22:33:35Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "I'm happy to add a SHOULD if folks think it wise.\r\n\r\nFWIW, I did not intend to imply list-like semantics for `current-batch` -- I attempted to be careful not to imply anything about the ordering of batches returned by `current-batch`. A few plausible implementation strategies:\r\n\r\n* A queue, as discussed upthread. (and yep, I think `current-batch` would allow a fairly nice implementation based on GCP Pub/Sub, AWS SQS, or similar technologies)\r\n* Randomly choose an acceptable batch with each call to `current-batch`.\r\n* For a sharded implementation: choose a random shard, and ask it for a current batch. (each shard might have its own queue, select randomly, or use some other strategy)\r\n\r\nThoughts? I purposefully left the implementation strategy ambiguous (on the idea that it's often wise to be less prescriptive rather than more prescriptive when it comes to implementation strategies), but if everyone thinks a queue is most appropriate then we can certainly suggest that. A queue or queue-like implementation is what I'd go with, but I didn't want to block implementations from experimenting with different selection methods.\r\n\r\n===\r\n\r\n> Anyway, the best the leader can do given what's in this PR is to advance the cursor regardless of the message being received by the collector. This means that some batches could be lost, but this proposal is still workable in the common case where the collector doesn't crash.\r\n\r\nCan you elaborate? With `current-batch`, the batch can continue to be returned until the collector actually attempts collection for the first time. This means that an ill-timed crash can be recovered from by polling `current-batch` again -- the same batch will eventually be returned again. The expectation is that the collector will durably store the collect URL before beginning to poll.\r\n\r\nOr, to put it another way:\r\n* If the crash happens before the collector durably stores the collect URI, the collector will eventually receive the same batch back from `current-batch` again -- no data loss.\r\n* If the crash happens after the collector durably stores the collect URI, the collector has the collect URI in its durable storage & can begin/continue polling -- no data loss.",
              "createdAt": "2022-10-18T22:33:34Z",
              "updatedAt": "2022-10-18T22:33:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EV-TP",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T22:48:55Z",
          "updatedAt": "2022-10-18T22:48:55Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "Fair points, both :) We probably SHOULDn't then.",
              "createdAt": "2022-10-18T22:48:55Z",
              "updatedAt": "2022-10-18T22:48:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EV-wE",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T22:51:55Z",
          "updatedAt": "2022-10-18T22:51:55Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "@branlwyd I think this hasn't actually been resolved yet? Basically the ask is: change \"-\" to \"_\" in enum variants",
              "createdAt": "2022-10-18T22:51:55Z",
              "updatedAt": "2022-10-18T22:51:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EWAD4",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T23:00:45Z",
          "updatedAt": "2022-10-18T23:00:45Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "Looking again: I added text that the Leader `SHOULD` return a batch that has not yet began collection, as the expected semantics there were IMO not clear enough. I left off the ordering part for now, but I'm happy to add it if folks think we should enforce queue-like behavior.",
              "createdAt": "2022-10-18T23:00:45Z",
              "updatedAt": "2022-10-18T23:00:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EWAJK",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-18T23:01:20Z",
          "updatedAt": "2022-10-18T23:01:21Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "LGTM",
              "createdAt": "2022-10-18T23:01:20Z",
              "updatedAt": "2022-10-18T23:01:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EWMVG",
          "commit": {
            "abbreviatedOid": "0a3452a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-10-19T00:26:40Z",
          "updatedAt": "2022-10-19T00:26:40Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5EYxNY",
          "commit": {
            "abbreviatedOid": "0a3452a"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-10-19T11:10:42Z",
          "updatedAt": "2022-10-19T11:10:42Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5EagaF",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-19T15:17:52Z",
          "updatedAt": "2022-10-19T15:17:52Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "I'm wary of responding to a resolved thread in a PR that I approved already but I want to note one more thing in this context.\r\n\r\nbranlwyd:\r\n>Can you elaborate? With current-batch, the batch can continue to be returned until the collector actually attempts collection for the first time. This means that an ill-timed crash can be recovered from by polling current-batch again -- the same batch will eventually be returned again. The expectation is that the collector will durably store the collect URL before beginning to poll.\r\n\r\nIIUC, you're saying that the collector doing `POST /collect` with `current-batch` is akin to a subscriber pulling a message, and then the first `GET {collect-uri}` is akin to acking the message, in that it indicates to the leader that the collector has received the collect URI and thus the leader can then respond to `current-batch` with a different collect URI? I think that makes sense, and I see how this is fault tolerant, but now it seems like `GET {collect-uri}` has a side effect of changing how the server responds to `POST /collect`, which seems a bit off. So I wonder if we should change it to `POST {collect-uri}`. But I don't think we should make that change here, as we need to consider more broadly what abstraction we want for delivering aggregate shares from aggregators to collectors.",
              "createdAt": "2022-10-19T15:17:52Z",
              "updatedAt": "2022-10-19T15:17:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EcUEE",
          "commit": {
            "abbreviatedOid": "26c8612"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-19T21:04:38Z",
          "updatedAt": "2022-10-19T21:04:38Z",
          "comments": [
            {
              "originalPosition": 38,
              "body": "> IIUC, you're saying that the collector doing POST /collect with current-batch is akin to a subscriber pulling a message, and then the first GET {collect-uri} is akin to acking the message, in that it indicates to the leader that the collector has received the collect URI and thus the leader can then respond to current-batch with a different collect URI?\r\n\r\nCorrect, though I would note that the Leader is free to return a different batch even before the Collector \"acks\" it by beginning collection. (This would be handy to support use-cases where the Collector wants to collect potentially many batches in parallel -- no need to synchronize the \"receive a `current-batch` for collection\" logic with the \"poll a collect URI\" logic, they can run more-or-less independently.) Or, to put it another way: \"acking\" a batch removes it from the set of batches that can be returned from `current-batch`; there is no requirement for `current-batch` to continuously return the same batch until it is \"acked\".\r\n\r\n\r\n> I think that makes sense, and I see how this is fault tolerant, but now it seems like GET {collect-uri} has a side effect of changing how the server responds to POST /collect, which seems a bit off. So I wonder if we should change it to POST {collect-uri}. But I don't think we should make that change here, as we need to consider more broadly what abstraction we want for delivering aggregate shares from aggregators to collectors.\r\n\r\nOh, that's a good point -- this is indeed a state change via a `GET`. While I think it is technically-speaking safe/non-problematic in this particular instance, it's worthy of being cleaned up to follow best practices. The most direct way would be to switch to `POST`, as you suggest.",
              "createdAt": "2022-10-19T21:04:38Z",
              "updatedAt": "2022-10-19T21:04:38Z"
            }
          ]
        }
      ]
    },
    {
      "number": 318,
      "id": "PR_kwDOFEJYQs4-no2E",
      "title": "Updating HTTPCaching Ref",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/318",
      "state": "MERGED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #317.",
      "createdAt": "2022-09-08T16:58:45Z",
      "updatedAt": "2022-09-09T18:00:00Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-1",
      "headRefOid": "7cbdd8aa0a76d98c5bf3ef2fb26c4703964fa7b5",
      "closedAt": "2022-09-09T18:00:00Z",
      "mergedAt": "2022-09-09T18:00:00Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "256544ebac4856595c8443e032a174fd408b5fb8"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BoWcK",
          "commit": {
            "abbreviatedOid": "7cbdd8a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks!",
          "createdAt": "2022-09-08T17:02:02Z",
          "updatedAt": "2022-09-08T17:02:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BoXOl",
          "commit": {
            "abbreviatedOid": "7cbdd8a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-08T17:04:41Z",
          "updatedAt": "2022-09-08T17:04:41Z",
          "comments": []
        }
      ]
    },
    {
      "number": 320,
      "id": "PR_kwDOFEJYQs4-nrLj",
      "title": "Alphabetizing Terms",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/320",
      "state": "MERGED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #319.",
      "createdAt": "2022-09-08T17:06:54Z",
      "updatedAt": "2022-09-09T17:59:46Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-2",
      "headRefOid": "91ccb6952de7de51e004ac84d05459fa8d714ceb",
      "closedAt": "2022-09-09T17:59:46Z",
      "mergedAt": "2022-09-09T17:59:46Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "26d9055c20cd8250be1a5cbc970e5c455f12a01b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BofnU",
          "commit": {
            "abbreviatedOid": "91ccb69"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-08T17:25:44Z",
          "updatedAt": "2022-09-08T17:25:44Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Boj7c",
          "commit": {
            "abbreviatedOid": "91ccb69"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-08T17:39:57Z",
          "updatedAt": "2022-09-08T17:39:57Z",
          "comments": []
        }
      ]
    },
    {
      "number": 322,
      "id": "PR_kwDOFEJYQs4-nszF",
      "title": "Adding Helper to Term Section",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/322",
      "state": "MERGED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Not wed to the wording.\r\n\r\nCloses #321.",
      "createdAt": "2022-09-08T17:12:26Z",
      "updatedAt": "2022-09-09T18:00:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-3",
      "headRefOid": "d49c2b122dc660bc2358ebf80ca6a399b67f2c22",
      "closedAt": "2022-09-09T18:00:30Z",
      "mergedAt": "2022-09-09T18:00:30Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "418d9c8253dbf55fcbdc8ae948c51bafe3665d7f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Bofid",
          "commit": {
            "abbreviatedOid": "d49c2b1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-08T17:25:28Z",
          "updatedAt": "2022-09-08T17:25:28Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Boj0e",
          "commit": {
            "abbreviatedOid": "d49c2b1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-08T17:39:32Z",
          "updatedAt": "2022-09-08T17:39:32Z",
          "comments": []
        }
      ]
    },
    {
      "number": 323,
      "id": "PR_kwDOFEJYQs4-sWaB",
      "title": "friendly editorial suggestions",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/323",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I did a pass through the document to s7. All of these are intended to be editorial even though some of them affect the presentation syntax. Hopefully, I didn't introduce any errors.",
      "createdAt": "2022-09-09T17:06:22Z",
      "updatedAt": "2022-09-13T17:20:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1fb88fdd0be5ff2f41486c6984fe77193fa63c81",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-4",
      "headRefOid": "a71739d10741fac672e0a058c273af6e98e2762b",
      "closedAt": "2022-09-13T17:20:41Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> (Also note that the build is failing.)\r\n\r\nThe Duo2 reference was [] originally. I changed it to {{}}, and then got the build error. I think that's because I didn't include the ?. I change it and see if it addresses the build error.",
          "createdAt": "2022-09-13T03:37:54Z",
          "updatedAt": "2022-09-13T03:37:54Z"
        },
        {
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Yep adding the ? with the ref fixed the build error.",
          "createdAt": "2022-09-13T03:42:27Z",
          "updatedAt": "2022-09-13T03:42:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @seanturner! Please rebase (there are merge conflicts) and squash when you have a second.",
          "createdAt": "2022-09-13T04:01:57Z",
          "updatedAt": "2022-09-13T04:01:57Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@seanturner @cjpatton is `//` an invalid comment for RFC8446 representation language?",
          "createdAt": "2022-09-13T15:05:30Z",
          "updatedAt": "2022-09-13T15:05:30Z"
        },
        {
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "body": "If I am reading 8446 correctly:\r\n\r\n    Comments begin with \"/*\" and end with \"*/\".",
          "createdAt": "2022-09-13T15:09:22Z",
          "updatedAt": "2022-09-13T15:09:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@seanturner if you don't want to have to muck around with rebasing, I've opened #339 as an alternative.",
          "createdAt": "2022-09-13T16:55:40Z",
          "updatedAt": "2022-09-13T16:55:40Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Closed in favor of #339.",
          "createdAt": "2022-09-13T17:20:41Z",
          "updatedAt": "2022-09-13T17:20:41Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BuiQs",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-09T18:05:16Z",
          "updatedAt": "2022-09-09T18:05:16Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Bu8Zi",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T19:51:18Z",
          "updatedAt": "2022-09-09T19:51:18Z",
          "comments": [
            {
              "originalPosition": 320,
              "body": "```suggestion\r\nthe collector using {{!HPKE}}.\r\n```",
              "createdAt": "2022-09-09T19:51:18Z",
              "updatedAt": "2022-09-09T19:51:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bu8c9",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T19:51:31Z",
          "updatedAt": "2022-09-09T19:51:32Z",
          "comments": [
            {
              "originalPosition": 329,
              "body": "```suggestion\r\na public key held by the collector using {{!HPKE}}.\r\n```",
              "createdAt": "2022-09-09T19:51:31Z",
              "updatedAt": "2022-09-09T19:51:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bu8ix",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T19:51:54Z",
          "updatedAt": "2022-09-09T19:51:54Z",
          "comments": [
            {
              "originalPosition": 388,
              "body": "```suggestion\r\n   attacks {{?Dou02}} could be used to amplify this capability.\r\n```",
              "createdAt": "2022-09-09T19:51:54Z",
              "updatedAt": "2022-09-13T03:39:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bu8sW",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T19:52:35Z",
          "updatedAt": "2022-09-09T19:52:36Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "```suggestion\r\nan effective mitigation against Sybil {{?Dou2}} attacks in deployments where it is\r\n```",
              "createdAt": "2022-09-09T19:52:35Z",
              "updatedAt": "2022-09-13T03:40:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Bu8uv",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-09T19:52:45Z",
          "updatedAt": "2022-09-09T19:52:46Z",
          "comments": [
            {
              "originalPosition": 169,
              "body": "```suggestion\r\nshare to a public key held by the respective aggregator using {{!HPKE}}.\r\n```",
              "createdAt": "2022-09-09T19:52:45Z",
              "updatedAt": "2022-09-09T19:52:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5BvrGW",
          "commit": {
            "abbreviatedOid": "50f2c06"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "GREAT feedback, thanks for this. Just a few nits. (Also note that the build is failing.)",
          "createdAt": "2022-09-10T00:40:10Z",
          "updatedAt": "2022-09-10T00:41:48Z",
          "comments": [
            {
              "originalPosition": 109,
              "body": "Hmm, why make this change? I'm not a huge fan :)",
              "createdAt": "2022-09-10T00:40:10Z",
              "updatedAt": "2022-09-10T00:41:48Z"
            },
            {
              "originalPosition": 175,
              "body": "Don't we want {{Dou2}} so that the existence of the reference is enforced?",
              "createdAt": "2022-09-10T00:40:59Z",
              "updatedAt": "2022-09-10T00:41:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B3ve4",
          "commit": {
            "abbreviatedOid": "456d12f"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-13T05:07:59Z",
          "updatedAt": "2022-09-13T05:08:00Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "```suggestion\r\nan effective mitigation against Sybil {{?Dou02}} attacks in deployments where it is\r\n```",
              "createdAt": "2022-09-13T05:07:59Z",
              "updatedAt": "2022-09-13T05:08:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B3v8c",
          "commit": {
            "abbreviatedOid": "a169a0c"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-13T05:11:03Z",
          "updatedAt": "2022-09-13T05:11:03Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "```suggestion\r\nan effective mitigation against Sybil {{Dou02}} attacks in deployments where it is\r\n```",
              "createdAt": "2022-09-13T05:11:03Z",
              "updatedAt": "2022-09-13T05:11:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B3wAu",
          "commit": {
            "abbreviatedOid": "d9332f2"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-13T05:11:27Z",
          "updatedAt": "2022-09-13T05:11:27Z",
          "comments": [
            {
              "originalPosition": 388,
              "body": "```suggestion\r\n   attacks {{Dou02}} could be used to amplify this capability.\r\n```",
              "createdAt": "2022-09-13T05:11:27Z",
              "updatedAt": "2022-09-13T05:11:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B7TNL",
          "commit": {
            "abbreviatedOid": "a71739d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-13T15:54:08Z",
          "updatedAt": "2022-09-13T15:54:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 325,
      "id": "PR_kwDOFEJYQs4-sYgd",
      "title": "Sizes for enums",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/325",
      "state": "MERGED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Assumed 255.\r\n\r\nCloses #324.",
      "createdAt": "2022-09-09T17:15:14Z",
      "updatedAt": "2022-09-10T00:42:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-5",
      "headRefOid": "a3796848a1a4634cbe8542f1e9ab37c4eb023802",
      "closedAt": "2022-09-10T00:42:38Z",
      "mergedAt": "2022-09-10T00:42:38Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "c46b90e06fb86d5412a95ac89d4087b54ef7595f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BuhzJ",
          "commit": {
            "abbreviatedOid": "a379684"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-09T18:03:23Z",
          "updatedAt": "2022-09-09T18:03:23Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BvrLd",
          "commit": {
            "abbreviatedOid": "a379684"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "These limits look reasonable \ud83d\udc4d ",
          "createdAt": "2022-09-10T00:42:25Z",
          "updatedAt": "2022-09-10T00:42:25Z",
          "comments": []
        }
      ]
    },
    {
      "number": 326,
      "id": "PR_kwDOFEJYQs4-sZdR",
      "title": "confid_id vs id for HpkeConfigId in HpkeConfig",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/326",
      "state": "CLOSED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Not sure about this one.",
      "createdAt": "2022-09-09T17:19:15Z",
      "updatedAt": "2022-09-12T23:55:25Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-6",
      "headRefOid": "7ea11a5e0bb54a128660f0d409a03a8866ae8fbc",
      "closedAt": "2022-09-12T23:55:25Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I prefer `id` in spots like this because if you imagine referring to it as `HpkeConfig.config_id`, then `config` is repeated unnecessarily, whereas `HpkeConfig.id` is concise and I hope unambiguous. However the names of these fields doesn't ever go on the wire so i don't object to changing them if others think this is more clear.",
          "createdAt": "2022-09-09T18:03:09Z",
          "updatedAt": "2022-09-09T18:03:09Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with Tim, I think we should leave as-is.",
          "createdAt": "2022-09-10T00:43:24Z",
          "updatedAt": "2022-09-10T00:43:24Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 327,
      "id": "PR_kwDOFEJYQs4-sauf",
      "title": "Fixing FixedSizeQuerry presentation syntax",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/327",
      "state": "MERGED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "I think this is right based on RFC 8446 always have a prefix like \"FixedSizeQuery.\" when the choice comes after a type.",
      "createdAt": "2022-09-09T17:23:05Z",
      "updatedAt": "2022-09-12T23:54:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-7",
      "headRefOid": "235038260e2d2c4154ea8c25f720bc8e75fe8718",
      "closedAt": "2022-09-12T23:54:34Z",
      "mergedAt": "2022-09-12T23:54:34Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "1fb88fdd0be5ff2f41486c6984fe77193fa63c81"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I can't find explicitly rules regarding this in RFC8446 section3, but I think this is the right change even if just to keep it consistent with the other occurrences of such variant, which is `select (PrepareStep.prepare_step_result)`",
          "createdAt": "2022-09-12T14:26:08Z",
          "updatedAt": "2022-09-12T14:26:08Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BvrSl",
          "commit": {
            "abbreviatedOid": "2350382"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "No objection, but I was under the impression we were supposed to skip this.",
          "createdAt": "2022-09-10T00:44:32Z",
          "updatedAt": "2022-09-10T00:44:32Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5B0b_9",
          "commit": {
            "abbreviatedOid": "2350382"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-12T14:38:26Z",
          "updatedAt": "2022-09-12T14:38:26Z",
          "comments": []
        }
      ]
    },
    {
      "number": 331,
      "id": "PR_kwDOFEJYQs4-shxH",
      "title": "IANA Considerations for Query Types Registry",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/331",
      "state": "MERGED",
      "author": "seanturner",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #330.",
      "createdAt": "2022-09-09T17:56:44Z",
      "updatedAt": "2022-09-12T13:43:59Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b999a149faefb573283e137cc7cbaa1112c0fa21",
      "headRepository": "seanturner/draft-ietf-ppm-dap",
      "headRefName": "patch-8",
      "headRefOid": "f985891ba2ae377af245e92b7104c2cee14bbd59",
      "closedAt": "2022-09-12T13:43:59Z",
      "mergedAt": "2022-09-12T13:43:59Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "a464af7f5d5aadfffb62605842b3ba9759974438"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan I think we should wait for one of @chris-wood or @ekr before merging.",
          "createdAt": "2022-09-10T00:53:00Z",
          "updatedAt": "2022-09-10T00:53:00Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5BuifP",
          "commit": {
            "abbreviatedOid": "4137873"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-09T18:06:12Z",
          "updatedAt": "2022-09-09T18:06:12Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5BvrVh",
          "commit": {
            "abbreviatedOid": "4137873"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-10T00:45:04Z",
          "updatedAt": "2022-09-10T00:45:04Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5B0EMr",
          "commit": {
            "abbreviatedOid": "4137873"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-12T13:43:21Z",
          "updatedAt": "2022-09-12T13:43:44Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "```suggestion\r\nspecifications can introduce new query types as needed (see {{query-type-reg}}).\r\n```",
              "createdAt": "2022-09-12T13:43:21Z",
              "updatedAt": "2022-09-12T13:43:44Z"
            }
          ]
        }
      ]
    },
    {
      "number": 333,
      "id": "PR_kwDOFEJYQs4-tNoQ",
      "title": "Clarify handling of replays in \"Upload Requests\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/333",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We didn't have any text that actually instructed leaders to reject uploads deemed to be replays based on their nonce. The text in {{anti-replay}} says leaders should do this, but then refers to the uploads section, which doesn't contain any text about checking nonces or what the leader should do.\r\n\r\nWe now define a new problem document type \"reportRejected\", which replaces the previous \"reportTooLate\", previously used in the case where the leader detects a report pertaining to an already-collected batch. The rationale for using a less specific error type for these two cases is twofold:\r\n\r\n  - In these cases, we don't expect the client to do anything but give up on the report, so nothing more specific than a generic rejection is needed;\r\n  - If we did provide more specific error codes, we'd be giving attackers an oracle for what nonces have already been uploaded or what reports have already been collected.\r\n\r\nWhile I was in this text, I also:\r\n\r\n  - resolved #191.\r\n  - clarified that the client should do if it gets \"reportTooEarly\". If the client shouldn't do anything special in that situation, then I argue the error should be the generic \"reportRejected\".\r\n  - downgraded a MUST to a SHOULD in the case where the client gets \"outdatedConfig\" twice. There's no way the protocol can meaningfully enforce that MUST, and if the client continues to upload reports referencing an outdated or non-existent HPKE config, the leader can just continue to ignore the reports.\r\n\r\nResolves #191",
      "createdAt": "2022-09-09T21:55:18Z",
      "updatedAt": "2022-12-09T23:06:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "debb056f8e966c3f41e4106077b2f495baf3868c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/reject-duplicate-nonces",
      "headRefOid": "72d6b4c7f0213e8fe7e30e6ef9830acace1b6dd9",
      "closedAt": "2022-12-09T23:06:40Z",
      "mergedAt": "2022-12-09T23:06:40Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "d383dbfa1ebee741c827dcb93355a65395ac271d"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't think we should hold up draft-02 for this, unless it takes long enough for the fixed-task stuff to land anyway.",
          "createdAt": "2022-09-09T21:56:26Z",
          "updatedAt": "2022-09-09T21:56:26Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Responding to David--\r\n\r\n> The term \"replay\" is overloaded between two contexts, clients uploading reports to the leader and the leader sending report shares to the helper. I think this change is mixing up the two.\r\n\r\nI see your point, and maybe we need to invent some new terms to clearly distinguish between anti-replay as in \"you tried to use the same report ID with different timestamps or content\" and anti-replay as in \"you tried to aggregate a single report too many times\". But I do think the helper needs to defend against the former, not just the latter, and so the {{anti-replay}} section does seem pertinent to it. In my latest commit I added text to this effect.\r\n\r\n> When we add text covering what the leader should do when it receives a new report with a nonce it has seen previously, I suggest that the leader should ignore the report and return 200 OK, on the theory that this will occur most commonly when retrying HTTP requests. If we do define an error type for this situation, we should not require that the leader reply with it upon repeated nonces in uploads, because that would require increased coordination between concurrent upload requests.\r\n\r\nI agree. The API changes proposed in #367 would mean that multiple PUT requests for the same report will all yield 200 OK (or 201 Created, I guess). So if you send the same `(report ID, timestamp, input share)` multiple times, that's OK. But I do think we want to enable (though not require, to Chris' point about allowing implementations to defer checking anti-replay to the aggregation sub-protocol) leaders alert clients if it's detected that they've tried to use an existing report ID with a new timestamp or input share.",
          "createdAt": "2022-11-17T17:15:08Z",
          "updatedAt": "2022-11-17T18:25:01Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan can you please rebase this?",
          "createdAt": "2022-12-09T02:05:02Z",
          "updatedAt": "2022-12-09T02:05:02Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I've updated this change. The section on handling uploads no longer references {{input-share-validation}}, per @divergentdave's argument that leader checks during upload and aggregator checks during aggregation are not the same thing. Instead we plainly state in the text of \"Upload Requests\" that leaders MUST ignore replays and MAY alert the client.",
          "createdAt": "2022-12-09T18:39:47Z",
          "updatedAt": "2022-12-09T18:39:47Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5B1tW2",
          "commit": {
            "abbreviatedOid": "0a35b24"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "The term \"replay\" is overloaded between two contexts, clients uploading reports to the leader and the leader sending report shares to the helper. I think this change is mixing up the two.\r\n\r\nIn the context of `/upload` requests, leaders need to at least ignore reports with repeated nonces for the following reasons.\r\n\r\n- To protect the privacy of misbehaving clients that choose nonces with insufficient entropy\r\n- To ensure the rest of the protocol can be executed consistently while indexing reports, report shares, preparation message shares, etc. by report nonce\r\n- To make `/upload` an idempotent endpoint, so that clients can safely retry uploads in the event of network-level issues\r\n\r\nAt the time of `/upload`, the \"anti-replay\" logic we require is just ensuring that the leader stores and uses at most one report with any given nonce. (Whether this is resolved during the upload request or after, so long as aggregation only ever uses one report with a given nonce) There is also a requirement that subsequent uploads that fall within previously completed collect requests be ignored in future collect requests, but this seems subjectively more distant from the umbrella of \"anti-replay\". (In fact, it could just as well be enforced during aggregation or collection, by not including it in aggregation jobs, validating AggregateShareReqs against past requests, etc.)\r\n\r\nThis differs from the anti-replay requirements in the context of the collect flow. In that case aggregators need to account for how many aggregations a report (or its nonce) has been included in, and enforce the `max_batch_lifetime` condition before servicing either a CollectReq or AggregateShareReq. This is the subject of section 4.5.7.\r\n\r\nWhen we add text covering what the leader should do when it receives a new report with a nonce it has seen previously, I suggest that the leader should ignore the report and return 200 OK, on the theory that this will occur most commonly when retrying HTTP requests. If we do define an error type for this situation, we should not require that the leader reply with it upon repeated nonces in uploads, because that would require increased coordination between concurrent upload requests.\r\n\r\n> If we did provide more specific error codes, we'd be giving attackers an oracle for what nonces have already been uploaded or what reports have already been collected.\r\n\r\nI think combining the two error conditions in this manner is only a minor impediment for an attacker, as they would instead have an oracle for the disjunction of the two error conditions. Instead, the unpredictability of the random component of the nonce (from an honest client) is what will prevent an attacker from gaining any advantage from `/upload` error codes. With that said, I support combining these different conditions into `reportRejected` for the first reason, because we don't expect the client to make any remedial change either way.",
          "createdAt": "2022-09-12T17:57:54Z",
          "updatedAt": "2022-09-12T19:10:54Z",
          "comments": [
            {
              "originalPosition": 34,
              "body": "nit: typo\r\n```suggestion\r\n{{anti-replay}}, the report MUST be ignored and the leader SHOULD alert the\r\n```",
              "createdAt": "2022-09-12T17:57:54Z",
              "updatedAt": "2022-09-12T19:10:54Z"
            },
            {
              "originalPosition": 35,
              "body": "The anti-replay section this links to is largely focused on the collect flow. There is one sentence that addresses upload requests, but it links back up here to {{upload-flow}}. We need to spell out the conditions for the \"ignore or abort\" behavior up here to avoid a cycle of references, but I think the paragraph below here is what {{anti-replay}} is pointing at. (\"each aggregator keeps track of the set of nonces pertaining to reports that were previously aggregated for a given task. If the leader receives a report from a client whose nonce is in this set, it either ignores it or aborts the upload sub-protocol as described in Section 4.3.\" vs. \"The leader MUST ignore any report pertaining to a batch that has already been collected...\")\r\n\r\nWe still don't have text covering the client trying to upload a report with a nonce that has already been received. (Note that the text in {{upload-flow}} only says the leader compares the nonce against nonces for reports that have been aggregated, not nonces for reports that have been received)",
              "createdAt": "2022-09-12T18:13:42Z",
              "updatedAt": "2022-09-12T19:10:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B3LGC",
          "commit": {
            "abbreviatedOid": "0a35b24"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Modulo changing the {{anti-replay}} reference to {{input-share-batch-validation}}, I think this change is in good enough shape to land. I think changing \"reportTooLate\" to \"reportRejected\" is strictly and improvement, since it better reflects the feedback the Leader gets from the Helper during the aggregation flow. We might consider adding more signaling, but we should do so in a later PR.\r\n\r\nI want to remind folks of one of our design goals for the upload flow. Checking whether a report would be early rejected (due to decryption failure, repeated nonce, collected batch, etc.) requires interacting with backend storage. Thus the Leader is permitted to perform these checks offline, just before running an aggregation job. That way this (potentially expensive) work is not on the hot path with the client. Of course, the Leader is free to provide this feedback if it wishes. (In fact the spec even says it SHOULD.)\r\n\r\nIf revisiting this requirement is desirable, then we should open an issue.",
          "createdAt": "2022-09-13T00:47:04Z",
          "updatedAt": "2022-09-13T01:04:28Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "{{anti-replay}} is painfully out of date at this point and can use a refresh. The best normative reference is {{input-share-batch-validation}}, which spells out how the Leader (resp. Helper) handles a report prior to issuing (resp. before responding to)  an AggregateInitializeReq.",
              "createdAt": "2022-09-13T00:47:05Z",
              "updatedAt": "2022-09-13T01:03:43Z"
            },
            {
              "originalPosition": 51,
              "body": "```suggestion\r\nsituation, clients MAY re-upload the report later on.\r\n```",
              "createdAt": "2022-09-13T00:48:46Z",
              "updatedAt": "2022-09-13T01:03:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CUejs",
          "commit": {
            "abbreviatedOid": "0a35b24"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Merge conflicts",
          "createdAt": "2022-09-19T18:02:50Z",
          "updatedAt": "2022-09-19T18:02:50Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5GnXGf",
          "commit": {
            "abbreviatedOid": "0a35b24"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-17T17:11:20Z",
          "updatedAt": "2022-11-17T17:11:20Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "Reviewing Chris' #382 reminded me to revisit this change. David's point about a circular reference is a good one. What I want to do is repurpose {{anti-replay}} to discuss the criteria for determining if a report is a replay, but then leave it up to either {{upload-flow}} or {{early-input-share-validation}} to specify what should be done when a replay is detected. So I'll do that after #382 lands, since that PR is changing the text of {{anti-replay}}.",
              "createdAt": "2022-11-17T17:11:20Z",
              "updatedAt": "2022-11-17T17:11:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IP3Pd",
          "commit": {
            "abbreviatedOid": "d506c8b"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-09T19:11:28Z",
          "updatedAt": "2022-12-09T19:11:28Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5IQbbV",
          "commit": {
            "abbreviatedOid": "d506c8b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-09T21:17:41Z",
          "updatedAt": "2022-12-09T21:19:21Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "I think this check is too stringent. For anti-replay, we require tracking the report IDs at a minimum.",
              "createdAt": "2022-12-09T21:17:41Z",
              "updatedAt": "2022-12-09T21:19:21Z"
            },
            {
              "originalPosition": 31,
              "body": "In case we end up hard-coding two aggregators in the future.\r\n```suggestion\r\nviolation. Note that this is enforced by all Aggregators, not just the Leader. In addition, the leader\r\n```",
              "createdAt": "2022-12-09T21:18:42Z",
              "updatedAt": "2022-12-09T21:19:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IQnxp",
          "commit": {
            "abbreviatedOid": "d506c8b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-09T22:21:32Z",
          "updatedAt": "2022-12-09T22:21:33Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "I changed the wording to discuss report ID. I think this needs the same nuance about sharding as the corresponding item in {{input-share-validation}} but I'm not sure how to effectively reference that here. We could just copy paste in the \"implementation note\".",
              "createdAt": "2022-12-09T22:21:32Z",
              "updatedAt": "2022-12-09T22:21:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IQrxc",
          "commit": {
            "abbreviatedOid": "d506c8b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-09T22:48:35Z",
          "updatedAt": "2022-12-09T22:48:35Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "Or just point to that section?",
              "createdAt": "2022-12-09T22:48:35Z",
              "updatedAt": "2022-12-09T22:48:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IQr0n",
          "commit": {
            "abbreviatedOid": "6f7ed35"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-09T22:48:58Z",
          "updatedAt": "2022-12-09T22:48:59Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "yes :)",
              "createdAt": "2022-12-09T22:48:58Z",
              "updatedAt": "2022-12-09T22:48:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IQt1H",
          "commit": {
            "abbreviatedOid": "72d6b4c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-09T23:05:14Z",
          "updatedAt": "2022-12-09T23:05:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 335,
      "id": "PR_kwDOFEJYQs4-w852",
      "title": "Align agg_id with role.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/335",
      "state": "CLOSED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Resolve #328 ",
      "createdAt": "2022-09-12T09:18:16Z",
      "updatedAt": "2022-09-15T19:25:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c46b90e06fb86d5412a95ac89d4087b54ef7595f",
      "headRepository": "simon-friedberger/draft-ietf-ppm-dap",
      "headRefName": "agg_id",
      "headRefOid": "ffe75d31ca8ead7010433ff0dbdaa50bf233dcf5",
      "closedAt": "2022-09-15T19:25:56Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't have a helpful suggestion here, but this reminds me of this issue in VDAF that I wrote some time ago: https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/40\r\n\r\nI haven't thought it all the way through, but if VDAF had its own notion of Leader and Helper, then DAP and VDAF could independently maintain numberings of roles and I think we'd achieve the property Chris W. wants.",
          "createdAt": "2022-09-12T18:11:25Z",
          "updatedAt": "2022-09-12T18:11:25Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I don't have a helpful suggestion here, but this reminds me of this issue in VDAF that I wrote some time ago: [cfrg/draft-irtf-cfrg-vdaf#40](https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/40)\r\n> \r\n> I haven't thought it all the way through, but if VDAF had its own notion of Leader and Helper, then DAP and VDAF could independently maintain numberings of roles and I think we'd achieve the property Chris W. wants.\r\n\r\nThis works for SHARES==2, but for SHARES>2 it breaks down because the agg_id is used for domain separation in the Fiat-Shamir step for Prio3. We can do whatever we want in DAP, but in Prio3, agg_id is going to have to be distinct for each aggregator.",
          "createdAt": "2022-09-12T18:19:21Z",
          "updatedAt": "2022-09-12T18:19:21Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks for looking at this @simon-friedberger! Because this breaks compatibility with VDAF, I'm going to decline this PR. There is still plenty of time to address #328 before we cut DAP-02, so feel free to send a PR.",
          "createdAt": "2022-09-15T19:25:56Z",
          "updatedAt": "2022-09-15T19:25:56Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Bzmzo",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-12T12:43:15Z",
          "updatedAt": "2022-09-12T12:43:15Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5B0ZgV",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:32:36Z",
          "updatedAt": "2022-09-12T14:32:37Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "This would cause problems given the current text of draft-irtf-cfrg-vdaf. There, `agg_id` is expected to be less than the total number of shares. The pseudocode also hinges some decisions on whether `agg_id == 0`. To simplify this, we either need to subtract two here, renumber the Role enum above, or coordinate some larger rearrangement with a future VDAF draft.",
              "createdAt": "2022-09-12T14:32:37Z",
              "updatedAt": "2022-09-12T14:32:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0akN",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-09-12T14:35:05Z",
          "updatedAt": "2022-09-12T14:35:53Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "This is a breaking change for VDAF. We could either:\r\n1. Update the agg ID semantics in VDAF and cut a draft\r\n2. Align server role with agg ID\r\n\r\nOption (2.) might look like this:\r\n\r\n```suggestion\r\n`agg_id` is the aggregator ID, equal to `server_role - 0x02`, where `server_role` is the aggregators' role in {{protocol-definition}};\r\n```\r\nAlternatively, we can re-assign the server roles to align with the VDAF draft.",
              "createdAt": "2022-09-12T14:35:05Z",
              "updatedAt": "2022-09-12T14:35:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0bqO",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:37:39Z",
          "updatedAt": "2022-09-12T14:37:39Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "ha, just saw this :)",
              "createdAt": "2022-09-12T14:37:39Z",
              "updatedAt": "2022-09-12T14:37:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0fvd",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:46:58Z",
          "updatedAt": "2022-09-12T14:46:58Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "It seems bad that an arbitrary change in DAP breaks VDAF. Let's do whatever is needed to decouple these things so that the aggregator ID can be freely assigned. ",
              "createdAt": "2022-09-12T14:46:58Z",
              "updatedAt": "2022-09-12T14:46:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0hrN",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:51:35Z",
          "updatedAt": "2022-09-12T14:51:36Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "Duplicates @divergentdave's comment.",
              "createdAt": "2022-09-12T14:51:35Z",
              "updatedAt": "2022-09-12T14:51:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0hyh",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:51:53Z",
          "updatedAt": "2022-09-12T14:51:53Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "In general the computations of each of the aggregator's aren't identical. For example, for Prio3 the first aggregator's (agg_id==0) is larger than the other's. On the other hand, for Poplar1 they're all roughly the same size. In general there is no reason to think that there will always be a notion of a \"leader share\", but the VDAF spec tries to guide algorithms to give most of the heavy lifting to the first aggregator.\r\n\r\nDo you have thoughts about how to clean this up? David suggested one fix. An alternative would be to re-label the aggregators here.",
              "createdAt": "2022-09-12T14:51:53Z",
              "updatedAt": "2022-09-12T14:52:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0j8S",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T14:56:28Z",
          "updatedAt": "2022-09-12T14:56:28Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "Oh, indeed it is being used basically as an array index to identify each aggregator's data.\r\n\r\nI have no strong opinion.\r\nThis was based on the desire to expand to more aggregators later. We could also use 0 = leader, 1 = helper (and 2, 3, .. if necessary) 254 = collector, 255 = client and change role accordingly?\r\n\r\nOr just leave everything as is, though I was equally surprised as Sean about the discrepancy.",
              "createdAt": "2022-09-12T14:56:28Z",
              "updatedAt": "2022-09-12T14:56:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0nAi",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T15:03:47Z",
          "updatedAt": "2022-09-12T15:03:47Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "I like you're suggestion for relabeling Simon, let's go with that and see what it looks like. Note that you'll have to propagate the changes to wherever we HPKE encrypt or decrypt something.",
              "createdAt": "2022-09-12T15:03:47Z",
              "updatedAt": "2022-09-12T15:03:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5B0o7U",
          "commit": {
            "abbreviatedOid": "ffe75d3"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-12T15:08:28Z",
          "updatedAt": "2022-09-12T15:08:28Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "ISTM @chris-wood is advocating for just leaving it as is. We can but we somehow need to specify how to convert the arbitrary IDs from the DAP spec to continuous numbers with leader=0 for the VDAF spec.",
              "createdAt": "2022-09-12T15:08:28Z",
              "updatedAt": "2022-09-12T15:12:27Z"
            }
          ]
        }
      ]
    },
    {
      "number": 336,
      "id": "PR_kwDOFEJYQs4-0snb",
      "title": "Add report time to `struct PrepareStep`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/336",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "PR #297 (which added fixed chunk tasks) broke the report nonce apart into a timestamp and a nonce. This was not reflected in the `PrepareStep` structures in an `AggregateInitResp` or `AggregateContinueReq`.",
      "createdAt": "2022-09-12T23:32:02Z",
      "updatedAt": "2022-09-14T20:42:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a464af7f5d5aadfffb62605842b3ba9759974438",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/prepare-step-time-and-nonce",
      "headRefOid": "226102518b2e953bd37c109e7c8bf2f656f6bb03",
      "closedAt": "2022-09-14T20:42:22Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Elsewhere we use `ReportMetadata` to get a single handle to a report's nonce and timestamp, but since that structure includes the optional extensions, it feels incorrect to include it in `PrepareStep`.",
          "createdAt": "2022-09-12T23:32:51Z",
          "updatedAt": "2022-09-12T23:32:51Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Unless we have a compelling reason to add the times tamp back, I'd suggest we axe it. To me it's conceptually simpler to rely strictly on the nonce for indexing the reports.\r\n\r\nThat's an interesting proposal. Certainly a 16 byte random nonce ought to be sufficient to uniquely identify a nonce. Let me see how this plays out as I wire up some of this in Janus. In the meantime, I'll make this PR a draft.",
          "createdAt": "2022-09-13T01:39:35Z",
          "updatedAt": "2022-09-13T01:39:35Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Indeed, I was able to implement everything without having the report time in the PrepareStep, because a nonce is enough to uniquely identify a report. Closing this -- it's easy enough to make a new PR if we do decide we need this.",
          "createdAt": "2022-09-14T20:42:21Z",
          "updatedAt": "2022-09-14T20:42:21Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5B3Psc",
          "commit": {
            "abbreviatedOid": "2261025"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Good catch, this might be a regression. I didn't end up needing this in my implementation. Is this useful for Janus?\r\n\r\nUnless we have a compelling reason to add the times tamp back, I'd suggest we axe it. To me it's conceptually simpler to rely strictly on the nonce for indexing the reports.",
          "createdAt": "2022-09-13T01:27:15Z",
          "updatedAt": "2022-09-13T01:28:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 337,
      "id": "PR_kwDOFEJYQs4-05TP",
      "title": "Mention that the public_share might be empty",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/337",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #329.\r\n\r\ncc/ @seanturner ",
      "createdAt": "2022-09-13T01:09:28Z",
      "updatedAt": "2022-09-16T00:29:28Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9b5b4d496f6ae417de58fa6fa2f7b22e89b76891",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/329",
      "headRefOid": "4b6ee7f8980c9b7317402c77bcf6368946dd14f6",
      "closedAt": "2022-09-14T22:57:10Z",
      "mergedAt": "2022-09-14T22:57:10Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "25caba91564e46cf67951eaf607a303d00f717a8"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5B3i0Z",
          "commit": {
            "abbreviatedOid": "984de8e"
          },
          "author": "seanturner",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-13T03:34:20Z",
          "updatedAt": "2022-09-13T03:34:20Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CD1G8",
          "commit": {
            "abbreviatedOid": "4b6ee7f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-14T22:49:30Z",
          "updatedAt": "2022-09-14T22:49:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 338,
      "id": "PR_kwDOFEJYQs4-1DRk",
      "title": "Allow per-report artificats to be dropped before task expiration",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/338",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #180.\r\n\r\nClarify that `report-dropped` is used when it's impossible to determine if an input share is valid. One situation we need this for is when an Aggregator drops some anti-replay state in order to reduce storage costs. This should result in data loss in the worst case, not a loss of privacy.",
      "createdAt": "2022-09-13T02:25:16Z",
      "updatedAt": "2022-09-16T00:29:18Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5d2e6dc44bc68bece7249495d256e0539af7cf5a",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/180",
      "headRefOid": "be3a175c2cdc4dc978e2bddf7af0a228f3e8ab53",
      "closedAt": "2022-09-15T19:20:42Z",
      "mergedAt": "2022-09-15T19:20:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "5fed67957d54d146ca4dfe312e62520867a8948b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5B8F2X",
          "commit": {
            "abbreviatedOid": "5fb348a"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-13T18:25:05Z",
          "updatedAt": "2022-09-13T18:27:22Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "```suggestion\r\nalleviate this burden, DAP allows Aggregators to drop this state as needed, so\r\n```",
              "createdAt": "2022-09-13T18:25:05Z",
              "updatedAt": "2022-09-13T18:27:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CD1uI",
          "commit": {
            "abbreviatedOid": "5fb348a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-14T22:51:37Z",
          "updatedAt": "2022-09-14T22:57:49Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "```suggestion\r\n1. Finally, if an Aggregator cannot determine if an input share is valid, it\r\n```",
              "createdAt": "2022-09-14T22:51:38Z",
              "updatedAt": "2022-09-14T22:57:49Z"
            },
            {
              "originalPosition": 114,
              "body": "```suggestion\r\nlong as reports are dropped properly as described in\r\n```",
              "createdAt": "2022-09-14T22:52:31Z",
              "updatedAt": "2022-09-14T22:57:49Z"
            },
            {
              "originalPosition": 70,
              "body": "This would also happen if for instance a helper received an `AggregateContinueReq` that contained a `PrepareStep` referencing a report that the helper has never seen before, right? If so, maybe worth noting as much in the \"Helper Continuation\" section that discusses handling of `AggregateContinueReq`.",
              "createdAt": "2022-09-14T22:57:00Z",
              "updatedAt": "2022-09-14T22:57:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CD5qk",
          "commit": {
            "abbreviatedOid": "f93c05a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-14T23:19:04Z",
          "updatedAt": "2022-09-14T23:19:29Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "That's not really the situation I'm thinking of. I suspect what I'm imagining will be much more common: Suppose a task runs for year. An Aggregator may choose to evict all report metadata that is older than a week; this text is meant to cover the situation where a report arrives over a week late.",
              "createdAt": "2022-09-14T23:19:04Z",
              "updatedAt": "2022-09-14T23:19:29Z"
            }
          ]
        }
      ]
    },
    {
      "number": 339,
      "id": "PR_kwDOFEJYQs4-4VqK",
      "title": "friendly editorial suggestions (resubmit)",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/339",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Resubmission of #323 in order to resolve conflicts. Originally authored by @seanturner:\r\n\r\nI did a pass through the document to s7. All of these are intended to be editorial even though some of them affect the presentation syntax.\r\n\r\n",
      "createdAt": "2022-09-13T16:54:40Z",
      "updatedAt": "2022-09-16T00:29:29Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1fb88fdd0be5ff2f41486c6984fe77193fa63c81",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/seanturner-edits/1",
      "headRefOid": "b5633f19c6b51719d29a76782cde9f94949e9689",
      "closedAt": "2022-09-13T17:24:13Z",
      "mergedAt": "2022-09-13T17:24:13Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "9b5b4d496f6ae417de58fa6fa2f7b22e89b76891"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5B7wws",
          "commit": {
            "abbreviatedOid": "b5633f1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-13T17:22:17Z",
          "updatedAt": "2022-09-13T17:22:57Z",
          "comments": [
            {
              "originalPosition": 110,
              "body": "nit: fix line wrapping here.",
              "createdAt": "2022-09-13T17:22:17Z",
              "updatedAt": "2022-09-13T17:22:57Z"
            }
          ]
        }
      ]
    },
    {
      "number": 341,
      "id": "PR_kwDOFEJYQs4--2P_",
      "title": "Ensure all protocol messages are self-describing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/341",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Closes #340.\r\n\r\nCurrently parsing AggreateInitializeReq, AggregateshareReq, CollectReq, and CollectResp requires the query type associated with the task. The query type is determined by the task, but it is necessary to parse the task ID from the message before the remainder can be parsed.\r\n\r\nThis change ensures that parsing of these messages does not require the query type.",
      "createdAt": "2022-09-14T23:13:32Z",
      "updatedAt": "2022-09-16T00:29:26Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "25caba91564e46cf67951eaf607a303d00f717a8",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/340",
      "headRefOid": "0bdf2bb420506082b1a28294eccb9438947e72c8",
      "closedAt": "2022-09-15T18:19:59Z",
      "mergedAt": "2022-09-15T18:19:59Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "ec4968f251ff0936797455bfa3fd5597b1d2cc7a"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CEAOk",
          "commit": {
            "abbreviatedOid": "6943955"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This accomplishes what it sets out to, modulo some typos.",
          "createdAt": "2022-09-14T23:55:07Z",
          "updatedAt": "2022-09-14T23:59:18Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "```suggestion\r\n  select (AggregateInitializeReq.query_type) {\r\n```",
              "createdAt": "2022-09-14T23:55:07Z",
              "updatedAt": "2022-09-14T23:59:18Z"
            },
            {
              "originalPosition": 66,
              "body": "```suggestion\r\n  indicated query type MUST match the task's query type.\r\n```",
              "createdAt": "2022-09-14T23:55:30Z",
              "updatedAt": "2022-09-14T23:59:18Z"
            },
            {
              "originalPosition": 78,
              "body": "```suggestion\r\n  select (BatchSelector.query_type) {\r\n```",
              "createdAt": "2022-09-14T23:55:40Z",
              "updatedAt": "2022-09-14T23:59:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CHlr4",
          "commit": {
            "abbreviatedOid": "0bdf2bb"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "LGTM, one typo needs fixing",
          "createdAt": "2022-09-15T14:18:38Z",
          "updatedAt": "2022-09-15T14:21:45Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "indicatd -> indicated",
              "createdAt": "2022-09-15T14:18:38Z",
              "updatedAt": "2022-09-15T14:21:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CJA9H",
          "commit": {
            "abbreviatedOid": "0bdf2bb"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-15T18:19:55Z",
          "updatedAt": "2022-09-15T18:19:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 343,
      "id": "PR_kwDOFEJYQs4_D7nq",
      "title": "Editorial: Make paragraph formatting consistent",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/343",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "* Justify text after bullets consistenly\r\n* Make sure all lines break at 80 characters",
      "createdAt": "2022-09-15T23:26:41Z",
      "updatedAt": "2023-03-06T23:48:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "74c26a4f2a122104bf6e8d01418c95c41ec4f81c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/edit/1",
      "headRefOid": "2c3986e4475eece58b81422f2300eb6f7b919565",
      "closedAt": "2022-09-19T18:47:15Z",
      "mergedAt": "2022-09-19T18:47:14Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0dd9a4a669bd0dcd24168cc920a692e698505850"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CTqAT",
          "commit": {
            "abbreviatedOid": "8c9f2a4"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM, modulo conflicts. It'd be nice if we had a linter that blocked PRs until their formatting was right, since changes like this taint git history.",
          "createdAt": "2022-09-19T15:20:52Z",
          "updatedAt": "2022-09-19T15:20:52Z",
          "comments": []
        }
      ]
    },
    {
      "number": 344,
      "id": "PR_kwDOFEJYQs4_D8d2",
      "title": "Editorial: Run spell checker",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/344",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #343 (merge that first).",
      "createdAt": "2022-09-15T23:34:11Z",
      "updatedAt": "2023-03-06T19:16:45Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0dd9a4a669bd0dcd24168cc920a692e698505850",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/edit/2",
      "headRefOid": "133114724923991fa6644c06aaca91afcf5d867a",
      "closedAt": "2022-09-19T18:53:40Z",
      "mergedAt": "2022-09-19T18:53:40Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "2e858f0ab1428e877d1e9a958c483b1c46b6a3f9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CTqlW",
          "commit": {
            "abbreviatedOid": "0a17c7b"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-19T15:22:28Z",
          "updatedAt": "2022-09-19T15:22:45Z",
          "comments": [
            {
              "originalPosition": 118,
              "body": "```suggestion\r\n   individual clients or a coalition of clients from compromising the robustness\r\n```",
              "createdAt": "2022-09-19T15:22:28Z",
              "updatedAt": "2022-09-19T15:22:45Z"
            }
          ]
        }
      ]
    },
    {
      "number": 345,
      "id": "PR_kwDOFEJYQs4_EBPJ",
      "title": "Update change log for draft 02",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/345",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #315.\r\nBased on #344 (merge that first).",
      "createdAt": "2022-09-16T00:16:28Z",
      "updatedAt": "2023-03-06T19:16:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2e858f0ab1428e877d1e9a958c483b1c46b6a3f9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/edit/3",
      "headRefOid": "f7c6e587e3dd423ecb850702443533e69368b7ba",
      "closedAt": "2022-09-19T18:59:14Z",
      "mergedAt": "2022-09-19T18:59:14Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "6c65376419634f4260753066b254345cf2dad191"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CTrAL",
          "commit": {
            "abbreviatedOid": "6785782"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-09-19T15:23:37Z",
          "updatedAt": "2022-09-19T15:27:38Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "```suggestion\r\n  current draft, the Collector specifies a \"query\", which the Aggregators use to\r\n```",
              "createdAt": "2022-09-19T15:23:37Z",
              "updatedAt": "2022-09-19T15:27:38Z"
            },
            {
              "originalPosition": 22,
              "body": "```suggestion\r\n  scheme. (Draft 01 required the use of the `DAP-Auth-Token` header; this is now optional.)\r\n```\r\nTo be pedantic, we didn't require bearer tokens in draft-01. In HTTP, a bearer token is when you have an `Authorization` header whose value is like `Bearer <token>`.\r\n\r\nhttps://datatracker.ietf.org/doc/html/rfc6750#section-2.1\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication",
              "createdAt": "2022-09-19T15:26:57Z",
              "updatedAt": "2022-09-19T15:27:38Z"
            },
            {
              "originalPosition": 35,
              "body": "```suggestion\r\n- Clarify when it is safe for an Aggregator to evict various data artifacts\r\n```",
              "createdAt": "2022-09-19T15:27:25Z",
              "updatedAt": "2022-09-19T15:27:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CUOVU",
          "commit": {
            "abbreviatedOid": "f9643c8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-19T17:08:50Z",
          "updatedAt": "2022-09-19T17:08:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 346,
      "id": "PR_kwDOFEJYQs4_ECRL",
      "title": "Editorial: Use \"_\" isntead of \"-\" in enum variants",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/346",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #314.\r\nBased on #345 (merge that first).\r\n\r\nThis is to match RFC 8446.",
      "createdAt": "2022-09-16T00:25:19Z",
      "updatedAt": "2023-03-06T19:16:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "6c65376419634f4260753066b254345cf2dad191",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/edit/4",
      "headRefOid": "19649b5d865db2c1f2b857bdbec9af6b2e7f5e35",
      "closedAt": "2022-09-19T19:07:44Z",
      "mergedAt": "2022-09-19T19:07:44Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0d3be2e58442ff69445155e4d605539cd8a05ee7"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CTsmY",
          "commit": {
            "abbreviatedOid": "10a08ee"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-19T15:28:11Z",
          "updatedAt": "2022-09-19T15:28:11Z",
          "comments": []
        }
      ]
    },
    {
      "number": 347,
      "id": "PR_kwDOFEJYQs4_IBaj",
      "title": "Editorial: rename fields from prepare_shares -> prepare_steps.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/347",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The prepare_shares fields were always of type PrepareStep. To match the convention used for most other field names, I change the name of the field to match its type. (There is no \"PrepareShare\" type.)",
      "createdAt": "2022-09-16T20:28:30Z",
      "updatedAt": "2023-11-27T23:10:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5fed67957d54d146ca4dfe312e62520867a8948b",
      "headRepository": null,
      "headRefName": "bran/rename-prepare-shares",
      "headRefOid": "823d08d2344209032c19e659e15e6f91ac4fc024",
      "closedAt": "2022-09-17T02:09:31Z",
      "mergedAt": "2022-09-17T02:09:30Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "d3cf2f177ed239aa74663a0fcc349a3539d649e3"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CO5nV",
          "commit": {
            "abbreviatedOid": "823d08d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-16T20:40:48Z",
          "updatedAt": "2022-09-16T20:40:48Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CPKk9",
          "commit": {
            "abbreviatedOid": "823d08d"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-16T21:46:23Z",
          "updatedAt": "2022-09-16T21:46:23Z",
          "comments": []
        }
      ]
    },
    {
      "number": 348,
      "id": "PR_kwDOFEJYQs4_Ilep",
      "title": "Editorial: \"batch interval\" -> \"batch selector\".",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/348",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The Aggregate Share Encryption section still had a few references to \"batch interval\", but the encryption process now uses the \"batch selector\".",
      "createdAt": "2022-09-17T01:28:19Z",
      "updatedAt": "2023-11-27T23:10:36Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5fed67957d54d146ca4dfe312e62520867a8948b",
      "headRepository": null,
      "headRefName": "bran/rename-batch-selector-field",
      "headRefOid": "d7ffeded81fa96323c4fb968767f40f4dec3dc65",
      "closedAt": "2022-09-17T02:09:41Z",
      "mergedAt": "2022-09-17T02:09:41Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "74c26a4f2a122104bf6e8d01418c95c41ec4f81c"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CPnTl",
          "commit": {
            "abbreviatedOid": "d7ffede"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-17T02:09:08Z",
          "updatedAt": "2022-09-17T02:09:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 351,
      "id": "PR_kwDOFEJYQs4_SERN",
      "title": "Correct TaskId with TaskID",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/351",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-09-20T14:48:03Z",
      "updatedAt": "2022-09-20T14:52:02Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0d3be2e58442ff69445155e4d605539cd8a05ee7",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "unify-task-id-case",
      "headRefOid": "e7e06592cbd823a68dcdb82abf0ddc5b5475b848",
      "closedAt": "2022-09-20T14:52:02Z",
      "mergedAt": "2022-09-20T14:52:02Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "1b341dc602ebd62f6b11cf61b55d8be88f20d2ca"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CZgM6",
          "commit": {
            "abbreviatedOid": "e7e0659"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-20T14:51:56Z",
          "updatedAt": "2022-09-20T14:51:56Z",
          "comments": []
        }
      ]
    },
    {
      "number": 352,
      "id": "PR_kwDOFEJYQs4_SzjG",
      "title": "Editorial: Define PartialBatchSelector",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/352",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "The BatchSelector is the value agreed upon by the Aggregators during the collect flow. It is used to guide selection of a batch of reports for computing the aggregate result.\r\n\r\nConceptually, a PartialBatchSelector is \"promoted\" to a BatchSelector once the Collector's query is determined. The PartialBatchSelector is used during the aggregation flow to assign reports to batches. For example, for fixed size queries, the BatchID is selected during the Leader during the aggregation flow.\r\n\r\n\r\nThis is not a functional change, but I ended up wanting this abstraction in my implementation, so I thought I'd upstream it.\r\n\r\ncc/ @wangshan.",
      "createdAt": "2022-09-20T17:43:40Z",
      "updatedAt": "2023-03-06T19:16:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1b341dc602ebd62f6b11cf61b55d8be88f20d2ca",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/edit/5/part-batch-sel",
      "headRefOid": "a9f4727789a67db376e65a7e93e94c5fb53ae4af",
      "closedAt": "2022-09-22T19:48:37Z",
      "mergedAt": "2022-09-22T19:48:37Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "32ea3aec1bfe49d3b725e441021bcbc4fe197c5d"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I'm not sure the \"promoted\" behaviour is clear, it feels like just for reducing duplication? I'd also prefer to call it something closer to its true purpose.",
          "createdAt": "2022-09-20T19:17:42Z",
          "updatedAt": "2022-09-20T19:17:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm not sure the \"promoted\" behavior is clear, it feels like just for reducing duplication? I'd also prefer to call it something closer to its true purpose.\r\n\r\nDeduplication is the point, yes -- this change is editorial.\r\n\r\nLet me try another stab at explaining: A batch is determined by a \"batch selector\". In turn, the batch selector is determined by the Collector's query. But, there are parts of the protocol where the batch needs to be \"partially determined\" prior to the Collector's query. (In particular, the Leader needs to be able to pick the batch ID before the Collector makes its query.)",
          "createdAt": "2022-09-20T19:23:41Z",
          "updatedAt": "2022-09-20T19:28:14Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "LGTM. Talked to Chris P. offline about how `Query` and `BatchSelector` have the same structure, but learned that `Query` might change in the future with the current-batch PR.",
          "createdAt": "2022-09-22T17:33:36Z",
          "updatedAt": "2022-09-22T17:33:36Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Ch5cg",
          "commit": {
            "abbreviatedOid": "a9f4727"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I think that this change would slightly simplify some of Janus' implementation, so LGTM.",
          "createdAt": "2022-09-21T23:04:04Z",
          "updatedAt": "2022-09-21T23:04:04Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CmtOW",
          "commit": {
            "abbreviatedOid": "a9f4727"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-22T17:33:42Z",
          "updatedAt": "2022-09-22T17:33:42Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CnPNt",
          "commit": {
            "abbreviatedOid": "a9f4727"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-22T19:36:00Z",
          "updatedAt": "2022-09-22T19:36:00Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CnSll",
          "commit": {
            "abbreviatedOid": "a9f4727"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-22T19:48:33Z",
          "updatedAt": "2022-09-22T19:48:33Z",
          "comments": []
        }
      ]
    },
    {
      "number": 353,
      "id": "PR_kwDOFEJYQs4_S1Oi",
      "title": "Editorial: nit: Be consistent about tabs in pseudocode",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/353",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-09-20T17:51:02Z",
      "updatedAt": "2023-03-06T19:16:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1b341dc602ebd62f6b11cf61b55d8be88f20d2ca",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/edit/6/struct-spacing",
      "headRefOid": "55083cb00bc2d388417739ee4728d1e2d9fc389c",
      "closedAt": "2022-09-20T19:41:34Z",
      "mergedAt": "2022-09-20T19:41:34Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "dba0f3f9ead4414fd4fff81c35b00236a3eaf725"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "should we consider to be consistent with 4 spaces instead of 2?",
          "createdAt": "2022-09-20T19:07:36Z",
          "updatedAt": "2022-09-20T19:07:36Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> should we consider to be consistent with 4 spaces instead of 2?\r\n\r\nThe majority of tabs are 2, so that's what I went with.",
          "createdAt": "2022-09-20T19:10:22Z",
          "updatedAt": "2022-09-20T19:10:22Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CbAAH",
          "commit": {
            "abbreviatedOid": "55083cb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-20T19:34:24Z",
          "updatedAt": "2022-09-20T19:34:24Z",
          "comments": []
        }
      ]
    },
    {
      "number": 354,
      "id": "PR_kwDOFEJYQs4_THgh",
      "title": "Reduce QueryType from 2 bytes to 1",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/354",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Closes #350.",
      "createdAt": "2022-09-20T19:10:41Z",
      "updatedAt": "2023-03-06T19:16:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "dba0f3f9ead4414fd4fff81c35b00236a3eaf725",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/350",
      "headRefOid": "f137f1f1f696f03b11fd86bc307c667e64e40e0e",
      "closedAt": "2022-09-22T19:44:55Z",
      "mergedAt": "2022-09-22T19:44:55Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "4714fb9511b3c42f488ee409cf4557ee4389ca83"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Cf2_J",
          "commit": {
            "abbreviatedOid": "f137f1f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-21T15:29:29Z",
          "updatedAt": "2022-09-21T15:29:29Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CmjWi",
          "commit": {
            "abbreviatedOid": "f137f1f"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-22T17:00:49Z",
          "updatedAt": "2022-09-22T17:00:49Z",
          "comments": []
        }
      ]
    },
    {
      "number": 356,
      "id": "PR_kwDOFEJYQs4_Yfu4",
      "title": "Move and rename \"Nonce\" to \"ReportID\".",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/356",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-02"
      ],
      "body": "Historically, the Nonce included a time component. Now that the time component is split out & Nonce is effectively an opaque identifier, ReportID is clearer & more closely matches similar identifiers (TaskID, AggregationJobID, BatchID).\r\n\r\nCloses #355.",
      "createdAt": "2022-09-21T21:44:03Z",
      "updatedAt": "2023-11-27T23:10:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "dba0f3f9ead4414fd4fff81c35b00236a3eaf725",
      "headRepository": null,
      "headRefName": "bran/rename-nonce",
      "headRefOid": "0d9e9c0f137c30b6b5539c9fbf53907691f5b479",
      "closedAt": "2022-09-22T20:17:04Z",
      "mergedAt": "2022-09-22T20:17:04Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b16ca5aeec3ae5ebae409b12cbd2c3567b0b8407"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Note this PR includes a single wire-format change: `ReportMetadata` had its fields reordered so that the `ReportID` comes first, since the ID is the primary identifier. I am happy to revert this part of the PR if folks would prefer a purely-editorial PR.",
          "createdAt": "2022-09-21T21:57:33Z",
          "updatedAt": "2022-09-21T22:26:39Z"
        },
        {
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "body": "LGTM. Any thoughts on replacing `time` and `report_id` with a Nonce so then it has both?",
          "createdAt": "2022-09-22T16:58:13Z",
          "updatedAt": "2022-09-22T16:58:13Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> LGTM. Any thoughts on replacing `time` and `report_id` with a Nonce so then it has both?\r\n\r\nI'm pretty happy leaving `time` & `report_id` as separate fields -- for practical purposes, IMO `ReportMetadata` binds them together closely enough (along with the report extensions).\r\n\r\nConceptually, DAP is changing such that the primary identifier for a report is its opaque ID (with this PR, called `ReportID`). Previously, the primary identifier for a report was `(report timestamp, opaque report ID)`. The technical aspects of this change were made by cjpatton in his changes to introduce `fixed-size` tasks.\r\n\r\nIf we do decide to introduce a struct that is effectively `(timestamp, opaque report ID)`, I'd prefer if it was called something other than `Nonce` -- while we do expect that this value is a \"value to be used only once\", it's no longer used as a nonce in any cryptographic operations. And I'd argue there's no better justification to call the report ID (or `(timestamp, report ID)`) a nonce than, say, the aggregation job ID/batch ID/task ID. That is, IMO \"report ID\" more accurately names what this type is than \"nonce\" -- \"nonce\" is not technically incorrect, just less clear.",
          "createdAt": "2022-09-22T19:05:54Z",
          "updatedAt": "2022-09-22T20:40:48Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Cmin_",
          "commit": {
            "abbreviatedOid": "c248a8b"
          },
          "author": "MichaelScaria",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-22T16:58:34Z",
          "updatedAt": "2022-09-22T16:58:34Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CnTVF",
          "commit": {
            "abbreviatedOid": "0d9e9c0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Yeah I think this is a big win :) good idea @branlwyd ",
          "createdAt": "2022-09-22T19:51:25Z",
          "updatedAt": "2022-09-22T19:51:25Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CnZq6",
          "commit": {
            "abbreviatedOid": "0d9e9c0"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nice change \ud83d\udc4d ",
          "createdAt": "2022-09-22T20:15:59Z",
          "updatedAt": "2022-09-22T20:15:59Z",
          "comments": []
        }
      ]
    },
    {
      "number": 357,
      "id": "PR_kwDOFEJYQs4_dGLK",
      "title": "Editorial: Rename max_batch_lifetime",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/357",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-09-22T19:54:38Z",
      "updatedAt": "2022-09-22T20:19:21Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "32ea3aec1bfe49d3b725e441021bcbc4fe197c5d",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "rename-max-batch-lifetime",
      "headRefOid": "a411367a62984a700db8b2ccf320c55380214932",
      "closedAt": "2022-09-22T20:19:21Z",
      "mergedAt": "2022-09-22T20:19:21Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "ef08d3f44f56cec9cff86209eb9d0b1cc257de11"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@tgeoghegan please have another look",
          "createdAt": "2022-09-22T20:16:45Z",
          "updatedAt": "2022-09-22T20:16:45Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5CnUoT",
          "commit": {
            "abbreviatedOid": "8f298dd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This seems useful, since it disambiguates \"task_lifetime\" and \"batch_lifetime\". The former, as i understand it, is a timestamp; and the latter is a number of queries.\r\n\r\ncc/ @tgeoghegan for thoughtful review here",
          "createdAt": "2022-09-22T19:56:21Z",
          "updatedAt": "2022-09-22T19:56:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CnXMC",
          "commit": {
            "abbreviatedOid": "8f298dd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-09-22T20:06:05Z",
          "updatedAt": "2022-09-22T20:06:12Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "This error type should be renamed to no longer reference \"lifetime\"",
              "createdAt": "2022-09-22T20:06:05Z",
              "updatedAt": "2022-09-22T20:06:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CnZ_J",
          "commit": {
            "abbreviatedOid": "a411367"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udc4d\ud83c\udffb ",
          "createdAt": "2022-09-22T20:17:11Z",
          "updatedAt": "2022-09-22T20:17:11Z",
          "comments": []
        }
      ]
    },
    {
      "number": 358,
      "id": "PR_kwDOFEJYQs4_dfIl",
      "title": "Note change in the Report structure in the change log",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/358",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-09-22T21:57:37Z",
      "updatedAt": "2023-03-06T19:16:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ef08d3f44f56cec9cff86209eb9d0b1cc257de11",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-02/nit",
      "headRefOid": "b0e18cf8d8900e112eb65dee646c795d1bcb0fd6",
      "closedAt": "2022-09-22T21:58:19Z",
      "mergedAt": "2022-09-22T21:58:19Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "81cf892d8aafebe9cbb367b73346d0c50d687734"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 359,
      "id": "PR_kwDOFEJYQs4_f9FD",
      "title": "Remove redundant HpkeAeadKdfId and fix type alias errors",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/359",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-09-23T13:34:53Z",
      "updatedAt": "2022-09-23T14:51:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "81cf892d8aafebe9cbb367b73346d0c50d687734",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "missing-and-wrong-tls-types",
      "headRefOid": "9d2c2c3ccf77c27c9f1141ef7973e7e09fce6bbf",
      "closedAt": "2022-09-23T14:51:55Z",
      "mergedAt": "2022-09-23T14:51:55Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "7fb11ea15e5253b87de919217836672dfc55411f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Cqs_e",
          "commit": {
            "abbreviatedOid": "1816bc5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-09-23T13:41:04Z",
          "updatedAt": "2022-09-23T13:41:04Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "Gah! Nice catch.",
              "createdAt": "2022-09-23T13:41:04Z",
              "updatedAt": "2022-09-23T13:41:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5CqtB0",
          "commit": {
            "abbreviatedOid": "1816bc5"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-23T13:41:09Z",
          "updatedAt": "2022-09-23T13:41:09Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CrHv0",
          "commit": {
            "abbreviatedOid": "9d2c2c3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-23T14:49:25Z",
          "updatedAt": "2022-09-23T14:49:25Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5CrJBs",
          "commit": {
            "abbreviatedOid": "9d2c2c3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-23T14:51:50Z",
          "updatedAt": "2022-09-23T14:51:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 363,
      "id": "PR_kwDOFEJYQs4_s4gN",
      "title": "Specify unrecognized task ID error explicitly in all APIs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/363",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Addresses issue https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/361",
      "createdAt": "2022-09-27T14:38:39Z",
      "updatedAt": "2022-09-30T15:20:53Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7fb11ea15e5253b87de919217836672dfc55411f",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "specify-unrecognized-task-error",
      "headRefOid": "02bec41d73d03fca23f9fbf274696eeeca10b804",
      "closedAt": "2022-09-30T15:20:53Z",
      "mergedAt": "2022-09-30T15:20:53Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f9590df96acedeb66ca0ca50604a5b5a8494248d"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": ">     1. The status code is implied by the definition of \"abort\" in {{errors}}, so this doesn't need to be mentioned.\r\nI don't think that's true. [3.2 says](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-3.2):\r\n>DAP servers can return responses with an HTTP error response code (4XX or 5XX). For example, if the client submits a request using a method not allowed in this document, then the server MAY return HTTP status code 405 Method Not Allowed.\r\n\r\nSo maybe we do need to specify that the status should be 400 (client error) and not 500 (server error). FWIW, [ACME](https://datatracker.ietf.org/doc/html/rfc8555) has several places where it dictates HTTP 400 inline in the text, so I think it's OK for us to do that. \r\n\r\n>     2. It's always best to use the keyword \"MUST\" when prescribing behavior that is security critical.\r\n\r\nI agree we should be consistent about using MUST throughout the document. Curiously, ACME has some examples where the 400 error response is a MUST, but the 200 success response has no all-caps applied to it. [From ACME 7.6](https://datatracker.ietf.org/doc/html/rfc8555#section-7.6):\r\n\r\n> If the revocation succeeds, the server responds with status code 200\r\n>   (OK).  If the revocation fails, the server returns an error.  For\r\n>   example, if the certificate has already been revoked, the server\r\n>   returns an error response with status code 400 (Bad Request) and type\r\n>   \"urn:ietf:params:acme:error:alreadyRevoked\".\r\n",
          "createdAt": "2022-09-28T12:40:26Z",
          "updatedAt": "2022-09-28T12:40:26Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@tgeoghegan @cjpatton thanks for the comments, I've specified 400 in the errors section and shortened the words elsewhere. ",
          "createdAt": "2022-09-28T14:44:57Z",
          "updatedAt": "2022-09-28T14:44:57Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5C7tqv",
          "commit": {
            "abbreviatedOid": "48b830f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "A couple high level comments:\r\n1. The status code is implied by the definition of \"abort\" in {{errors}}, so this doesn't need to be mentioned.\r\n2. It's always best to use the keyword \"MUST\" when prescribing behavior that is security critical.\r\n\r\nBased on this, I think the best verbiage is:\r\n\r\n> MUST abort with error \"unrecognizedTask\"\r\n\r\n",
          "createdAt": "2022-09-28T03:04:13Z",
          "updatedAt": "2022-09-28T03:04:13Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5DASwp",
          "commit": {
            "abbreviatedOid": "02bec41"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-09-28T18:07:20Z",
          "updatedAt": "2022-09-28T18:07:20Z",
          "comments": []
        }
      ]
    },
    {
      "number": 365,
      "id": "PR_kwDOFEJYQs4_7lBb",
      "title": "Add a missing Empty type definition",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/365",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2022-09-30T11:27:57Z",
      "updatedAt": "2022-10-11T00:29:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7fb11ea15e5253b87de919217836672dfc55411f",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "define-empty",
      "headRefOid": "8a208a7b2c2f548d884df91da6d1ecb44c692d2a",
      "closedAt": "2022-10-11T00:29:33Z",
      "mergedAt": "2022-10-11T00:29:33Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "ed1685a92b8c3670c5b6f42306780f32fcb97499"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5DLIBg",
          "commit": {
            "abbreviatedOid": "8a208a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "No objection.",
          "createdAt": "2022-09-30T15:19:02Z",
          "updatedAt": "2022-09-30T15:19:02Z",
          "comments": []
        }
      ]
    },
    {
      "number": 367,
      "id": "PR_kwDOFEJYQs5AxASd",
      "title": "DO NOT MERGE sketch of resource oriented DAP API",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/367",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "This is a sketch of a resource oriented API for DAP. It's presented\r\nseparately from the main body of protocol text because before taking on\r\nthe work of integrating it in, I want to begin a discussion about this\r\nproposal, leading up to a presentation at IETF 115.\r\n\r\nThe objectives are to refactor the DAP API into a resource-oriented API\r\nmore in line with conventional HTTP semantics and REST. In particular, I\r\nam trying to pay close attention to idempotence and ensuring that\r\nprotocol participants can recover gracefully from network messages being\r\nlost.\r\n\r\nDesigning the API around resources identified by a URI also simplifies\r\nmessage handling: servers no longer need to parse the first part of a\r\nmessage to get a task ID to then figure out what the rest of the message\r\ncontains, because now things like a task ID are in a resource's path.\r\n\r\nWe also should be able to write less protocol text, because if all of\r\nour resources are boring HTTP nouns, then the usual semantics for things\r\nlike HTTP 202 Accepted, 3xx redirects or 404 Not Found apply.\r\n\r\nI do _not_ introduce an ACME style API directory, because I'm not\r\nconvinced that it is appropriate to introduce an extra level of\r\nindirection for DAP API requests, which are going to be more performance\r\nsensitive than ACME.\r\n\r\nI also deliberately avoid ACME's POST-as-GET requests, because our\r\nexperience at Let's Encrypt has been that those make it hard to cache\r\nresponses, because CDNs don't expect POST results to be cacheable.\r\n\r\nI've put this up on GitHub as a PR so that it's out in public and folks\r\ncan leave comments inline reasonably gracefully, although I don't intend\r\nto ever merge this.\r\n\r\nRelevant to #278",
      "createdAt": "2022-10-13T20:07:04Z",
      "updatedAt": "2023-01-20T19:18:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "4e08fa0c860797f395d57d0546c0d3ca5fad13ed",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/resource-api-sketch",
      "headRefOid": "134d5305849f52a59061a543e843387564b24a69",
      "closedAt": "2023-01-20T19:18:04Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@tgeoghegan thanks for putting this together. I have a couple of comments about this proposal:\r\n1. I'm not sure idempotency is achievable with today's protocol, for two main reasons: anti-replay check & various time related expiration rules. We maybe able to do it with /upload endpoint if we ask Leader to always ignore repeated uploads. \r\n2. The `collection-id` proposed requires the whole agg-flow only starts after an colleciton-id has been identified and collect request issued. But in today's standard Leader is free to proactively aggregate reports before a collect is requested, this is important to minimise resources needed on aggregators. PR https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/313 is attempt to flesh out details about this.   ",
          "createdAt": "2022-10-17T15:44:55Z",
          "updatedAt": "2022-10-17T15:44:55Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> * It would be useful to flesh out a bit more the requirements for resource limits. How long do I have to keep data artifacts around before I can throw them away?\r\n\r\nI agree that DAP needs to deal with this, however I don't think any new ambiguities are introduced by this change. Yes, there are circumstances where a helper must respond to `PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` with an error indicating it doesn't have the report shares anymore, but that exact same situation exists when it's `POST /aggregate` instead.\r\n\r\n> * (Not sure what the answer is here.) Does this close the door on DAP deployments with multiple helpers, or does the design anticipate this?\r\n\r\nI don't see any way this change forbids more than one helper, though I think DAP should commit to exactly two aggregators anyway.\r\n \r\n> * I think this definitely warrants discussion at IETF 115, if at all possible.\r\n\r\nWG involvement and consensus is obviously important. But I don't know if a slide deck and a few brief minutes of discussion will let us make much progress. I think a call to action to come hash out the design in the WG mailing list or here on GitHub will be most effective.",
          "createdAt": "2022-10-20T14:15:24Z",
          "updatedAt": "2022-10-20T14:15:24Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I've been putting together some sequence diagrams of the sub-protocols to illustrate how the API flows, where side effects are incurred by POST requests, and some error recovery. Disclose any triangle for biiiiiiiig sequence diagram PNGs and also links to editable source diagrams.\r\n  \r\n<details>\r\n  <summary>Aggregate sub-protocol with leader crashes (obsoleted by <a href=https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/367#issuecomment-1319267422>this diagram</a>)</summary>\r\n\r\n  ![image](https://user-images.githubusercontent.com/19212995/200424632-f124c936-01bb-401b-b34c-b578d8453917.png)\r\n  [editable source](https://sequencediagram.org/index.html#initialData=CoSwLgNgpgBAggcwQJygghmWBnArgIxgAdkB7MUgY1IhgHdwALGadAEymRkuXW0agAoQUXTIwISiFEA7MDAAyUdpxFiJU2fIASUCEVWDk+UgA8YpAG6dFyjsgA0u-ZwBcAZTCZY74HABKwIIm5lY2SirIHpQCbLjQMOgyiUioGBKkyQBWpPgAOjJJbDBxvPgQAJ7EnNgg2PLg2DAAkgAiDgUgMpQQuBzFqESk4tgOMFBglAB0ghH2ALQAfM4GUQAKAKrAMAD0XtgA1tg7AN77B-MgbAC+O+ipaJggmQD6OfjHAGakpMFmFtYuCs3ABhUgAWyIuCwMAAjPUYGRcDJiiQoEQCuCoNhsPdsYkUSVcGVKgVVrUEWABOCjCEATZgVFPN4YP4APIbAByrThf1CgNskQAxKg2K45jYeHwBNhBIz5iKoGKAMymCVReHyJGEtGiVAwLE4vGyulhLjq8V2GyoerqUbcTJ4CBgbBkmp1LDFW1YWn-M2C+weLww3wBILqpaM1ybbZ7PhHU7nS43O4PdLPGRvXJfH58+lAvSrVyM7gCShHarICmemDeqAdGSoagIGQgABe+PuCBg7xg31IYxtQxRrpkDCptYoorhCO1qMGmOxuIQ2Llhc4SwtmsRpGR8-RYlghuXq4jy3X6zZvl252OZ3jyduXbST1e7xzpAKACFSGwKq4dz3ABeWEG23OcYHwMh2EoPh5F1Q8DSXY0839KNqEhaFYAAJkJCDdUXI0VyaIoiRJCo3SrD0YCpKBwQKAoAB5qAgYZXCFYYkhXRZ3CuWAoE+T4oEoMBGJ2FjhkWACS1gmQCjYOpYOQYpPhAKstV3HVBkPQiTxIwlqHiAYBNwbAoAKCha04SxJBwAgzIAR1wKA5AKGMb3jO8kyuJ801fTN3x2ftfX5BkLyDFl2S5HlsNQgV1UVMV1W4Xh+FXeVEtcVULVwgZNP3PUj2Q4i4vCK0omSm0vBGMZqBkJ0XUo6slUnbwQvzAM3GZGEou5XkzyjNYr1jW9EwfHzUxQR4MgC7Mgp+b9f3-QCURAsDZ3yyDoLYWCEQQ-VjxQ00BSjGSywrRpK2ar1g3rAozPxKlMBgcADUwGJ8Qg2E1xcZBN3K1xcpWgrEMOkrjrKyJXDBTCYSBiCoNIGC4LJQYkKIztCVKdBygomRyWo2iaWMP14oB7rYF6mLZnKyNwqG6840OLzxpTZ9pozLMPnmz8ZB-P8ALnIDsIbeHNsR5G9u0g7itXCGC1+1xVMKCB21gLooXkfhEIIlFiRxyoruo-aWo16FRyYiSog43gZG43iOHGQThNE8SaEk6SL24JISkUsQ2Hu4TMjyvdiGloqMdHUjsFIN6qlQT5TNgSyzOQGzKHM+r7KgJyXPkBngAKJmE3vQ5H0ml8Zq5j8ClQXP6iacdmCF76SdChWiwpmAADFmk5Zp3G0ABRVoftWf6ofcHPnO6WBSE+GA1nDzx0QACmVupYgASna-0LS73v+8HkfBCAA)\r\n</details>\r\n\r\n\r\n<details>\r\n  <summary>Collect protocol with time interval query</summary>\r\n  \r\n  ![image](https://user-images.githubusercontent.com/19212995/200425561-882a59dc-a1db-4db4-8e62-af6b5c0ac01b.png)\r\n  [editable source](https://sequencediagram.org/index.html#initialData=CoSwLgNgpgBAwgewtAxmGAHATgsCVIwDu4AFjGCALawgB2YUWAbgIYQwCOArkwJ4AoARlZZKKECIYwAMlFYATJsNHjJraQAkoEDMpFiQEqekTIoaBFiFnUeLAFoAfHMVMAXAAUAqsBgB6MFYAZwBrYP8AbyCwhxAFAF9-AnM0EAQ6YIAdOgAlKB4oYPRgvQkAMxAimGCEGgpqWgYmNggBACMEAA8YBGYmWXklLHcULHlGGFYYFLt0uhgAKwR2nI0FGAVuLFZ2iD5MJmCQYphwHIAKcGCYb1yASQAaM7oUCG4lDfGMKzBgx5y9DeHygG1YAHNweNwaxKBklitggBKASuYbOWwWezuTTAYCeGAAZgADISYOMFCBxmk6OCKAgcoEQuEojFQnFEskkHMMhFyggEB1ur1+lh4NysVZ3AARba7faHLDHYo5WZY24PASYyyOFxDDyeADyAGU-EywhFosyOUk1Wlef5+YK0UwMRKdTi8QSAEzE70CVhpNiTbX2AR0XCwLAgcGkdAIcri1LYxXpSkodgKn7Ieh0u3oO5PHLgqBgSi0mC4-EwX3e8lFH6ZIqo-W67S6A2+AJsy1sm3+CFQqAwxgAfWCpFEzc6PT6A3behGsp2ewOi+VYDWkOhsNgE6nCNWdCufw1TxewM+9Z+YmydCB7yvg53cIWy3ayIEC9dercIyrBIkmSFJUliub0oyPasta8RJM+w67uOk7jHyApCrOoqDH+MpyquiobluQ4jnuyGwIWAgQAKGAwNwDAgBw8HETUpGHmcNwKBkUAtn+zjfiMADiACiZpQVasSwQO24IWO+4oY6aF8c4Lr-l6NZ+gIUB0Ao3Hok4fHuEJInMr2MGcoxiGyUU8mCopv7DJ61a+sSOR8VMUlMZZAhKIGlDBrAoZWDY7r2EprZeCaRkWtB4mcvm8yoc6rZusmUoAWpzl0KG8z1sE3AQGAAhAA)\r\n</details>\r\n\r\n<details>\r\n  <summary>Collect protocol with fixed batch query</summary>\r\n\r\n  ![image](https://user-images.githubusercontent.com/19212995/200425737-26e120f5-27f1-46f2-afab-3eea1eee00e0.png)\r\n  [editable source](https://sequencediagram.org/index.html#initialData=CoSwLgNgpgBAwgewtAxmGAHATgsCVIwDu4AFjAGYgAeUAJjAEYCGYK5AjgK5RYCeAKAEZmWMCBQgRAO3QAZKMzq9ho8ZJnoAElAgYVIsRKnNZ8JKjxYhjBNRgIAbrxgKlvAFwARKAGcQAObSrLC+CAC2sFzSBMhQaPQAOtIsbOTMvjAARChcWFhQslkAdDBeecyMEHzJ+lj+vuhEpBLkqewwIJk5eQVFxQKIcWgIWAC0AHxuylgeAAoAqsAwAPRgGQDWvisA3uu+G2MgdAC+K7GWIAjSvskASlDcfui++pJUfjC5+YVgY+2kAS2exOFzTTxwAohGDML4WeLia4wABWCEYlFGyWy3z6YCyTFYHVMDDoFSqNWkdQa6HAMAAFOBMgs7gBJAA0nRiEC4yjoWIKGFGYF8HJAXJ59BhAQCBQCrCu0mSqMYvgAlANweMJkNLKMPFpgMA5jAAMwABhNMAKdBABTQYoCMDwyTWm22e02R1O53h9uu2woCAQQLsDmcWHMwys3jJ1UwvGpyQuCJgzJZg19VkmmvmAHkAMrLV0Hd37Q7HM7Jv03FaB4PAsNgxQzbx+QLBMChCKwXCkFzRKudvkpQnpbo434lMqxvjx+pdMDJZqtAlpTrj3qT4rJZIAHlieoAxKNTAEoBN88dYFAKBQEbufRBRhMPDBfFwVY8eGYCk9GrdZAQGBFiLMtS09CtH0uf1kkDCMJzMAFiBAZAYD4EBdBJEBb14X5kgKd8IDAARNUmHUET1A0jRgAAmM0aIEZh7UcaFyJGaxpFwWAsECUh0AQChI11WY5yuG0UGYZBZ0FZAHThKNU1ZNlkjPMBxGkR0qONOiaKtPxBRuPwSObXhJh0PRPBA1YwN2MsvTOZhpVlEIAH1fFIUQjIbUEI3MuoYywSo4ypBdkkcmUoDlTs3w8goUTRZIGWFRT2U5FBuV5PTBTEACxXSiUGHC5zEWkeKVVVAQ-NMqYTNmLTTQtPSbTtdTHWdaRiy2WyIO9IrItc9zPIDIMQxBcNXFqgKgukhNQuCJz+uiwa4rTAQnwQDAYGicQIClCKotCWLYGVdcYDoa4oGM9wtSq2YAHEAFFQLdbqDnslY+oOtyjuG4NbuzSb6p0gRCjoK6ZjM3R-Me56S1e8teoWr7lr8WsRv+mrrv1Q1tLNM1kluvbisOzyBGUJjxBY6K2KsIQadGAGsbmAtYa6j03sgwcFV+8HqvpuqcdovHkhphU9MI4igA)\r\n</details>",
          "createdAt": "2022-11-07T22:09:13Z",
          "updatedAt": "2022-11-17T22:06:03Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "During the PPM WG session at IETF115, @martinthomson suggested that DAP clients shouldn't be allowed to choose report IDs. So instead of `PUT /tasks/{task-id}/reports/{report-id}`, clients would do `POST /tasks/{task-id}/reports`. The server would choose a report ID and store the report under it. This has to be a POST (I think) because it has the side effect of generating a report ID.\r\n\r\nI like the idea of servers being able to synthesize the report IDs totally internally, but we need to think through how we would enable clients to retry report uploads without the risk of duplicate submissions.\r\n\r\nI think it might be a bit easier to do this for aggregation jobs, since the partial batch selector might suffice to coalesce requests together.",
          "createdAt": "2022-11-08T18:09:39Z",
          "updatedAt": "2022-11-08T18:09:39Z"
        },
        {
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "body": "In a design that uses POST, the server takes more responsibility for managing duplicate reports. the idempotency key draft might provide some help there, but there isn\u2019t really a magical answer.",
          "createdAt": "2022-11-08T18:39:08Z",
          "updatedAt": "2022-11-08T18:39:08Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "In the latest commit, I've taken a stab at making the helper responsible for constructing opaque aggregation job URIs. I think this may also be possible for reports, if we can convince ourselves that there is some means of de-duplicating uploaded reports without a report ID. For instance, the leader could hash the input share, or we could use an [idempotency-key header](https://datatracker.ietf.org/doc/draft-ietf-httpapi-idempotency-key-header/).",
          "createdAt": "2022-11-17T21:28:44Z",
          "updatedAt": "2022-11-17T21:28:44Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "<details>\r\n  <summary>Updated aggregate sub-protocol sequence diagram, illustrating an n-round VDAF and better highlighting side effects.</summary>\r\n\r\n  ![image](https://user-images.githubusercontent.com/19212995/202569506-db0180b0-2d51-4c8f-adf9-ecd9ffe5ffef.png)\r\n  [editable source](https://sequencediagram.org/index.html#initialData=CoSwLgNgpgBAdgagIwFoBOB7ArnAJjAQwHMi0oiCxYBnLAIxgAdMwMBjDCGAd3AAsY0ArihoYbNAWp8o1AFBzGBNGBBsQSuGBgAZKMNGLlq9Zu0AJKBEaG5BBhgBuo3fpFoANJeuiAXAGUwSlh-YABBACVgOToMAA8YJxc9AzQAthlcLGhCOEISMgpVDDyAKww6AB04AjwYLMk6CABPJlFqEGptcGoYAEkAEQ9qkDg2CCwRfDJGDBVqDxgoMDYAOjkU9xQAPm8bNIAFAFVgGAB6IOoAa2ozgG9Lq5QQXABfM+JSckoQEoB9cp0eSxBJJMR7PwAYRKXTQWDY2lq+S+RV+ZQqMCOET61WhAFtGFgqDAkF0YJgcPhmFBGNU8bJqMRZLl8A17C1qvsOmSwDI8XI0CDEs5wVZ9gEgsSIgB5I4AOQGJJi8WFyTcogAxGRcL5Ni4JFIZPIIWgUFqoDqAMxxPVpczAYAHGAAJgADEgYJCyMFcNUdOwfiVfDA7oCUFg0CBXtUAEIYXDNYOk7QUurUpRkGD06iMoiyZWgkWuVK69ViMhdYwLcQw7Jgaic9qdKj4StUAVCsHF9wS4IwUKRaK2nYm3zHU4XKQ3e6PZ5vD4Fb7FOAAirAlVd0cm8QyNg3NpobktmBtqDDOB4ygZZklGB0K8CahWKAIuYs6qfJjKAh4xYV2Z4L0vC8tUXRzBaJJkqmVIzFmDJMsaYqiDstq+Pajouu6nrei2foBsuwahhU4aRtGcBxgmwZwCgvLktgaYzMosDZrm+adkWqH4oSxLJnRlJ3pgwhsFIYCcoxmYsQhLL1BG7LNI2h7NjAvJQPygobhxZa9lKsoKkqw67EhhzSqEIZhhGUaxvGiZ8XgAC8SDnrx0ECRgQkiUw4nMfBebroWLijhwBJErAzp1C56Z0j5zK1KyslNPJcBckpKn8vYGkBUZGpzLUeYBC8sBQAAZkVL5gMG27CXA1S4J0wloNMNJzNo0hMb0sXVBw2SNUVWBPspGAnqIjhqFAoH0E+ACOWBQFoMAHCZpxEXQJGWXAwECNBDkdhloo+GkgR9jK8qKs6cgmihWlhdM9EwTSTFwTmCFyHZdn+FAIoEFweLgbZuDtZmUBxC+IW4K9O3+WItpeEZ2mwMdelwBsZYjrDC2mctq1kRRNlbXA57UbRLl0IJuDCWS6YPZJvkFqqe3ikF3GwIgqBE7dnk0lFT2+dJbIJQpR7KXydjsZl+3ZZIcB5f4BVLCVZUVUZ4i1DVdXKPg1Gs5t7OU2QXOse1dRdRAPV9WNWiDU+aAjWwNATVA02zaJcDo0t5mkTw-B-XZmsQ3TMCjodOknfAyC012trmjqtriJI0j5hdUe+NaqEszR2v8br3nc2xu3dn4McVkE8yLBwcC0BA9YC82EGnn7EdaUH8O6YqSMGaOrtmcRFnY9ZwZ4+e6d-a57kU15j0G+HRZbkr157r0PQHketeSmeoFQMyvKUDA4BZg+zIuUjF3bKnyBDxF4-U7nkP52kXEhaHWvDyTblkyJYk0hPUmxTJjQcklTYeTC3UjfVCTcYAI1bmHduaNFpdxWj3KylFvYswJmfNm-EX6j20FnL+NNRb0z8EVUYX0QAAC9YCjG4iePgD1Ip4Hii0JeSks74CoUSeQ6Ub4mglrlKA+URBy1KgiRW+1lZ5FqtQeqvo4BpwwQxe6ElooNhqHUagg1LytDIL1fqrAhrW1GuNIEDsZpzU7pjRBcAyCOy6EBL2eMw4gP9oHVeMAABifQ5R9H8OYAAogMc6RlLolneo7MYsAMBFXml5QINIAAUxC4CdEyAASnrppEJriPFeJ8f4uQQA)\r\n</details>",
          "createdAt": "2022-11-17T22:04:22Z",
          "updatedAt": "2022-11-17T22:04:22Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The discussion here has run its course and the changes are being pursued in #395, #398, #399 and #400. I'm closing this PR because we have no intention to merge it and to declutter the repository's open PR list.",
          "createdAt": "2023-01-20T19:18:04Z",
          "updatedAt": "2023-01-20T19:18:04Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ECrTN",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-13T21:48:13Z",
          "updatedAt": "2022-10-13T21:56:28Z",
          "comments": [
            {
              "originalPosition": 322,
              "body": "```suggestion\r\n`collection-id` uniquely identifies a collection. It is chosen by the collector when it PUTs to the resource.\r\n```",
              "createdAt": "2022-10-13T21:48:13Z",
              "updatedAt": "2022-10-13T21:56:28Z"
            },
            {
              "originalPosition": 217,
              "body": "I think the list responses will require some additional metadata to indicate when to stop requesting pages.",
              "createdAt": "2022-10-13T21:56:25Z",
              "updatedAt": "2022-10-13T21:56:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EHhdw",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-14T18:20:01Z",
          "updatedAt": "2022-10-14T18:20:01Z",
          "comments": [
            {
              "originalPosition": 217,
              "body": "Yeah, I should have put a TODO there, because the pagination stuff isn't completely thought through. In fact we should follow the lead of [ACME](https://datatracker.ietf.org/doc/html/rfc8555#section-7.1.2.1) here and use link relations. [RFC 8288](https://datatracker.ietf.org/doc/html/rfc8288) appears to be the relevant reference.\r\n\r\nHowever, it's possible we can get away without providing the endpoints for listing aggregation jobs, aggregate shares and collections if we can instead work out a way for protocol participants to query for exactly the aggregation job, aggregate share or collection they care about, perhaps by putting things like aggregation parameters, batch selectors or queries into query parameters.",
              "createdAt": "2022-10-14T18:20:01Z",
              "updatedAt": "2022-10-14T18:21:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ENRpn",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-17T15:02:01Z",
          "updatedAt": "2022-10-17T15:02:01Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "I think it can work with taskprov, which specifies how to provision a task in-band, it could end up calling the `PUT tasks/<task-id>` API. But so far the out-of-band task provision is not specified, so we may want to flesh that out (if we want to flesh that out) first.",
              "createdAt": "2022-10-17T15:02:01Z",
              "updatedAt": "2022-10-17T15:02:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ENdja",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-17T15:25:14Z",
          "updatedAt": "2022-10-17T15:25:14Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "I don't think this is idempotent if aggregator proactively aggregates reports. If two identical AggregateJobPutReq is received before and after a batch is collected, then the later one will end up in error.",
              "createdAt": "2022-10-17T15:25:14Z",
              "updatedAt": "2022-10-17T15:25:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ENebd",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-17T15:27:22Z",
          "updatedAt": "2022-10-17T15:27:23Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "even without collect flow, anti-reply will make this non-idempotent too:\r\n\r\nhttps://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/main/draft-ietf-ppm-dap.md#early-input-share-validation-early-input-share-validation\r\n\r\n",
              "createdAt": "2022-10-17T15:27:22Z",
              "updatedAt": "2022-10-17T15:27:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ENxvD",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-17T16:18:44Z",
          "updatedAt": "2022-10-17T19:48:31Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "I kind of like `/tasks/{task-id}/hpke_config`. This is more in-line with how other resources are specified, specifically it does not require a query parameter.\r\n\r\nDAP implementations that want to use an effectively-global config would arrange for every task to return the same HPKE config information.",
              "createdAt": "2022-10-17T16:20:26Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 14,
              "body": "nit: no need to fix this document, but once this eventually lands in the RFC, could we better clarify between path arcs that are literals (like `tasks` or `reports` here) and those that are standing in for a value (like `task-id` or `report-id` here).\r\n\r\nNot sure what best RFC practices are but something like `{task-id}` would be fine to remove ambiguity, and has some precedent being used in this way.",
              "createdAt": "2022-10-17T16:22:39Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 95,
              "body": "What happens if an attempt is made to upload two different reports with the same (task, report) IDs? PUT semantics might allow it to be updated, but I don't think we want to allow that -- specify?",
              "createdAt": "2022-10-17T16:24:55Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 175,
              "body": "This may be too much detail for this doc, but I think we should mention that once the aggregate job has \"stepped\" forward, further `PUT` requests must fail -- the way things are worded right now I think a helper could receive the same `AggregateJobPutReq` twice, and either silently accept it w/o updating state or \"reset\" the aggregation job back to its initial state. Both are probably undesirable.",
              "createdAt": "2022-10-17T16:37:53Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 183,
              "body": "Alternate aggregation idempotency suggestion:\r\n\r\nThe current counter-based suggestion will work. However, it requires the leader to realize that it is out of sync with the helper (requiring specific error codes from the helper?), then run protocol steps/make requests that it would never otherwise run (GET of the aggregation job, followed by an aggregate-continue based on the results of that GET).\r\n\r\nAnother approach, based on my previous suggestion, would be to have the helper remember (a hash of) the last `AggregateJob` \"step\" request it received from the leader for each aggregation job. When the helper receives a request, if that request is for the current round, handle it normally. If the request is for the previous round and the request matches the (hash of) the previous request, return the current state. Otherwise, fail with an appropriate error.\r\n\r\nAdvantages, as I see them:\r\n* Leader does not need to be able to realize it is out of sync with the helper -- just by sending the requests it \"normally\" would based on its current state, the protocol will move forward & recover from being out-of-sync.\r\n* Added complexity to helper does not require any additional \"states in the state machine\" -- all additional logic is in handling of a single request, rather than being split over multiple requests.\r\n* GET of aggregation jobs can be dropped (I think -- I don't see that they're needed for anything else).\r\n* One fewer network request to recover from falling out-of-sync.\r\n\r\nDisadvantages:\r\n* Helper does have to store (a hash of) the last request it received from the leader.",
              "createdAt": "2022-10-17T16:48:37Z",
              "updatedAt": "2022-10-17T19:56:25Z"
            },
            {
              "originalPosition": 211,
              "body": "Why do we need a paginated list of aggregation jobs?\r\n\r\nI think this can be dropped from the specification (& left up to implementations, if they find it handy).",
              "createdAt": "2022-10-17T18:46:57Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 234,
              "body": "Q: This aggregate share ID is new, correct?\r\n\r\nCould this ID be the batch ID (for fixed-size tasks) or the batch's time interval (for time-interval tasks) rather than being opaque? [The hardest part to be is how to represent the interval in a URI path.]",
              "createdAt": "2022-10-17T18:52:47Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 279,
              "body": "> This implies that `time_interval`-type queries now also have a batch ID, chosen by the leader.\r\n\r\nThis might be somewhat outside the scope of review of this document, but: the equivalent of \"batch ID\" for a time-interval task is the interval being collected. There is no need for a separate opaque batch ID in this case.",
              "createdAt": "2022-10-17T18:55:16Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 281,
              "body": "I think `AggregateShareReq` should use `BatchSelector` -- `Query` will likely morph to allow e.g. the collector to ask the aggregators to pick a batch to collect (and in general may change further to support Collector->Aggregator requests), while `AggregateShareReq` will always want to directly specify exactly one batch by ID.",
              "createdAt": "2022-10-17T18:57:25Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 299,
              "body": "Why do we need a paginated list of aggregate shares?\r\n\r\nI think this can be dropped from the specification (& left up to implementations, if they find it handy).",
              "createdAt": "2022-10-17T19:02:05Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 322,
              "body": "Currently DAP doesn't specify collection IDs. Could that be done as part of this work? I think a `CollectionID` as a 16-byte opaque identifier would fit will with similar identifiers in the spec. I think explicitly defining collections would as a side effect make some of the validation logic easier to specify/follow.\r\n\r\n(BatchID doesn't work, unfortunately: a collection is logically identified by a batch ID along with an aggregation parameter. A more radical change might be to specify something like `/tasks/{task-id}/batches/{batch-id}/collections/{agg-param}` to collect. This would be very clean in terms of API layout & allowing our validation to be written in terms of explicitly-defined entities, but I don't think we want to commit to putting an aggregation parameter in the URI path -- we don't know how large it is, or that it will fit well in the URL-safe character set.)",
              "createdAt": "2022-10-17T19:04:02Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 379,
              "body": "> a collection is already uniquely identified by the combination of the aggregation parameter and the query\r\n\r\nnit: this is true today but won't be true once #342 is resolved; at that point, there will be some mechanism for the Collector's `Query` to say something like \"I don't know the specific batch I want, just give me one that's ready (if any)\" -- i.e. the Collector's `Query` will not directly identify a single specific batch.\r\n\r\nI guess the more accurate way of saying this would be that a collection is identified by an aggregation parameter and a batch identifier (literally `BatchID` for fixed-size, a time interval for time-interval).",
              "createdAt": "2022-10-17T19:21:44Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 385,
              "body": "I'm confused, wouldn't the leader ignore the reports that fall into interval _i_ after the first collection request at _t1_? \r\n\r\nFrom DAP-02: \"The Leader MUST ignore any report pertaining to a batch that has already been collected.\"\r\n\r\nMultiple collections for a single batch are to allow aggregation parameter to vary; I don't think the current wording of DAP-02 allows for collecting \"the same batch but with a few more reports that have arrived since the last collection\" to avoid potentially breaking privacy of those new reports.",
              "createdAt": "2022-10-17T19:27:45Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 401,
              "body": "Why do we need a paginated list of collection jobs?\r\n\r\nI think this can be dropped from the specification (& left up to implementations, if they find it handy).",
              "createdAt": "2022-10-17T19:28:53Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            },
            {
              "originalPosition": 299,
              "body": "```suggestion\r\nA paginated list of aggregate shares for the task.\r\n```",
              "createdAt": "2022-10-17T19:47:18Z",
              "updatedAt": "2022-10-17T19:48:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EO2mV",
          "commit": {
            "abbreviatedOid": "a4cc2ec"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-17T20:07:56Z",
          "updatedAt": "2022-10-18T20:35:49Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "@cjpatton explained to me how your extension works by putting a task definition into the extensions of a report. We could instead do a PUT to `/tasks/task-id`, but then that requires an extra request in the client, possibly every time they upload a report (otherwise how do they know whether the task already exists?), so the taskprov report extension still seems valuable.",
              "createdAt": "2022-10-17T20:07:56Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 44,
              "body": "I agree, but deployments that use the task-prov extension may not know the task ID at the time of requesting the HPKE config. [The taskprov draft](https://github.com/wangshan/draft-wang-ppm-dap-taskprov/blob/main/draft-wang-ppm-dap-taskprov.md#supporting-hpke-configurations-independent-of-tasks-hpke-config-no-task-id) includes some language recommending the use of a global HPKE config to work around this, but it would still be reasonable for aggregators to reject `/tasks/{task-id}/hpke_config` requests if `{task-id}` is not yet known to them. This could be fixed if taskprov asserts that aggregators should return a global HPKE config from `/tasks/{task-id}/hpke_confifg` if `{task-id}` is unrecognized. I'm curious what @wangshan and @cjpatton think.",
              "createdAt": "2022-10-17T20:12:55Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 95,
              "body": "You're right. IMO the specified behavior should be to reject the second PUT request, because the first report may already have been accumulated into an aggregate share by the aggregator, and frankly it sounds like a PITA to service the update: you'd have to substract/un-accumulate the previous input share from the aggregate and then add the new one, and I don't think VDAF spells out how to do that.\r\n\r\nFWIW I think this problem exists in DAP-02, as well.\r\n\r\n- [x] note that resources are immutable (may not be updated with a second PUT)",
              "createdAt": "2022-10-17T20:15:21Z",
              "updatedAt": "2022-10-20T14:25:47Z"
            },
            {
              "originalPosition": 175,
              "body": "It's idempotent in the sense that repeated `PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` requests won't change the state, the key point being that it's safe for the leader to resend that request to the helper multiple times.\r\n\r\nI also think that collection of a batch doesn't need to affect how the helper responds to this request. Let me sketch out a hypothetical:\r\n\r\n1. Leader sends `PUT /tasks/{task-id}/aggregation_jobs/agg-job-1` (here, `agg-job-1` is some specific aggregation job identifier).\r\n2. Helper replies with `struct AggregateJob` containing first-round prepare message\r\n3. Leader sends `POST /tasks/{task-id}/aggregation_jobs/agg-job-1` with first-round broadcast prepare message\r\n4. Helper prepares the reports, consuming batch query count for the relevant reports, and replies with `struct AggregateJob` containing a sequence of `PrepareStep` that are all in state `finished`.\r\n\r\nAt this point, if the leader sends `PUT /tasks/{task-id}/aggregation_jobs/agg-job-1` again (for whatever reason) _and if all the members of the `struct AggregateJobPutReq` are unchanged_, then the helper shouldn't re-evaluate anti-replay, because it can check its own storage, notice it's already prepared all those inputs, and return the same response it did in step (4). This doesn't violate anti-replay or privacy budgets because it's the exact same set of reports under the same aggregation parameter.\r\n\r\nHowever, if the leader sends `PUT /tasks/{task-id}/aggregation_jobs/agg-job-1` _but varies any of the fields of the body_, then the helper should reject it. So I think we should specify that the aggregation job resource may not be mutated once created, except to advance its state through a POST request.\r\n\r\nFurther, if the leader sends `PUT /tasks/{task-id}/aggregation_jobs/agg-job-2` and the reports in that request's body overlap with the ones prepared by `agg-job-1`, then the helper has to apply anti-replay and either consume query limit or refuse to service the request, as appropriate.",
              "createdAt": "2022-10-18T20:00:20Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 175,
              "body": "I think you are raising the same question as Shan did [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/367/files#r997208345), so I would direct you to my response there. But you bring up an additional good point about mutating an aggregation job with a PUT, which I think we should explicitly forbid.",
              "createdAt": "2022-10-18T20:04:45Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 211,
              "body": "Given that aggregation job IDs are assigned by the leader, I believe you are right, because the leader should be able to maintain an authoritative list of agg jobs itself instead of relying on the helper.",
              "createdAt": "2022-10-18T20:08:18Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 234,
              "body": "Yes, aggregate share ID is new, and introduced for consistency with other resources, and for the same fundamental reason that I introduce `collection-id`. The helper's aggregate share resource is strongly analogous to the leader's collection resource, so a lot of the same considerations apply, including the question of whether or not an aggregate share or collection is uniquely identified by the (aggregation parameter, query) tuple. Anyway, I'll discuss that question further in the collection-id portions of the doc [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/367/files#r997427041).",
              "createdAt": "2022-10-18T20:22:10Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 279,
              "body": "I think this again is the question of what uniquely identifies an aggregate share or collection, which I'd like to consolidate [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/367/files#r997427041).",
              "createdAt": "2022-10-18T20:23:14Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 299,
              "body": "Agreed, I will delete this resource.",
              "createdAt": "2022-10-18T20:24:21Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 385,
              "body": "You are correct, I forgot about that requirement in DAP-02. I need to revisit this text after thinking harder about `current-batch` and fixed batch queries in general.",
              "createdAt": "2022-10-18T20:25:59Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 401,
              "body": "Deleted with the other list resources",
              "createdAt": "2022-10-18T20:32:59Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            },
            {
              "originalPosition": 379,
              "body": "A recurring set of questions here is (1) whether aggregate shares and collections need an opaque ID to uniquely identify them beyond the (aggregate parameter, query) tuple and (2) who should choose that ID if it is needed. I'm still ruminating on this; more thoughts to follow in this thread.",
              "createdAt": "2022-10-18T20:35:46Z",
              "updatedAt": "2022-10-18T20:35:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EYzT9",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-19T11:17:10Z",
          "updatedAt": "2022-10-19T11:17:10Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "@tgeoghegan what I meant was after the task has been provisioned following task_prov extension, the leader/helper can call a _PUT to /tasks/task-id_ to store the task object. The /tasks endpoint should not be exposed to client.",
              "createdAt": "2022-10-19T11:17:10Z",
              "updatedAt": "2022-10-19T11:17:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EY23w",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-19T11:28:20Z",
          "updatedAt": "2022-10-19T11:28:21Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "@tgeoghegan makes sense, I think it's safe privacy wise since the agg flow is for preparation. My other concern is time related states, if the 2nd `PUT /tasks/{task-id}/aggregation_jobs/agg-job-1` come at a time the task_id has expired, would helper responds differently? If not then we are potentially asking helper to store output share or aggregate share forever.",
              "createdAt": "2022-10-19T11:28:20Z",
              "updatedAt": "2022-10-19T11:28:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EaW5P",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-19T14:57:10Z",
          "updatedAt": "2022-10-19T14:57:11Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "Good suggestion, I adopted it!",
              "createdAt": "2022-10-19T14:57:11Z",
              "updatedAt": "2022-10-19T14:57:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EcfHj",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "We obviously have some details to work out, as you've noted, but overall I strongly support this change. The main concern I have is that there may be hidden costs in terms of implementation complexity. (I haven't done a thorough review yet, but I'll be looking out for things that might require big refactors in Daphne.) If we can mitigate that, I think the obvious benefit is a much, much cleaner spec that will be easier to implement fresh.\r\n\r\nHigh-level things:\r\n\r\n1. It would be useful to flesh out a bit more the requirements for resource limits. How long do I have to keep data artifacts around before I can throw them away?\r\n2. (Not sure what the answer is here.) Does this close the door on DAP deployments with multiple helpers, or does the design anticipate this?\r\n3. I think this definitely warrants discussion at IETF 115, if at all possible.",
          "createdAt": "2022-10-19T21:51:40Z",
          "updatedAt": "2022-10-19T22:32:55Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "\"collection jobs\"?",
              "createdAt": "2022-10-19T21:51:40Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 24,
              "body": "I'm not sure this is our most compelling example: What if a response is dropped during the aggregation flow?",
              "createdAt": "2022-10-19T21:54:10Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 28,
              "body": "I wouldn't worry too much about stepping on taskprov; let's just worry about spelling out the core protocol. The considerations may overlap, they may not. At the end of the day, it's up to taskprov to make sure that it  fits in appropriately with the core protocol.\r\n\r\nThat said, I think the operative question here is: Who is using this task end point, and why would we trust them to configure tasks? In taskprov we define an additional role, called the Author, that is semi-trusted for configuring tasks. The protocol spells out how to opt-in/opt-out of a task, but I wouldn't expect this mechanism to be suitable for all deployments.\r\n\r\nI would lean towards keeping task provisioning out of this change. Perhaps @divergentdave could focus on building out the plumbing for this in the interop draft for if/when we need it in the core protocol? (That is, spelling out task configuration for internal test endpoints.)",
              "createdAt": "2022-10-19T21:56:48Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 40,
              "body": "As ChrisW would no doubt ask at this juncture: What are your requirements here? Why would we need multiple HPKE configs?\r\n\r\nOne reason was previously pointed out by @ekr: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/248. TL;DR: The Client and Aggregator may not initially agree on what HPKE ciphersuite to use. But we may not need to expose multiple configs simultaneously, if the Client can indicate in its request which suites it supports.",
              "createdAt": "2022-10-19T22:08:50Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 93,
              "body": "Who needs this functionality, and for what purpose? How do we think about how long this report needs to be kept around?\r\n\r\nThis might be a bit tricky for Daphne because we may want to shard reports by timestamp.",
              "createdAt": "2022-10-19T22:13:22Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 143,
              "body": "Who needs to implement this: the Leader, Helper, or both? Presumably just the Helper? How long do we need to keep this around?",
              "createdAt": "2022-10-19T22:15:55Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 193,
              "body": "Nice idea.",
              "createdAt": "2022-10-19T22:22:00Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 279,
              "body": "Hmm, are you confusing \"batchID\" with \"aggJobID\" here? I don't see as batch ID in the agg job endpoint.",
              "createdAt": "2022-10-19T22:26:32Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 317,
              "body": "This would change if we took #313.",
              "createdAt": "2022-10-19T22:28:40Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            },
            {
              "originalPosition": 331,
              "body": "nit: Avoid using italics \"_blah_\" in I-Ds. this doesn't get rendered the way you want. (Look at the draft-ietf-ppm-dap.txt.)",
              "createdAt": "2022-10-19T22:30:15Z",
              "updatedAt": "2022-10-19T22:32:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgMrp",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T13:50:50Z",
          "updatedAt": "2022-10-20T13:50:50Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "From DAP-02 [5.4.1](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-5.4.1):\r\n\r\n> Furthermore, the aggregators must store data related to a task as long as the current time has not passed this task's task_expiration. Aggregator MAY delete the task and all data pertaining to this task after task_expiration. Implementors SHOULD provide for some leeway so the collector can collect the batch after some delay.\r\n\r\nSo if a task expires, then from that point, a helper MAY respond to `PUT /tasks/{task-id}/aggregation_jobs/agg-job-1` with an error. It could also choose to continue providing the results of the aggregation job indefinitely. It's up to the implementation to decide when it discards data belonging to a task like aggregate results.\r\n\r\nI think there will have to be some text explaining that leaders should be prepared for helpers to have discarded the results of an aggregation job, but I don't think this is an idempotence problem, because the state change (helper discarding aggregate job results) wasn't a result of repeated `PUT` requests by the leader.",
              "createdAt": "2022-10-20T13:50:50Z",
              "updatedAt": "2022-10-20T13:50:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgMtx",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T13:50:54Z",
          "updatedAt": "2022-10-20T13:50:54Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "I think you're right. The most substantial thing I could come up with was allowing clients to enumerate tasks, but without a more concrete use case, we're better off deleting this resource.\r\n\r\n- [x] delete task(s) resource(s)",
              "createdAt": "2022-10-20T13:50:54Z",
              "updatedAt": "2022-10-20T14:07:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgO1F",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T13:54:24Z",
          "updatedAt": "2022-10-20T13:54:25Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "I think you (quite understandably) read this as meaning a DAP client, but here I am using \"client\" in the sense of an HTTP client, such that the leader is a client when it makes requests to the helper, and the collector is a client when it makes requests to the leader. I'll clarify this.\r\n\r\n- [x] clarify meaning of \"client\" to mean HTTP client, not DAP client",
              "createdAt": "2022-10-20T13:54:24Z",
              "updatedAt": "2022-10-20T18:53:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgQNY",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T13:56:46Z",
          "updatedAt": "2022-10-20T13:56:47Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "This change will be complicated enough, so I think I will choose not to solve #248, too, so I lean toward leaving the `hpke_config` exactly as it is.\r\n\r\n- [x] put `hpke_config` back the way it was",
              "createdAt": "2022-10-20T13:56:46Z",
              "updatedAt": "2022-10-20T14:07:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgQxZ",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T13:57:57Z",
          "updatedAt": "2022-10-20T13:57:58Z",
          "comments": [
            {
              "originalPosition": 93,
              "body": "You're right, I was just putting it in because semantically it's obvious, but there's no protocol use case, so we shouldn't specify it (implementations are free to implement this if they find it useful).\r\n\r\n- [x] remove GETters we don't explicitly need in the protocol",
              "createdAt": "2022-10-20T13:57:58Z",
              "updatedAt": "2022-10-20T14:16:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgRU3",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T13:59:08Z",
          "updatedAt": "2022-10-20T13:59:08Z",
          "comments": [
            {
              "originalPosition": 331,
              "body": "Yeah, but this isn't an I-D and will get drastically re-written into a different PR when it gets integrated into the text.",
              "createdAt": "2022-10-20T13:59:08Z",
              "updatedAt": "2022-10-20T13:59:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EghK1",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T14:29:12Z",
          "updatedAt": "2022-10-20T14:29:13Z",
          "comments": [
            {
              "originalPosition": 143,
              "body": "Just the helper, as noted above, but we do need to--\r\n\r\n- [x] add some language discussing how long servers should have to respond to GET aggregate jobs, aggregate shares, collections, considering task expiration and DELETE requests against these",
              "createdAt": "2022-10-20T14:29:12Z",
              "updatedAt": "2022-10-20T19:22:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EghvM",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T14:30:18Z",
          "updatedAt": "2022-10-20T14:30:19Z",
          "comments": [
            {
              "originalPosition": 279,
              "body": "This is aggregate share, not aggregate job, which does reference batch ID in [DAP-02](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-02.html#section-4.5.2-3)",
              "createdAt": "2022-10-20T14:30:18Z",
              "updatedAt": "2022-10-20T14:30:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EgiUE",
          "commit": {
            "abbreviatedOid": "d9336f7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T14:31:31Z",
          "updatedAt": "2022-10-20T14:31:32Z",
          "comments": [
            {
              "originalPosition": 379,
              "body": "- [x] Do we need an API level concept of collection ID or aggregate share ID? If so, who chooses those IDs? Can the same solution work for both time interval and chunky DAP queries?",
              "createdAt": "2022-10-20T14:31:32Z",
              "updatedAt": "2022-10-21T19:30:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EiKoY",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-20T18:53:07Z",
          "updatedAt": "2022-10-20T18:53:08Z",
          "comments": [
            {
              "originalPosition": 217,
              "body": "Resolving since the list resources have been deleted anyway, but if/when we want something like this again, RFC 8288 link relations are the way to go.",
              "createdAt": "2022-10-20T18:53:07Z",
              "updatedAt": "2022-10-20T18:53:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EoF5X",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-21T18:04:41Z",
          "updatedAt": "2022-10-21T18:04:42Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "I like how this proposal eliminates an extra `GET` roundtrip. My intuition though is that the helper has to hold on not just to a hash of the previous request, but possibly also its response from the previous round. I've got to try to sketch out some failure scenarios in many-round VDAFs to see how this plays out.",
              "createdAt": "2022-10-21T18:04:42Z",
              "updatedAt": "2022-10-21T18:04:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Eob_t",
          "commit": {
            "abbreviatedOid": "b478641"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-21T19:21:32Z",
          "updatedAt": "2022-10-21T19:21:32Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "I think I get it now -- your scheme assumes that the leader would record its state before trying to advance state in the helper, which means that in the case of a crash, the helper can never be ahead of the leader.\r\n\r\nI think that same assumption eliminates the need for the `GET` in the `round` solution. Suppose the helper is on round `n`. After recovering from a crash, the leader sends `POST` to the aggregate job with either:\r\n\r\n- `round = n`: helper has seen this message before the leader crashed and sends its current state (round _n+1_ prepare messages). Protocol continues from there. \r\n- `round = n+1`: helper advances its state to the next round. Protocol continues from there.\r\n- any other value of `round`: the leader and helper are too far out of sync. The aggregate job must be abandoned. The leader could recover by creating a new aggregate job with the same parameters, essentially going back to the first round. \r\n\r\nSo I think either notion (request hash or round number) can achieve the functionality we want. Question is: which is \"better\"?",
              "createdAt": "2022-10-21T19:21:32Z",
              "updatedAt": "2022-10-21T19:23:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EwlaX",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-24T18:25:51Z",
          "updatedAt": "2022-10-24T18:25:52Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "> I like how this proposal eliminates an extra GET roundtrip. My intuition though is that the helper has to hold on not just to a hash of the previous request, but possibly also its response from the previous round.\r\n\r\nThe design written in this document also requires the helper to store the response from the previous round -- the response from `GET /tasks/{task-id}/aggregate_jobs/{aggregation-job-id}` suggested by the written design is directly the response from the previous round. The data storage requirements to the helper are the same in both the written design & in my suggested change.\r\n\r\n===\r\n\r\n> I think I get it now -- your scheme assumes that the leader would record its state before trying to advance state in the helper, which means that in the case of a crash, the helper can never be ahead of the leader.\r\n\r\nAh, I didn't intend to suggest this. Specifically, I don't think there is much any design can do to avoid the helper's state being out of sync with the leader's state in light of arbitrary crashes. (Also, I am not suggesting the removal of the `round` parameter.)\r\n\r\nThe only difference between my suggestion and the written design is, in a \"crash-recovery\" scenario:\r\n* In the written design, the leader will perform a `GET /tasks/{task-id}/aggregate_jobs/{aggregation-job-id}` to retrieve the relevant `prep_msgs`, then use those `prep_msgs` along with the prepare state it does have in order to recover its own next state & the next message to send. Presumably, the leader realizes it needs to send this `GET` by attempting a `POST` with the information it does have, and receiving an error message from the helper communicating something like \"incorrect round number\" -- that is, the leader needs to realize it is in a \"crash-recovery\" scenario by receiving & correctly interpreting a particular error code.\r\n* In my suggestion, the leader simply sends a `POST` with the state/message it does have. Since the `round` parameter it sends is one less than what the helper expects but matches the previous-round request, the helper notices this and returns its current state (i.e. what would be returned by a `GET` against the aggregation job). The leader does not need to realize it is in a crash-recovery scenario at all -- just by following the \"normal\" aggregation protocol, things will get back in sync.",
              "createdAt": "2022-10-24T18:25:51Z",
              "updatedAt": "2022-10-24T18:28:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Ew9dR",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-24T19:45:06Z",
          "updatedAt": "2022-10-24T19:45:07Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "For one thing, I think we agree that `GET /tasks/{task-id}/aggregate_jobs/{aggregation-job-id}` is unnecessary, so I'm going to remove it for the sake of simplicity and concision.\r\n\r\nPast that, I'm wondering whether the helper keeping track of a hash of the previous request is necessary for the helper to gracefully resume. Because `Vdaf.init`, `Vdaf.prep_next` and `Vdaf.prep_shares_to_prep` are deterministic ([VDAF](https://www.ietf.org/archive/id/draft-irtf-cfrg-vdaf-03.html#section-5.2)), the helper can assume that even if the leader restarts and were to re-run its side of the preparation protocol from scratch, the prepare messages computed by the helper the first time around remain valid.\r\n\r\nI made a [sequence diagram](https://sequencediagram.org/index.html#initialData=A4QwTgLglgxloDsIAIAyBTEATdYBQoks8ISyAEugDbC555gBGA9gB7LMBuuamOYALgDKEEBHTIhAFQCCAJSkMW7Lj0o1cw0eMmyFeDNlwBaAHzraggAoBVRco7cwFapYEBhZgFtgAVx0AjADOKGDMvghYyMBg6MAAOghe6EFBIADmKcikUVi+YCCMVACeiZZBUCHIEAAW6F5KbI5qrpoiYhJyAPI2AHIAIsgBeBYmpob8AsGh4ZHRsYQSyakZKXgOqs4Tmp4+-hLTyGERUYxh2DAgIWULyMtpmUHZc3kFRaUI5ZUotfWNKk5eEZBO0dN0+oNhtswGZRtYutJEgAhZhYYoCI6zLAAXgCABpEodjnMzswLlcUDE4uAlikHmsNoC4QIYN4-DoAExzYlRKkJJJ01ZPHLIV6FEplXAVKq-BpMJqbFwaEHaTo9AbIDnrBWA6EAYliWAE0OQMAKQTqQRGrRhBvQRoAzKxoQIuVEefNqbE7oLHtqATwXSbYiFwBAgnjTcwEEFfFRw5KwNLxFFQ+J-s0tnw2qrkOCNVDszDzDaBFYEVJkaj0ZiTriCQgiVjkKTyVU+TSfSs-YyWsqBHDTXUYABrJ5QcPRKXfe3INPoBtBdBZWpiZATu5iGCW2tzYZwswut273kLTv3IX+zNAya7dkSY8e1tYS7XT63C+PZ65fLij5fGU6jlXss2BLQOjzdVBi1aFYVLctEQQFE0QxHlsQ5BtH2bZ9X0pM9vU-BkdT7NwADMoAQEAqCgAAvCQKPZOcak7PlEjFd4pyTb5PUWKIGP8K15QDZxmVBCQADEAElekkoRyAAUX6a1lUPIthHQABHXx0AQGAJGYUjkCsfD0BEOIAApyIQSo6iwABKK9FRdMTkCkmS5MUvAgA) illustrating how I think a leader and helper would recover from a leader crash during round 2 of a 3-round VDAF.\r\n\r\n![Crash recovery in 3 round VDAF](https://user-images.githubusercontent.com/19212995/197611360-4c81629d-676d-4c72-b6dd-10414018b59d.svg)\r\n\r\nIt seems to me that the leader restarting, putting itself back into State = ROUND 1, and then resending the 1st round broadcast prepare message suffices for it and the helper to gracefully resume the protocol. Where would the helper hashing the leader's PUT request help? I guess it would catch the case where for some reason, the leader sends a different list of 1st round prepare messages after restarting than it did the first time?",
              "createdAt": "2022-10-24T19:45:06Z",
              "updatedAt": "2022-10-24T19:45:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ExOBL",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-24T20:26:24Z",
          "updatedAt": "2022-10-24T20:26:24Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "> Where would the helper hashing the leader's PUT request help?\r\n\r\nThe only purpose of storing the (hash of) the previous request is to ensure that the response the Helper gives is coherent (i.e. matches the request).\r\n\r\nWhile you are correct that the Leader should (if properly following DAP) always send the same request, there is nothing practically ensuring this if the Helper doesn't check somehow. A sufficiently-buggy Leader could send any request at all, and without the hash (or some other mechanism) to allow the request to be verified, the Helper would give the answer a non-buggy Leader would have received rather than the error the actually-buggy Leader should receive.\r\n\r\nIf folks don't think it's valuable to avoid this scenario, storage of the (hash of) the request can be dropped. IMO, I wouldn't recommend this (it's unlikely to come up in practice, but if it does, the results will likely be _very_ confusing to operators).",
              "createdAt": "2022-10-24T20:26:24Z",
              "updatedAt": "2022-10-24T20:26:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ExflT",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-24T21:25:43Z",
          "updatedAt": "2022-10-24T21:25:43Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "Yeah, I think you're on to something. I had hoped that cases where the leader is sending the wrong prepare messages after a restart would be caught by failures in the helper's `Vdaf.prep_next` call, but at best they'd be caught on the next round.\r\n\r\nWhat do you think about considering this fault recovery case separately from the resource oriented API change? The PR that introduces that into the protocol text will already be quite large, and this scenario that we are discussing is nuanced enough that it would benefit from having a spotlight shone on it, which is to say, a PR dedicated to making just that change.",
              "createdAt": "2022-10-24T21:25:43Z",
              "updatedAt": "2022-10-24T21:25:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ExuUj",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-24T22:25:23Z",
          "updatedAt": "2022-10-24T22:25:23Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "Sure -- I think the issues are separable. Probably the resource-oriented API should be specified first, then can be tweaked if necessary to achieve idempotency or otherwise solve the fault-recovery issue.",
              "createdAt": "2022-10-24T22:25:23Z",
              "updatedAt": "2022-10-24T22:25:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5EzIiH",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-10-25T07:15:08Z",
          "updatedAt": "2022-10-25T07:15:09Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "It occurs to me it might wiser to leave it up to the implementation: the fault-recovery logic `MUST` check the request's round number, and `SHOULD`(?) check the entire request [worded to allow checking equality or by hash or some other technically-approximate method]. WDYT?",
              "createdAt": "2022-10-25T07:15:09Z",
              "updatedAt": "2022-10-25T07:15:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fmn1j",
          "commit": {
            "abbreviatedOid": "b478641"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-03T22:45:10Z",
          "updatedAt": "2022-11-03T22:45:11Z",
          "comments": [
            {
              "originalPosition": 175,
              "body": "I misunderstood idempotency, your explanation is correct: it's not about the response being the same but the state not changed by multiple calls.",
              "createdAt": "2022-11-03T22:45:11Z",
              "updatedAt": "2022-11-03T22:45:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FmohY",
          "commit": {
            "abbreviatedOid": "c3729db"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-03T22:49:29Z",
          "updatedAt": "2022-11-03T22:49:29Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "@tgeoghegan returning global HPKE config when taskID unrecognized should work. But I prefer the `/hpke_config` endpoint, because it's a resource identified by hpke config id, task_id as a query parameter seems appropriate here.",
              "createdAt": "2022-11-03T22:49:29Z",
              "updatedAt": "2022-11-03T22:49:29Z"
            }
          ]
        }
      ]
    },
    {
      "number": 370,
      "id": "PR_kwDOFEJYQs5BHsbY",
      "title": "Align media type of aggregate request/response",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/370",
      "state": "MERGED",
      "author": "easonchan1213",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "In IANA all media types seem to be of \"application\" type instead of \"message\" type, this PR is to align the mismatch in the protocol description and adapt to the media types specified in IANA.",
      "createdAt": "2022-10-19T15:31:55Z",
      "updatedAt": "2022-10-19T20:25:40Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ed1685a92b8c3670c5b6f42306780f32fcb97499",
      "headRepository": "easonchan1213/draft-ietf-ppm-dap",
      "headRefName": "align-media-type",
      "headRefOid": "73a75731dbf8b2190ee227c74fbe9dd67a317911",
      "closedAt": "2022-10-19T19:42:27Z",
      "mergedAt": "2022-10-19T19:42:27Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b849420524188648fce90845cfaa03449e9984c5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Eb6Ks",
          "commit": {
            "abbreviatedOid": "73a7573"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Nice catch!",
          "createdAt": "2022-10-19T19:32:55Z",
          "updatedAt": "2022-10-19T19:32:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 372,
      "id": "PR_kwDOFEJYQs5B7EPp",
      "title": "Use structs for AAD encoding",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/372",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "This revises the sections dealing with encryption and decryption of input shares and aggregate shares, rewriting the construction of each AAD input to be in terms of RFC 8446-style structs instead of byte string concatenation. This fixes #371.",
      "createdAt": "2022-10-31T22:32:34Z",
      "updatedAt": "2022-11-09T16:23:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "66aa97a400784cfe11836d385552230257cbd1d0",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "aad-message-structs",
      "headRefOid": "9078bbe3dc2b60c59725036bcad31a04a24a7cb3",
      "closedAt": "2022-11-09T16:23:11Z",
      "mergedAt": "2022-11-09T16:23:11Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "97ec44665a446ab8798543a3246765dfe648c2f1"
      },
      "comments": [
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "I squashed this back into two commits and then rebased to fix some conflicts.",
          "createdAt": "2022-11-08T23:30:48Z",
          "updatedAt": "2022-11-08T23:30:48Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@ekr would you mind taking a quick look?",
          "createdAt": "2022-11-08T23:37:41Z",
          "updatedAt": "2022-11-08T23:37:41Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5FZJuU",
          "commit": {
            "abbreviatedOid": "1e279d7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Solid implementation, and a clear win \ud83d\udc4d \r\n",
          "createdAt": "2022-11-02T00:27:02Z",
          "updatedAt": "2022-11-02T00:27:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Fd-gy",
          "commit": {
            "abbreviatedOid": "1e279d7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-02T15:23:19Z",
          "updatedAt": "2022-11-02T15:23:19Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5FeQ6n",
          "commit": {
            "abbreviatedOid": "1e279d7"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Generally LGTM.",
          "createdAt": "2022-11-02T15:59:21Z",
          "updatedAt": "2022-11-02T16:01:55Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nencoded message  of type InputShareAAD, constructed from the same values as\r\n```",
              "createdAt": "2022-11-02T15:59:21Z",
              "updatedAt": "2022-11-02T16:01:55Z"
            },
            {
              "originalPosition": 59,
              "body": "```suggestion\r\n`server_role` is `0x02` for the leader and `0x03` for a helper, and\r\n`agg_share_aad` is a value of type `AggregateShareAAD` with its values set from the corresponding fields of the `AggregateShareReq`.\r\n```",
              "createdAt": "2022-11-02T16:01:03Z",
              "updatedAt": "2022-11-02T16:01:55Z"
            },
            {
              "originalPosition": 89,
              "body": "```suggestion\r\nresponse to its query as follows:\r\n```",
              "createdAt": "2022-11-02T16:01:48Z",
              "updatedAt": "2022-11-02T16:01:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FmLEH",
          "commit": {
            "abbreviatedOid": "a5d4b45"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-03T20:39:57Z",
          "updatedAt": "2022-11-03T20:39:58Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "nit: if we follow he same camel case convention of types, this should be `InputShareAad`. Same applies to `AggregateShareAad`",
              "createdAt": "2022-11-03T20:39:57Z",
              "updatedAt": "2022-11-03T20:39:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fmio1",
          "commit": {
            "abbreviatedOid": "c695d81"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-03T22:13:52Z",
          "updatedAt": "2022-11-03T22:13:52Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5F-ugm",
          "commit": {
            "abbreviatedOid": "9078bbe"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM",
          "createdAt": "2022-11-09T13:43:58Z",
          "updatedAt": "2022-11-09T13:43:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 373,
      "id": "PR_kwDOFEJYQs5CEw9A",
      "title": "Adopt must-implement semantics for extensions",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/373",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Closes #369.",
      "createdAt": "2022-11-02T16:47:49Z",
      "updatedAt": "2023-03-06T19:16:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "97ec44665a446ab8798543a3246765dfe648c2f1",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/369/hidden",
      "headRefOid": "421a252ec98001b8452ba61e868e37859d4d9435",
      "closedAt": "2022-11-16T22:55:43Z",
      "mergedAt": "2022-11-16T22:55:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "46e6890e419ef2be23ed0e73fd9e9a86694b5f2c"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2022-11-08T23:22:12Z",
          "updatedAt": "2022-11-08T23:22:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2022-11-09T17:43:32Z",
          "updatedAt": "2022-11-09T17:43:32Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan I've removed the shared extensions as discussed. Can you take another look and re-approve?",
          "createdAt": "2022-11-09T17:52:20Z",
          "updatedAt": "2022-11-09T17:52:20Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5FetJ3",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-02T17:04:07Z",
          "updatedAt": "2022-11-02T17:04:07Z",
          "comments": [
            {
              "originalPosition": 53,
              "body": "This is a bit awkward. The requirement I'm trying to meet here is: The Leader shouldn't have to have its HPKE decryption keys available during the upload sub-protocol. One way to avoid this would be to move the extensions out of the plaintext.\r\n\r\nNote that \"must implement\" semantics is required during the aggregation flow; see below.",
              "createdAt": "2022-11-02T17:04:07Z",
              "updatedAt": "2022-11-02T17:05:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FfhiO",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-11-02T19:30:54Z",
          "updatedAt": "2022-11-02T19:37:19Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Do we need a new error type here, as well as the corresponding prescription elsewhere that an implementation MUST provide this error type in some scenario?\r\n\r\nThe question I think we should apply to all error types (including the existing ones) is whether any protocol participant actually behaves differently than they would if they just got some kind of generic \"Bad Request\" error.\r\n\r\nSuppose a leader rejects a report because it contains an unrecognized extension. If the client sees a problem document with `unrecognizedExtension`, what is it supposed to do? Try again, trying different subsets of the extensions until it finds a combination the leader accepts? That doesn't make sense to me. If the client is configured to send some list of extensions, but the leader is configured to accept a different list of extensions, then I don't think the protocol can recover in-band. The client and leader operators would need to dig into why the configuration disagrees and take some action that is outside the scope of DAP.\r\n\r\nThat leads me to conclude that an error of type `unrecognizedMessage`, in response to which the client simply stops trying to upload that report, should suffice for the case of unrecognized extensions. The detail field of the problem document constructed by the server can contain detailed information on what's wrong with the message that aids operators in debugging, but I don't think the protocol needs to dictate a particular error code.",
              "createdAt": "2022-11-02T19:30:54Z",
              "updatedAt": "2022-11-02T19:37:19Z"
            },
            {
              "originalPosition": 51,
              "body": "```suggestion\r\nIf the Report contains an unrecognized extension, then the Leader MAY abort the\r\n```",
              "createdAt": "2022-11-02T19:32:24Z",
              "updatedAt": "2022-11-02T19:37:19Z"
            },
            {
              "originalPosition": 64,
              "body": "```suggestion\r\nauthenticated information in the Report. Extensions that need to be handled by\r\n```",
              "createdAt": "2022-11-02T19:32:47Z",
              "updatedAt": "2022-11-02T19:37:19Z"
            },
            {
              "originalPosition": 66,
              "body": "I worry that putting extensions in two different places will lead to complexity in the implementations and ambiguities, too: what if the same extension type appears in both places but with different values? Either DAP or the specification of each extension has to spell out how to resolve such conflicts.\r\n\r\nUnless I'm missing something, the reason to allow extensions in two places is to avoid transmitting the same extension in each aggregator's ciphertext. If I'm right and this is just about that performance optimization, could we avoid doing this until operational experience shows it to be a problem worth the extra complexity?",
              "createdAt": "2022-11-02T19:36:24Z",
              "updatedAt": "2022-11-02T19:37:19Z"
            },
            {
              "originalPosition": 123,
              "body": "Same observation here that I had about the problem document types: it's not clear to me how the leader will handle this differently than some more generic error code.",
              "createdAt": "2022-11-02T19:37:10Z",
              "updatedAt": "2022-11-02T19:37:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FgLLC",
          "commit": {
            "abbreviatedOid": "38cb0c3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-02T22:00:32Z",
          "updatedAt": "2022-11-02T22:08:01Z",
          "comments": [
            {
              "originalPosition": 66,
              "body": "We already have this problem for ReportMetadata.extensions, but I agree that de-duplicating in two places adds a bit more complexity. Basically instead of:\r\n\r\n1. Parse report\r\n2. De-duplicate Report.metadata.extensions\r\n3. Decrypt input share\r\n\r\nWe have to do\r\n\r\n1. Parse report\r\n2. Decrypt input share\r\n3. De-duplicate Report.metadata.extensions and plaintext_input_share.extensions\r\n\r\n> Unless I'm missing something, the reason to allow extensions in two places is to avoid transmitting the same extension in each aggregator's ciphertext. If I'm right and this is just about that performance optimization, could we avoid doing this until operational experience shows it to be a problem worth the extra complexity?\r\n\r\nGiven how important a consideration bandwidth has been, I think it's worth adding a bit of complexity to avoid increasing overhead here. This is particularly concerning for the taskprov extension, FWIW.",
              "createdAt": "2022-11-02T22:00:32Z",
              "updatedAt": "2022-11-02T22:08:01Z"
            },
            {
              "originalPosition": 4,
              "body": "I think it's better to reserve unrecognizedMessage for true parsing errors, but I take your point. There's no reason to spell out this extra error case because there's no automated response to it.",
              "createdAt": "2022-11-02T22:06:47Z",
              "updatedAt": "2022-11-02T22:08:01Z"
            },
            {
              "originalPosition": 123,
              "body": "Here I have a different view: Knowing what went wrong is useful telemetry because if we observe a spike in rejection rates, we'll want to investigate what's going on.",
              "createdAt": "2022-11-02T22:07:55Z",
              "updatedAt": "2022-11-02T22:08:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fgf4G",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-03T00:08:36Z",
          "updatedAt": "2022-11-03T00:08:37Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I make the distinction between unrecognized extension and parsing error because it's possible to parse an unrecognized extension without knowing its structure. This is by virtue of the length tag.",
              "createdAt": "2022-11-03T00:08:36Z",
              "updatedAt": "2022-11-03T00:08:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fkh3-",
          "commit": {
            "abbreviatedOid": "38cb0c3"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-03T15:49:01Z",
          "updatedAt": "2022-11-03T15:53:07Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "nit: field name mismatch\r\n```suggestion\r\nField `extensions` is set to the list of extensions intended to be consumed by\r\n```",
              "createdAt": "2022-11-03T15:49:01Z",
              "updatedAt": "2022-11-03T15:53:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fk7Qx",
          "commit": {
            "abbreviatedOid": "38cb0c3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-03T16:41:40Z",
          "updatedAt": "2022-11-03T16:48:33Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I maintain that for now, we should use `unrecognizedMessage`, and perhaps rename that error to just `badRequest` or something more generic. In particular I don't think this PR should add new error types.",
              "createdAt": "2022-11-03T16:41:40Z",
              "updatedAt": "2022-11-03T16:48:33Z"
            },
            {
              "originalPosition": 123,
              "body": "Yes, I buy that we need a way for implementations to diagnose problems. In this case, you're arguing that you want the leader to be able to tell why a report share was rejected by the helper? This is handled reasonably gracefully anywhere we use HTTP problem documents because the `details` field allows adding arbitrary information for use in logging or metrics, but here we only have a single error code to insert in the list of `PrepareStep` structures. Maybe we could change `ReportShareError` so that instead of being just an enum, it's a structure that contains a value of the current `ReportShareError` enum and a `details` field that can optionally provide an implementation specific integer error code? Then the protocol wouldn't have to say anything specific about unrecognized extensions but your leader could still get something to track metrics on, and then ask your peer operating the helper why you're suddenly getting so many errors with `details = 18674749`, or whatever error code their implementation assigns to this case.",
              "createdAt": "2022-11-03T16:48:26Z",
              "updatedAt": "2022-11-03T16:48:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FsbR_",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-04T23:56:28Z",
          "updatedAt": "2022-11-04T23:56:28Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Done!",
              "createdAt": "2022-11-04T23:56:28Z",
              "updatedAt": "2022-11-04T23:56:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fsbxs",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-05T00:04:39Z",
          "updatedAt": "2022-11-05T00:04:40Z",
          "comments": [
            {
              "originalPosition": 123,
              "body": "Hmmm, that might be OK. I'm sort of used to enumerating different error types, based on my experience with TLS. (See [\"AlertDescription\"](https://datatracker.ietf.org/doc/html/rfc8446#section-6). Though I take your point that we should do our best minimize these.\r\n\r\nOne idea is to change \"unrecognized_extension\" to \"unrecognized_message\" and let this be a catch-all for messages that could not be handled ((to match our decision for upload flow). This is slightly odd, though, because there are cases where the Helper would not abort with \"unrecognizedMessage\" but would reject a report with \"unrecognized_message\".",
              "createdAt": "2022-11-05T00:04:39Z",
              "updatedAt": "2022-11-05T00:04:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FxV5b",
          "commit": {
            "abbreviatedOid": "a02caec"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-11-07T15:03:02Z",
          "updatedAt": "2022-11-07T15:06:00Z",
          "comments": [
            {
              "originalPosition": 123,
              "body": "The precedent of TLS 1.3 does seem like a compelling argument, but I wonder if there were specific use cases for each of those errors.\r\n\r\nI'll propose this as a way forward for this change:\r\n\r\n- In this PR, use the generic `unrecognized_message` error. The principle here is to do the simplest thing that solves the problem we have, and to keep this PR focused on its problem statement.\r\n- File an issue to extend `ReportShareError` as I suggest to allow implementation-specific error information in the aggregate sub-protocol.\r\n- File an issue about whether we should use more specific error codes, and do more research on whether it's a good idea. We could for instance reach out to the TLS WG to see how they feel in retrospect about the error codes in TLS 1.3.",
              "createdAt": "2022-11-07T15:03:02Z",
              "updatedAt": "2022-11-07T15:06:00Z"
            },
            {
              "originalPosition": 66,
              "body": "I'm still concerned about this, but I'm not sure how to break the tie between us. I'm OK moving forward with this change as it is (especially since you helpfully filed #375), but perhaps we could take this question to the WG to get more opinions?",
              "createdAt": "2022-11-07T15:05:53Z",
              "updatedAt": "2022-11-07T15:06:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Fyiu2",
          "commit": {
            "abbreviatedOid": "a02caec"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-07T18:14:15Z",
          "updatedAt": "2022-11-07T18:44:05Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "nit: typo\r\n```suggestion\r\nthe given Aggregator. (See {{upload-extensions}}.) Field `payload` is set to the\r\n```",
              "createdAt": "2022-11-07T18:14:15Z",
              "updatedAt": "2022-11-07T18:44:05Z"
            },
            {
              "originalPosition": 66,
              "body": "I think separate places for per-report and per-share extensions makes sense. We may want to add guidance in the extensions section or security considerations section for extension designers. Specifically, I think each extension should have to define whether it is allowable as a per-report extension, per-share extension, or both. If an extension implicitly relies on both aggregators seeing the same extension value, then failure to reject that extension inside `PlaintextInputShare` would be an easy implementation mistake to make.",
              "createdAt": "2022-11-07T18:39:21Z",
              "updatedAt": "2022-11-07T18:44:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5F5JRV",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-08T18:01:34Z",
          "updatedAt": "2022-11-08T18:01:35Z",
          "comments": [
            {
              "originalPosition": 66,
              "body": "One datapoint from IETF 115: There is not much enthusiasm for taskprov extension as-it-is, so I think it would be fair to disregard that particular use case as a consideration. That said I imagine there might be use cases for which shared extensions are useful.\r\n\r\nLet's take this to the list, as @tgeoghegan suggests. Hopefully someone will give us a perspective that makes the decision tree more clear.",
              "createdAt": "2022-11-08T18:01:34Z",
              "updatedAt": "2022-11-08T18:01:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5F6onQ",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-08T23:39:35Z",
          "updatedAt": "2022-11-08T23:39:35Z",
          "comments": [
            {
              "originalPosition": 123,
              "body": "I took the suggestion to use \"unrecognized_message\" here ... we can file issues for follow-up things once this PR merges.",
              "createdAt": "2022-11-08T23:39:35Z",
              "updatedAt": "2022-11-08T23:39:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5F_pCw",
          "commit": {
            "abbreviatedOid": "717a072"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-09T15:51:06Z",
          "updatedAt": "2022-11-09T15:51:06Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5GAYwB",
          "commit": {
            "abbreviatedOid": "213ef65"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-09T17:51:53Z",
          "updatedAt": "2022-11-09T17:51:54Z",
          "comments": [
            {
              "originalPosition": 66,
              "body": "Update the PR to remove shared extensions.",
              "createdAt": "2022-11-09T17:51:54Z",
              "updatedAt": "2022-11-09T17:51:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5GAaDJ",
          "commit": {
            "abbreviatedOid": "e09ecfb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Noticed a typo, otherwise LGTM.",
          "createdAt": "2022-11-09T17:55:48Z",
          "updatedAt": "2022-11-09T17:56:46Z",
          "comments": [
            {
              "originalPosition": 73,
              "body": "```suggestion\r\nall Aggregators; others may only be intended for a specific Aggregator. (For\r\n```",
              "createdAt": "2022-11-09T17:55:48Z",
              "updatedAt": "2022-11-09T17:56:46Z"
            }
          ]
        }
      ]
    },
    {
      "number": 374,
      "id": "PR_kwDOFEJYQs5CE4fT",
      "title": "editorial: Add referneces for `SealBase()` and `OpenBase()`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/374",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Closes #368.",
      "createdAt": "2022-11-02T17:13:07Z",
      "updatedAt": "2023-03-06T19:16:37Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "cc7fe02c8210349a52c959c930e51057ed852f27",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/368",
      "headRefOid": "86836b42a188f9b854c1009090868a5e42713307",
      "closedAt": "2022-11-08T23:14:15Z",
      "mergedAt": "2022-11-08T23:14:15Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "311fb8ca373be4b2e2a40b032cad40adb5bcaddb"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5FfBEM",
          "commit": {
            "abbreviatedOid": "86776d9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-11-02T17:56:58Z",
          "updatedAt": "2022-11-02T17:57:11Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "The strings \"SealBase\" and \"OpenBase\" never actually occur in RFC 9180. Instead, [section 6.1](https://www.rfc-editor.org/rfc/rfc9180.html#section-6.1) describes \"Seal<MODE>\" and \"Open<MODE>\" and then enumerates the possible modes, among which is \"Base\". Just to make this a little easier on folks reading DAP, could we refer specifically to RFC 9180 section 6.1?",
              "createdAt": "2022-11-02T17:56:58Z",
              "updatedAt": "2022-11-02T17:57:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FfCDm",
          "commit": {
            "abbreviatedOid": "86776d9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-02T17:59:35Z",
          "updatedAt": "2022-11-02T17:59:36Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Sure, mr. pedantic-pants",
              "createdAt": "2022-11-02T17:59:35Z",
              "updatedAt": "2022-11-02T17:59:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FfDRr",
          "commit": {
            "abbreviatedOid": "86776d9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-02T18:02:41Z",
          "updatedAt": "2022-11-02T18:02:41Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "(seriously, great idea.)",
              "createdAt": "2022-11-02T18:02:41Z",
              "updatedAt": "2022-11-02T18:02:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5FfeFT",
          "commit": {
            "abbreviatedOid": "86836b4"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-02T19:21:44Z",
          "updatedAt": "2022-11-02T19:21:44Z",
          "comments": []
        }
      ]
    },
    {
      "number": 378,
      "id": "PR_kwDOFEJYQs5CPqWS",
      "title": "Change the length-tag for agg_param to 32 bits",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/378",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Closes #377.\r\n\r\nThis will be needed to accommodate Poplar1, which is likely to need sequences of candidate prefixes to exceed 65K.",
      "createdAt": "2022-11-04T23:47:20Z",
      "updatedAt": "2023-03-06T19:16:36Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "cc7fe02c8210349a52c959c930e51057ed852f27",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/377",
      "headRefOid": "2e6f8c56504274d020507eef46ecddfd7e73bb6c",
      "closedAt": "2022-11-08T23:14:50Z",
      "mergedAt": "2022-11-08T23:14:50Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "66aa97a400784cfe11836d385552230257cbd1d0"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Fs9AH",
          "commit": {
            "abbreviatedOid": "2e6f8c5"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-05T14:02:06Z",
          "updatedAt": "2022-11-05T14:02:06Z",
          "comments": []
        }
      ]
    },
    {
      "number": 379,
      "id": "PR_kwDOFEJYQs5DCmbR",
      "title": "Treat PlaintextInputShare with repeated extensions as unrecognized",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/379",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "Closes #375.\r\nBased on #373 (merge that first).",
      "createdAt": "2022-11-16T16:25:54Z",
      "updatedAt": "2023-03-06T19:16:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "46e6890e419ef2be23ed0e73fd9e9a86694b5f2c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/375",
      "headRefOid": "e90e5a6643c74f6e58820a75141836546a2daa75",
      "closedAt": "2022-11-16T22:58:27Z",
      "mergedAt": "2022-11-16T22:58:27Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "4e08fa0c860797f395d57d0546c0d3ca5fad13ed"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Gg96U",
          "commit": {
            "abbreviatedOid": "bf5799e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-16T17:39:47Z",
          "updatedAt": "2022-11-16T17:39:47Z",
          "comments": []
        }
      ]
    },
    {
      "number": 380,
      "id": "PR_kwDOFEJYQs5DC8Mk",
      "title": "Add extension points to aggregate and collect flows",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/380",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #373 (merge that first).\r\n\r\nGeneralizes \"upload extensions\" to \"protocol extensions\" and lifts this section to a subsection of the protocol definition.\r\n\r\nAdds a list of extensions to AggregateInitializeReq and CollectReq.",
      "createdAt": "2022-11-16T17:32:44Z",
      "updatedAt": "2023-03-06T19:16:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "cjpatton/369/hidden",
      "baseRefOid": "421a252ec98001b8452ba61e868e37859d4d9435",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/369/agg-extensions",
      "headRefOid": "c8b52d7634dc63aca150b239cd7ab6b4185dba43",
      "closedAt": "2022-12-08T21:31:54Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I've decided this is probably not a good idea. In any case, it's not needed for the use case I have in mind (taskprov).",
          "createdAt": "2022-12-08T21:31:54Z",
          "updatedAt": "2022-12-08T21:31:54Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5GhBj-",
          "commit": {
            "abbreviatedOid": "c8b52d7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-16T17:46:07Z",
          "updatedAt": "2022-11-16T17:51:37Z",
          "comments": [
            {
              "originalPosition": 95,
              "body": "I think you should explain how this relates to the `extensions` field in `CollectReq`. In particular, I think the only reason that a `CollectReq` would ever contain extensions is so that they can be relayed to the helper. If the collector wanted to send something special to the leader, it could send an extra header in the HTTP request that sends the `CollectReq`. A leader that wants to add something non-standard to an `AggregateInitializeReq` could do much the same thing. It's only the collector<->helper case that needs some kind of in-band extensions scheme, since they never communicate directly (yet).",
              "createdAt": "2022-11-16T17:46:07Z",
              "updatedAt": "2022-11-16T17:51:38Z"
            },
            {
              "originalPosition": 125,
              "body": "Continuing from my remark above: I think we should at least recommend against the leader ever parsing extensions. HTTP already provides plenty of mechanisms for the collector to provide extra information or parameters to the leader out-of-band from DAP protocol messages. Extensions should only ever be needed so the collector can say something extra to the helper, and at that point we have to think about how the leader could forge extensions. Maybe there should be a recommendation that anything put into an extension by the collector needs to be authenticated so the helper can verify where it came from?",
              "createdAt": "2022-11-16T17:51:33Z",
              "updatedAt": "2022-11-16T17:51:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Hr_Ur",
          "commit": {
            "abbreviatedOid": "c8b52d7"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-02T15:56:20Z",
          "updatedAt": "2022-12-02T15:56:20Z",
          "comments": [
            {
              "originalPosition": 95,
              "body": "Is this `extensions` meant to replace the `ReportShare.encrypted_input_share .payload.extensions` eventually? If not, what's the relation between these two extensions, can people introduce an extension only relevant to `AggregateInitializeReq` but not clients? ",
              "createdAt": "2022-12-02T15:56:20Z",
              "updatedAt": "2022-12-02T15:56:20Z"
            }
          ]
        }
      ]
    },
    {
      "number": 381,
      "id": "PR_kwDOFEJYQs5DDy0y",
      "title": "Bump version tag from dap-02 to dap-03.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/381",
      "state": "MERGED",
      "author": "bhalleycf",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-03"
      ],
      "body": "The computations for the seals have changed in DAP-03, so we should increment the version tag too for consistency.",
      "createdAt": "2022-11-16T20:57:37Z",
      "updatedAt": "2022-11-29T16:59:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "4e08fa0c860797f395d57d0546c0d3ca5fad13ed",
      "headRepository": "bhalleycf/draft-ietf-ppm-dap",
      "headRefName": "main",
      "headRefOid": "5817d583b13bda25a0e8ea53ada401086badb42e",
      "closedAt": "2022-11-29T16:59:32Z",
      "mergedAt": "2022-11-29T16:59:32Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "5fbb7f90667fd44b219b32ba2e3cf3fb4e7e1a2b"
      },
      "comments": [
        {
          "author": "bhalleycf",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Hmm, after further review of the diffs I see the actual computation of the seals didn't change, but given other parts of the wire protocol changed, it still seems good to me to bump the \"dap-02\" strings to \"dap-03\".",
          "createdAt": "2022-11-16T21:17:54Z",
          "updatedAt": "2022-11-16T21:17:54Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Hmm, after further review of the diffs I see the actual computation of the seals didn't change, but given other parts of the wire protocol changed, it still seems good to me to bump the \"dap-02\" strings to \"dap-03\".\r\n\r\nCurrently we plan to bump this string with each draft, so this would be needed in any case. See #376.",
          "createdAt": "2022-11-16T22:52:20Z",
          "updatedAt": "2022-11-16T22:52:20Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "There's been no objections to this fairly mechanical change so I think we can merge this one.",
          "createdAt": "2022-11-29T16:59:27Z",
          "updatedAt": "2022-11-29T16:59:27Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5GidqP",
          "commit": {
            "abbreviatedOid": "49eafac"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-16T22:52:48Z",
          "updatedAt": "2022-11-16T22:52:48Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Gm1g3",
          "commit": {
            "abbreviatedOid": "5817d58"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-17T15:52:34Z",
          "updatedAt": "2022-11-17T15:52:34Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5GuxsW",
          "commit": {
            "abbreviatedOid": "5817d58"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-18T19:29:19Z",
          "updatedAt": "2022-11-18T19:29:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 382,
      "id": "PR_kwDOFEJYQs5DEhHq",
      "title": "Allow implementations to relax anti-replay requirement",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/382",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #362.\r\n\r\nHaving to track all report IDs is a burden for sharding anti-replay storage across time. This change clarifies that, while tracking report IDs is sufficient, it is also sufficient to track any report metadata that contains the reprot ID.",
      "createdAt": "2022-11-17T00:14:21Z",
      "updatedAt": "2023-03-06T19:16:31Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5fbb7f90667fd44b219b32ba2e3cf3fb4e7e1a2b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/362/1",
      "headRefOid": "a4b7037db0751cb1d7322fa0e52a2ef8fe19c3ed",
      "closedAt": "2022-12-08T21:41:23Z",
      "mergedAt": "2022-12-08T21:41:23Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "196fb5fc7ec97f787607563044b3f4c1a07b59fe"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "cc/ @bhalleycf",
          "createdAt": "2022-11-17T00:14:47Z",
          "updatedAt": "2022-11-17T00:14:47Z"
        },
        {
          "author": "bhalleycf",
          "authorAssociation": "CONTRIBUTOR",
          "body": "LGTM",
          "createdAt": "2022-11-17T14:41:10Z",
          "updatedAt": "2022-11-17T14:41:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed",
          "createdAt": "2022-12-08T21:29:57Z",
          "updatedAt": "2022-12-08T21:29:57Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Gmt5L",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-11-17T15:36:39Z",
          "updatedAt": "2022-11-17T15:52:05Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "```suggestion\r\nrejected during the aggregation sub-protocol as described in\r\n```",
              "createdAt": "2022-11-17T15:36:39Z",
              "updatedAt": "2022-11-17T15:52:05Z"
            },
            {
              "originalPosition": 24,
              "body": "I acknowledge that I'm critiquing text that was already present and is not modified by your change but--\r\n```suggestion\r\nA malicious leader may attempt to force a replay by replacing the report ID\r\ngenerated by the client with a report ID the helper has not yet seen. To prevent\r\n```\r\nIs there any reason not to be explicit about which aggregators we're talking about?",
              "createdAt": "2022-11-17T15:38:39Z",
              "updatedAt": "2022-11-17T15:52:05Z"
            },
            {
              "originalPosition": 10,
              "body": "I think we need to be more clear about what the \"given time window\" is. My reading of the arguments in #362 is that we're instructing implementations to choose a window of time relative to the instant at which a report being evaluated for anti-replay, and only check for unique report IDs within that window. Is the behavior the same regardless of time interval or fixed batch tasks? Is there a risk of the helper and leader applying inconsistent anti-replay policy, either because they construct their time windows differently or because the leader might be evaluating anti-replay at upload time, well before the helper gets a chance to do so at aggregation time? Finally, if we say nothing about how to construct the time windows, then doesn't that allow a conforming implementation to not do anti-replay checks at all?",
              "createdAt": "2022-11-17T15:51:56Z",
              "updatedAt": "2022-11-17T15:52:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5GuTIc",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-11-18T17:49:57Z",
          "updatedAt": "2022-11-18T17:49:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5IJxgj",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T17:48:49Z",
          "updatedAt": "2022-12-08T17:48:49Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "I think this is useful if at some point we move to direct-upload architecture.",
              "createdAt": "2022-12-08T17:48:49Z",
              "updatedAt": "2022-12-08T17:48:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IJ2bz",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T18:05:06Z",
          "updatedAt": "2022-12-08T18:05:06Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Maybe we can just say \"To detect replay attacks for a given task,\" since this replay state is required to exist for the duration of an entire task, right?",
              "createdAt": "2022-12-08T18:05:06Z",
              "updatedAt": "2022-12-08T18:05:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IJ_ah",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T18:30:31Z",
          "updatedAt": "2022-12-08T18:30:31Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "I don't think so: the point of this PR is to allow aggregators to evaluate anti-replay on a narrower scope than the entire task. Bob's [argument](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/362#issuecomment-1301108181), I think, is that aggregators only need to look at reports that wouldn't get rejected for being too old or from too far in the future, and I think perhaps this paragraph could be written in those terms.",
              "createdAt": "2022-12-08T18:30:31Z",
              "updatedAt": "2022-12-08T18:30:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IKEzW",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T18:46:02Z",
          "updatedAt": "2022-12-08T18:46:02Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "The \"given time window\" refers to the window of time for which an aggregator chooses to check for replayed reports; any reports outside of that timestamp must be rejected.",
              "createdAt": "2022-12-08T18:46:02Z",
              "updatedAt": "2022-12-08T18:46:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IKFZO",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T18:48:15Z",
          "updatedAt": "2022-12-08T18:48:15Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Perfect. Then how about we just keep \"given time window\" as is, say that implementations can choose this to fit their needs, and add a line at the end of the paragraph saying \"Any report that is outside this time window MUST be rejected\" (or whatever). Would that work?",
              "createdAt": "2022-12-08T18:48:15Z",
              "updatedAt": "2022-12-08T18:48:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IKHzB",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T18:55:05Z",
          "updatedAt": "2022-12-08T18:55:06Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "I've pushed @chris-wood's suggestion of simply dropping discussion about the time window at that point. To @tgeoghegan point about not needing to do anti-replay for reports too far in the past or in the future, this is already premitted in {{early-input-share-validation}}.\r\n\r\nIn fact, {{early-input-share-validation}} is really the main text that matters. I think the only normative text we want to add is that anti-replay MUST be enforced per nonce, but MAY be enforced per nonce/timestamp. What do y'all think about axing this {{anti-replay}} altogether and merging this normative text into {{early-input-share-validation}}?",
              "createdAt": "2022-12-08T18:55:05Z",
              "updatedAt": "2022-12-08T18:55:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IKJNJ",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T18:58:36Z",
          "updatedAt": "2022-12-08T18:58:37Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "If we rename that section to \"Input Share Validation\" and promote the replay rejection step to the first thing in the list, I'd be OK with that.",
              "createdAt": "2022-12-08T18:58:36Z",
              "updatedAt": "2022-12-08T18:58:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IKK7S",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T19:04:43Z",
          "updatedAt": "2022-12-08T19:04:43Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Going with \"Report Share Validation\" :)\r\n",
              "createdAt": "2022-12-08T19:04:43Z",
              "updatedAt": "2022-12-08T19:04:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IKo59",
          "commit": {
            "abbreviatedOid": "76bac13"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-08T20:38:56Z",
          "updatedAt": "2022-12-08T20:38:56Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5IK0uZ",
          "commit": {
            "abbreviatedOid": "42ff6db"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T21:20:53Z",
          "updatedAt": "2022-12-08T21:20:53Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "Done. I ended up going with \"Input Share Validation\", as @chris-wood suggested, as this better matches the language in that section.",
              "createdAt": "2022-12-08T21:20:53Z",
              "updatedAt": "2022-12-08T21:20:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IK2q1",
          "commit": {
            "abbreviatedOid": "f609e80"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-08T21:28:41Z",
          "updatedAt": "2022-12-08T21:28:41Z",
          "comments": []
        }
      ]
    },
    {
      "number": 386,
      "id": "PR_kwDOFEJYQs5E1KKE",
      "title": "Allow the server to provide multiple HPKE configurations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/386",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #248 ",
      "createdAt": "2022-12-08T20:52:03Z",
      "updatedAt": "2022-12-08T21:38:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5fbb7f90667fd44b219b32ba2e3cf3fb4e7e1a2b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/config-list",
      "headRefOid": "33b4e84941b32ce8ac223c3be8634e53be793972",
      "closedAt": "2022-12-08T21:38:12Z",
      "mergedAt": "2022-12-08T21:38:12Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b17ec2a34be145012c7feb5341184e2369c2ff77"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5IK19m",
          "commit": {
            "abbreviatedOid": "ba72d23"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I'm game. One minor editorial nit.",
          "createdAt": "2022-12-08T21:25:55Z",
          "updatedAt": "2022-12-08T21:26:24Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "```suggestion\r\nmultiple HPKE configurations with non-distinct configuration identifiers, it can use trial decryption with each\r\n```",
              "createdAt": "2022-12-08T21:25:55Z",
              "updatedAt": "2022-12-08T21:26:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IK3Qh",
          "commit": {
            "abbreviatedOid": "33b4e84"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-08T21:30:55Z",
          "updatedAt": "2022-12-08T21:30:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 387,
      "id": "PR_kwDOFEJYQs5E1jhy",
      "title": "Update changelog for 03",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/387",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Note that this might need to be updated once we land #333.",
      "createdAt": "2022-12-08T22:12:28Z",
      "updatedAt": "2023-03-06T19:15:57Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d383dbfa1ebee741c827dcb93355a65395ac271d",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/03/changelog",
      "headRefOid": "ca3f884f3511b1314c6d49ebc38d16054dc52b3d",
      "closedAt": "2022-12-10T00:09:32Z",
      "mergedAt": "2022-12-10T00:09:32Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "d66aae7fd877489d62ee9c4dbdc9d8b5876f98da"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ILB3u",
          "commit": {
            "abbreviatedOid": "3e13fa8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T22:14:07Z",
          "updatedAt": "2022-12-08T22:14:08Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Not 100% sure if is actually a wire breaking change.",
              "createdAt": "2022-12-08T22:14:07Z",
              "updatedAt": "2022-12-08T22:14:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILUel",
          "commit": {
            "abbreviatedOid": "de84435"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:48:27Z",
          "updatedAt": "2022-12-08T23:48:28Z",
          "comments": [
            {
              "originalPosition": 34,
              "body": "TODO: \r\n\r\n- [x] #333\r\n- [x] #390, #389\r\n- [x] #391",
              "createdAt": "2022-12-08T23:48:27Z",
              "updatedAt": "2022-12-09T23:45:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILpwM",
          "commit": {
            "abbreviatedOid": "de84435"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-09T02:05:44Z",
          "updatedAt": "2022-12-09T02:05:44Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "I think it's fine to say so. ",
              "createdAt": "2022-12-09T02:05:44Z",
              "updatedAt": "2022-12-09T02:05:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IQy6R",
          "commit": {
            "abbreviatedOid": "ca3f884"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-09T23:56:05Z",
          "updatedAt": "2022-12-09T23:56:05Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5IQ0BQ",
          "commit": {
            "abbreviatedOid": "ca3f884"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-10T00:09:28Z",
          "updatedAt": "2022-12-10T00:09:28Z",
          "comments": []
        }
      ]
    },
    {
      "number": 388,
      "id": "PR_kwDOFEJYQs5E1k3b",
      "title": "Don't specify that the response to upload request is empty",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/388",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #191.",
      "createdAt": "2022-12-08T22:17:48Z",
      "updatedAt": "2023-03-06T19:16:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "196fb5fc7ec97f787607563044b3f4c1a07b59fe",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/191",
      "headRefOid": "45cdddf39e00d9ef86dfc0177b6d907e1752184b",
      "closedAt": "2022-12-08T22:50:24Z",
      "mergedAt": "2022-12-08T22:50:24Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "a56497f0d7b4e2f08b287cb01268438c2b9e29c8"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ILC56",
          "commit": {
            "abbreviatedOid": "45cdddf"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-08T22:18:37Z",
          "updatedAt": "2022-12-08T22:18:37Z",
          "comments": []
        }
      ]
    },
    {
      "number": 389,
      "id": "PR_kwDOFEJYQs5E1sOX",
      "title": "Address Problem Details issues pointed out in #209",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/389",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #209.\r\n",
      "createdAt": "2022-12-08T22:49:26Z",
      "updatedAt": "2023-03-06T19:16:30Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a56497f0d7b4e2f08b287cb01268438c2b9e29c8",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/209/1",
      "headRefOid": "a25f96ba1c6492617ce6f0aec378f0c2fd0270cf",
      "closedAt": "2022-12-08T23:44:57Z",
      "mergedAt": "2022-12-08T23:44:57Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "ab798f534c140cd1ffbb05b41227f56256598e8f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ILJ5I",
          "commit": {
            "abbreviatedOid": "15cf138"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T22:51:06Z",
          "updatedAt": "2022-12-08T22:51:06Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "@divergentdave is this guidance \"instance\" useful, but sufficiently ambiguous?",
              "createdAt": "2022-12-08T22:51:06Z",
              "updatedAt": "2022-12-08T22:51:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILLPX",
          "commit": {
            "abbreviatedOid": "15cf138"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2022-12-08T22:58:03Z",
          "updatedAt": "2022-12-08T23:05:23Z",
          "comments": [
            {
              "originalPosition": 18,
              "body": "```suggestion\r\nWhen the task ID is known (see {{task-configuration}}), the problem document SHOULD\r\n```",
              "createdAt": "2022-12-08T22:58:03Z",
              "updatedAt": "2022-12-08T23:05:23Z"
            },
            {
              "originalPosition": 19,
              "body": "```suggestion\r\ninclude an additional \"taskid\" member containing the ID encoded in Base 64 using\r\n```",
              "createdAt": "2022-12-08T22:58:28Z",
              "updatedAt": "2022-12-08T23:05:23Z"
            },
            {
              "originalPosition": 30,
              "body": "I think we should delete this. I'm not sure it makes sense for a standard that governs the interactions between machines should say anything about what to tell humans. Plus, it's not correct in all cases: if a client receives `unknownHpkeConfig`, we recommend that it re-fetch HPKE configs and try re-encrypting, which wouldn't require telling the user anything at all.",
              "createdAt": "2022-12-08T22:59:58Z",
              "updatedAt": "2022-12-08T23:05:23Z"
            },
            {
              "originalPosition": 14,
              "body": "This requirement on `title` doesn't make sense: if the `title` field is a deterministic function of the problem type, then there's no value in including it in problem documents because everyone can just look it up in this table. [RFC 7807 3.1](https://www.rfc-editor.org/rfc/rfc7807#section-3.1) says the title field is \"human readable\", which I think can be read as \"implementation defined\" and \"not to be relied upon by machines\". So I don't think DAP should say anything at all about `title`.",
              "createdAt": "2022-12-08T23:03:23Z",
              "updatedAt": "2022-12-08T23:05:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILNOj",
          "commit": {
            "abbreviatedOid": "15cf138"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:07:01Z",
          "updatedAt": "2022-12-08T23:07:01Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "I think this is still pointing implementers in the wrong direction compared to RFC 7807. The RFC says that the instance URI \"will always identify a specific occurrence of a problem\". Thus, the URI would have to include some identifier pointing back to error information from the particular request/trace.\r\n\r\nI think it would make sense for DAP to be silent on the `instance` member. It's optional and it's sufficiently specified in RFC 7807 to the extent that DAP doesn't need to add anything further. Plus I have a hunch most DAP implementations won't go to the extent of storing error information such that consumers could dereference `instance` URIs for \"further information\" as described in RFC 7807 section 3.1.",
              "createdAt": "2022-12-08T23:07:01Z",
              "updatedAt": "2022-12-08T23:07:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILNYF",
          "commit": {
            "abbreviatedOid": "15cf138"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:07:52Z",
          "updatedAt": "2022-12-08T23:07:52Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "I agree, I was trying to preserve the (ill-defined) requirement, as it wasn't actually mentioned in the issue. I'll delete it anyway.",
              "createdAt": "2022-12-08T23:07:52Z",
              "updatedAt": "2022-12-08T23:07:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILOQJ",
          "commit": {
            "abbreviatedOid": "15cf138"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:12:40Z",
          "updatedAt": "2022-12-08T23:12:41Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "Done. I was following @divergentdave recommendation in the issue, but I see his thumbs up here :)",
              "createdAt": "2022-12-08T23:12:40Z",
              "updatedAt": "2022-12-08T23:12:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILOj3",
          "commit": {
            "abbreviatedOid": "15cf138"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:14:28Z",
          "updatedAt": "2022-12-08T23:14:29Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "Done.",
              "createdAt": "2022-12-08T23:14:29Z",
              "updatedAt": "2022-12-08T23:14:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILOvm",
          "commit": {
            "abbreviatedOid": "9dafaca"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:15:33Z",
          "updatedAt": "2022-12-08T23:15:33Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "@tgeoghegan would this close #360?\r\n```suggestion\r\nWhen the task ID is known (see {{task-configuration}}), the problem document MAY\r\n```",
              "createdAt": "2022-12-08T23:15:33Z",
              "updatedAt": "2022-12-08T23:15:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILS_D",
          "commit": {
            "abbreviatedOid": "9dafaca"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2022-12-08T23:42:13Z",
          "updatedAt": "2022-12-08T23:42:14Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "Yeah I think it does!",
              "createdAt": "2022-12-08T23:42:13Z",
              "updatedAt": "2022-12-08T23:42:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ILS_l",
          "commit": {
            "abbreviatedOid": "9dafaca"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-08T23:42:18Z",
          "updatedAt": "2022-12-08T23:42:18Z",
          "comments": []
        }
      ]
    },
    {
      "number": 390,
      "id": "PR_kwDOFEJYQs5E186Z",
      "title": "Make \"taskid\" member of Problem Details optional",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/390",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #360 ",
      "createdAt": "2022-12-08T23:47:08Z",
      "updatedAt": "2023-03-06T19:16:29Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ab798f534c140cd1ffbb05b41227f56256598e8f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/360",
      "headRefOid": "656fd41fad4f72190e27ddf6c58e827b374a7423",
      "closedAt": "2022-12-08T23:51:18Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm not opposed to this change but I think the `SHOULD` in conjunction with the acknowledgement that task ID is not always known was good enough.\r\n\r\nAh, OK, let's leave it there then. I'll close that issue.",
          "createdAt": "2022-12-08T23:51:16Z",
          "updatedAt": "2022-12-08T23:51:16Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ILUt_",
          "commit": {
            "abbreviatedOid": "656fd41"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I'm not opposed to this change but I think the `SHOULD` in conjunction with the acknowledgement that task ID is not always known was good enough.",
          "createdAt": "2022-12-08T23:50:08Z",
          "updatedAt": "2022-12-08T23:50:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 391,
      "id": "PR_kwDOFEJYQs5E71oC",
      "title": "Add more input share validation items",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/391",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": " - Allows helpers to reject reports from too far in the future, matching the leader check detailed in \"Upload Requests\". We add `ReportShareError.report_too_early` for this case, and allow the leader to try the report again later.\r\n - Requires helpers to mark undecodable decrypted input shares as invalid with `ReportShareError.unrecognized_message` (#255).\r\n\r\nResolves #255",
      "createdAt": "2022-12-09T19:05:58Z",
      "updatedAt": "2022-12-09T22:51:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ab798f534c140cd1ffbb05b41227f56256598e8f",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/more-input-share-validation",
      "headRefOid": "6e4daa6a62003f4c71f73f6df24a4ec14407cd41",
      "closedAt": "2022-12-09T22:51:56Z",
      "mergedAt": "2022-12-09T22:51:56Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "debb056f8e966c3f41e4106077b2f495baf3868c"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5IQIeR",
          "commit": {
            "abbreviatedOid": "5cf84d9"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-09T19:48:44Z",
          "updatedAt": "2022-12-09T19:48:44Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5IQOHi",
          "commit": {
            "abbreviatedOid": "5cf84d9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2022-12-09T20:13:46Z",
          "updatedAt": "2022-12-09T20:18:43Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "```suggestion\r\nIf the type is `finished`, then the leader aborts with `unrecognizedMessage`. If\r\n```",
              "createdAt": "2022-12-09T20:13:46Z",
              "updatedAt": "2022-12-09T20:18:44Z"
            },
            {
              "originalPosition": 35,
              "body": "```suggestion\r\n1. Check if the report is too far into the future. Implementors can provide for\r\n   some small leeway, usually no more than a few minutes, to account for clock\r\n   skew. If a report is rejected for this reason, the Aggregator SHOULD mark\r\n   the input share as invalid with the error `report_too_early`.\r\n```\r\nTo both relax normative language around something that's hard to be normative about, and use consistent language with the other checks.",
              "createdAt": "2022-12-09T20:17:28Z",
              "updatedAt": "2022-12-09T20:18:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5IQajg",
          "commit": {
            "abbreviatedOid": "f409230"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udc4d ",
          "createdAt": "2022-12-09T21:13:27Z",
          "updatedAt": "2022-12-09T21:13:27Z",
          "comments": []
        }
      ]
    },
    {
      "number": 392,
      "id": "PR_kwDOFEJYQs5E9LPI",
      "title": "Last-minute editorial things for draft-03",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/392",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "1. Run spellcheck\r\n2. Make all paragraphs wrap at 80 characters",
      "createdAt": "2022-12-10T00:10:28Z",
      "updatedAt": "2023-03-06T19:15:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d66aae7fd877489d62ee9c4dbdc9d8b5876f98da",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/03/spellcheck",
      "headRefOid": "e92069f79f45928eac7458f86a12986dee07f7f5",
      "closedAt": "2022-12-10T00:49:37Z",
      "mergedAt": "2022-12-10T00:49:37Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "c835232c6119a0ba5b3e9dc89cb6ba23dfaa51e1"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 393,
      "id": "PR_kwDOFEJYQs5FlfHQ",
      "title": "Specify one Helper & compute two VDAF rounds per DAP aggregation step.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "This is based on the \"ping-pong\" communication pattern described in the\r\nVDAF specification, and reduces the number of network round-trips\r\nrequired to complete an aggregation by about half for all VDAFs, from\r\n`ROUNDS + 1` to `ceil((ROUNDS + 1) / 2)`. In particular, for 1-round\r\nVDAFs like those in the Prio family, this reduces aggregation to a\r\nsingle network round-trip.\r\n    \r\nImplementing this change requires specifying to exactly two aggregators,\r\ni.e. exactly one Helper. The text of the aggregation section is updated\r\nto explicitly specify one Helper.",
      "createdAt": "2022-12-15T22:06:00Z",
      "updatedAt": "2023-11-27T23:10:14Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f006e450641aa1703c89f9c26cc3ddc6c9b2f9a8",
      "headRepository": null,
      "headRefName": "bran/aggregation-fast-path",
      "headRefOid": "bbbeffb1f694a4db4a21da548be0af19d38f91ee",
      "closedAt": "2023-06-15T22:17:55Z",
      "mergedAt": "2023-06-15T22:17:55Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "c5868ea9854cef7379cef6712aa2d702888f01c9"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd given the scope of the change, please file an issue to track this change and then take it to the list for discussion.",
          "createdAt": "2022-12-15T22:25:14Z",
          "updatedAt": "2022-12-15T22:25:14Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @chris-wood: given the scope of the change, please file an issue to track this change and then take it to the list for discussion.\r\n\r\nFWIW, I asked Brandon to write this up just to see what it would look like. We're thinking about experimenting with this to see if it's worth making this change. I don't think we're quite ready to try to get consensus on it.",
          "createdAt": "2022-12-15T22:41:04Z",
          "updatedAt": "2022-12-15T22:41:04Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Chris P pointed out in some off-PR discussion that this specializes DAP to two aggregators: ~~all aggregators now run `prep_shares_to_prep` independently~~ [edit] a helper now runs the first `prep_shares_to_prep` call, and running `prep_shares_to_prep` requires knowing the preparation shares generated by all of the aggregators, so we couldn't use the trick in this PR with more than one helper without incurring some kind of broadcast communication.\r\n\r\nDAP is currently written assuming only one helper, but it's an open question as to whether we want to keep things this way. If not, I suspect we'll have to unwind this change into a separate endpoint.",
          "createdAt": "2022-12-15T23:02:57Z",
          "updatedAt": "2023-01-12T17:21:57Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Moved this to \"draft\" while we figure out if we want to try to merge this.",
          "createdAt": "2022-12-16T00:26:31Z",
          "updatedAt": "2022-12-16T00:26:31Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Making both aggregators run `Vdaf.prep_shares_to_prep` could be unfortunate, because for Prio, that does require running the FLPCP `decide` algorithm, which can be expensive. In DAP-03, the helpers trust the leader to run `Vdaf.prep_shares_to_prep` and then broadcast the resulting combined message in the first `AggregateContinueReq`. In this change, we could have the helper run `Vdaf.prep_shares_to_prep` when it handles `AggregateInitializeReq` and then \"broadcast\" the combined prep message back to the leader in the `AggregateInitializeResp`. For 1-round VDAFs, this also means that the helper should be able to reach the `finished` state while handling `AggregateInitializeReq`, so I think you'd need `case finished` in `PrepareStep.prepare_step_result` to be like `case finished: opaque finished_prepare_msg<0..2^32-1>;`\r\n> \r\n> I wrote up some notes months and months ago on how we might do this. This is (roughly) what the DAP-03 flow looks like:\r\n> \r\n> ```\r\n> 1 round VDAF\r\n> LEADER  MESSAGE                                                HELPER\r\n> START                                                          START\r\n> WAITING--AggregateInitReq(ReportShare,AggParam)--------------->WAITING\r\n> WAITING<---AggregateResp(Continue(helper's PrepareMessage))----WAITING\r\n> WAITING--AggregateReq(Continue(combined PrepareMessage))------>WAITING // leader \"broadcasts\" first prepare message\r\n> FINISH<----AggregateResp(Finish)-------------------------------FINISH\r\n> ```\r\n> \r\n> If we make it the helper's job to combine and \"broadcast\" prepare messages:\r\n> \r\n> ```\r\n> 1 round VDAF short circuit:\r\n> LEADER  MESSAGE                                                             HELPER\r\n> START                                                                       START\r\n> WAITING--AggregateInitReq(ReportShare,AggParam,leader's PrepareMessage)---->FINISH\r\n> FINISH<--AggregateResp(Finish(combined PrepareMessage))---------------------FINISH // helper \"broadcasts\" first prepare message\r\n> ```\r\n\r\nAh, apologies: taking another look, this PR does not increase the number of times `prep_shares_to_prep` is called -- it only moves first call from the leader to the helper. (The leader still calls all further `prep_shares_to_prep` calls.) I think I was confused because the need to call `prep_shares_to_prep` during aggregation initialization was previously left implicit! That, along with the fact that initialization now calls `prep_shares_to_prep` twice (once by the helper during `prep_init`; once by the leader for the first `prep_next`), led to the confusion.",
          "createdAt": "2023-01-10T19:45:42Z",
          "updatedAt": "2023-01-10T19:45:42Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I updated this PR to address review comments thus far, and to factor the extra `ReportID` out of the aggregation initialize response.",
          "createdAt": "2023-02-21T23:03:33Z",
          "updatedAt": "2023-02-21T23:10:44Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "~~From https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/439#issuecomment-1520917332, I should change \"preparation message/share\" to \"prep message/share\" when I next touch this PR.~~ [edit: this is done]",
          "createdAt": "2023-04-24T22:50:29Z",
          "updatedAt": "2023-04-28T23:42:37Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This change resolves #190 so we should close that once this lands.",
          "createdAt": "2023-05-26T21:16:39Z",
          "updatedAt": "2023-05-26T21:16:39Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "This PR has been updated to use the new \"ping-pong\" API described in https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240.",
          "createdAt": "2023-06-02T22:03:56Z",
          "updatedAt": "2023-06-02T22:03:56Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> But I think it's reasonable to make the reader jump around a bit if it makes the text simpler.\r\n\r\nIMO, we should optimize for the reader, rather than the writer -- while writers will have to maintain two small sections that are very similar, IMO it's much simpler for the reader if we define \"where to go next\" at the end of a given section, rather than the beginning of the next section.",
          "createdAt": "2023-06-12T22:05:36Z",
          "updatedAt": "2023-06-12T22:11:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Everything has either been resolved or punted to future issues except that we want to re-work the message definitions to eliminate the cases `PrepareStepState.initialize` and `PrepareStepState.continue`. Then this is good to go.\r\n\r\nI agree we can eliminate `PrepareStepState.initialize`, but I don't agree that we can eliminate `continue`. The reason is that the Helper responds to a request with one of `continue`, `finish`, and `reject`, and the Leader needs to distinguish between these.\r\n\r\nWe could eliminate the `PrepareStepState` continue wrapper in the `AggregationJobContinueReq`, but we can't remove it from `AggregationJobResp`.",
          "createdAt": "2023-06-14T19:42:17Z",
          "updatedAt": "2023-06-14T19:42:17Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> We could eliminate the `PrepareStepState` continue wrapper in the `AggregationJobContinueReq`, but we can't remove it from `AggregationJobResp`.\r\n\r\nYou're correct, I jumped the gun there.",
          "createdAt": "2023-06-14T20:13:40Z",
          "updatedAt": "2023-06-14T20:13:40Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Comments addressed & commits squashed.",
          "createdAt": "2023-06-14T22:49:47Z",
          "updatedAt": "2023-06-14T22:49:47Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Iyq9Q",
          "commit": {
            "abbreviatedOid": "af18352"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Making both aggregators run `Vdaf.prep_shares_to_prep` could be unfortunate, because for Prio, that does require running the FLPCP `decide` algorithm, which can be expensive. In DAP-03, the helpers trust the leader to run `Vdaf.prep_shares_to_prep` and then broadcast the resulting combined message in the first `AggregateContinueReq`. In this change, we could have the helper run `Vdaf.prep_shares_to_prep` when it handles `AggregateInitializeReq` and then \"broadcast\" the combined prep message back to the leader in the `AggregateInitializeResp`. For 1-round VDAFs, this also means that the helper should be able to reach the `finished` state while handling `AggregateInitializeReq`, so I think you'd need `case finished` in `PrepareStep.prepare_step_result` to be like `case finished: opaque finished_prepare_msg<0..2^32-1>;`\r\n\r\nI wrote up some notes months and months ago on how we might do this. This is (roughly) what the DAP-03 flow looks like:\r\n\r\n```\r\n1 round VDAF\r\nLEADER  MESSAGE                                                HELPER\r\nSTART                                                          START\r\nWAITING--AggregateInitReq(ReportShare,AggParam)--------------->WAITING\r\nWAITING<---AggregateResp(Continue(helper's PrepareMessage))----WAITING\r\nWAITING--AggregateReq(Continue(combined PrepareMessage))------>WAITING // leader \"broadcasts\" first prepare message\r\nFINISH<----AggregateResp(Finish)-------------------------------FINISH\r\n```\r\n\r\nIf we make it the helper's job to combine and \"broadcast\" prepare messages:\r\n\r\n```\r\n1 round VDAF short circuit:\r\nLEADER  MESSAGE                                                             HELPER\r\nSTART                                                                       START\r\nWAITING--AggregateInitReq(ReportShare,AggParam,leader's PrepareMessage)---->FINISH\r\nFINISH<--AggregateResp(Finish(combined PrepareMessage))---------------------FINISH // helper \"broadcasts\" first prepare message\r\n```",
          "createdAt": "2022-12-16T17:55:32Z",
          "updatedAt": "2022-12-16T18:31:59Z",
          "comments": [
            {
              "originalPosition": 39,
              "body": "Since `AggregateInitializeResp` contains a sequence of `ReportInitialization`, then we only need a single `init_prep_step` and a single `stepped_prepare_step`, right?\r\n```suggestion\r\n  PrepareStep init_prep_step;\r\n  PrepareStep stepped_prepare_step;\r\n```\r\n",
              "createdAt": "2022-12-16T17:55:32Z",
              "updatedAt": "2022-12-16T18:32:00Z"
            },
            {
              "originalPosition": 30,
              "body": "IIUC, `ReportInitialization.init_prep_step` and `ReportInitialization.stepped_prepare_step` should always be in the same state (that is, both continued, both finished, or both failed), right? If so, then it's unfortunate that this representation allows constructing invalid combinations of `init_prep_step` and `stepped_prepare_step`. How about:\r\n\r\n```suggestion\r\n    case continued: {\r\n        opaque init_prep_msg<0..2^32-1>; /* VDAF preparation share or message */\r\n        opaque stepped_prepare_msg<0..2^32-1>; /* VDAF preparation share or message */\r\n    }\r\n```\r\n(or substitute something that is valid in the presentation language)\r\n\r\nAnd then `struct ReportInitialization` would contain a single `PrepareStep`. This would also avoid transmitting the same `report_id` value twice in each `ReportInitialization`.",
              "createdAt": "2022-12-16T18:01:39Z",
              "updatedAt": "2022-12-16T18:32:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KEsxd",
          "commit": {
            "abbreviatedOid": "af18352"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T18:12:58Z",
          "updatedAt": "2023-01-10T18:12:58Z",
          "comments": [
            {
              "originalPosition": 39,
              "body": "er, yeah, I think this was initially `opaque` and then I didn't update it properly.",
              "createdAt": "2023-01-10T18:12:58Z",
              "updatedAt": "2023-01-10T18:12:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KEyfx",
          "commit": {
            "abbreviatedOid": "af18352"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T18:31:39Z",
          "updatedAt": "2023-01-10T18:31:39Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "`init_prep_step` and `stepped_prepare_step` won't necessarily be in the same state: see the Input Share Preparation section. `init_prep_step` includes the helper's first call to `prep_next`, `stepped_prepare_step` includes the helper's second call to `prep_next`.\r\n\r\n(note that the use of `PrepareStep` in this way does repeat the `report_id` unnecessarily; `PrepareStep` is still used in a number of places that need to include a `report_id`. It'd be easy enough to factor the messages into `PrepareStep`-without-report-ID & `PrepareStep`-with-report-ID, but I'll do so once we're happy with the general approach.)",
              "createdAt": "2023-01-10T18:31:39Z",
              "updatedAt": "2023-01-10T18:31:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OOuFT",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I'm about half way through. It's looking good so far, but there's enough little things to address that I think it would be helpful to pause.\r\n\r\nThis is a big-ish PR, but I don't think there's a good way to split it up.",
          "createdAt": "2023-02-24T04:13:32Z",
          "updatedAt": "2023-02-24T04:29:41Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "* I think the notion of \"report share\" remains useful even after this change. I think it would be useful to keep the struct around.\r\n* I'd have to double check, but are we bumping the length tag for the prep share from 16 to 32 bits in this change? 16 ought to be sufficient.\r\n\r\n```suggestion\r\n  ReportShare report_share;\r\n  opaque leader_prep_share<0..2^32-1>;\r\n```",
              "createdAt": "2023-02-24T04:13:32Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 43,
              "body": "```suggestion\r\n  Leader's VDAF preparation initialization for this report.\r\n```",
              "createdAt": "2023-02-24T04:15:22Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 67,
              "body": "Maybe be specific here?\r\n```suggestion\r\ncarrying the first-round preparation message for each report.\r\n```",
              "createdAt": "2023-02-24T04:17:23Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 113,
              "body": "Hmm, what's going on here? IIUC this should carry the first-round prep message. Why do we have two PrepareStep's",
              "createdAt": "2023-02-24T04:19:14Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 128,
              "body": "Unless we take my suggestion above, we need to change \"ReportShare\" here.",
              "createdAt": "2023-02-24T04:20:17Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 134,
              "body": "Shouldn't it be \"round 1\", since the Helper has processed the round 1 prep message?",
              "createdAt": "2023-02-24T04:21:39Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 140,
              "body": "What's the reason for changing AggregationJobResp to AggregationJobInitResp?",
              "createdAt": "2023-02-24T04:22:19Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 178,
              "body": "Only the Helper runs prep_shares_to_prep.\r\n\r\nPreviously, the Leader had gathered the shares, combined them into the message, then sends the message to the Helper; now the Helper gathers the shares, combines them into the message, and sends the message to the Leader.",
              "createdAt": "2023-02-24T04:25:25Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 225,
              "body": "Hmm, doesn't the Leader need the previous PrepareStep in order to compute its next prep share?",
              "createdAt": "2023-02-24T04:27:16Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            },
            {
              "originalPosition": 237,
              "body": "I think you used the term \"leader_prep_share\" above; perhaps replace \"leader_outbound\" with \"leader_prep_share\" for consistency? (The new name is much better, IMO!)",
              "createdAt": "2023-02-24T04:28:22Z",
              "updatedAt": "2023-02-24T04:29:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OWBWP",
          "commit": {
            "abbreviatedOid": "ebc001f"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T23:54:42Z",
          "updatedAt": "2023-02-25T01:11:05Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "> I think the notion of \"report share\" remains useful even after this change. I think it would be useful to keep the struct around.\r\n\r\nHmm -- `ReportShare` was replaced (renamed & modified) with `ReportAggregationInitReq`, for consistency with `ReportAggregation{Init,Continue}{Req,Resp}` naming. I did this because I needed to split these messages out, since they now contain different fields. I could rename to something like `ReportShare{Init,Continue}{Req,Resp}` if that's preferable to folks?\r\n\r\n\r\n> I'd have to double check, but are we bumping the length tag for the prep share from 16 to 32 bits in this change? 16 ought to be sufficient.\r\n\r\nI think the only place prepare shares were transmitted prior to this PR was as part of a `PrepareStep` message, and `PrepareStep` uses a 32-bit length. We could shorten both, though `PrepareStep` (both before & after this PR) sometimes puts a `prep_msg` in the same field -- we should be careful that it's safe to shorten prepare messages, too.",
              "createdAt": "2023-02-24T23:54:42Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            },
            {
              "originalPosition": 134,
              "body": "It's sort of arbitrary -- as long as we are consistent, things will work out. I decided to maintain that the first recorded round value is 0, even though we are now shifting more of the work to live in the initialization step.",
              "createdAt": "2023-02-24T23:58:17Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            },
            {
              "originalPosition": 140,
              "body": "`AggregationJobResp` was previously used as the response for both aggregation initialization requests & aggregation continuation requests. I split these out to two messages because the response types now need to convey different information. Specifically, the aggregation initialization response now needs to include two prepare steps, while the aggregation continue response needs to include only one.",
              "createdAt": "2023-02-25T00:00:04Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            },
            {
              "originalPosition": 113,
              "body": "See the `input-share-prep` section for more detail, but in short:\r\n* `init_prep_step` contains the first preparation message (from the Helper's call to `prep_shares_to_prep`).\r\n* `stepped_prep_step` contains the preparation share the Helper retrieved from calling `prep_next` on the preparation message included in `init_prep_step`.\r\n\r\nIncluding both of these messages is required for the Leader to be able to perform the necessary computations to be sync'ed up & ready for the aggregation continuation step.",
              "createdAt": "2023-02-25T00:28:27Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            },
            {
              "originalPosition": 225,
              "body": "It has it: during initialization, this is `AggregateJobInitResp.report_shares[i].stepped_prep_step`. During continuation, this is `AggregateJobContinueResp.report_shares[i].prep_step`.",
              "createdAt": "2023-02-25T00:37:55Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            },
            {
              "originalPosition": 237,
              "body": "I like this. I also renamed variations of `inbound` to variations of `prep_msg`.",
              "createdAt": "2023-02-25T00:45:52Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            },
            {
              "originalPosition": 178,
              "body": "Argh, that's what I meant to write. I split this out explicitly now.\r\n\r\nOne thought: the `input-share-prep` section now describes two slightly-different procedures for the Leader & Helper. WDYT about moving this section into the `leader-init` & `helper-init` sections? I think this would likely be clearer, especially now that the aggregators are following slightly different processes, and would match the structure of the continuation section.",
              "createdAt": "2023-02-25T01:06:58Z",
              "updatedAt": "2023-02-25T01:11:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OVZ8V",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "There's a few other places in the document where we discuss multiple helpers that should be cleaned up. Off the top of my head:\r\n\r\n- In \"Upload Request\"`, we define:\r\n```\r\nstruct {\r\n  ReportMetadata report_metadata;\r\n  opaque public_share<0..2^32-1>;\r\n  HpkeCiphertext encrypted_input_shares<1..2^32-1>;\r\n} Report;\r\n```\r\nI think a lot of text gets simpler if we instead define:\r\n```\r\nstruct {\r\n  ReportMetadata report_metadata;\r\n  opaque public_share<0..2^32-1>;\r\n  HpkeCiphertext leader_encrypted_input_share;\r\n  HpkeCiphertext helper_encrypted_input_share;\r\n} Report;\r\n```\r\n\r\n- `{{task-configuration}}`'s discussion of `aggregator_endpoints`\r\n- `{{collect-aggregate}}`'s discussion of sending aggregate share reqs",
          "createdAt": "2023-02-24T20:50:28Z",
          "updatedAt": "2023-02-25T02:13:31Z",
          "comments": [
            {
              "originalPosition": 41,
              "body": "If we change the definition of `struct Report` as I suggest, then instead of this messy language about indices, we can just say \"the `helper_encrypted_report_share` from the report\" (or thereabouts).",
              "createdAt": "2023-02-24T20:50:28Z",
              "updatedAt": "2023-02-25T02:13:31Z"
            },
            {
              "originalPosition": 45,
              "body": "I think `{helper}` is more clear, since we don't have to worry about multiple helpers anymore.",
              "createdAt": "2023-02-24T20:53:47Z",
              "updatedAt": "2023-02-25T02:13:31Z"
            },
            {
              "originalPosition": 140,
              "body": "I think this is because `AggregationJobInitResp` is made up of `ReportAggregationInitResp`, which include both `init_prep_step` and `stepped_prep_step`, while the `ReportAggregationContinueResp`s that make up an `AggregationJobContinueResp` only contain a single `prep_step`, but I suspect the helper doesn't need to send `init_prep_step` during initialization and so the same message can be used for both init and continuation response.",
              "createdAt": "2023-02-25T01:57:24Z",
              "updatedAt": "2023-02-25T02:13:31Z"
            },
            {
              "originalPosition": 134,
              "body": "Yes, and then the `AggregationJobInitResp` the helper sends to leader is the message instructing the leader to join the helper on round 1.",
              "createdAt": "2023-02-25T01:58:10Z",
              "updatedAt": "2023-02-25T02:13:31Z"
            },
            {
              "originalPosition": 113,
              "body": "I agree, there should just be the current round combined/broadcast prepare message in this message. Let's look at what the Helper does during initialization, and let's imagine this is a VDAF with more than one round.\r\n\r\n~~~\r\nprep_msg = VDAF.prep_shares_to_prep(agg_param, [leader_prep_share, helper_prep_share])\r\n(next_prep_state, next_prep_share) = VDAF.prep_next(prep_state, prep_msg)\r\n~~~\r\n\r\n`leader_prep_share` is the leader's share of the 1st round prepare message, and is what was transmitted in the `AggregationJobInitReq`. `helper_prep_share` is the helper's share of the 1st round prepare message was derived from the encrypted report share by the helper. `prep_msg` is the combined/broadcast 1st round prepare message.\r\n\r\nHelper then uses `prep_msg` and `prep_state`, which is its round 0 state, to advance to round 1. The outputs are the helper's round 1 state and the helper's share of round 2's prepare message. In order to advance the leader to round 1, the helper needs to send the leader `prep_msg`, so that the leader can then use its round 0 state and the round 1 combined/broadcast prepare message to advance to round 1. The leader doesn't need `next_prep_share` because when it continues, it's going to send its round 2 prepare message share to the helper in an AggregationJobContinueReq.\r\n\r\nHowever, this discussion suggests a further possible optimization: if the helper did send the round 2 prepare message share along with the combined round 1 prepare message, then the leader could:\r\n\r\n- advance to round 1 using the round 1 prepare message, obtaining its share of the round 2 prepare message\r\n- combine its round 2 prepare message with the helper's share of the round 2 prepare message, yielding the round 2 combined prepare message\r\n- advance itself to round 2\r\n- transmit the round 2 combined prepare message to the helper in an `AggregationJobContinueReq`\r\n\r\nBasically, the leader and helper would take turns combining prepare message shares and instructing the other aggregator to advance its round, which could eliminate round trips for VDAFs with more than one round.",
              "createdAt": "2023-02-25T02:13:16Z",
              "updatedAt": "2023-02-25T02:13:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OWvck",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-25T21:54:59Z",
          "updatedAt": "2023-02-25T21:54:59Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "This change makes the first prepare message be computed from the prepare message shares by the Helper, and keeps all following prepare messages computed by the Leader. I went this way because it keeps more of the work being done by the Leader, and it's the \"minimal\" change to behavior in the sense that it changes the least about the protocol.\r\n\r\nTim, your first suggestion is equivalent to what is written in this PR except that the Helper is responsible for computing all of the prepare messages, because it changes things to have the leader send a prepare message share rather than a prepare message with each continuation. Your second suggestion is what is written down in this PR -- except that the Leader/Helper doesn't \"take turns\" (I'm not sure there's an advantage to implementing things this way). Both suggestions are the same number of round-trips, equal to the VDAF's `ROUNDS` parameter, and thus optimal in number of round-trips.",
              "createdAt": "2023-02-25T21:54:59Z",
              "updatedAt": "2023-02-25T21:54:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OWvlU",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-25T22:02:26Z",
          "updatedAt": "2023-02-25T22:02:26Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "(I guess the question is: who do we want to compute the prepare message, i.e. call `prep_shares_to_prep`, after initialization? Using this trick requires the Helper to call `prep_shares_to_prep` during initialization, but after that we can spell the protocol out to allow either the Leader or the Helper call `prep_shares_to_prep` on each continuation round. As-written, the Leader makes these calls; we could change things to have the Helper make these calls instead, or something more advanced like a turn-taking strategy. IMO, I'm happy with either the Leader or the Helper taking on this role, but don't think more advanced strategies where the Leader & Helper take turns are worth the protocol complexity.)",
              "createdAt": "2023-02-25T22:02:26Z",
              "updatedAt": "2023-02-25T22:02:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PFtt7",
          "commit": {
            "abbreviatedOid": "ebc001f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T17:51:57Z",
          "updatedAt": "2023-03-06T17:58:02Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "> Hmm -- ReportShare was replaced (renamed & modified) with ReportAggregationInitReq, for consistency with ReportAggregation{Init,Continue}{Req,Resp} naming.\r\n\r\nThis PR is quite large; to minimize reviewer effort, can we try to limit it to essential changes only?",
              "createdAt": "2023-03-06T17:51:58Z",
              "updatedAt": "2023-03-06T17:58:03Z"
            },
            {
              "originalPosition": 113,
              "body": "@tgeoghegan I don't think the change you suggest actually reduces the number of HTTP requests, does it? I don't see the benefit, and I think it makes things more complex.\r\n\r\n@branlwyd  re what is the minimal change: I think this PR should make the following change: Instead of having the Leader run prep_shares_to_prep(), have the Helper do it. That would mean the Leader's request contains its perp share, and the Helper's response contains the prep message (i.e., the broadcast message).",
              "createdAt": "2023-03-06T17:57:52Z",
              "updatedAt": "2023-03-06T17:58:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PFyoD",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T17:59:56Z",
          "updatedAt": "2023-03-06T17:59:56Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "In particular, in general I think we should try to avoid renaming structures, even if their contents change. In this particular case I don't think we need change the meaning (or even the contents of) ReportShare. The report share can still just be the encrypte dinput share and meta data; then the prep share could be a separate structure.",
              "createdAt": "2023-03-06T17:59:56Z",
              "updatedAt": "2023-03-06T17:59:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHubz",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-07T00:18:07Z",
          "updatedAt": "2023-03-07T00:18:08Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "Hopefully this will be more clear after #422.",
              "createdAt": "2023-03-07T00:18:07Z",
              "updatedAt": "2023-03-07T00:18:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QjH8c",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-21T21:07:41Z",
          "updatedAt": "2023-03-21T21:07:41Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "Fair enough -- the new \"ping-pong\" approach uses the same response type for both aggregation initialization & continuation, so renaming back to `ReportShare` is natural to do.",
              "createdAt": "2023-03-21T21:07:41Z",
              "updatedAt": "2023-03-21T21:07:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QjIUF",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-21T21:08:55Z",
          "updatedAt": "2023-03-21T21:08:55Z",
          "comments": [
            {
              "originalPosition": 113,
              "body": "We went with an approach that implements Tim's ping-pong idea, reducing the number of network round trips by about half.",
              "createdAt": "2023-03-21T21:08:55Z",
              "updatedAt": "2023-03-21T21:08:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QjI2Y",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-21T21:10:45Z",
          "updatedAt": "2023-03-21T21:10:45Z",
          "comments": [
            {
              "originalPosition": 134,
              "body": "I think that it is clearer if we maintain that the `round` counter counts DAP rounds (i.e. network round-trips), rather than VDAF rounds, since the DAP round is the \"unit of continuation\" -- if something goes wrong in transit during an aggregation continuation attempt, we'll be restarting from the beginning of the previous DAP round, rather than from a VDAF round that is traversed partway through a DAP round.",
              "createdAt": "2023-03-21T21:10:45Z",
              "updatedAt": "2023-03-21T21:10:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QjI6x",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-21T21:10:59Z",
          "updatedAt": "2023-03-21T21:11:00Z",
          "comments": [
            {
              "originalPosition": 140,
              "body": "(this no longer applies with the new \"ping-pong\" approach)",
              "createdAt": "2023-03-21T21:11:00Z",
              "updatedAt": "2023-03-21T21:11:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QjJZg",
          "commit": {
            "abbreviatedOid": "8e5e69d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-21T21:12:27Z",
          "updatedAt": "2023-03-21T21:12:27Z",
          "comments": [
            {
              "originalPosition": 41,
              "body": "I don't mind doing this, but it can be added on in a later PR -- let's get the technical bits correct here before we update all of the other text, to keep scope in check?",
              "createdAt": "2023-03-21T21:12:27Z",
              "updatedAt": "2023-03-21T21:12:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QjtXp",
          "commit": {
            "abbreviatedOid": "ae6ca7d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-03-21T23:54:09Z",
          "updatedAt": "2023-03-23T01:04:24Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "```suggestion\r\n- Initialization: Begin the aggregation flow by sharing report and verifier shares with the\r\n```",
              "createdAt": "2023-03-21T23:54:09Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 103,
              "body": "Instead of fiddling with indices, we should change message definitions so that instead of vectors of objects indexed by aggregator, you have two fields, one for the leader and one for the helper. e.g., `Report` becomes:\r\n\r\n```\r\n   struct {\r\n     ReportMetadata report_metadata;\r\n     opaque public_share<0..2^32-1>;\r\n-    HpkeCiphertext encrypted_input_shares<1..2^32-1>;\r\n+    HpkeCiphertext leader_encrypted_input_share;\r\n+    HpkeCiphertext helper_encrypted_input_share;\r\n   } Report;\r\n```\r\n\r\nThis will simplify a few paragraphs in the protocol text and should reduce the encoded size by removing the length prefix.",
              "createdAt": "2023-03-22T00:04:38Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 137,
              "body": "```suggestion\r\nIf the Helper finished but the Leader did not, then the report cannot be\r\nprocessed further and MUST be removed from the candidate set. Otherwise, `out`\r\n```",
              "createdAt": "2023-03-22T00:10:00Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 181,
              "body": "This text seems just as important as the sentences around it and so shouldn't be subordinated into a parenthetical.\r\n```suggestion\r\nIf `ReportShare.leader_prep_share` is empty, the Helper must\r\nretrieve its output share during the VDAF initialization; if it does not, the\r\nreport must be marked as invalid with a \"vdaf_prep_error\" error. Then the\r\n```",
              "createdAt": "2023-03-22T00:16:21Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 231,
              "body": "```suggestion\r\n`prep_msg` and `prep_share`, or is marked as finished if the VDAF preparation\r\n```",
              "createdAt": "2023-03-22T00:20:27Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 233,
              "body": "```suggestion\r\naggregation job is now in round 1.\r\n```\r\n\r\nBefore this change, all the helper was doing was `Vdaf.prep_init` to obtain its round 1 prep message share and thus was in round 0. Now, it uses its own round 1 prep message share + the leader's round 1 prep message share and advances to round 1, and then responds to the leader with the broadcast prep message, enabling the leader to advance to round 1 too.",
              "createdAt": "2023-03-22T00:21:59Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 251,
              "body": "```suggestion\r\naggregator (`0x02` for the Leader and `0x03` for the Helper). The `OpenBase()`\r\n```",
              "createdAt": "2023-03-22T00:24:53Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 169,
              "body": "We discuss \"the process\" (referring to the overall VDAF preparation) and then immediately after \"this process\" referring to helper initialization) which I think is confusing. How about:\r\n```suggestion\r\nuse to continue preparing the report.\r\n\r\nTo begin this process, the Helper first checks if it recognizes the task ID. If\r\n```",
              "createdAt": "2023-03-23T00:19:12Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 431,
              "body": "```suggestion\r\nin the aggregation job, including a preparation message and possibly a preparation\r\n```",
              "createdAt": "2023-03-23T00:32:05Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 432,
              "body": "```suggestion\r\nmessage-share as described above. The media type is set to\r\n```",
              "createdAt": "2023-03-23T00:32:30Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 415,
              "body": "```suggestion\r\npreparation state and `prep_share` is the Leader's next preparation share; the\r\n```",
              "createdAt": "2023-03-23T00:34:28Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 417,
              "body": "```suggestion\r\n`PrepareStep` with the `prep_msg` it held and the `prep_share` it computed. (If\r\n```",
              "createdAt": "2023-03-23T00:34:39Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 432,
              "body": "What does \"above\" refer to here? The text in `{#aggregation-helper-init}` discussing how to construct `AggregationJobResp`? An explicit reference to that section, or just spelling out how to construct the `PrepareStep`s again would make this more clear.",
              "createdAt": "2023-03-23T00:37:35Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 520,
              "body": "```suggestion\r\n`PrepareStep` with an empty `prep_msg` field. If one aggregator finished but\r\nthe other did not, then the report cannot be processed further and the Helper\r\nMUST respond with a \"vdaf_prep_error\" failure. Otherwise, `out` is the pair\r\n```",
              "createdAt": "2023-03-23T00:39:26Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 522,
              "body": "```suggestion\r\npreparation state and `helper_prep_share` is the Helper's next preparation share;\r\n```",
              "createdAt": "2023-03-23T00:39:43Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 525,
              "body": "```suggestion\r\nfollows.\r\n```\r\n\r\nI think this repeats the line at the beginning of the paragraph on line 1680.",
              "createdAt": "2023-03-23T00:40:59Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 563,
              "body": "```suggestion\r\n`PrepareStep` containing the computed `prep_msg` and `prep_share`.\r\n```",
              "createdAt": "2023-03-23T00:41:35Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 554,
              "body": "This paragraph gets repeated more or less verbatim something like four times, which seems unfortunate. I wonder if we could factor this text into a section describing how an aggregator (regardless of role) transforms a round _n_ `PrepareStep` into a round _n+1_ `PrepareStep`, because I'm pretty sure it's exactly the same for leader and helper. Then, the leader continuation and helper continuation sections could both refer to that section. This is similar to how we refer to `{#input-share-prep}` from both `{#leader-init}` and `{#aggregation-helper-init}`.",
              "createdAt": "2023-03-23T00:47:46Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            },
            {
              "originalPosition": 376,
              "body": "I don't think the new text discusses what the leader should do if the `PrepareStep` is of type `failed`.",
              "createdAt": "2023-03-23T01:03:47Z",
              "updatedAt": "2023-03-23T01:04:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5QxIsm",
          "commit": {
            "abbreviatedOid": "ae6ca7d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I'm gratified by how simple this turned out to be. However so much is changing here that we need to make sure we implement this before merging the PR.\r\n\r\nOne high-level comment:\r\n\r\n* In the overview section, let's say how many HTTP requests the aggregation flow requires, as a function of the number of rounds for the VDAF. Also, give a brief explanation as to why.\r\n",
          "createdAt": "2023-03-23T16:47:15Z",
          "updatedAt": "2023-03-23T17:06:39Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "Logically, the \"Report Share\"  (see conventions) is the encrypted input share and corresponding metadata. I don't think we should overload this term by including the leader_prep_share. I would suggest renaming this struct to something like \"ReportPrepInit\" or something. In fact, I would go one step further refactoring as follows:\r\n\r\n```\r\nstruct {\r\n    ReportShare report_share;\r\n    opaque leader_prep_share<0..2^32-1>;\r\n} ReportPrepInit;\r\n```\r\n\r\nand update AggregateJobInitReq accordingly.",
              "createdAt": "2023-03-23T16:47:15Z",
              "updatedAt": "2023-03-23T17:06:39Z"
            },
            {
              "originalPosition": 103,
              "body": "Yup, this is a benefit of simplifying the protocol.",
              "createdAt": "2023-03-23T16:47:57Z",
              "updatedAt": "2023-03-23T17:06:39Z"
            },
            {
              "originalPosition": 148,
              "body": "Why don't we compute the next prep message here?",
              "createdAt": "2023-03-23T16:55:25Z",
              "updatedAt": "2023-03-23T17:06:39Z"
            },
            {
              "originalPosition": 179,
              "body": "Why would this ever be empty? For the moment we should presume the VDAF is at least 1-round. (I think this assumption is already implicit?) \r\n\r\nIf the consideration is DAF compatibility, then we can worry about that later.",
              "createdAt": "2023-03-23T16:55:51Z",
              "updatedAt": "2023-03-23T17:06:39Z"
            },
            {
              "originalPosition": 196,
              "body": "For consistency with the variable names we use for processing the Leader's transition.\r\n```suggestion\r\n{{collect-flow}}. Otherwise, `out` is the pair `(helper_prep_state, helper_prep_share)`, where\r\n```",
              "createdAt": "2023-03-23T16:56:38Z",
              "updatedAt": "2023-03-23T17:06:39Z"
            },
            {
              "originalPosition": 214,
              "body": "Sanity check: Is it the case that the finished state carries a prep message for (1) even-round VDAFs and (2) odd-round VDAFs?",
              "createdAt": "2023-03-23T16:59:12Z",
              "updatedAt": "2023-03-23T17:06:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Qw5-J",
          "commit": {
            "abbreviatedOid": "7727d59"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-23T16:15:27Z",
          "updatedAt": "2023-03-23T23:50:19Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "Done, but I went with \"prepare message-shares\" to match the wording of the rest of the spec.",
              "createdAt": "2023-03-23T16:15:27Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 233,
              "body": "I don't think we should make this change: I think that we should maintain that the DAP round counter counts \"DAP steps\" (i.e. network round-trips), rather than \"VDAF preparation steps\" (i.e. calls to `prep_next`), because the \"DAP step\" is the unit of retry/continuation in DAP. That is, if something happens to necessitate a retry here, we'll be restarting from 0 (i.e. the beginning of aggregation initialization) anyway -- there isn't much point to tracking \"how far\" we made it into the step that failed and will be retried entirely.",
              "createdAt": "2023-03-23T16:44:15Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 251,
              "body": "Done.",
              "createdAt": "2023-03-23T17:07:19Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 563,
              "body": "Done.",
              "createdAt": "2023-03-23T17:08:09Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 525,
              "body": "This line applies to the operation in L1655, the latter line applies to the operation in L1673. Still, this can be reworked: I merged \"VDAF operation fails\" handling with \"one aggregator finished & the other did not\" handling since both are responded to in the same way. This also structures the paragraphs to be more similar to one another.",
              "createdAt": "2023-03-23T17:24:03Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 522,
              "body": "Done.",
              "createdAt": "2023-03-23T17:24:29Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 520,
              "body": "(parenthesis removed in a different way)",
              "createdAt": "2023-03-23T17:25:20Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 432,
              "body": "Done.",
              "createdAt": "2023-03-23T17:26:08Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 432,
              "body": "\"Above\" refers to the beginning of this section, where the result of the `prep_next` call determines what kind of `PrepareStep` is sent for each report. Updated the text to be more explicit in its reference.",
              "createdAt": "2023-03-23T17:27:03Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 431,
              "body": "Done.",
              "createdAt": "2023-03-23T17:30:28Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 137,
              "body": "(parenthesis removed in a different way.)",
              "createdAt": "2023-03-23T17:44:43Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 148,
              "body": "This operation does compute the next prepare message; do you mean why not compute the next prepare message-share via another call to `prep_next`?\r\n\r\nIf so, it's technically feasible to compute the next prepare message-share here [as long as we make a similar change to the aggregation continuation logic]. In fact, implementations could choose to compute the prepare message-share here & at the end of aggregation continuation, rather than at the beginning of aggregation continuation, and still interoperate with aggregators that made the opposite choice.\r\n\r\nThe main consideration for writing things this way is this makes it clear what the \"minimal durable storage\" between rounds is: as written, it's clear that implementations can be written that only store the prepare message between rounds. If we wrote it the other way, implementations written \"to the word\" of that spec would end up storing both the prepare message & the prepare message-share between rounds.\r\n\r\nA secondary consideration is that this keeps the spec shorter: if written this way, the relevant `prep_next` call is described only at the beginning of aggregation continuation. If we computed the next prepare message-share here, we would need to describe the relevant `prep_next` call both here & at the end of aggregation continuation.",
              "createdAt": "2023-03-23T17:53:49Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 179,
              "body": "DAF-compatibility was indeed the concern. I went ahead and ripped out the relevant text here & in the Leader Initialization section -- no objection from me in keeping the specification simple for now. (at least it's good to know that supporting DAFs will be a minor addition to the DAP protocol, though perhaps the spec change will end up somewhat larger due to e.g. security considerations)",
              "createdAt": "2023-03-23T18:04:33Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 181,
              "body": "(parenthetical text removed for other reasons)",
              "createdAt": "2023-03-23T18:04:58Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 196,
              "body": "Done. (though note I used `prep_state` throughout -- I maintained that here, can update everywhere if y'all prefer)",
              "createdAt": "2023-03-23T20:11:04Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 214,
              "body": "It's true that a `finished` message will always carry a prepare message, rather than a prepare message-share, for all VDAFs. However, even-round VDAFs will terminate in a way that leaves this field empty during the Helper's resposne to the leader in the final DAP network round-trip. This works on the assumption that no valid prepare message will ever be 0 bytes. (this oddity is actually my least-favorite technical issue with this change, but I think I prefer to leave the field empty rather than introduce a new enum value which indicates \"finished, but no `prep_msg`\")",
              "createdAt": "2023-03-23T20:15:50Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 231,
              "body": "Done.",
              "createdAt": "2023-03-23T20:16:00Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 415,
              "body": "Done.",
              "createdAt": "2023-03-23T20:31:01Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 417,
              "body": "Done.",
              "createdAt": "2023-03-23T20:31:10Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            },
            {
              "originalPosition": 376,
              "body": "Added some text saying that the Leader should also fail.\r\n\r\n[BTW, why is `report_too_early` handling defined in aggregation continuation? wouldn't right place to check mention this be in aggregation initialization, since (I think) that's where the error would be encountered? I moved the consideration for `report_too_early` to that section]",
              "createdAt": "2023-03-23T20:43:38Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 554,
              "body": "I think I disagree:\r\n* All overall operations (leader initialization, helper initialization, leader continuation, helper continuation) are different in terms of the VDAF operations they perform -- I don't think we can write a useful generalization that covers all of them.\r\n* We could try to generalize this paragraph describing handling of the outcome of `prep_next`, specifically. However:\r\n  * The handling of `out` differs in every case. So we'd switch from an explicit paragraph to a paragraph that cites another section, then has a similar amount of text describing the handling of each possible outcome.\r\n  * This wouldn't save much in terms of total text, and the additional indirection & generality would make the text harder to read.\r\n\r\nI think this really is a case where the thing we could generalize over is too small to be worth factoring out -- repetition is better here. Thoughts/other approaches?",
              "createdAt": "2023-03-23T20:57:18Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 80,
              "body": "Done (I think I updated all the field references appropriately).",
              "createdAt": "2023-03-23T21:10:11Z",
              "updatedAt": "2023-03-23T23:50:20Z"
            },
            {
              "originalPosition": 103,
              "body": "OK -- I was avoiding updating parts of the spec other than the aggregation section to avoid scope creep in this PR, but I went ahead and updated the spec for all text I could find that can usefully be updated.",
              "createdAt": "2023-03-23T21:26:13Z",
              "updatedAt": "2023-03-23T23:50:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Q0Kmy",
          "commit": {
            "abbreviatedOid": "ae6ca7d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-24T00:38:31Z",
          "updatedAt": "2023-03-24T00:38:32Z",
          "comments": [
            {
              "originalPosition": 554,
              "body": "I'm happy to move forward with this as-is and we can try reducing this text in the future.",
              "createdAt": "2023-03-24T00:38:32Z",
              "updatedAt": "2023-03-24T00:38:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5RL6M_",
          "commit": {
            "abbreviatedOid": "ae6ca7d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-28T23:59:52Z",
          "updatedAt": "2023-03-28T23:59:53Z",
          "comments": [
            {
              "originalPosition": 233,
              "body": "I get the distinction you're drawing between DAP round and VDAF round, and agree that for a 1-round VDAF, it won't really matter, because on retry, the leader will re-send `AggregationJobInitReq`. But then I think we need to make sure the leader and helper continuation sections make it clear when either advances to the next round. Right now, the discussion of how the leader constructs `AggregationJobContinueReq` reads (lines 1596-1597):\r\n\r\n>The `round` field is the round of **VDAF preparation** that the Leader just reached\r\n>and wants the Helper to advance to.\r\n\r\nEmphasis mine. I think it might suffice to change that from \"VDAF preparation\" to something like \"DAP aggregation\".\r\n\r\nI put together a pair of diagrams to work out just when the DAP and VDAF round transitions occur to make sure I understand this. Would you take a look and confirm to me that we're understanding this the same way?\r\n\r\n[2 round VDAF, no failures](https://sequencediagram.org/index.html#initialData=CoSwLgNgpgBATDATgewK4DsAmMBqARAQQDEYAKKAOgHMKYAFZABwgENEBKAKE8bbBADGIXujAwAMlBaYoiHn0HCWomAAkoERrO4AjZAA8YyAG6yJUmYgBc0aWYDOYFmFgAGGAF5cmFgDMKjIhQjAD6IOjgpLaWSMHIiGL2ABZsUOwAOuhRFg5OLjAAjAA0MNFmgcEwyakwruye3n4BQaHoUPpg2XaIVXluJQByyG1ceoYmZpLdVo7xsCxUVEFUziDDMABWyDowAO7gSUhoWA2uJWU9js6wBZyIY0amPVOWVlCiZvjERxjYrpnvFw9Qh0H4nVycF6yAC0AD51JpZFYCItlqthgApbYASQiYAASlAAI6kJIaLQ9FrxRIpILnHI9CqMKq0tyjAyPMwIilWMmIy59WoNHA+fxMsJ40nksxUhIs1IZLJ8im9a6FErK8oteVBWr1LwipritodKX81X5M4wIYjO4PCY9blIwGfQgkFC-Wqce1PNTS6xMmAAW3sVCFBtFzWCIWqQXsITAyBCTLNKsDsf6pQZMHTrL1mVTuTVcA1-pz2ozhX1jTFLRCJs6moFauK5cqIbDdW9HIdfv5M0TuoWSygK346y2O32YEOHpOXitTYtsDgdp7vqd1hdPS+7uO2AKAI+wIIoLnf04m7hUOsKJHY7W6CxOkJ9kYpDoLVSAGUXIwrFYADCwz8OgqBQKQgYdrUpbmrmNQFOw7LjL6N5WL4UBgAIhzDmi47oJs2x7AcYLYAu9LdMuhTdihkwMlYXQxFc+QllmlHwbqiHCpGxrtJ0FxUa2UGhnqa60c89HbrgbqkdRPp0dMwlhgU3FGnWGbxomyYtIxWqVJWrZLhxNyKmgNI1BGanRg2unNixJRKVWNGchJ0y9HMMC4aO6IEZOxEzrJXitmZOpQGJLnmNMUm7rJcBHkCMAgrJtw3nCm7Iqi3n4c+wGiOE4GEiS54NK2n7BD+f4AUQ4QgMkUCYJB2rQYhyERelGFYThmUPhORHTrO+6nLBKrMSuzm9ulIWVpZtbWXxhZ2SuDlNSJiHjRu-oDh5Xk9b5fUkcVQUlFNrLhRNm3RTJxVxW0x6JaeyWXv6170XeeGPs+r7vmVvBBL+wRVTVdUNQA3gAvuwQA)\r\n\r\n[2 round VDAF, response dropped during continuation](https://sequencediagram.org/index.html#initialData=CoSwLgNgpgBATDATgewK4DsAmMBqARAQQDEYAKKAOgHMKYAFZABwgENEBKGAd3AAsYAZixARUiKACgJjNmBABjEDPRgYAGSgtMURNNkKlLFTAASUCIx1SARsgAeMZADcd6zdsQAuaFtcBnMBYwWAAGGABeXEwWAQpGcUYAfRB0cFIfDyQoRmREVT9eNih2AB10dPd-QOCYAEYAGhgM13jsmAKimBDOSJxo2Nak9Cg7MArfRHbq0MaAOWRh9glbB2dXDQnPANzYFioqcSogkAWYACtka24+JDQsCK7G5smAoNhaiUQVxxdJjY9PFAVK58MRbhhsCEykDgpNCHRwfcQhJ-joALQAPjMFh0ngI+0OxwWAClLgBJVJgABKUAAjqReOZLJMErl8oVxE9KpNBu0OaElt81pNsczPIycS9pl0Hn0YnEEslKQyma5WXk+UVSuUJcypm86o1dS0EprxF0elF5YNEsNRirJfqaiE5gtip8hb9TKqvDCQYQSCgIV1lvYfq5RbjeQBbPxUGW9foK7KJDriPyJMDIRKDB163lpmZNbkwAv8i1lPNVA1wI0+0umwt1S1ygaKu1jY1Sg0NBttWPx7qh1ZeyNebbmvYHKBHOSnC5XHhgfhB+6RF0wLtO2BwD1h4XeyWA4GTUGBu7YWrQk8weGIyESMeY1FefHT2cndCk6w0vyMUh0AkRQAMrBIwnieAAwgscjoKgUCkDGcaPJu9Zlp0tTsIK+5ei+ngCFAYDyPwU6EnO6DnJc1zLveDwbs8251MO4Z-NynjjJkrw1LWxYTH2jBmu8LZJjaHYcdWNS9khg5LF8OHrGxfqngGtEfJ6CmbNJdSyiJiqFhmWY5gk4k8o25a9lu6HmphZRoOynSJta7YjGMDFcTujRaZhzEHnhUw7DApEzkSFELtRK4Xg8vZ2YJe4jhpAJKbgKmrtgcDXrCt4EAiqVMS+mJjniBLBeR37QSoKTwTS9K5ZEvaAdkIFgRBRApCABRQJgiGmgOzbYfFIo+vhhHEYFxUfvOVFLhFwbrnWjrufAPmjkNMVNo5bYpmJW6LTxXn9Sxh5ihOuzjSFlGLjctWGo4qD2eIcWHYVSVnrR6XDDed65R8T4AMTiJgngAMx2Hhb5kZ+36-v+DUyOIoHZC1bUdV1ADeAC+B2+Wx4hgIgACeUzZIwKTxkFE2hZcKLcgVQ3gyVkOXOVsFVXSpDXfVQHw81nitakKPdf2yGYVjK1HgRREkWd5EXeFtF1Y0a38stEZDeT51he1gUQOIWiE6cX2Rb2CSsIT2jBPIwSYI+PrPmx9MU1DUB-gBXNQAj4G88jjJo5jEhAA)",
              "createdAt": "2023-03-28T23:59:52Z",
              "updatedAt": "2023-03-28T23:59:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Rvf4J",
          "commit": {
            "abbreviatedOid": "ae6ca7d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-04T16:56:11Z",
          "updatedAt": "2023-04-04T16:56:12Z",
          "comments": [
            {
              "originalPosition": 233,
              "body": "Good point -- I switched the description of the `round` field to explicitly describe DAP rounds rather than VDAF rounds.\r\n\r\nI think the sequence diagrams are roughly correct. (I'm not sure what it means to \"enter into\" a round, or why the rounds are entered at the points where they are in the sequence, but the counts are correct vs network round trips.)\r\n\r\nUltimately I think it's more useful to count DAP rounds over VDAF rounds because the purpose of the `round` field is to enable retries. The unit of retries in DAP is the network request, rather than a VDAF round. (this is confounded by the fact that, before this change, a DAP network request could be 1-1 mapped to a VDAF round.)",
              "createdAt": "2023-04-04T16:56:11Z",
              "updatedAt": "2023-04-04T16:56:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TA5p_",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-19T19:26:22Z",
          "updatedAt": "2023-04-19T23:38:52Z",
          "comments": [
            {
              "originalPosition": 676,
              "body": "In the overview I would only state what the number of HTTP requests will be. The explanation should go somewhere else, since it requires understanding some of the details that are not introduced yet.",
              "createdAt": "2023-04-19T19:26:22Z",
              "updatedAt": "2023-04-19T23:38:52Z"
            },
            {
              "originalPosition": 474,
              "body": "This a wire breaking change that is not strictly needed. I would like to consider punting to a future PR in order to minimize code changes.",
              "createdAt": "2023-04-19T19:57:47Z",
              "updatedAt": "2023-04-19T23:38:52Z"
            },
            {
              "originalPosition": 720,
              "body": "```suggestion\r\n  ReportShare helper_report_share;\r\n```",
              "createdAt": "2023-04-19T20:02:39Z",
              "updatedAt": "2023-04-19T23:38:52Z"
            },
            {
              "originalPosition": 214,
              "body": "~As discussed on the 2023-04-18 sync: There are cases where the prep message may be the empty string. Therefore we will need some way of distinguishing between \"I've commited, but there's no message to send (I'm just ACK'ing)\" and \"I've committed, here's the last message you'll need before you commit\".~\r\n\r\n~WDYTA \"commit(prep_msg)\" and \"committed\"?~\r\n",
              "createdAt": "2023-04-19T22:57:37Z",
              "updatedAt": "2023-04-21T19:46:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TCF_R",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-20T00:52:44Z",
          "updatedAt": "2023-04-20T00:52:44Z",
          "comments": [
            {
              "originalPosition": 474,
              "body": "It was a while ago, but [we actually both asked Bran to do this](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393#discussion_r1144102260). I think we should keep this change in this PR. Plus, if you're concerned about the code changes needed to test this against Janus, well, the message format changes are already implemented [there](https://github.com/divviup/janus/pull/1234), so backing this out would be more code changes for us!",
              "createdAt": "2023-04-20T00:52:44Z",
              "updatedAt": "2023-04-20T00:52:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TGs8P",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-20T15:29:41Z",
          "updatedAt": "2023-04-20T15:29:41Z",
          "comments": [
            {
              "originalPosition": 474,
              "body": "ACK, no problem.",
              "createdAt": "2023-04-20T15:29:41Z",
              "updatedAt": "2023-04-20T15:29:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TOIFU",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T18:07:49Z",
          "updatedAt": "2023-04-21T19:32:39Z",
          "comments": [
            {
              "originalPosition": 762,
              "body": "```suggestion\r\n  `leader_prep_share` is the Leader's initial preparation share as computed during\r\n```",
              "createdAt": "2023-04-21T18:07:49Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 772,
              "body": "Not strictly related to this PR, but I'm worried that the distinction is not clear between a \"VDAF round\" and a \"DAP-aggregation round\". I think it would be helpful to introduce the notion of DAP-agg rounds in the overview and distinguish it from VDAF rounds.",
              "createdAt": "2023-04-21T18:08:55Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 787,
              "body": "A \"MAY\" inside of a paranthetical is somewhat icky, since it implies normative text. Is this behavior spelled out elsewhere? If so, I'd remove it; otherwise, if this is meant to be normative, let's be more explicit.\r\n```suggestion\r\nreturned a `failed` `PrepareStep`, the Leader marks the report as failed. Unless the error is `report_too_early`, the Leader MUST NOT include the report in a subsequent aggregation job. Otherwise, the Leader MAY include the report in a future aggregaton job.\r\n```",
              "createdAt": "2023-04-21T18:13:09Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 782,
              "body": "The rest of this section feels out of order, as it's what happens after the Helper sends the `AggregationJobResp`. However it might make sense to leave it here, because of the way the \"Leader Continuation\" section is structured.",
              "createdAt": "2023-04-21T18:23:52Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 829,
              "body": "There is one and only one preparation message for each round.\r\n```suggestion\r\nsuccessful, it includes the preparation message in its response that the Leader will\r\n```",
              "createdAt": "2023-04-21T18:27:43Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 846,
              "body": "```suggestion\r\nFor each remaining input share, the Helper proceeds as follows. Let\r\n```",
              "createdAt": "2023-04-21T18:29:17Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 864,
              "body": "It also needs to send `prep_msg` to the Leader.",
              "createdAt": "2023-04-21T18:31:56Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 867,
              "body": "I don't think this needs to be said.\r\n```suggestion\r\n`helper_prep_share` is its next preparation message-share.\r\n```",
              "createdAt": "2023-04-21T18:32:38Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 896,
              "body": "Not strictly part of this PR but I think this is a good idea. (If an implementer missed this they could not interop with Daphne :D )\r\n```suggestion\r\nThe message is a sequence of `PrepareStep` values, the order of which MUST match\r\n```",
              "createdAt": "2023-04-21T18:36:08Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 923,
              "body": "```suggestion\r\nby the HPKE configuration. If the aggregator supports multiple HPKE configurations\r\n```",
              "createdAt": "2023-04-21T18:41:36Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 14,
              "body": "FWIW, in the VDAF spec we're using capital-\"A\" for Aggregator. I see why it makes sense to not treat it as a proper noun here, but I think we could also argue for keeping it consistent with the VDAF spec.",
              "createdAt": "2023-04-21T18:43:16Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            },
            {
              "originalPosition": 1045,
              "body": "Nice catch, but we could be a bit more specific:\r\n```suggestion\r\n  `AggregationJobInitReq` (if this is the first round of the aggregation job) or the\r\n```",
              "createdAt": "2023-04-21T18:48:00Z",
              "updatedAt": "2023-04-21T19:32:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TOpag",
          "commit": {
            "abbreviatedOid": "ae6ca7d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T20:14:50Z",
          "updatedAt": "2023-04-21T20:14:50Z",
          "comments": [
            {
              "originalPosition": 214,
              "body": "The ladder diagram looks something like this:\r\n\r\n| | |\r\n|-|-|\r\n| Leader -> Helper | report_share, [leader_prep_1] |\r\n| Helper-> Leader | prep_1, [helper_prep_2] |\r\n| Leader -> Helper | prep_2, [leader_prep_3] |\r\n| Helper -> Leader | prep_3, [leader_prep_4] |\r\n\r\n... and so on. The square brackets indicate an optional message that is only sent if there is another round.\r\n\r\n* First message: helper_report_share, leader_prep_share_1\r\n* Middle message: prep_I, prep_share_I+1 (either leader or helper)\r\n* Odd round VDAF, penultimate message: prep_R-1, leader_prep_share_R\r\n* Odd round VDAF, last message: prep_R\r\n* Even round VDAF, penultimate message: prep_R\r\n* Even round VDAF, last message: (nil)\r\n\r\nThus I think we have the following variants for PrepareStep (subsuming ReportInit, which, IMO would be a bit clearer):\r\n```\r\nstruct {\r\n  PrepareStepState prepare_step_state;\r\n  select (PrepareStep.prepare_step_state) {\r\n    case initialize:\r\n      opaque report_share<0..2^32-1>;\r\n      opaque prep_share<0..2^32-1>;\r\n    case continue:\r\n      ReportId report_id;\r\n      opaque prep_msg<0..2^32-1>;\r\n      opaque prep_share<0..2^32-1>;\r\n    case finish:\r\n      ReportId report_id;\r\n      opaque prep_msg<0..2^32-1>;\r\n    case finished:\r\n      ReportId report_id;\r\n    case reject:\r\n      ReportId report_id;\r\n      ReportShareError error;\r\n  };\r\n} PrepareStep;\r\n```\r\n\r\nThe reason I changed the verb tense is to distinguish between finish and finished, the latter being the variant without the payload sent by the Helper in even-round VDAFs. I've also changed \"failed\" to \"reject\", as the latter is maybe more inline with the verbage we have.",
              "createdAt": "2023-04-21T20:14:50Z",
              "updatedAt": "2023-04-21T20:14:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TO_gc",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T22:06:58Z",
          "updatedAt": "2023-04-21T22:06:58Z",
          "comments": [
            {
              "originalPosition": 720,
              "body": "(also cleaned up a few places that were still referring to `AggregateJobInitReq.report_shares` instead of `AggregateJobInitReq.report_inits`)",
              "createdAt": "2023-04-21T22:06:58Z",
              "updatedAt": "2023-04-21T22:06:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPArC",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T22:15:46Z",
          "updatedAt": "2023-04-21T22:15:46Z",
          "comments": [
            {
              "originalPosition": 762,
              "body": "(cleaned up all instances of \"prepare/preparation message-share\" throughout.)",
              "createdAt": "2023-04-21T22:15:46Z",
              "updatedAt": "2023-04-21T22:15:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPCk6",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T22:30:17Z",
          "updatedAt": "2023-04-21T22:30:18Z",
          "comments": [
            {
              "originalPosition": 829,
              "body": "(also cleaned up \"prepare message\" -> \"preparation message\" throughout.)",
              "createdAt": "2023-04-21T22:30:17Z",
              "updatedAt": "2023-04-21T22:30:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPFVV",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T22:42:38Z",
          "updatedAt": "2023-04-21T22:42:39Z",
          "comments": [
            {
              "originalPosition": 864,
              "body": "Yep -- the structure of the response message is defined later (in the second-to-last paragraph of this section). This paragraph determines how the values that eventually go into the message are computed.\r\n\r\nWould you like to see an update here?",
              "createdAt": "2023-04-21T22:42:39Z",
              "updatedAt": "2023-04-21T22:42:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPF7t",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T22:47:41Z",
          "updatedAt": "2023-04-21T22:47:42Z",
          "comments": [
            {
              "originalPosition": 867,
              "body": "OK -- I've made updates to the preparation state implicit.",
              "createdAt": "2023-04-21T22:47:41Z",
              "updatedAt": "2023-04-21T22:47:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPGNC",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T22:49:25Z",
          "updatedAt": "2023-04-21T22:49:25Z",
          "comments": [
            {
              "originalPosition": 896,
              "body": "Nor with Janus, I think a `MUST` makes sense here.",
              "createdAt": "2023-04-21T22:49:25Z",
              "updatedAt": "2023-04-21T22:49:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPJ-0",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T23:32:13Z",
          "updatedAt": "2023-04-21T23:32:14Z",
          "comments": [
            {
              "originalPosition": 772,
              "body": "I agree; I've updated the Overview as suggested, and updated the use of \"rounds\" throughout to be more clear.",
              "createdAt": "2023-04-21T23:32:14Z",
              "updatedAt": "2023-04-21T23:32:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPKK0",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-21T23:35:06Z",
          "updatedAt": "2023-04-21T23:35:06Z",
          "comments": [
            {
              "originalPosition": 782,
              "body": "I think I'd like to leave this as-is -- the obvious fix that comes to mind would be to break leader/helper initialization into three sections: \"leader initialization up to sending the request\", \"helper initialization\", \"leader initialization after receiving the response\". I think this would be less understandable than the current forward-reference.",
              "createdAt": "2023-04-21T23:35:06Z",
              "updatedAt": "2023-04-21T23:36:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TPNK3",
          "commit": {
            "abbreviatedOid": "1f67ee1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-04-22T00:13:01Z",
          "updatedAt": "2023-04-22T00:13:02Z",
          "comments": [
            {
              "originalPosition": 214,
              "body": "Done. I like dropping `ReportPrepInit`. Please review closely that everything is updated appropriately for the new messages -- there's probably at least a few places I didn't explain halding the newly-possible unexpected prepare step states explicitly enough.",
              "createdAt": "2023-04-22T00:13:02Z",
              "updatedAt": "2023-04-22T00:13:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5T6uqJ",
          "commit": {
            "abbreviatedOid": "466cea1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-01T18:06:02Z",
          "updatedAt": "2023-06-02T20:31:37Z",
          "comments": [
            {
              "originalPosition": 118,
              "body": "```suggestion\r\n: An endpoint which receives report shares. Each Aggregator works with its\r\n  co-Aggregator to compute the aggregate result. Any given measurement task\r\n```",
              "createdAt": "2023-05-01T18:06:02Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 257,
              "body": "```suggestion\r\n* `leader_aggregator_endpoint`: A URL relative to which the Leader's API\r\n```",
              "createdAt": "2023-05-01T18:08:34Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 346,
              "body": "nit: Less redundant\r\n```suggestion\r\n* `leader_encrypted_input_share` is the Leader's encrypted input share.\r\n\r\n* `helper_encrypted_input_share` is the Helper's encrypted input share.\r\n```",
              "createdAt": "2023-05-01T18:09:59Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 507,
              "body": "```suggestion\r\n      opaque prep_share<0..2^32-1>;\r\n```",
              "createdAt": "2023-05-01T19:56:34Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 511,
              "body": "The comments here aren't super useful, given that we've defined `prep_msg` and `prep_share` above. However, it *would* be helpful to comment to say what round each field pertains to.",
              "createdAt": "2023-05-01T19:57:12Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 30,
              "body": "There is one Leader\r\n```suggestion\r\n: The Aggregator that coordinates aggregation and collection with the Helper.\r\n```",
              "createdAt": "2023-06-02T20:17:33Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 262,
              "body": "FWIW I wish this were called \"collector_hkpe_config\" (\"collector_config\" is a bit vague)",
              "createdAt": "2023-06-02T20:19:50Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 443,
              "body": "Suggestion: Now that this explained in {{!VDDAF}} I think we skip this here.",
              "createdAt": "2023-06-02T20:21:27Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            },
            {
              "originalPosition": 460,
              "body": "Nice :)",
              "createdAt": "2023-06-02T20:21:57Z",
              "updatedAt": "2023-06-02T20:31:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5W7yH5",
          "commit": {
            "abbreviatedOid": "146703b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-02T21:52:25Z",
          "updatedAt": "2023-06-02T21:52:26Z",
          "comments": [
            {
              "originalPosition": 507,
              "body": "Hmm, I kind of like calling out that this will always be the leader's preparation share; thoughts on keeping this as-is / maybe there's a great reason to rename that I'm overlooking?",
              "createdAt": "2023-06-02T21:52:25Z",
              "updatedAt": "2023-06-02T21:52:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5W7x_v",
          "commit": {
            "abbreviatedOid": "e46e011"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-02T21:51:35Z",
          "updatedAt": "2023-06-02T21:52:58Z",
          "comments": [
            {
              "originalPosition": 492,
              "body": "Let's add a TODO here describing the current state of affairs: We'll need to cut a draft of VDAF with the new API and update the references here.",
              "createdAt": "2023-06-02T21:51:35Z",
              "updatedAt": "2023-06-02T21:52:58Z"
            },
            {
              "originalPosition": 507,
              "body": "Actually, feel free to disrgard. I think we've moved beyond where this suggestion is helpful :)",
              "createdAt": "2023-06-02T21:52:53Z",
              "updatedAt": "2023-06-02T21:52:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5W7y_N",
          "commit": {
            "abbreviatedOid": "466cea1"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-02T21:58:01Z",
          "updatedAt": "2023-06-02T21:58:02Z",
          "comments": [
            {
              "originalPosition": 262,
              "body": "I don't remember why I made this change, and no disagreement here that `collector_hpke_config` is easier to understand. I switched back.",
              "createdAt": "2023-06-02T21:58:02Z",
              "updatedAt": "2023-06-02T21:58:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5W7yb1",
          "commit": {
            "abbreviatedOid": "3829175"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-02T21:54:27Z",
          "updatedAt": "2023-06-02T22:46:01Z",
          "comments": [
            {
              "originalPosition": 599,
              "body": "nit: I think we still need to clarify which variant of `PrepareStep` to use here. Also, this includes the report share as well, right?",
              "createdAt": "2023-06-02T21:54:27Z",
              "updatedAt": "2023-06-02T22:46:01Z"
            },
            {
              "originalPosition": 679,
              "body": "```suggestion\r\n{{ping-pong-translation}}. The Helper computes:\r\n```",
              "createdAt": "2023-06-02T21:56:25Z",
              "updatedAt": "2023-06-02T22:46:01Z"
            },
            {
              "originalPosition": 262,
              "body": "Actually I think you were right to change this, as it is `collector_config` above as well.",
              "createdAt": "2023-06-02T21:59:41Z",
              "updatedAt": "2023-06-02T22:46:01Z"
            },
            {
              "originalPosition": 496,
              "body": "For consistency with the VDAF draft (here and below)\r\n```suggestion\r\n                             True,\r\n```",
              "createdAt": "2023-06-02T22:02:29Z",
              "updatedAt": "2023-06-02T22:46:01Z"
            },
            {
              "originalPosition": 931,
              "body": "Runon sentence. Can we split it up?",
              "createdAt": "2023-06-02T22:23:19Z",
              "updatedAt": "2023-06-02T22:46:01Z"
            },
            {
              "originalPosition": 950,
              "body": "In order to avoid conflating \"VDAF rounds\" with \"DAP rounds\" I think it would be helpful to rename `round`. How about `request_num`?",
              "createdAt": "2023-06-02T22:25:06Z",
              "updatedAt": "2023-06-02T22:46:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5W8G-G",
          "commit": {
            "abbreviatedOid": "3829175"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-02T23:37:15Z",
          "updatedAt": "2023-06-02T23:37:15Z",
          "comments": [
            {
              "originalPosition": 599,
              "body": "This is defined in the ping-pong-translation section; the text earlier in this section  specifying that any non-`PingPongInitialize` messages must be dropped ensures this translation will produce an `initialize` `PrepareStep`.",
              "createdAt": "2023-06-02T23:37:15Z",
              "updatedAt": "2023-06-02T23:37:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5XNPr5",
          "commit": {
            "abbreviatedOid": "3829175"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-05T17:53:11Z",
          "updatedAt": "2023-06-05T17:53:11Z",
          "comments": [
            {
              "originalPosition": 950,
              "body": "(I renamed \"DAP round\" to \"step\" or \"aggregation step\", as \"request number\" led to text that was somewhat more awkward/confusing, but I definitely agree with the intent of the suggestion.)",
              "createdAt": "2023-06-05T17:53:11Z",
              "updatedAt": "2023-06-05T17:53:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Xg2vd",
          "commit": {
            "abbreviatedOid": "4fe9db2"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": ">* \"Leader Continaution\" section didn't correctly handle 1-round VDAFs as\r\n>  written.\r\n\r\nCan you comment as to how? The intent was for 1-round VDAFs to terminate with the process described in the Leader/Helper Initialization sections, such that the Leader/Helper Continuation sections are not used at all. We should probably make it clearer in those sections that once a Finished state is reached no further aggregation steps are taken, but AFAICT the process was OK with that in mind.",
          "createdAt": "2023-06-07T17:28:35Z",
          "updatedAt": "2023-06-07T18:26:42Z",
          "comments": [
            {
              "originalPosition": 646,
              "body": "```suggestion\r\n  opaque payload<0..2^32-1> = outbound;\r\n```",
              "createdAt": "2023-06-07T17:28:35Z",
              "updatedAt": "2023-06-07T18:26:42Z"
            },
            {
              "originalPosition": 645,
              "body": "editorial nit: I think this `report_share = helper_report_share` notation is not \"standard\", at least I haven't seen it before & it is not used in this document. Describe the values placed in the fields in text?",
              "createdAt": "2023-06-07T17:30:14Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 746,
              "body": "This seems overly harsh; I think we should just reject such reports (though this likely indicates a scary \"impossible\" bug that an implementation might flag for investigation).",
              "createdAt": "2023-06-07T17:37:17Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 494,
              "body": "```suggestion\r\n\"finished\", and \"reject\". The initial message includes a `ReportShare` message:\r\n```",
              "createdAt": "2023-06-07T17:44:11Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 1060,
              "body": "```suggestion\r\nstate, yielding an output share for both Aggregators or a rejection. This phase\r\n```",
              "createdAt": "2023-06-07T17:50:22Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 1078,
              "body": "I think it'd be clearer to write this at the end of the `Leader Initialization` section, i.e. we only start the aggregation continuation process if we aren't already done.",
              "createdAt": "2023-06-07T17:52:00Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 1185,
              "body": "```suggestion\r\n1. Else if the inbound prep step type is \"reject\", then the Leader rejects the\r\n```",
              "createdAt": "2023-06-07T17:54:38Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 1176,
              "body": "```suggestion\r\n   where `inbound` is the payload of the prep step. If `state == Rejected()`,\r\n```",
              "createdAt": "2023-06-07T17:54:45Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 1201,
              "body": "The logic described in this section is triggered on receipt of an aggregation-continuation message from the Leader. There is no valid case where the Helper has reached `Finished` but the Leader sends another continuation message, right? If so, I think we should effectively fail out in this case.\r\n\r\n(well, this can happen if the aggregators are skewed in terms of which round they are handling, but that is not what is covered by this text -- but whatever text we land on will need to account for this.)",
              "createdAt": "2023-06-07T17:57:43Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            },
            {
              "originalPosition": 1256,
              "body": "There are a few questions to me as to whether the step tracking is still needed; I think it is, and this text is why. Without tracking aggregation progress in some way, we wouldn't be able to recognize when we're in a skew state.\r\n\r\nOf course, the DAP step is now redundant with the VDAF round, which as of the latest VDAF ping-pong iteration is tracked in the state. So, we could drop the DAP step & track retries with VDAF round. But I think this isn't a good idea for two reasons:\r\n\r\n1. The DAP round goes 1, 2, 3...; the VDAF rounds that are recorded will go 2, 4, 6... This would mean that a continuation request could be sent with an odd round; I suppose we should fail in this case, but an implementation will be happier if we arrange things such that this error condition can't exist at all.\r\n\r\n2. The VDAF round only needs to be tracked in state because `ping_pong_next` needs to use it to parse the `inbound` message. But this means that parsing the `inbound` message will require shared context (i.e. correct parsing requires the aggregators to agree on the VDAF round). Since the current round information is sent in the same `AggregationContinueReq` as the `inbound` message itself, this means that aggregator implementations will need to ensure they split parsing. If they don't, they'll see rare errors that occur only during round-skew.",
              "createdAt": "2023-06-07T18:18:22Z",
              "updatedAt": "2023-06-07T18:26:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5XiqMU",
          "commit": {
            "abbreviatedOid": "7225f1f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-07T22:54:01Z",
          "updatedAt": "2023-06-07T23:25:37Z",
          "comments": [
            {
              "originalPosition": 645,
              "body": "I'm borrowing (and potentially abusing :/) from RFC 8446, e.g.: https://www.rfc-editor.org/rfc/rfc8446#section-7.1\r\n\r\nI think something like this would be useful. It's a little clearer than writing out \"PrepareStep message of type \"initialize\" with the report share field set to `helper_report_share\".",
              "createdAt": "2023-06-07T22:54:02Z",
              "updatedAt": "2023-06-07T23:25:37Z"
            },
            {
              "originalPosition": 746,
              "body": "In my opinion this is an \"invalidMessage\" situation: We don't want to allow the code to do something else here, because it gets us into the realm of unspecified behavior. (We don't want to silently finish preparation here.)",
              "createdAt": "2023-06-07T22:55:30Z",
              "updatedAt": "2023-06-07T23:25:37Z"
            },
            {
              "originalPosition": 1078,
              "body": "Right now the control flow is something like this:\r\n\r\n```\r\nleader init\r\n- send an aggregate init request, handle response\r\n- go to leader continue\r\n\r\nleader continue\r\n- If there is nothing to do then halt. Otherwise send aggregate continue request and handle response\r\n- go to leader continue\r\n```\r\n\r\nIt sounds like your suggesting something like this?\r\n\r\n```\r\nleader init\r\n- send an aggregate init request, handle response\r\n- go to leader continue\r\n\r\nleader continue\r\n- If there is something to do, then send aggregate continue request and handle response\r\n- If there is nothing to do, then halt.\r\n- go to leader continue\r\n```\r\n\r\nThat seems fine, although maybe a bit more complicated to write in English? Up to you, please feel free to push to your own PR :D",
              "createdAt": "2023-06-07T23:02:50Z",
              "updatedAt": "2023-06-07T23:25:38Z"
            },
            {
              "originalPosition": 1201,
              "body": "Actually at this point the Helper has sent a response to `AggregateInitReq`. If the VDAF is 1-round, then the payload is the prep message and it has nothing more to do.",
              "createdAt": "2023-06-07T23:16:56Z",
              "updatedAt": "2023-06-07T23:25:38Z"
            },
            {
              "originalPosition": 1256,
              "body": "> There are a few questions to me as to whether the step tracking is still needed; I think it is, and this text is why. Without tracking aggregation progress in some way, we wouldn't be able to recognize when we're in a skew state.\r\n\r\nI agree that continuation step tracking is necessary in order to implement skew recovery -- what I'm questioning is whether we need such explicit language about it. For instance we say things like:\r\n\r\n> The Leader is now in DAP step 0.\r\n> The Leader advances to the next DAP step.\r\n> The Leader sends a message to the Helper to say that it should advance to the next DAP step.\r\n\r\nI think it would be simpler to just say that the first continuation request has `step` set to `1`, the second has `step` set to `0` and so on, and say that the Helper uses this field for step-skew recovery. In fact I don't think any of the paragraph needs to change as long as it's clear what the value of `step` is supposed to be. We might even stick this in the ladder diagram to make sure it's crystal clear.\r\n\r\n> Of course, the DAP step is now redundant with the VDAF round, which as of the latest VDAF ping-pong iteration is tracked in the state. So, we could drop the DAP step & track retries with VDAF round. But I think this isn't a good idea for two reasons:\r\n\r\nTotally agreed, let's not even bring up the VDAF round number in this section.",
              "createdAt": "2023-06-07T23:25:33Z",
              "updatedAt": "2023-06-07T23:25:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5XqjlE",
          "commit": {
            "abbreviatedOid": "d4afe7e"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-08T21:14:34Z",
          "updatedAt": "2023-06-08T23:39:11Z",
          "comments": [
            {
              "originalPosition": 625,
              "body": "```suggestion\r\nHelper. (These are coalesced into a single HTTP request to the Helper as\r\n```",
              "createdAt": "2023-06-08T21:14:34Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 746,
              "body": "OK -- I think this isn't critical. But for all but the last aggregation step, we wouldn't be in unspecified behavior: the Leader would reflect an error to the Helper as with any other error. (Errors on the last step are are unspecified behavior, before & with this PR.)",
              "createdAt": "2023-06-08T21:33:38Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 808,
              "body": "```suggestion\r\n`inbound` denote the payload of the prep step sent by the Leader):\r\n```",
              "createdAt": "2023-06-08T21:34:52Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 1078,
              "body": "Sure, I can write another commit against this PR once we're happy with this commit.\r\n\r\nI am suggesting something like:\r\n```\r\nleader init\r\n- send aggregate init request, handle response\r\n- stop if we recovered output shares, go to leader continue otherwise\r\n\r\nleader continue\r\n- send aggregate continue request, handle response\r\n- stop if we recovered output shares, go to leader continue otherwise\r\n\r\nhelper init\r\n- receive aggregate init request, handle response (including storing state, or output shares if we recovered them)\r\n\r\nhelper continue\r\n- receive aggregate continue request, handle response (including storing state, or output shares if we recovered them)\r\n```\r\n\r\nFor the record, my reasons for wanting to write it up this up this way are:\r\n* this describes all of the logic that an implementation will need to process an aggregation step (initialization or continuation) in a single section -- i.e. IMO, deciding whether to keep going after a given step is more naturally a part of that step, rather than the following step. (though of course, an implementation could make either choice & still interoperate) Or to put it another way, I think part of \"handling the response\" is deciding whether there is another step to be taken.\r\n* this matches the preexisting structure of the text.\r\n\r\nThis reasoning is somewhat marginal/opinionated for the Leader sections, but IMO is stronger for the Helper section: currently, the Helper Continuation section contains the only text telling the Helper when to stop. But before this PR, that section was effectively \"the logic to run to handle an `AggregateContinueReq` from the Leader\" -- now, spec readers will have to realize they should implement the first part of the continuation logic at the end of their previous step (since if they don't, no further requests will be arriving from the Leader to trigger that logic, so it will never be run), but the rest of the logic is triggered by an incoming aggregate continue request.",
              "createdAt": "2023-06-08T21:59:01Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 1123,
              "body": "```suggestion\r\nNext, the Leader sends a POST request to the aggregation job URI used during\r\n```",
              "createdAt": "2023-06-08T22:22:59Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 1191,
              "body": "```suggestion\r\nan additional round of continuation is required and the Helper proceeds as\r\n```",
              "createdAt": "2023-06-08T22:34:47Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 1256,
              "body": "Yeah, I think we can simplify the step language -- at least, we certainly don't need to define the Leader's step until we're about to send the first aggregate continue request, so the Leader Initialization text can be dropped.\r\n\r\nI'll survey the description of step handling & simplify in the commit I'm going to push after this one.",
              "createdAt": "2023-06-08T22:42:44Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            },
            {
              "originalPosition": 1201,
              "body": "(continuing this in the other thread about where to write the text about aggregators noticing they are done & stopping)",
              "createdAt": "2023-06-08T23:13:08Z",
              "updatedAt": "2023-06-08T23:39:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Xyv91",
          "commit": {
            "abbreviatedOid": "4fe9db2"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-09T19:04:22Z",
          "updatedAt": "2023-06-09T19:04:23Z",
          "comments": [
            {
              "originalPosition": 1078,
              "body": "I wrote this up, PTAL.",
              "createdAt": "2023-06-09T19:04:22Z",
              "updatedAt": "2023-06-09T19:04:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Xzgae",
          "commit": {
            "abbreviatedOid": "aa84c12"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "In my opinion the new control flow is strictly worse, at least for the Leader. The reason is we end up with repetitive text, like: \r\n\r\n> 1. [...] where `inbound` is the payload of the prep step. If `state == Rejected()`,\r\n>   then the Leader rejects the report and removes it from the candidate set. If\r\n>   `state == Continued(prep_state)`, then another continuation step is required;\r\n>   the Leader stores `state` and `outbound` for the next continuation step. If\r\n>   `state == Finished(out_share)`, then preparation is complete and the Leader\r\n>   stores the output share for use in the collection flow ({{collect-flow}}).\r\n>\r\n> 2. Else if the type is \"finished\" and `state == Finished(out_share)`, then\r\n>  preparation is complete and the Leader stores the output share for use in\r\n>   the collection flow ({{collect-flow}}).\r\n\r\nWith the previous reason, we have just one \"goto\" to {{collect-flow}}, at the top of the continuation section. (Similar for the Helper.) The less pointers we have the better.\r\n\r\nAlso note that there is a bug here I think: see inline comments.\r\n\r\n> For the record, my reasons for wanting to write it up this up this way are:\r\n> * this describes all of the logic that an implementation will need to process an aggregation step (initialization or continuation) in a single section -- i.e. IMO, deciding whether to keep going after a given step is more naturally a part of that step, rather than the following step. (though of course, an implementation could make either choice & still interoperate) Or to put it another way, I think part of \"handling the response\" is deciding whether there is another step to be taken.\r\n\r\nThat sounds reasonable, but let me add one more consideration: Simpler control flow makes the spec easier to maintain. It seems to me that we end up with a few more if-else statements with your branch, some of which have the same endpoint (think return statement). Better to have fewer branches if we can so that we don't have a lot of redundant text.\r\n\r\nUltimately I think this is a matter of taste, so I'm fine either way I think. \r\n\r\n> * this matches the preexisting structure of the text.\r\n\r\nAt this stage I think we're free to pick whatever control flow makes the most sense.\r\n\r\n> This reasoning is somewhat marginal/opinionated for the Leader sections, but IMO is stronger for the Helper section: currently, the Helper Continuation section contains the only text telling the Helper when to stop. But before this PR, that section was effectively \"the logic to run to handle an `AggregateContinueReq` from the Leader\" -- now, spec readers will have to realize they should implement the first part of the continuation logic at the end of their previous step (since if they don't, no further requests will be arriving from the Leader to trigger that logic, so it will never be run), but the rest of the logic is triggered by an incoming aggregate continue request.\r\n\r\nI see your point, and I'd just add that it would be the more \"conventional\" approach. But I think it's reasonable to make the reader jump around a bit if it makes the text simpler. One thing to note is that we're dealing with what is effectively a protocol with an arbitrary number of rounds, which is really unusual for specs of this nature.",
          "createdAt": "2023-06-09T22:18:08Z",
          "updatedAt": "2023-06-09T22:38:25Z",
          "comments": [
            {
              "originalPosition": 888,
              "body": "* Let's parse out the `out_share`, since {{collect-flow}} knows nothing about `state`.\r\n* Missing closed paran\r\n* collect flow -> sub-protocol\r\n* let's try to be as precise as we can, i.e., instead of \"aggregation step\", which is a bit ill-defined, let's say \"continuation step\", which we use elsewhere.\r\n```suggestion\r\nFinally, if `state == Continued(prep_state)` then the Helper stores `state` for the next continuation step\r\n({{aggregation-helper-continuation}}). Otherwise, if `state == Finished(out_share)`, then the Helper stores `out_share` for use in the collection sub-protocol ({{collect-flow}}).\r\n```",
              "createdAt": "2023-06-09T22:18:08Z",
              "updatedAt": "2023-06-09T22:38:25Z"
            },
            {
              "originalPosition": 730,
              "body": "I believe this is incorrect: If `state == Finished(out_share)` **but** `outbound != None`, then we need to send one more continuation request.",
              "createdAt": "2023-06-09T22:22:34Z",
              "updatedAt": "2023-06-09T22:38:25Z"
            },
            {
              "originalPosition": 1164,
              "body": "This is also incorrect I'm pretty sure: if `outbound != None` but `state == Finished(out_share)` then we need to be sure to send the last outbound message.",
              "createdAt": "2023-06-09T22:24:11Z",
              "updatedAt": "2023-06-09T22:38:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5X99E9",
          "commit": {
            "abbreviatedOid": "80e4dca"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-12T21:34:59Z",
          "updatedAt": "2023-06-12T22:05:47Z",
          "comments": [
            {
              "originalPosition": 888,
              "body": "Done, though \"flow\" is used in many places -- I updated a few that are touched by this PR, but I didn't attempt to unify usage of \"flow\" vs \"sub-protocol\" throughout the spec.",
              "createdAt": "2023-06-12T21:34:59Z",
              "updatedAt": "2023-06-12T22:05:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5X-rjf",
          "commit": {
            "abbreviatedOid": "80e4dca"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Just a couple of easy fixes left from me.",
          "createdAt": "2023-06-13T01:05:46Z",
          "updatedAt": "2023-06-13T01:18:17Z",
          "comments": [
            {
              "originalPosition": 1155,
              "body": "I think the condition here should be `inbound is \"continue\" && state == Continued(prep_state)`. If the Leader is instate `Finished`, then inbound prep step type being `\"continue\"` is a Helper-side bug and we should abort.",
              "createdAt": "2023-06-13T01:08:24Z",
              "updatedAt": "2023-06-13T01:18:17Z"
            },
            {
              "originalPosition": 1076,
              "body": "If the VDAF is 2-round, then `state == Finished(out_share)`. I'd suggest just not asserting the type of `state` here.\r\n\r\n```suggestion\r\n`state` and an outbound message\r\n```",
              "createdAt": "2023-06-13T01:16:38Z",
              "updatedAt": "2023-06-13T01:18:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YEzYH",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM. Before we merge, can you squash the stack and update the commit message?",
          "createdAt": "2023-06-13T16:47:54Z",
          "updatedAt": "2023-06-13T18:09:38Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YFVAU",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "First review pass with some wording and message definition gripes. I'll do another this afternoon where I read this and https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240 from the implementer's perspective and may have more substantial structural feedback.",
          "createdAt": "2023-06-13T18:11:09Z",
          "updatedAt": "2023-06-13T20:33:39Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "This paragraph doesn't define what `p` is, although that's not a problem introduced by this change. Should we separately fix this?",
              "createdAt": "2023-06-13T18:11:09Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 494,
              "body": "I think it'd be more clear to move the definition of `struct PrepareStep` above this discussion of its four variants.",
              "createdAt": "2023-06-13T18:18:57Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 520,
              "body": "```suggestion\r\nincludes a `ReportShareError`:\r\n```",
              "createdAt": "2023-06-13T18:19:30Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 630,
              "body": "I think we'll want to make a follow-up change to tidy up references: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/470",
              "createdAt": "2023-06-13T18:25:02Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 573,
              "body": "This phrasing is awkward. Let's try:\r\n```suggestion\r\n1. Determine which input report shares are valid and recover output shares from them.\r\n```\r\n\r\nOr we can punt that to a separate PR since this text is not new to this PR.",
              "createdAt": "2023-06-13T18:33:09Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 641,
              "body": "I'm not sure about this notation. Per [RFC 8446](https://datatracker.ietf.org/doc/html/rfc8446#section-3.7), this would mean that `struct PrepareStep` may only be constructed with `prepare_step_state = 0`, which is not our intent. I don't see anything in RFC 8446 that lays out a syntax for literal values. Maybe there's something in another TLS WG document that could help?",
              "createdAt": "2023-06-13T18:41:18Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 641,
              "body": "The Python definitions in https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240 have to use integer enum values with comments (I guess?) but since this is in RFC 8446 presentation language, we can do \r\n```suggestion\r\n  PrepareStepState prepare_step_state = initialize;\r\n```\r\nper [3.5 enumerateds](https://datatracker.ietf.org/doc/html/rfc8446#section-3.5):\r\n\r\n```\r\n   The names of the elements of an enumeration are scoped within the\r\n   defined type.  In the first example, a fully qualified reference to\r\n   the second element of the enumeration would be Color.blue.  Such\r\n   qualification is not required if the target of the assignment is well\r\n   specified.\r\n\r\n      Color color = Color.blue;     /* overspecified, legal */\r\n      Color color = blue;           /* correct, type implicit */\r\n```",
              "createdAt": "2023-06-13T18:43:25Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 742,
              "body": "This is structured awkwardly, because it reads as \"[stuff about `report_too_early`] then [stuff about the general case] then [stuff about `report_too_early` again]\".\r\n```suggestion\r\n   removes it from the candidate set. The Leader MUST NOT include the report\r\n   in a subsequent aggregation job, nless the error is `report_too_early`, in\r\n   which case the Leader MAY include the report in a subsequent aggregation\r\n   job.\r\n```",
              "createdAt": "2023-06-13T19:58:09Z",
              "updatedAt": "2023-06-14T17:43:58Z"
            },
            {
              "originalPosition": 773,
              "body": "Technically it's the `report_share` inside the `PrepareStep` that has a report ID. I'm not sure if that's worth clarifying.",
              "createdAt": "2023-06-13T20:00:48Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 777,
              "body": "If it's illegal for the `prepare_steps` in `AggregationJobInitReq` to ever be anything but `PrepareStep::initialize`, why bother with the enum at all? We could instead have something like `struct PrepareStepInit { ReportShare report_share; opaque payload<0..2^32-1>; }` and remove one possible source of bugs in implementations.",
              "createdAt": "2023-06-13T20:03:42Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 1175,
              "body": "Do we also need the exception here for `report_too_early` to match that in `{{leader-init}}`, or do we claim that this is impossible because any too early reports would have been rejected previously?",
              "createdAt": "2023-06-13T20:11:24Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            },
            {
              "originalPosition": 1213,
              "body": "Similarly to my comment on the init case: we could define `AggregationJobContinueReq` to contain a sequence of `struct PrepareStepContinue { ReportId report_id; opaque payload<0..2^32-1>; }` and make this easier to implement.",
              "createdAt": "2023-06-13T20:14:16Z",
              "updatedAt": "2023-06-13T20:33:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YGQqR",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I have some more nits and suggestions for message changes but I think this is mechanically sound and its with the corresponding VDAF change.",
          "createdAt": "2023-06-13T20:52:36Z",
          "updatedAt": "2023-06-13T21:37:14Z",
          "comments": [
            {
              "originalPosition": 483,
              "body": "```suggestion\r\n  to each report share in the aggregation job.\r\n```",
              "createdAt": "2023-06-13T20:52:36Z",
              "updatedAt": "2023-06-13T21:37:14Z"
            },
            {
              "originalPosition": 573,
              "body": "On second reading, I see that it's incorrect to discuss recovering _output shares_ in this context. I'm still shaky on this notion of \"recovering\" report shares (did they get lost?) but that's not what this PR is about, so I'll resolve this thread so we can focus.",
              "createdAt": "2023-06-13T20:57:06Z",
              "updatedAt": "2023-06-13T21:37:15Z"
            },
            {
              "originalPosition": 586,
              "body": "This requirement that the aggregation job ID be unique in the scope of the task seems correct, but doesn't quite align with what is said at the top of `{{agg-init}}`, which just says \"unique\", and I think that can only be read as \"globally unique\".\r\n\r\nAnyway, the requirements for aggregation job ID should only be stated once, either here or at the top of `{{agg-init}}`.",
              "createdAt": "2023-06-13T21:04:51Z",
              "updatedAt": "2023-06-13T21:37:14Z"
            },
            {
              "originalPosition": 619,
              "body": "nit: I think we should clarify that the report ID is used as the nonce.",
              "createdAt": "2023-06-13T21:07:03Z",
              "updatedAt": "2023-06-13T21:37:14Z"
            },
            {
              "originalPosition": 1178,
              "body": "If we take my other suggestion to change the definition of `AggregationJobInitReq` (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393/files#r1228639257), then I think we could change the definition of `struct PrepareStep` such that invalid types are impossible, and then this is one more check implementations don't have to worry about.\r\n\r\nOrthogonally to that suggestion, I think this text could be moved into `{{aggregation-job-validation}}`, just to avoid repeating it here and in the aggregation init section.",
              "createdAt": "2023-06-13T21:23:20Z",
              "updatedAt": "2023-06-13T21:37:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNWrX",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T16:38:55Z",
          "updatedAt": "2023-06-14T17:33:36Z",
          "comments": [
            {
              "originalPosition": 494,
              "body": "I agree, though this puts the definition of `PrepareStep` before the definition of some of the messages included in `PrepareStep`. I think that's OK (they're still very close, and contextually this section is introducing the message & its contents) but I can make further changes if anyone disagrees.",
              "createdAt": "2023-06-14T16:38:55Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 586,
              "body": "We definitely want \"unique in the scope of the task.\" I removed the requirement here.",
              "createdAt": "2023-06-14T16:41:44Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 641,
              "body": "I agree. @cjpatton pushed back on a change here before, as this notation is more compact than an English-language description of the field contents; Chris, is there an RFC or other doc that would support this notation?\r\n\r\nMy opinion is that this notation is both more compact & less ambiguous than alternative methods of specifying the struct values; I like it even if other RFCs have not had need of this notation. But I also see the value of strictly sticking to preexisting notation, so ultimately I am happy to let the editors decide here.",
              "createdAt": "2023-06-14T16:49:10Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 641,
              "body": "(waiting to fix until the other notational comment is resolved.)",
              "createdAt": "2023-06-14T16:49:32Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 773,
              "body": "IMO, no. We consider the `PrepareStep` to include a report ID throughout the document, even though the particular location of the report ID will vary depending on the prepare step state. We could be more specific here because the state will always be `initialize` but I don't think it makes the document clearer.",
              "createdAt": "2023-06-14T16:53:38Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 1175,
              "body": "I think it's the latter. (An implementation could guard against internal clock skew by adding the check here too, I suppose, but I think that's best left to implementations.)",
              "createdAt": "2023-06-14T17:05:51Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 777,
              "body": "I agree--in fact, splitting things into separate messages for initialization/continuation was the initial approach taken in this PR--but we decided to merge in order to be able to use the same types for initialization & continuation and to minimize the size of the PR. (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393#discussion_r1117814328)\r\n\r\nI'd prefer something like you suggest (that is, step-specific message types), but I would like consensus before making a change. @cjpatton?",
              "createdAt": "2023-06-14T17:15:25Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 1178,
              "body": "(waiting to resolve the step-specific-message issue before addressing the first part of this comment)\r\n\r\nI disagree with the second part -- if anything, I'd say we should \"inline\" the content of `{{aggregation-job-validation}]` and then eliminate that section entirely. The reasoning for this is that documents should be optimized for the reader over the writer/maintainer (IMO), readers have a much easier time comprehending a document that is readable in a linear fashion, and this particular section contains a single small validation check. Repetition is IMO justified in this case, as it leads to a more understandable document with very little cost to maintainability.\r\n\r\nAlso, note that the `Leader Initialization` & `Leader Continuation` next-step logic is slightly different, i.e. in handling of a `finished` incoming message. We could call out the special case in the section we factored this out to, but if we have to have a special case for every place we are referring to the section, IMO that makes the proposed factoring less valuable.",
              "createdAt": "2023-06-14T17:22:04Z",
              "updatedAt": "2023-06-14T17:33:36Z"
            },
            {
              "originalPosition": 1213,
              "body": "(waiting to resolve the step-specific-message issue before addressing this comment)",
              "createdAt": "2023-06-14T17:25:03Z",
              "updatedAt": "2023-06-14T17:34:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNvn6",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T17:43:00Z",
          "updatedAt": "2023-06-14T17:43:00Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "I like this notation, too, but since we refer to RFC 8446 section 3, I think we would need to explain that this document's usage of that presentation language differs in this one respect. I think the paragraph just before `{{overview}}` where we introduce the presentation language:\r\n\r\n>This document uses the presentation language of {{!RFC8446}} to define messages\r\n>in the DAP protocol. Encoding and decoding of these messages as byte strings\r\n>also follows {{RFC8446}}.\r\n\r\nwould be a good place to add some text explaining our object literal notation.",
              "createdAt": "2023-06-14T17:43:00Z",
              "updatedAt": "2023-06-14T17:43:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNxIe",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T17:47:14Z",
          "updatedAt": "2023-06-14T17:47:15Z",
          "comments": [
            {
              "originalPosition": 586,
              "body": "Bump, the inconsistent uniqueness requirements are still there at dcced627426a8db038b99c1c9918fe97de93299e",
              "createdAt": "2023-06-14T17:47:15Z",
              "updatedAt": "2023-06-14T17:47:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNxgK",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T17:48:15Z",
          "updatedAt": "2023-06-14T17:48:15Z",
          "comments": [
            {
              "originalPosition": 619,
              "body": "Bump, still no mention of the VDAF nonce in this section at dcced627426a8db038b99c1c9918fe97de93299e",
              "createdAt": "2023-06-14T17:48:15Z",
              "updatedAt": "2023-06-14T17:48:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNyJy",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T17:50:07Z",
          "updatedAt": "2023-06-14T17:50:08Z",
          "comments": [
            {
              "originalPosition": 1178,
              "body": "Given that there's only a single item in `{{aggregation-job-validation}}`, I'm inclined to agree that we could inline that section. I think we could punt that to a later change since (IIRC) this PR does not modify `{{aggregation-job-validation}}`.",
              "createdAt": "2023-06-14T17:50:07Z",
              "updatedAt": "2023-06-14T17:50:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN2sb",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:01:35Z",
          "updatedAt": "2023-06-14T18:01:36Z",
          "comments": [
            {
              "originalPosition": 586,
              "body": "Looking to https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/commit/dcced627426a8db038b99c1c9918fe97de93299e, `agg-init` now says \"The Leader begins an aggregation job by choosing a set of candidate reports that\r\npertain to the same DAP task and a job ID which MUST be unique within the scope\r\nof the task.\" while `leader-init` now only says \"Generate a fresh AggregationJobID.\"",
              "createdAt": "2023-06-14T18:01:35Z",
              "updatedAt": "2023-06-14T18:01:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN28x",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:02:17Z",
          "updatedAt": "2023-06-14T18:02:18Z",
          "comments": [
            {
              "originalPosition": 619,
              "body": "Looking to https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/commit/dcced627426a8db038b99c1c9918fe97de93299e, the relevant text now says: \"`report_id` is the report ID, used as a nonce\"",
              "createdAt": "2023-06-14T18:02:17Z",
              "updatedAt": "2023-06-14T18:02:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN5OH",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:08:52Z",
          "updatedAt": "2023-06-14T18:08:52Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "This notation is borrowed from [Section 7.2](https://datatracker.ietf.org/doc/html/rfc8446#section-7.1). I don't believe it's formalized anywhere.\r\n\r\nMeta comment here: Let's not let perfection be the enemy of the good. We have a lot of work to do on the spec, and a this stage the most important thing is that it's unambiguous. If adding some text to clarify this notation would help, then go for it.",
              "createdAt": "2023-06-14T18:08:52Z",
              "updatedAt": "2023-06-14T18:08:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN6ye",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:12:58Z",
          "updatedAt": "2023-06-14T18:12:58Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "Both are valid I think. I'm fine with this change, but I'd err on the side of being consistent with the VDAF spec. There I'm not sure we want to change this since the pseudocode is Python and this would look slightly less Pythonic.",
              "createdAt": "2023-06-14T18:12:58Z",
              "updatedAt": "2023-06-14T18:12:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN7fl",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:15:03Z",
          "updatedAt": "2023-06-14T18:15:03Z",
          "comments": [
            {
              "originalPosition": 777,
              "body": "I'm fine with this change.",
              "createdAt": "2023-06-14T18:15:03Z",
              "updatedAt": "2023-06-14T18:15:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN8S0",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:16:28Z",
          "updatedAt": "2023-06-14T18:16:28Z",
          "comments": [
            {
              "originalPosition": 1175,
              "body": "Good catch. In the absence of a MUST or MUST NOT stating otherwise, `report_too_early` is valid here. I would copy the language here.",
              "createdAt": "2023-06-14T18:16:28Z",
              "updatedAt": "2023-06-14T18:16:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN87_",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:18:19Z",
          "updatedAt": "2023-06-14T18:18:19Z",
          "comments": [
            {
              "originalPosition": 1178,
              "body": "This text is still needed: There is the case that the Leader is in state `Finished` but the inbound message is `continue`.",
              "createdAt": "2023-06-14T18:18:19Z",
              "updatedAt": "2023-06-14T18:18:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN9ZI",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:19:30Z",
          "updatedAt": "2023-06-14T18:19:30Z",
          "comments": [
            {
              "originalPosition": 1213,
              "body": "I believe that's incorrect. There are three possible variants: \"continue\", \"finished\", and \"reject\". \"finished\" and \"reject\" would be invalid at this point.",
              "createdAt": "2023-06-14T18:19:30Z",
              "updatedAt": "2023-06-14T18:20:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN9q7",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:20:22Z",
          "updatedAt": "2023-06-14T18:20:23Z",
          "comments": [
            {
              "originalPosition": 586,
              "body": "Oops, never mind, I was holding it wrong.",
              "createdAt": "2023-06-14T18:20:22Z",
              "updatedAt": "2023-06-14T18:20:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN9tc",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:20:30Z",
          "updatedAt": "2023-06-14T18:20:30Z",
          "comments": [
            {
              "originalPosition": 619,
              "body": "Oops, never mind, I was holding it wrong.",
              "createdAt": "2023-06-14T18:20:30Z",
              "updatedAt": "2023-06-14T18:20:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN-Cm",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:21:31Z",
          "updatedAt": "2023-06-14T18:21:31Z",
          "comments": [
            {
              "originalPosition": 777,
              "body": "Note that there are still more edge cases like this that we can't eliminate as easily. In general, validating the prep step type is necessary.",
              "createdAt": "2023-06-14T18:21:31Z",
              "updatedAt": "2023-06-14T18:21:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YN-j6",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:23:01Z",
          "updatedAt": "2023-06-14T18:23:03Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "I agree with Chris that this notation is good and we should use it. I scanned a handful of other documents in the TLS WG looking for prior art on this and couldn't find anything. I think our best path forward will be to add a paragraph or two to DAP explaining this notation. I wrote up https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/472 for us to clarify this notation.",
              "createdAt": "2023-06-14T18:23:02Z",
              "updatedAt": "2023-06-14T18:23:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YOAiJ",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:26:43Z",
          "updatedAt": "2023-06-14T18:26:43Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "You're going to quite correctly accuse me of being a pedant, but this particular snippet is not a Python description of calling a VDAF method but rather TLS presentation language explaining how to construct a wire message. If we look at [an example from RFC 8446](https://datatracker.ietf.org/doc/html/rfc8446#appendix-B.1):\r\n\r\n```\r\nenum {\r\n          invalid(0),\r\n          change_cipher_spec(20),\r\n          alert(21),\r\n          handshake(22),\r\n          application_data(23),\r\n          heartbeat(24),  /* [RFC 6520](https://datatracker.ietf.org/doc/html/rfc6520) */\r\n          (255)\r\n      } ContentType;\r\n\r\n      <snip>\r\n\r\n      struct {\r\n          ContentType opaque_type = application_data; /* 23 */\r\n          ProtocolVersion legacy_record_version = 0x0303; /* TLS v1.2 */\r\n          uint16 length;\r\n          opaque encrypted_record[TLSCiphertext.length];\r\n      } TLSCiphertext;\r\n```",
              "createdAt": "2023-06-14T18:26:43Z",
              "updatedAt": "2023-06-14T18:26:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YOCHy",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:31:00Z",
          "updatedAt": "2023-06-14T18:31:00Z",
          "comments": [
            {
              "originalPosition": 1178,
              "body": "Filed https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/473 so we don't have to do that change here.",
              "createdAt": "2023-06-14T18:31:00Z",
              "updatedAt": "2023-06-14T18:31:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YOCxR",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T18:32:51Z",
          "updatedAt": "2023-06-14T18:32:51Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "We'll have to revisit this in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/472 anyway, so @branlwyd I think you should make the call on whether you want to update these in this PR. It's perfectly readable either way.",
              "createdAt": "2023-06-14T18:32:51Z",
              "updatedAt": "2023-06-14T18:32:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YOEFL",
          "commit": {
            "abbreviatedOid": "dcced62"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Everything has either been resolved or punted to future issues except that we want to re-work the message definitions to eliminate the cases `PrepareStepState.initialize` and `PrepareStepState.continue`. Then this is good to go.",
          "createdAt": "2023-06-14T18:36:46Z",
          "updatedAt": "2023-06-14T18:36:46Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YPGsJ",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T21:45:52Z",
          "updatedAt": "2023-06-14T21:45:53Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "I like specifying the name, I don't think it's any less clear (and implementors will probably be writing a name rather than a number) & it's supported by existing notation.",
              "createdAt": "2023-06-14T21:45:52Z",
              "updatedAt": "2023-06-14T21:45:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPH0M",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T21:50:36Z",
          "updatedAt": "2023-06-14T21:50:37Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "@tgeoghegan I'm not sure what you mean? In VDAF we currently write:\r\n```python\r\n\ufeff\ufeff\ufeff    elif inbound.type == 1: # continue\r\n```\r\nIf we instead wrote something like\r\n```python\r\n    elif inbound.type == continue:\r\n```\r\nthen we'd need to define `continue` as a constant somewhere. Which, you know, we could do.",
              "createdAt": "2023-06-14T21:50:36Z",
              "updatedAt": "2023-06-14T21:50:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPMgb",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T22:04:59Z",
          "updatedAt": "2023-06-14T22:05:00Z",
          "comments": [
            {
              "originalPosition": 641,
              "body": "The snippet that we're discussing in this thread is:\r\n\r\n```\r\n~~~\r\nstruct {\r\n  PrepareStepState prepare_step_state = 0; /* initialize */\r\n  ReportShare report_share = helper_report_share;\r\n  opaque payload<0..2^32-1> = outbound;\r\n} PrepareStep;\r\n~~~\r\n```\r\n\r\nThat's not Python, it's RFC 8446 presentation language, so Python syntax is irrelevant here.",
              "createdAt": "2023-06-14T22:04:59Z",
              "updatedAt": "2023-06-14T22:05:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPVQN",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T22:46:01Z",
          "updatedAt": "2023-06-14T22:46:01Z",
          "comments": [
            {
              "originalPosition": 777,
              "body": "I introduced `PrepareInit` and `PrepareContinue` as messages that the Leader sends to the helper for initialization & continuation steps.\r\n\r\nIMO, `PrepareStep`'s name is now iffy -- perhaps it should be something like `PrepareResponse`? -- but I think we can rename in a later PR, if we decide we want to.",
              "createdAt": "2023-06-14T22:46:01Z",
              "updatedAt": "2023-06-14T22:46:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPVVB",
          "commit": {
            "abbreviatedOid": "4d111ce"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T22:46:27Z",
          "updatedAt": "2023-06-14T22:46:27Z",
          "comments": [
            {
              "originalPosition": 1213,
              "body": "I introduced `PrepareInit` & `PrepareContinue` as specializations of the Leader's messages.",
              "createdAt": "2023-06-14T22:46:27Z",
              "updatedAt": "2023-06-14T22:46:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPVf8",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-06-14T22:47:32Z",
          "updatedAt": "2023-06-14T22:51:48Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "This should say \"report share\" instead of \"input share\". The latter is the thing consumed by VDAF; the former is the thing consumed by DAP. We need to make this distinction clear, somehow, and use terminology consistently. (Here and below.)",
              "createdAt": "2023-06-14T22:47:32Z",
              "updatedAt": "2023-06-14T22:51:49Z"
            },
            {
              "originalPosition": 43,
              "body": "We still need \"finished\":\r\n```\r\ncase finished: Empty;\r\n```",
              "createdAt": "2023-06-14T22:48:22Z",
              "updatedAt": "2023-06-14T22:51:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPXIK",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T22:55:49Z",
          "updatedAt": "2023-06-14T22:55:50Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Actually it looks like there are multiple places where this regression happened.",
              "createdAt": "2023-06-14T22:55:49Z",
              "updatedAt": "2023-06-14T22:55:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPXfk",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T22:57:58Z",
          "updatedAt": "2023-06-14T22:57:59Z",
          "comments": [
            {
              "originalPosition": 668,
              "body": "I don't really think we need to clarify this, as this is already clear in the upload section. But if we do want to re-iterate, then let's make sure we're clear about which parameter we mean.\r\n```suggestion\r\n* `report_id` is the report ID, used as the nonce for VDAF sharding\r\n```",
              "createdAt": "2023-06-14T22:57:58Z",
              "updatedAt": "2023-06-14T22:57:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPX6t",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:00:18Z",
          "updatedAt": "2023-06-14T23:00:18Z",
          "comments": [
            {
              "originalPosition": 688,
              "body": "I think we should continue to define the `ReportShare` struct, even if we don't need to. This way it's clear what we mean by \"report share\".\r\n```suggestion\r\n  ReportShare report_share;\r\n```",
              "createdAt": "2023-06-14T23:00:18Z",
              "updatedAt": "2023-06-14T23:00:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPYWt",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:02:40Z",
          "updatedAt": "2023-06-14T23:02:40Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I think that's incorrect: this section has no concept of report shares (which were not formally defined until the aggregation sections, but were sometimes mentioned in previous sections), and indeed now report shares are gone entirely (as the `ReportShare` message was merged into the new `PrepareInit` message, since that was the only usage of the `ReportShare` message). Specifically, the message that would contain the extensions referred to in this text is called `PlaintextInputShare`.\r\n\r\nIMO, if we want to change terminology, we should do so in a later PR that updates all usages of \"input share\" in this section; but for now, this textual change is a simple bugfix.",
              "createdAt": "2023-06-14T23:02:40Z",
              "updatedAt": "2023-06-14T23:12:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPYjA",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:03:43Z",
          "updatedAt": "2023-06-14T23:03:43Z",
          "comments": [
            {
              "originalPosition": 862,
              "body": "Now that `PrepareStep` is never sent by the Leader by the Helper, perhaps it deserves a more distinguish name, like `PrepareResp` or something?",
              "createdAt": "2023-06-14T23:03:43Z",
              "updatedAt": "2023-06-14T23:03:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPYoo",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:04:13Z",
          "updatedAt": "2023-06-14T23:04:13Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Extensions occur in `struct PlaintextInputShare` (a DAP-level concept), so this is more accurate than `ReportShare. ",
              "createdAt": "2023-06-14T23:04:13Z",
              "updatedAt": "2023-06-14T23:04:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPY1m",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:05:18Z",
          "updatedAt": "2023-06-14T23:05:19Z",
          "comments": [
            {
              "originalPosition": 500,
              "body": "The Leader no longer sends prep step.",
              "createdAt": "2023-06-14T23:05:18Z",
              "updatedAt": "2023-06-14T23:05:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPY_h",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:06:10Z",
          "updatedAt": "2023-06-14T23:06:10Z",
          "comments": [
            {
              "originalPosition": 530,
              "body": "I believe we we'll want to find some way to convey the information in this paragraph. I agree that it now needs to be rewritten now that the term \"prep step\" is going away.",
              "createdAt": "2023-06-14T23:06:10Z",
              "updatedAt": "2023-06-14T23:06:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZAh",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:06:15Z",
          "updatedAt": "2023-06-14T23:06:16Z",
          "comments": [
            {
              "originalPosition": 668,
              "body": "I think it's helpful to mention the nonce here: consider the perspective of an implementer who is reading this and also reading VDAF's definition of `ping_pong_start()`, or looking at a VDAF implementation that takes an argument called \"nonce\". I think it's helpful to give them the hint that the report ID is the nonce, since it won't necessarily occur to them to go check the upload section if they're implementing the server.\r\n\r\nedit: Chris' text suggestion is good though.",
              "createdAt": "2023-06-14T23:06:15Z",
              "updatedAt": "2023-06-14T23:12:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZGy",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Sorry @branlwyd but this now needs a few more minor changes. (We're close, I can feel it!)",
          "createdAt": "2023-06-14T23:06:54Z",
          "updatedAt": "2023-06-14T23:06:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZQ_",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:07:34Z",
          "updatedAt": "2023-06-14T23:07:34Z",
          "comments": [
            {
              "originalPosition": 668,
              "body": "Works for me.",
              "createdAt": "2023-06-14T23:07:34Z",
              "updatedAt": "2023-06-14T23:07:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZcv",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:08:31Z",
          "updatedAt": "2023-06-14T23:08:31Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "From the glossary section:\r\n```\r\nReport:\r\n: A cryptographically protected measurement uploaded to the Leader by a Client.\r\n  Comprised of a set of report shares.\r\n\r\nReport Share:\r\n: An encrypted input share comprising a piece of a report.\r\n```",
              "createdAt": "2023-06-14T23:08:31Z",
              "updatedAt": "2023-06-14T23:08:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZqO",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:09:48Z",
          "updatedAt": "2023-06-14T23:09:48Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Conceptually a \"report share\" is everything an Aggregator needs in order to aggregate its share of the Client's input. Semantically this ought to include the report extensions.",
              "createdAt": "2023-06-14T23:09:48Z",
              "updatedAt": "2023-06-14T23:09:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZsY",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:10:02Z",
          "updatedAt": "2023-06-14T23:10:02Z",
          "comments": [
            {
              "originalPosition": 688,
              "body": "What if we just rename `struct PrepareInit` to `struct ReportShare`?\r\n\r\nedit: actually my suggestion is unsatisfactory because the message contains the leader's prep share plus the helper's report share, and suggesting that the leader's prep share is part of the report share would be icky.",
              "createdAt": "2023-06-14T23:10:02Z",
              "updatedAt": "2023-06-15T00:53:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPZ8k",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:11:20Z",
          "updatedAt": "2023-06-14T23:11:20Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "RFC 8446 presentation language omits cases which have no message, see e.g. https://datatracker.ietf.org/doc/html/rfc8446#appendix-B.3.",
              "createdAt": "2023-06-14T23:11:20Z",
              "updatedAt": "2023-06-14T23:11:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPbh9",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T23:20:04Z",
          "updatedAt": "2023-06-14T23:20:04Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "Ohh nice! We might consider leaving it for self-consistency:\r\n```\r\nstruct {\r\n  FixedSizeQueryType query_type;\r\n  select (query_type) {\r\n    by_batch_id: BatchID batch_id;\r\n    current_batch: Empty;\r\n  }\r\n} FixedSizeQuery;\r\n```\r\nbut I'm fine with it as it is.",
              "createdAt": "2023-06-14T23:20:04Z",
              "updatedAt": "2023-06-14T23:20:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPnLQ",
          "commit": {
            "abbreviatedOid": "17c4d13"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T00:33:33Z",
          "updatedAt": "2023-06-15T00:33:33Z",
          "comments": [
            {
              "originalPosition": 530,
              "body": "I restored most of this, I think defining the possible preparation response types is not useful (as it is defined as part of the message type).",
              "createdAt": "2023-06-15T00:33:33Z",
              "updatedAt": "2023-06-15T00:33:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPnny",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T00:37:05Z",
          "updatedAt": "2023-06-15T00:37:05Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "I made things consistent in the other direction, by removing \"empty\" cases.",
              "createdAt": "2023-06-15T00:37:05Z",
              "updatedAt": "2023-06-15T00:37:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPnze",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T00:38:18Z",
          "updatedAt": "2023-06-15T00:38:19Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "FWIW, it seems that there is some terminology confusion/disagreement around the difference between an \"input share\" and a \"report share\"; I suggest we discuss outside this PR, come to consensus, and fix things up throughout the spec in a later PR.",
              "createdAt": "2023-06-15T00:38:18Z",
              "updatedAt": "2023-06-15T00:38:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPpJ8",
          "commit": {
            "abbreviatedOid": "1daedde"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T00:48:44Z",
          "updatedAt": "2023-06-15T00:48:44Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I agree. Any ambiguity between input and report shares existed before this change so we shouldn't try to solve that problem here.",
              "createdAt": "2023-06-15T00:48:44Z",
              "updatedAt": "2023-06-15T00:48:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPpbA",
          "commit": {
            "abbreviatedOid": "1daedde"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T00:50:47Z",
          "updatedAt": "2023-06-15T00:50:48Z",
          "comments": [
            {
              "originalPosition": 681,
              "body": "This should be `struct { ... } ReportShare;` instead of `struct ReportShare { ... };` (GitHub won't let me make a suggestion to that effect or I would save you the trouble).",
              "createdAt": "2023-06-15T00:50:48Z",
              "updatedAt": "2023-06-15T00:50:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YPqVX",
          "commit": {
            "abbreviatedOid": "1daedde"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Couple more typos to resolve but I think this is sound.",
          "createdAt": "2023-06-15T00:58:15Z",
          "updatedAt": "2023-06-15T00:58:15Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YP6v6",
          "commit": {
            "abbreviatedOid": "1daedde"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T02:37:13Z",
          "updatedAt": "2023-06-15T02:37:14Z",
          "comments": [
            {
              "originalPosition": 681,
              "body": "d'oh, thanks :)",
              "createdAt": "2023-06-15T02:37:14Z",
              "updatedAt": "2023-06-15T02:37:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YQMwD",
          "commit": {
            "abbreviatedOid": "bbbeffb"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-15T04:13:34Z",
          "updatedAt": "2023-06-15T04:13:34Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YUgOn",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T14:48:01Z",
          "updatedAt": "2023-06-15T14:48:02Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "SGTM. Can you please add a note to the commit message this, as well as a reference to the text in RFC 8446? The reason is that @wangshan previously added the \"Empty\" bit, and since we're reverting that change we need to at least document why it's valid.",
              "createdAt": "2023-06-15T14:48:01Z",
              "updatedAt": "2023-06-15T14:48:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YUhx8",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T14:51:04Z",
          "updatedAt": "2023-06-15T14:51:04Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Agreed, let's make sure we get some clarity on this. But does it make sense to at least revert the change of replacing \"ReportShare\" to \"input share\" in this PR?",
              "createdAt": "2023-06-15T14:51:04Z",
              "updatedAt": "2023-06-15T14:51:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YUity",
          "commit": {
            "abbreviatedOid": "bbbeffb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM modulo a suggestion for the commit message. Feel free to override my other comment.",
          "createdAt": "2023-06-15T14:52:51Z",
          "updatedAt": "2023-06-15T14:52:51Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YUk9K",
          "commit": {
            "abbreviatedOid": "467321b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T14:55:54Z",
          "updatedAt": "2023-06-15T14:55:55Z",
          "comments": [
            {
              "originalPosition": 43,
              "body": "Hmmm, wait a second ... where in RFC 8446 does it say it's OK? Note that the \"_RESERVED\" fields in the referenced struct in Appendix B.3 is for backwards compatibility (these messages aren't used in 1.3).",
              "createdAt": "2023-06-15T14:55:54Z",
              "updatedAt": "2023-06-15T14:55:55Z"
            }
          ]
        }
      ]
    },
    {
      "number": 395,
      "id": "PR_kwDOFEJYQs5GqbSk",
      "title": "Introduce resource oriented API",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/395",
      "state": "CLOSED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "Changes the DAP API to orient the upload, aggregate and collect sub-protocols around some HTTP resources, namely:\r\n\r\nUpload:\r\n - HPKE configurations\r\n - Reports\r\n\r\nAggregate:\r\n - Aggregation jobs\r\n\r\nCollect:\r\n - Collections\r\n - Aggregate shares\r\n\r\nThere's a lot of change here, most of which is changing the spelling of HTTP endpoints from e.g. `POST /upload` to\r\n`PUT /tasks/{task-id}/reports/{report-id}`. But there are some more important improvements made here:\r\n\r\n - move some identifiers from message bodies into resource paths, removing the need to partially parse messages to get task ID\r\n - aggregation job messages now contain a `round` field to allow aggregators to recover from message loss\r\n\r\nThe other big goal here is to make better use of HTTP primitives and conventions. However, it's important not to get too precious about this: we expect that anything using this API is a DAP implementation rather than a generic HTTP client like a web browser, so it doesn't matter that much if we choose exactly the right HTTP method, because ultimately DAP gets to dictate the semantics of any message.\r\n\r\nResolves #278 ",
      "createdAt": "2023-01-04T22:34:52Z",
      "updatedAt": "2023-02-08T01:28:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c835232c6119a0ba5b3e9dc89cb6ba23dfaa51e1",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-resource-api",
      "headRefOid": "42f1099558c7654d5568dd36ff69a07afd6bacc0",
      "closedAt": "2023-02-08T01:28:34Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "See also https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/367 for some more discussion and analysis of this change.",
          "createdAt": "2023-01-04T22:50:52Z",
          "updatedAt": "2023-01-04T22:50:52Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Your point about how disruptive this change is is well-taken. I'll work on chunking it into less drastic changes.",
          "createdAt": "2023-01-17T23:24:31Z",
          "updatedAt": "2023-01-17T23:24:31Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton per your suggestion, #398 is just the message struct changes and request paths, without the round skew recovery stuff or any other behavioral changes.",
          "createdAt": "2023-01-19T03:31:52Z",
          "updatedAt": "2023-01-19T03:31:52Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I've downgraded this PR to a draft since any changes that get merged will happen via #398, #399 and #400.",
          "createdAt": "2023-01-20T19:18:59Z",
          "updatedAt": "2023-01-20T19:18:59Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing; the changes discussed here have materialized elsewhere.",
          "createdAt": "2023-02-08T01:28:34Z",
          "updatedAt": "2023-02-08T01:28:34Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5JtIKz",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-04T22:36:54Z",
          "updatedAt": "2023-01-04T22:39:54Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "[RFC 6570](https://datatracker.ietf.org/doc/rfc6570/) discusses URI templating, and I considered using it here, but decided not to because (1) we don't need a template scheme as complicated as the one in 6570 and (2) 6570 has rules about how variables in templates should be percent encoded to be URL safe, whereas we have already settled on using URL-safe unpadded base64. ",
              "createdAt": "2023-01-04T22:36:54Z",
              "updatedAt": "2023-01-04T22:39:54Z"
            },
            {
              "originalPosition": 1068,
              "body": "I'm not certain if explicitly spelling out HTTP 303 See Other and a Location header is a good idea. I think we could just say something like \"the Helper redirects to a URI for the aggregation job\", and rely on existing HTTP specifications for things like the status code and the headers and so on. On the other hand, if we specify this explicitly as I have here, then there's less ambiguity and less code to write for DAP implementations (with this text, they can simply ignore any 3xx status that isn't 303 instead of having to reason about the nuanced difference between 301 and 303). So I lean towards being specific.",
              "createdAt": "2023-01-04T22:39:34Z",
              "updatedAt": "2023-01-04T22:39:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5JzD22",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-05T22:20:01Z",
          "updatedAt": "2023-01-05T23:14:44Z",
          "comments": [
            {
              "originalPosition": 23,
              "body": "```suggestion\r\n* Computing the aggregate over sets of reports, called batches, using the aggregation job\r\n```",
              "createdAt": "2023-01-05T22:20:01Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 26,
              "body": "Should we call this a \"collection job\", to match \"aggregation job\"?",
              "createdAt": "2023-01-05T22:20:27Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 60,
              "body": "```suggestion\r\n/* Public metadata associated with a report. */\r\n```",
              "createdAt": "2023-01-05T22:23:18Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 145,
              "body": "Do you think this actually need to be spelled out? ",
              "createdAt": "2023-01-05T22:31:56Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 301,
              "body": "* Less pseudocode makes this easier to read I think.\r\n* In VDAF-04, the public_share will probably be non-empty \ud83d\ude2c \r\n\r\n```suggestion\r\n* `public_share` is the public share generated by the VDAF sharding algorithm.\r\n```",
              "createdAt": "2023-01-05T22:48:46Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 220,
              "body": "This SHOULD appears to be missing in the new text.",
              "createdAt": "2023-01-05T22:51:16Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 237,
              "body": "nit: \"art\" is generally supposed to wrap at around 70 characters. I would suggest breaking this across two lines:\r\n```suggestion\r\n(public_share, input_shares) =\r\n    VDAF.measurement_to_input_shares(measurement)\r\n```",
              "createdAt": "2023-01-05T22:52:04Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 382,
              "body": "```suggestion\r\nfor an Aggregator to verify that reports come from authenticated Clients. It\r\n```",
              "createdAt": "2023-01-05T22:58:03Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 446,
              "body": "```suggestion\r\n  Poplar1) cannot be prepared until the Collector provides the aggregation\r\n```",
              "createdAt": "2023-01-05T23:02:16Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 491,
              "body": "```suggestion\r\nshares\", one for each Aggregator. The Leader and Helpers then run the aggregate\r\n```",
              "createdAt": "2023-01-05T23:05:20Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            },
            {
              "originalPosition": 1056,
              "body": "* Editorial: It might be helpful to define the resource and message format before getting into the protocol flow.\r\n* Where is the aggregation job ID? We still need a way to link requests for the same job across rounds.",
              "createdAt": "2023-01-05T23:14:39Z",
              "updatedAt": "2023-01-05T23:14:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J3Lak",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-06T18:09:31Z",
          "updatedAt": "2023-01-06T18:09:31Z",
          "comments": [
            {
              "originalPosition": 1056,
              "body": "Summary of offline discussion: The aggregation job ID is missing here because it has been folded into the redirect URI that the Leader is supposed to post to next to move aggregation forward. (In other words, the aggreagtion job ID is now an implementation detail.) The ideas is to make aggreagtion jobs \"asynchronous\" the way collection jobs are. However there is an important difference between the two: Here the Helper computes its response immediately, i.e., aggregation is inherently synchronous. Collection is inherently asynchronous because the Leader usually won't be finished computing the aggregate at the time the request is made.\r\n\r\nWe decided that we should re-align this part with DAP-03, i.e., move the aggregation job ID into the API here.",
              "createdAt": "2023-01-06T18:09:31Z",
              "updatedAt": "2023-01-06T18:09:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J3tR9",
          "commit": {
            "abbreviatedOid": "7e54146"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-06T20:13:56Z",
          "updatedAt": "2023-01-06T20:25:02Z",
          "comments": [
            {
              "originalPosition": 23,
              "body": "This would be confusing, because in this bullet, we are discussing the aggregation sub-protocol, which works in terms of aggregation jobs, and those aren't necessarily equivalent to batches.",
              "createdAt": "2023-01-06T20:13:56Z",
              "updatedAt": "2023-01-06T20:25:21Z"
            },
            {
              "originalPosition": 26,
              "body": "I feel like that's trying to sneak a verb (\"job\") into a resource, which should be a noun. However this is purely a naming quibble so I can live with either. I'm curious what others think; I'd be happy to go with the majority consensus.",
              "createdAt": "2023-01-06T20:16:40Z",
              "updatedAt": "2023-01-06T20:25:02Z"
            },
            {
              "originalPosition": 145,
              "body": "No, I guess not, since we don't have any text that forbids this.",
              "createdAt": "2023-01-06T20:18:30Z",
              "updatedAt": "2023-01-06T20:25:02Z"
            },
            {
              "originalPosition": 220,
              "body": "I moved it down to line 928, in the discussion of `struct Report`:\r\n\r\n~~~\r\n* `time` is the time at which the report was generated. The Client SHOULD round\r\n  this value down to the nearest multiple of the task's `time_precision` in\r\n  order to ensure that that the timestamp cannot be used to link a report back\r\n  to the Client that generated it.\r\n~~~",
              "createdAt": "2023-01-06T20:19:41Z",
              "updatedAt": "2023-01-06T20:25:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J33oS",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-06T20:42:10Z",
          "updatedAt": "2023-01-06T20:42:11Z",
          "comments": [
            {
              "originalPosition": 1056,
              "body": "We reached a similar conclusion about the helper's aggregate share resource: the leader should always be able to wait until all relevant aggregation jobs are driven to completion before requesting an aggregate share from the helper. So servicing such a request should just be a matter of collapsing a bunch of output shares into an aggregate share (or, depending on the implementation, collapsing a bunch of partial aggregates over, say, an hour's worth of output shares into the requested aggregate share), which should be doable synchronously for the helper. So I'm also going to move the aggregate share resource back to how it worked in DAP-03 (though we'll still change the request path).",
              "createdAt": "2023-01-06T20:42:11Z",
              "updatedAt": "2023-01-06T20:42:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J36gI",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-06T20:52:59Z",
          "updatedAt": "2023-01-06T20:53:00Z",
          "comments": [
            {
              "originalPosition": 1056,
              "body": "> Editorial: It might be helpful to define the resource and message format before getting into the protocol flow.\r\n\r\nI'll give that a try after the dust has settled on other changes.\r\n\r\n- [x] put the description of aggregate job endpoints and messages before the prose discussion of the protocol",
              "createdAt": "2023-01-06T20:53:00Z",
              "updatedAt": "2023-01-12T01:10:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J-xCL",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-09T20:53:52Z",
          "updatedAt": "2023-01-09T20:53:52Z",
          "comments": [
            {
              "originalPosition": 1014,
              "body": "I've been working on implementing this in Janus (https://github.com/divviup/janus/pull/731) and I'm finding this definition of `AggregationJob` a little awkward. The idea here is to have a single representation of an aggregation job for both helper and leader across multiple rounds of preparation. But we end up with some extra complexity in implementations:\r\n\r\n- Not all `PrepareStep.prepare_step_state` values are valid in all rounds of preparation. For instance, when the leader first sends an `AggregationJob` to the helper, the helper must verify that each `PrepareStep` is in the `start` state. In subsequent rounds, the `start` state is illegal.\r\n- `PrepareStep`s in the `start` state contain a `ReportShare`, which contains `ReportMetadata`, which has `ReportID` in it. That duplicates the `ReportID` field in the outer `PrepareStep` structure, which is wasteful.\r\n- the `agg_param` and `part_batch_selector` fields are only really useful in the first leader->helper messages. Subsequently, the helper has to check that they haven't changed since an earlier round.\r\n\r\nAll of these mean extra code for servers to write to validate messages. I think we were better off the way things were before, with distinct representations for the initial creation of an aggregation job and then the subsequent stepping. I think it's less REST-y, but that doesn't seem like a good enough tradeoff to me against the implementation complexity and increased message size.",
              "createdAt": "2023-01-09T20:53:52Z",
              "updatedAt": "2023-01-09T20:54:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J-u0N",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-09T20:45:51Z",
          "updatedAt": "2023-01-09T22:54:34Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "Will query parameters always be optional?",
              "createdAt": "2023-01-09T20:45:51Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 698,
              "body": "Why would the response have the same structure as the request (i.e., AggregationJob)? This is wasteful if the aggregation parameter is large, which is likely to be the case for Poplar.",
              "createdAt": "2023-01-09T21:45:26Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 716,
              "body": "WDYTA putting the round number in the  in the resource path instead of serializing it in the message? It might make state management easier for the Helper.",
              "createdAt": "2023-01-09T21:48:33Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 725,
              "body": "```suggestion\r\ntimestamp), a public share sent to each Aggregator, and the recipient's\r\n```",
              "createdAt": "2023-01-09T21:51:09Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 114,
              "body": "For consistency with other resources? Also, the return value is a list of HPKE configs.\r\n```suggestion\r\n##### GET `/hpke_configs[?task_id={task-id}]`\r\n```",
              "createdAt": "2023-01-09T22:39:34Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1085,
              "body": "Is the Helper required to keep the agg job state around until it is DELETEd? As a deployer, I don't want to have to count on my Leader to manage my state responsibly.",
              "createdAt": "2023-01-09T22:40:45Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1014,
              "body": "+1, I think we should go back to what we had before (more or less). My main concern: We should not transmit the agg param more than once per agg job, since for Poplar this might be relatively large. (Think 1-2MBs.)",
              "createdAt": "2023-01-09T22:42:32Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1125,
              "body": "I don't recall if we agreed to be consistent about this, but just in case:\r\n```suggestion\r\nThis resource allows the Collector to create new collection jobs in the Leader and\r\n```",
              "createdAt": "2023-01-09T22:44:08Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1133,
              "body": "```suggestion\r\n##### PUT `/tasks/{task-id}/collection_jobs` {#collections-put}\r\n```",
              "createdAt": "2023-01-09T22:45:26Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1184,
              "body": "```suggestion\r\nThe Leader then asynchronously begins working with the Helper to aggregate the reports satisfying the query (or continues this process, depending on the\r\nVDAF) as described in {{aggregate-flow}}, and then invokes the aggregate share\r\n```",
              "createdAt": "2023-01-09T22:46:57Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1197,
              "body": "```suggestion\r\n* The Leader and Helper have aggregated a sufficient number of reports.\r\n```",
              "createdAt": "2023-01-09T22:48:07Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1238,
              "body": "?\r\n```suggestion\r\n} CollectJobResult;\r\n```",
              "createdAt": "2023-01-09T22:49:27Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1258,
              "body": "```suggestion\r\n###### A Note on Idempotence\r\n```",
              "createdAt": "2023-01-09T22:50:13Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            },
            {
              "originalPosition": 1266,
              "body": "I don't think I follow. For starters, you're talking about polling the collect URI here, correct? If yes, then the contents of the response, that is the aggregate result, is not going to change once computed.\r\n\r\n",
              "createdAt": "2023-01-09T22:53:35Z",
              "updatedAt": "2023-01-09T22:54:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_qx6",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T01:52:33Z",
          "updatedAt": "2023-01-10T01:52:33Z",
          "comments": [
            {
              "originalPosition": 698,
              "body": "As explained [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/395#discussion_r1065098317), we're going to change this.",
              "createdAt": "2023-01-10T01:52:33Z",
              "updatedAt": "2023-01-10T01:52:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_rAx",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T01:54:25Z",
          "updatedAt": "2023-01-10T01:54:26Z",
          "comments": [
            {
              "originalPosition": 40,
              "body": "I don't know, it depends on what query params we add in the future. For now I'm trying to make the path templates no more complicated than what is needed to describe the API we currently have.",
              "createdAt": "2023-01-10T01:54:25Z",
              "updatedAt": "2023-01-10T01:54:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_rPM",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T01:56:12Z",
          "updatedAt": "2023-01-10T01:56:12Z",
          "comments": [
            {
              "originalPosition": 716,
              "body": "I don't think that can work: in the case of the helper replying to the leader, there's no request path in which to indicate the round.",
              "createdAt": "2023-01-10T01:56:12Z",
              "updatedAt": "2023-01-10T01:56:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_rXd",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T01:57:15Z",
          "updatedAt": "2023-01-10T01:57:16Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Yes, excellent idea.",
              "createdAt": "2023-01-10T01:57:16Z",
              "updatedAt": "2023-01-10T01:57:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_rzb",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T02:00:40Z",
          "updatedAt": "2023-01-10T02:00:40Z",
          "comments": [
            {
              "originalPosition": 1085,
              "body": "No, the helper can implement its own GC policies and potentially discard aggregation jobs and other state before a DELETE request. We should add some text to make that clear:\r\n\r\n- [x] discuss in {{aggregation-job-put}} and {[aggregation-job-post}} that the request could fail because the helper has GCed the relevant state\r\n- [x] discuss in DELETE `/tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` what the helper should do if it has already deleted the aggregation job",
              "createdAt": "2023-01-10T02:00:40Z",
              "updatedAt": "2023-01-12T19:22:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_sQr",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T02:04:01Z",
          "updatedAt": "2023-01-10T02:04:01Z",
          "comments": [
            {
              "originalPosition": 1266,
              "body": "Yes, but that's not what affects idempotence. What matters is that polling a collection has the _side effect_ of changing what the leader considers to be the current batch. There's a much more verbose explanation of this [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/134d5305849f52a59061a543e843387564b24a69/api-resources.md?plain=1#L341).",
              "createdAt": "2023-01-10T02:04:01Z",
              "updatedAt": "2023-01-10T02:04:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_spn",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T02:06:54Z",
          "updatedAt": "2023-01-10T02:06:54Z",
          "comments": [
            {
              "originalPosition": 1197,
              "body": "I think this is less clear. We discuss elsewhere how the leader decides what reports will go into a batch, and \"sufficient\" isn't always the only criteria (for time interval batches, you have to have prepared _all_ the reports in the interval, not just enough to meet the task's minimum).",
              "createdAt": "2023-01-10T02:06:54Z",
              "updatedAt": "2023-01-10T02:06:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5J_tKS",
          "commit": {
            "abbreviatedOid": "0f1f0c2"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T02:10:29Z",
          "updatedAt": "2023-01-10T02:10:29Z",
          "comments": [
            {
              "originalPosition": 1125,
              "body": "I prefer \"collections\" over \"collection jobs\". The collection _job_ is the means by which we get the thing we actually care about, which is the collection. But at the end of the day, it doesn't really make a difference which string we use here.",
              "createdAt": "2023-01-10T02:10:29Z",
              "updatedAt": "2023-01-10T02:10:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KF1Ef",
          "commit": {
            "abbreviatedOid": "b640fcf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-10T22:08:47Z",
          "updatedAt": "2023-01-10T22:08:47Z",
          "comments": [
            {
              "originalPosition": 716,
              "body": "Discussed offline: This is just spelling.",
              "createdAt": "2023-01-10T22:08:47Z",
              "updatedAt": "2023-01-10T22:08:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KKnSo",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-11T17:06:46Z",
          "updatedAt": "2023-01-12T00:54:09Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "FWIW, I like \"collection job\" over \"collection\". Other than \"collection job\" matching \"aggregation job\", the term \"collection\" already has a generic meaning for many software engineers as a data type that contains multiple instances of some underlying structure. (e.g. a vector, map, etc)",
              "createdAt": "2023-01-11T17:06:46Z",
              "updatedAt": "2023-01-12T00:54:10Z"
            },
            {
              "originalPosition": 426,
              "body": "Nitpick, not caused by this PR:\r\n\r\n> Verification of a set of reports is referred to as an aggregation job.\r\n\r\nThis sentence is awkward, as it doesn't refer to the aggregation portion of the aggregation job at all. Maybe something like \"The unit of verification and aggregation of a set of reports is called an aggregation job.\"?",
              "createdAt": "2023-01-11T21:47:22Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 427,
              "body": "> a DAP task will have many aggregation jobs\r\n\r\n\"a DAP task can have many aggregation jobs\"? (or \"may\" instead of \"can\", if that isn't too close to a `MAY`.) I think the text intends to define a one-to-many relationship, but a DAP task may have exactly zero aggregation jobs.",
              "createdAt": "2023-01-11T21:52:14Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 488,
              "body": "```suggestion\r\nHelper's aggregation job resource. Only the Helper supports this resource.\r\n```",
              "createdAt": "2023-01-11T22:26:34Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 496,
              "body": "`PrepareStepState` needs to a `start` state to match the cases inside `PrepareStep` below.",
              "createdAt": "2023-01-11T22:36:40Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 618,
              "body": "```suggestion\r\n  ReportShare report_shares<1..2^32-1>;\r\n```",
              "createdAt": "2023-01-11T23:08:00Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 510,
              "body": "Q: when will the `start` state ever be used  in practice? Even the response from the helper for the first `PUT` will have moved on to `continued`/`finished`/`failed`.",
              "createdAt": "2023-01-11T23:12:33Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 722,
              "body": "Q: is there a reason this validation can't happen at the end of the previous round? (I'm not sure how much it matters, it just strikes me as a little strange to await validation of the previous round's response until the next round. Maybe there's a way to phrase things so that this validation can happen at either point? Or maybe in practice implementations can go ahead and do the validation at the end of the previous round, if it's indistinguishable from the outside?)",
              "createdAt": "2023-01-11T23:49:41Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 792,
              "body": "Shouldn't this storage happen after receiving the response from the helper?",
              "createdAt": "2023-01-12T00:09:43Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 819,
              "body": "```suggestion\r\nreport share. The Helper MUST check its current round against the\r\n```",
              "createdAt": "2023-01-12T00:13:23Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 851,
              "body": "Recovering from round skew only requires storing one old round of responses; I think it would be worthwhile to mention this.",
              "createdAt": "2023-01-12T00:18:30Z",
              "updatedAt": "2023-01-12T00:54:09Z"
            },
            {
              "originalPosition": 852,
              "body": "Here we say the helper should store the answer, but below we say the helper can either store or recompute the response (paragraph starting L1404).\r\n\r\nI'd suggest dropping the \"recompute\" part -- storing the answer is simpler & less error-prone. And recomputing the response still requires storing information to allow the recomputation to occur, I think.",
              "createdAt": "2023-01-12T00:22:48Z",
              "updatedAt": "2023-01-12T00:54:10Z"
            },
            {
              "originalPosition": 884,
              "body": "I think this text should allow returning e.g. `500` on error -- as written, the Helper must unconditionally respond with a `204`.",
              "createdAt": "2023-01-12T00:24:27Z",
              "updatedAt": "2023-01-12T00:54:10Z"
            },
            {
              "originalPosition": 1169,
              "body": "Since we are defining URIs for everything else, is there any reason not to introduce a `CollectJobID` and treat collect jobs as we do e.g. reports or aggregation jobs? (i.e. define a specific URI scheme, including the collect job ID, something like `/tasks/{task-id}/collect_jobs/{collect-job-id}`)\r\n\r\nThis would match the rest of the API much more closely, and I don't think there are any downsides. The collector would be generating the collect job IDs, but I don't think that is a problem.",
              "createdAt": "2023-01-12T00:47:27Z",
              "updatedAt": "2023-01-12T00:54:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMpPZ",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:06:18Z",
          "updatedAt": "2023-01-12T01:06:18Z",
          "comments": [
            {
              "originalPosition": 510,
              "body": "This is an error, left over from a previous iteration where a single `AggregationJob` message was used for all stages of the aggregate sub-protocol. There shouldn't be a `start` state here. Thank you for spotting that.\r\n\r\n- [x] delete stray reference to PrepareStepState::start",
              "createdAt": "2023-01-12T01:06:18Z",
              "updatedAt": "2023-01-12T16:58:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMp8A",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:10:07Z",
          "updatedAt": "2023-01-12T01:10:07Z",
          "comments": [
            {
              "originalPosition": 722,
              "body": "You're quite right that a leader could validate the helper's `AggregationJob` immediately. The reason I put this text here is so that I don't have to duplicate it at the bottom of both \"Leader Initialization\" and also \"Leader Continuation\" (in both cases, the Leader gets an `AggregationJob` from the helper and must validate it). But I think what we could do is add a section \"Aggregation Job Validation\" as a peer to \"Input Share Preparation\" and \"Input Share Validation\" and then refer to that from both \"Leader Initialization\" and \"Leader Continuation\".\r\n\r\n- [x] refactor aggregation job validation text into another section",
              "createdAt": "2023-01-12T01:10:07Z",
              "updatedAt": "2023-01-12T17:18:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMqIQ",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:11:28Z",
          "updatedAt": "2023-01-12T01:11:28Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "I put it back to \"collect job\" in the most recent round of changes. If you both like it better, we can change it again to \"collection job\".",
              "createdAt": "2023-01-12T01:11:28Z",
              "updatedAt": "2023-01-12T01:11:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMqOC",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:12:07Z",
          "updatedAt": "2023-01-12T01:12:07Z",
          "comments": [
            {
              "originalPosition": 496,
              "body": "As noted [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/395#discussion_r1067551046), there should not be a start state.",
              "createdAt": "2023-01-12T01:12:07Z",
              "updatedAt": "2023-01-12T01:12:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMqxv",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:16:05Z",
          "updatedAt": "2023-01-12T01:16:06Z",
          "comments": [
            {
              "originalPosition": 792,
              "body": "I think there's a few different points at which the leader could store different things. What we care about is that the leader checkpoint state so that it can recover from crashes, but the more I think about it, the more I think it's inappropriate for the spec to say anything about what an implementation stores or when. So maybe we shouldn't say anything about this at all.",
              "createdAt": "2023-01-12T01:16:05Z",
              "updatedAt": "2023-01-12T01:16:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMrDQ",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:17:59Z",
          "updatedAt": "2023-01-12T01:17:59Z",
          "comments": [
            {
              "originalPosition": 851,
              "body": "- [x] include implementation note explaining that helper only needs to store the most recent set of prepare messages for each aggregation job",
              "createdAt": "2023-01-12T01:17:59Z",
              "updatedAt": "2023-01-12T17:01:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMrPu",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:19:24Z",
          "updatedAt": "2023-01-12T01:19:24Z",
          "comments": [
            {
              "originalPosition": 884,
              "body": "- [x] clean up language about HTTP status code in {{aggregation-job-delete}}",
              "createdAt": "2023-01-12T01:19:24Z",
              "updatedAt": "2023-01-12T17:04:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KMrY8",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T01:20:27Z",
          "updatedAt": "2023-01-12T01:20:27Z",
          "comments": [
            {
              "originalPosition": 1169,
              "body": "Yeah the argument from consistency seems reasonable. The counter-argument is that not specifying the collect job URI is the least change from DAP-03, and thus the least disruptive to implementations. I can go either way on this. I'm curious what @cjpatton thinks.",
              "createdAt": "2023-01-12T01:20:27Z",
              "updatedAt": "2023-01-12T01:20:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KRWIn",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T17:04:59Z",
          "updatedAt": "2023-01-12T17:04:59Z",
          "comments": [
            {
              "originalPosition": 852,
              "body": "I think this is similar to the equivalent leader text discussed [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/395#discussion_r1067581728) so I'll pursue discussion there.",
              "createdAt": "2023-01-12T17:04:59Z",
              "updatedAt": "2023-01-12T17:04:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KRYOM",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T17:07:57Z",
          "updatedAt": "2023-01-12T17:07:58Z",
          "comments": [
            {
              "originalPosition": 792,
              "body": "We have the same problem in the equivalent helper text (noted by Brandon [here](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/395#discussion_r1067587961). These two passages are awkward, because there's a few different ways for implementations to checkpoint state such that they can implement round skew recovery, and I don't think it's appropriate for the spec to dictate implementation details (because maybe there's some nuance about a particular implementation's storage system that makes the prescribed strategy difficult or impossible). Maybe these SHOULDs about storing state should be deleted and instead we could have an implementation note elsewhere (Operational Considerations?) discussing the intent of the `round` field. and suggesting one possible solution.",
              "createdAt": "2023-01-12T17:07:58Z",
              "updatedAt": "2023-01-12T17:07:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KRhCY",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T17:24:59Z",
          "updatedAt": "2023-01-12T17:24:59Z",
          "comments": [
            {
              "originalPosition": 852,
              "body": "The only part of this thread that's different from the linked thread is that this thread suggests dropping the suggestion to \"recompute\" -- IIUC, recomputing is a strictly inferior strategy to storing the previously-computed result (since even in a recompute strategy, you still need to store the data required to recompute the result, i.e. recomputation also requires storage).\r\n\r\nI agree w/ your comment in the other thread that we should probably not be very specific about when storage occurs.",
              "createdAt": "2023-01-12T17:24:59Z",
              "updatedAt": "2023-01-12T17:25:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KRicR",
          "commit": {
            "abbreviatedOid": "c375f03"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T17:28:59Z",
          "updatedAt": "2023-01-12T17:29:00Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "I'd be happy with either \"collect job\" or \"collection job\". (I have a slight preference for \"collection job\" as it matches \"aggregation job\" & is slightly more grammatically correct, but this is very quibbly.)",
              "createdAt": "2023-01-12T17:29:00Z",
              "updatedAt": "2023-01-12T17:29:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KSLrW",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T18:42:49Z",
          "updatedAt": "2023-01-12T18:42:49Z",
          "comments": [
            {
              "originalPosition": 1169,
              "body": "I hear you; my thoughts are that we should aim to design things as they \"should be\" if we were writing the design down for the first time, without necessarily worrying about the size of the diff against previous drafts.",
              "createdAt": "2023-01-12T18:42:49Z",
              "updatedAt": "2023-01-12T18:42:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KSifD",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-12T19:44:09Z",
          "updatedAt": "2023-01-12T19:44:10Z",
          "comments": [
            {
              "originalPosition": 852,
              "body": "I deleted the SHOULDs about storing anything and punted the discussion of round skew to a new \"Recovering From Round Skew\". ",
              "createdAt": "2023-01-12T19:44:09Z",
              "updatedAt": "2023-01-12T19:44:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KaUHu",
          "commit": {
            "abbreviatedOid": "42f1099"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Thank you, @tgeoghegan, for all of your hard work on this. I remain supportive of this change and think it's going in a good direction. I have two big-picture concerns that I would like to see addressed before we merge.\r\n\r\n**Editorial flow changes.** This PR makes significant changes to the flow of the document in order to re-arrange the text around the new concept of \"resources\". Below is the TOC of this PR on the left; TOC of main on the right:\r\n<img width=\"830\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3453007/212445317-b0fcfa98-3edd-4655-a93e-06756b58ec73.png\">\r\n\r\nWhile I understand the reasoning for this, it has a major downside. The text that's changing here is hard-fought, and quite delicate; and changing it so much makes it very difficult to review for correctness. In addition, I fear that the protocol text will no longer align with people's mental model for how the protocol works. We are due for a major overhaul of the document flow, but I think we need to do so incrementally.\r\n\r\nI'll also just say that I'm not sure how useful it is to re-arrange the protocol flow in terms of \"resources\". It makes sense to describe the API in these terms, and even to make protocol changes that align better with this idea semantically; but at the end of the day we need to describe the sequence of interactions in some chronological fashion. The flow in this PR seems a bit at odds with this.\r\n\r\nWhat I believe our goal should be is to try to preserve all of the API changes, wire changes,  renaming of structs, and behavior changes that this PR entails -- that is, keep *all* of the substantive changes that are made -- but minimize code movement to the greatest possible extent.\r\n\r\nHere's what that this might look like. Starting on a fresh branch:\r\n\r\nFirst, before tthe upload, aggregation, and collection sections, add a new section specifying the new API endpoints and summarize what they do.\r\n\r\nSecond, go through each of the sections line-by-line modifying them as necessary. For example, rewrite\r\n  \r\n  > Clients retrieve the HPKE configuration from each aggregator by\r\n  > sending an HTTP GET request to [aggregator]/hpke_config, where\r\n  > [aggregator] is the aggregator's endpoint URL, obtained from the task\r\n  > parameters.\r\n  \r\nas\r\n\r\n > Clients retrieve the HPKE configuration via the Aggregator's \r\n> HPKE-config resource API ({{refer-to-section-where-this-is-defined}}).\r\n\r\nIt also be necessary to add text in certain places, e.g., the text around encoding and validating the round number in the aggregation flow. Also add any related, but technically orthogonal changes you want might want to to suggest.\r\n\r\nThird, in the API definition sections, add a forward reference to where the body of the request/response is defined in the text, e.g., \"The body of the HTTP request is specifed in {{refer-to-section-where-this-is-defined}}.\"\r\n\r\nAll of that said: Perhaps you feel you've done your best to minimize code movement and don't think we can minimize this change any more than you already have. I'm happy to engage on this.\r\n\r\n**Recovery.** Enabling data recovery mechanisms is a primary goal of this PR. However, I think these bits of new protocol behavior should be lifted into a future PR so that they can be discussed on their own merits. Here I'm including things like \"rewinding\" the Helper to a previous state: The costs associated with this, and any security implications, need to be discussed. On the other hand, \"plumbing\" changes that are required to specify such mechanisms, like the round number, should be kept.",
          "createdAt": "2023-01-13T20:14:37Z",
          "updatedAt": "2023-01-14T15:04:37Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "For consistency with VDAF spec\r\n```suggestion\r\n* Collecting aggregate results, using the aggregate share\r\n```",
              "createdAt": "2023-01-13T20:14:37Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 90,
              "body": "\"configuration\" is used as an adjective here (similarly for \"report\")\r\n```suggestion\r\nconfiguration ({{hpke-configs-resource}}) and report ({{report-resource}})\r\n```",
              "createdAt": "2023-01-13T20:19:43Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 233,
              "body": "```suggestion\r\nshares and the public share using the VDAF's\r\n```",
              "createdAt": "2023-01-13T22:44:25Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 300,
              "body": "```suggestion\r\n* `public_share` is the public share output by the VDAF sharding algorithm.\r\n```",
              "createdAt": "2023-01-13T22:51:12Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 307,
              "body": "```suggestion\r\n   Aggregators. The order of the encrypted input shares MUST match the\r\n```",
              "createdAt": "2023-01-13T22:51:47Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 430,
              "body": "Simpler\r\n\r\n```suggestion\r\norder to enable the system to handle very large batches of reports, multiple aggregation jobs can be executed concurrently.\r\n```",
              "createdAt": "2023-01-13T22:53:20Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 547,
              "body": "nit: Indefinite article emphasizes there are many aggregation jobs.\r\n```suggestion\r\nconcurrently creating multiple aggregation jobs. After choosing a set of\r\n```",
              "createdAt": "2023-01-13T22:58:40Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 578,
              "body": "```suggestion\r\nfor all valid candidate reports, it creates an aggregation job in the\r\n```",
              "createdAt": "2023-01-13T23:09:01Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 646,
              "body": "Reviewer note: This OPEN ISSUE is not new.",
              "createdAt": "2023-01-13T23:14:32Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 654,
              "body": "```suggestion\r\nLeader moves to the aggregation job continuation phase with the enclosed\r\n```",
              "createdAt": "2023-01-13T23:16:04Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 657,
              "body": "\"should the Helper response indicate that retry might work\": This is a heavily loaded parenthetical. What do you mean by \"retry\", and how does the Helper indicate that retry \"might work\"? Is this discussed at some point? If not, I would nix this text.",
              "createdAt": "2023-01-13T23:18:32Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 702,
              "body": "Invalid report shares are \"processed\" as well.\r\n```suggestion\r\nOnce the Helper has processed each report share in, it responds with an\r\n```",
              "createdAt": "2023-01-13T23:24:35Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 706,
              "body": "```suggestion\r\n`report_id` field). Each report that was marked as invalid as described in {{input-share-validation}} is assigned the\r\n```",
              "createdAt": "2023-01-13T23:27:21Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 715,
              "body": "```suggestion\r\nstate (either `finished` or `failed`), yielding an output share for all aggregators or an error. This phase may\r\n```",
              "createdAt": "2023-01-13T23:33:32Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 723,
              "body": "```suggestion\r\nsent by the Helpers in the previous round and its locally computed prepare message.\r\n```",
              "createdAt": "2023-01-13T23:36:50Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 729,
              "body": "Recovery: Is this recovery path new to this PR?\r\n* If \"no\", I think we should nix it. since it's a separate issue and should be reviewed on its own merits.\r\n* If \"yes\": This entails that the Helper needs to be willing re-process reports it previously rejected for this reason. Can we make sure this is spelled out? Note that Daphne will need to be patched to support this behavior, but shouldn't be too bad. (cc/ @bhalleycf).",
              "createdAt": "2023-01-13T23:54:35Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 736,
              "body": "```suggestion\r\nfinished, then the report cannot be processed further and MUST be removed from the candidate set.\r\n```",
              "createdAt": "2023-01-13T23:57:12Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 752,
              "body": "Wrap pseudocode blocks at 70 characters.\r\n```suggestion\r\ninbound = VDAF.prep_shares_to_prep(\r\n    agg_param, [leader_outbound, helper_outbound])\r\n```",
              "createdAt": "2023-01-13T23:58:45Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 782,
              "body": "Previously we also required the order to be preserved. I think we should continue to do so.",
              "createdAt": "2023-01-14T00:00:25Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 797,
              "body": "```suggestion\r\nthe aggregation job never existed or if it was deleted (see\r\n```",
              "createdAt": "2023-01-14T00:01:44Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 823,
              "body": "```suggestion\r\n1. An error, in which case the report is marked as invalid and the\r\n```",
              "createdAt": "2023-01-14T00:03:15Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 837,
              "body": "Recovery: I think we should consider any MUSTs relevant to recovery to a future change.",
              "createdAt": "2023-01-14T00:06:33Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 846,
              "body": "Consider reusing an existing error type. Unless, would this be useful for recovery?\r\n\r\n```suggestion\r\ncurrent round, then the Helper MUST abort with an error of type `unexpectedMessage`.\r\n```",
              "createdAt": "2023-01-14T01:02:21Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 860,
              "body": "Reviewer note: This text is not new.",
              "createdAt": "2023-01-14T01:03:20Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 870,
              "body": "```suggestion\r\nHelpers MAY delete aggregation jobs and their related state without an explicit\r\n```",
              "createdAt": "2023-01-14T01:04:12Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 872,
              "body": "editorial: Instead of mentioning \"respond with 204 No Content\", try to refactor this section so that it only is mentioned once.",
              "createdAt": "2023-01-14T01:05:08Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 1086,
              "body": "\"exected\"?",
              "createdAt": "2023-01-14T01:10:49Z",
              "updatedAt": "2023-01-14T02:25:48Z"
            },
            {
              "originalPosition": 1088,
              "body": "Remove Markdown italics syntax (it doesn't render the way you want it to, unfortunately). Here and below.\r\n```suggestion\r\nthe Helper successfully advances from round `n` to `n+1`, but its\r\n```",
              "createdAt": "2023-01-14T01:11:53Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1102,
              "body": "Whether there is an attack here is speculation. I'm also not certain the defense is sufficient.",
              "createdAt": "2023-01-14T01:13:51Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1083,
              "body": "Recovery: I suggest we push recovery to a separate change so that it can be discussed on its own merits.",
              "createdAt": "2023-01-14T01:14:58Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1169,
              "body": "I agree with @branlwyd, and if we view the collect URI as a \"cludge\" in the current API, I think the change is in scope for this PR.",
              "createdAt": "2023-01-14T01:19:43Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1274,
              "body": "Suggests that intermittent failures to obtain aggregate shares may not be fatal (retry is possible).\r\n\r\n```suggestion\r\nIf the Leader cannot obtain the Helper's aggregate share, then the Leader responds to\r\n```",
              "createdAt": "2023-01-14T01:26:39Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1306,
              "body": "1. This doesn't add much\r\n2. You describe collections before aggregate shares\r\n```suggestion\r\nall state related to it.\r\n```",
              "createdAt": "2023-01-14T01:27:56Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1392,
              "body": "Why SHOULD? I would think MUST is appropriate here since if the Leader fails to do this, the result will likely be batch inconsistency.",
              "createdAt": "2023-01-14T01:29:28Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            },
            {
              "originalPosition": 1449,
              "body": "```suggestion\r\ncomputed as in {{aggregate-share-encrypt}}.\r\n```",
              "createdAt": "2023-01-14T01:33:38Z",
              "updatedAt": "2023-01-14T02:25:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5KqQVR",
          "commit": {
            "abbreviatedOid": "525f016"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-17T23:17:50Z",
          "updatedAt": "2023-01-17T23:17:50Z",
          "comments": [
            {
              "originalPosition": 1169,
              "body": "OK, that sounds like a consensus.\r\n\r\n- [ ] specify a `CollectionJobId` and collection job URIs",
              "createdAt": "2023-01-17T23:17:50Z",
              "updatedAt": "2023-01-17T23:17:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LPg-w",
          "commit": {
            "abbreviatedOid": "42f1099"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-19T18:18:35Z",
          "updatedAt": "2023-01-19T18:18:35Z",
          "comments": [
            {
              "originalPosition": 729,
              "body": "This behavior is not new. The equivalent passage in DAP-03: https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-03#section-4.4.2.1-2.1",
              "createdAt": "2023-01-19T18:18:35Z",
              "updatedAt": "2023-01-19T18:18:35Z"
            }
          ]
        }
      ]
    },
    {
      "number": 396,
      "id": "PR_kwDOFEJYQs5HA9_K",
      "title": "upload: Revise guidance for aborting HPKE config request",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/396",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "The current text is out-dated. Update it to account for multiple HPKE configs returned by the Aggregator.",
      "createdAt": "2023-01-09T20:57:35Z",
      "updatedAt": "2023-03-06T19:15:55Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c835232c6119a0ba5b3e9dc89cb6ba23dfaa51e1",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/hpke-config-abort",
      "headRefOid": "f4e657aadebafad0662f0123a5a55997c36854cf",
      "closedAt": "2023-01-09T21:24:10Z",
      "mergedAt": "2023-01-09T21:24:10Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "ae9ddf08e2d0ed9aa07b991194666bc4dce5e6da"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5J-yos",
          "commit": {
            "abbreviatedOid": "f4e657a"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-09T20:59:31Z",
          "updatedAt": "2023-01-09T20:59:31Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5J-41r",
          "commit": {
            "abbreviatedOid": "f4e657a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-09T21:23:19Z",
          "updatedAt": "2023-01-09T21:23:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 398,
      "id": "PR_kwDOFEJYQs5IFnES",
      "title": "Barebones resource oriented API",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/398",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "Following from discussion in #395, this PR aims to only include the changes to request paths and message structures needed to absolve implementations of needing to partially parse messages to figure out how to parse the remainder. Further behavioral changes included in #395 will be submitted as further PRs.",
      "createdAt": "2023-01-19T03:31:01Z",
      "updatedAt": "2023-01-30T18:10:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ae9ddf08e2d0ed9aa07b991194666bc4dce5e6da",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-resource-api-again",
      "headRefOid": "d48384886e00ce9fba24530983a3bdc23e146bc0",
      "closedAt": "2023-01-30T18:10:15Z",
      "mergedAt": "2023-01-30T18:10:15Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "b6af92a3e4bdb6cafe8362768ef7cf9ba9a4b8a3"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5LQpy2",
          "commit": {
            "abbreviatedOid": "e0e4493"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udc4d  Thanks for taking the time to refactor. This is a nice, clean start.",
          "createdAt": "2023-01-19T21:31:04Z",
          "updatedAt": "2023-01-19T21:54:05Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "This guidance doesn't seem necessary to me given the definition of `[role]` below.\r\n\r\n```suggestion\r\nendpoint to construct a resource URI.\r\n```",
              "createdAt": "2023-01-19T21:31:05Z",
              "updatedAt": "2023-01-19T21:54:05Z"
            },
            {
              "originalPosition": 12,
              "body": "Suggestion: For consistency with the other \"variables\" in paths, use curly braces instead of square braces. Unless, do you intend `{}` to denote a variable that is expanded into base64 encoding?\r\n```suggestion\r\n{role}/resource_type/{resource-id}\r\n```",
              "createdAt": "2023-01-19T21:31:39Z",
              "updatedAt": "2023-01-19T21:54:05Z"
            },
            {
              "originalPosition": 85,
              "body": "Hrmm, I'm a bit wary. of moving the report ID out of the `Report` struct. Can you explain your thinking here?\r\n\r\nIn any case, we need to make sure that the task ID, report ID, timestamp, and any other metadata that we might have in the future are all included in the report metadata.",
              "createdAt": "2023-01-19T21:37:09Z",
              "updatedAt": "2023-01-19T21:54:05Z"
            },
            {
              "originalPosition": 532,
              "body": "Reviewer note: This MUST has been moved below.",
              "createdAt": "2023-01-19T21:52:20Z",
              "updatedAt": "2023-01-19T21:54:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LQx7t",
          "commit": {
            "abbreviatedOid": "e0e4493"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Oops, I was a bit too \"approval happy\" :) I just have one blocking comment: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/398#discussion_r1081893133",
          "createdAt": "2023-01-19T21:55:53Z",
          "updatedAt": "2023-01-19T21:55:53Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5LQ06_",
          "commit": {
            "abbreviatedOid": "e0e4493"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-19T22:05:44Z",
          "updatedAt": "2023-01-19T22:17:22Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "No, I was just preserving the existing usage of `[leader]`, `[helper]` and `[aggregator]`. Using `{}` is more consistent, though.",
              "createdAt": "2023-01-19T22:05:44Z",
              "updatedAt": "2023-01-19T22:17:23Z"
            },
            {
              "originalPosition": 85,
              "body": "The report ID now occurs in the request path `[leader]/tasks/{task-id}/reports/{report-id}`, so it does not need to occur in the request body. `struct ReportMetadata` does still contain `ReportID` so that it can be used to construct `InputShareAad` and `ReportShare`.\r\n\r\nI suppose it's not essential that the report ID be in the request path: thus far, it's only the task ID that's needed by servers to figure out how to parse the rest of the message. So we could put report ID back into the body, and then we can use `ReportMetadata` in `Report` again, which is a little nicer. ",
              "createdAt": "2023-01-19T22:10:08Z",
              "updatedAt": "2023-01-19T22:17:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LQ5_b",
          "commit": {
            "abbreviatedOid": "89b23c5"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-19T22:24:36Z",
          "updatedAt": "2023-01-19T22:24:37Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "The downside of this change is that it's inconsistent with the URIs for aggregation jobs and collections, but I don't really think that matters. ",
              "createdAt": "2023-01-19T22:24:36Z",
              "updatedAt": "2023-01-19T22:24:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LWJki",
          "commit": {
            "abbreviatedOid": "991f495"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-20T17:54:47Z",
          "updatedAt": "2023-01-20T17:54:47Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5LXbuy",
          "commit": {
            "abbreviatedOid": "89b23c5"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-20T22:42:31Z",
          "updatedAt": "2023-01-20T22:42:32Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Consistency is a good idea, I think. The curly brackets look nicer.",
              "createdAt": "2023-01-20T22:42:32Z",
              "updatedAt": "2023-01-20T22:42:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LXceh",
          "commit": {
            "abbreviatedOid": "89b23c5"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-20T22:44:59Z",
          "updatedAt": "2023-01-20T22:45:00Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "My vote would be to move report ID back into body so that the metadata appears in both report and report share. If you decide to to keep the report ID where it is, then my only request is to make sure to spell out how the metadata is constructed where we specify encryption of the input shares.",
              "createdAt": "2023-01-20T22:44:59Z",
              "updatedAt": "2023-01-20T22:45:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LXc1f",
          "commit": {
            "abbreviatedOid": "89b23c5"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-20T22:45:53Z",
          "updatedAt": "2023-01-20T22:45:53Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "Oh looks like you already updated this! Thanks.",
              "createdAt": "2023-01-20T22:45:53Z",
              "updatedAt": "2023-01-20T22:45:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LXc62",
          "commit": {
            "abbreviatedOid": "991f495"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-20T22:46:07Z",
          "updatedAt": "2023-01-20T22:46:07Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Let8y",
          "commit": {
            "abbreviatedOid": "991f495"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-23T21:10:49Z",
          "updatedAt": "2023-01-23T23:40:01Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "We should drop this note, as it's looking like Prio3 will have a non-empty public share in VDAF-04.",
              "createdAt": "2023-01-23T21:10:49Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 117,
              "body": "nit: clarity\r\n```suggestion\r\n* `encrypted_input_shares` is the encrypted input shares for each of the\r\n   Aggregators.\r\n```",
              "createdAt": "2023-01-23T21:11:27Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 134,
              "body": "nit: extra word left behind\r\n```suggestion\r\nThe leader responds to well-formed requests with HTTP status code 201\r\n```",
              "createdAt": "2023-01-23T21:12:14Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 205,
              "body": "Should we change MIME types like so for consistency?\r\n```suggestion\r\n\"application/dap-aggregation-job-init-req\".\r\n```",
              "createdAt": "2023-01-23T22:10:18Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 327,
              "body": "unrelated to PR, small wording improvement\r\n```suggestion\r\nif it should continue preparing the report share:\r\n```",
              "createdAt": "2023-01-23T22:52:40Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 6,
              "body": "Suggested rephrasing, to introduce the resources up front:\r\n```suggestion\r\nIn this section, we discuss each of these interactions, the resources they act on (report, aggregation job, and collection job), and the messages used to do so. A resource's path is resolved relative to a server's\r\nendpoint to construct a resource URI.\r\n```",
              "createdAt": "2023-01-23T23:01:53Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 21,
              "body": "rephrasing:\r\n```suggestion\r\nDAP resource identifiers are opaque byte strings, so any occurrence of\r\n`{resource-id}` in a URL template (e.g., `{task-id}` or `{report-id}`) MUST\r\nbe expanded to the URL-safe, unpadded Base 64 representation of the corresponding resource\r\nidentifier, as specified in sections 5 and 3.2 of {{!RFC4648}}.\r\n```",
              "createdAt": "2023-01-23T23:06:16Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            },
            {
              "originalPosition": 440,
              "body": "pre-existing typo\r\n```suggestion\r\na Retry-After header field to suggest a polling interval to the collector.\r\n```",
              "createdAt": "2023-01-23T23:15:11Z",
              "updatedAt": "2023-01-23T23:40:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Lfz9f",
          "commit": {
            "abbreviatedOid": "991f495"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-24T00:22:20Z",
          "updatedAt": "2023-01-24T00:22:20Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "We can make that change in a separate PR where DAP will adapt to VDAF-04, but I want to keep this PR focused on its stated goal.",
              "createdAt": "2023-01-24T00:22:20Z",
              "updatedAt": "2023-01-24T00:22:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Lf0IE",
          "commit": {
            "abbreviatedOid": "991f495"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-24T00:23:27Z",
          "updatedAt": "2023-01-24T00:23:27Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "I don't want to have to keep this list of resources in sync with the remainder of the document.",
              "createdAt": "2023-01-24T00:23:27Z",
              "updatedAt": "2023-01-24T00:23:27Z"
            }
          ]
        }
      ]
    },
    {
      "number": 399,
      "id": "PR_kwDOFEJYQs5IJ9-u",
      "title": "DAP-04 text cleanups",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/399",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "# Stacked on #398\r\n\r\nThis PR includes several text cleanups previously folded into #395:\r\n\r\n- The upload section explicitly describes the call to `VDAF.measurement_to_input_shares`\r\n- An obsolete TODO in the aggregation section is removed\r\n- Text that incorrectly instructed leaders to abort if the helper finishes aggregation is corrected.",
      "createdAt": "2023-01-19T18:42:56Z",
      "updatedAt": "2023-01-30T19:35:51Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b6af92a3e4bdb6cafe8362768ef7cf9ba9a4b8a3",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-text-cleanups",
      "headRefOid": "5b807a1f1c5e1690d1f69405b44fd8323115f1d2",
      "closedAt": "2023-01-30T19:35:50Z",
      "mergedAt": "2023-01-30T19:35:50Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "77cb75a6480a1777cf5ef7c1c6d1d9d12f3c08b2"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Lr_3_",
          "commit": {
            "abbreviatedOid": "21c24b4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "DISMISSED",
          "body": "",
          "createdAt": "2023-01-25T17:37:43Z",
          "updatedAt": "2023-01-30T18:10:52Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5MCgFr",
          "commit": {
            "abbreviatedOid": "5b807a1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-30T18:34:35Z",
          "updatedAt": "2023-01-30T18:34:35Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5MCkA2",
          "commit": {
            "abbreviatedOid": "5b807a1"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-30T18:43:10Z",
          "updatedAt": "2023-01-30T18:43:10Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5MCqoq",
          "commit": {
            "abbreviatedOid": "5b807a1"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-01-30T18:56:26Z",
          "updatedAt": "2023-01-30T18:56:48Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "@cjpatton when do we expect VDAF-04 to be cut?",
              "createdAt": "2023-01-30T18:56:26Z",
              "updatedAt": "2023-01-30T18:56:48Z"
            }
          ]
        }
      ]
    },
    {
      "number": 400,
      "id": "PR_kwDOFEJYQs5IPPu0",
      "title": "Aggregation: round skew recovery",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/400",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "Introduces the `round` field to `AggregationJob` and discusses how aggregators should recover from round skew.",
      "createdAt": "2023-01-20T19:10:52Z",
      "updatedAt": "2023-02-08T21:16:59Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3eeb626bb7c00d02caff754a49fb915e5a947c89",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-round-skew-recovery",
      "headRefOid": "1f9b9bac758c4e68395a17f81ff32590cbe8fc3f",
      "closedAt": "2023-02-08T21:16:59Z",
      "mergedAt": "2023-02-08T21:16:59Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "89e5dc32be05ebd4eb032c7f6d0aaa7074c69b08"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Nobody's asked for any changes on this for about week, so let's merge it!",
          "createdAt": "2023-02-08T21:16:54Z",
          "updatedAt": "2023-02-08T21:16:54Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5LWpSH",
          "commit": {
            "abbreviatedOid": "b1502b4"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-20T19:40:54Z",
          "updatedAt": "2023-01-20T19:40:55Z",
          "comments": [
            {
              "originalPosition": 190,
              "body": "[@cjpatton points out](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/395/files#r1070186916):\r\n\r\n> Whether there is an attack here is speculation. I'm also not certain the defense is sufficient.\r\n\r\nShould we drop this requirement until we have a stronger motivation for it? Or should we err on the side of caution?",
              "createdAt": "2023-01-20T19:40:55Z",
              "updatedAt": "2023-01-20T19:40:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LXl5v",
          "commit": {
            "abbreviatedOid": "b1502b4"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-20T23:34:48Z",
          "updatedAt": "2023-01-21T00:42:50Z",
          "comments": [
            {
              "originalPosition": 69,
              "body": "> (by report ID)\r\n\r\nbig nitpick: I think this parenthetical can be dropped -- \"the same report IDs in the same order\" already implies ordered-by-report-ID, IMO.",
              "createdAt": "2023-01-20T23:34:48Z",
              "updatedAt": "2023-01-21T00:42:50Z"
            },
            {
              "originalPosition": 190,
              "body": "IMO, we should keep some language suggesting that the helper check that the retransmitted message matches the original message. Less strongly, IMO, this should be a requirement (but perhaps this could be a suggestion instead, e.g. a `SHOULD` instead of a `MUST`).\r\n\r\nMy concern is less about an attack (I'm also not sure there is an attack here[1]), and more about consistency/coherence -- it would be _extremely_ confusing if some buggy Leader behavior caused them to retransmit a different message the second time around, and they got an answer as if they had sent the original message.\r\n\r\n[1] Not caused by this PR, but it looks like neither the leader nor the helper checks that the request & response have the same reports in the same order during aggregation continuance. (The spec does dictate the helper's responses `MUST` be in the same order, but no one is required to check this.) Am I missing something or should this be fixed?",
              "createdAt": "2023-01-21T00:41:36Z",
              "updatedAt": "2023-01-21T00:42:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LsLpH",
          "commit": {
            "abbreviatedOid": "b1502b4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-25T18:10:24Z",
          "updatedAt": "2023-01-25T18:10:29Z",
          "comments": [
            {
              "originalPosition": 190,
              "body": "I'm fine with the requirement for checking consistency. One question I have: What triggers the consistency check? Is it noticing that for a given agg job, the current agg request has the same round number as the current one?\r\n\r\nI also agree with Brandon: Consistency/coherence is a stronger case for this requirement. I think SHOULD is appropriate; if we want to lift to MUST, then I think we had better be clearer about the actual mechanism.\r\n\r\nThat said, I think we should leave a note about the potential attack vector. The problem is the Leader's ability to \"rewind\" and the Helper and retry on different inputs. For instance, if the VDAF verification key changes, then the Leader gets to see different verifier shares produced by the Helper. This is potentially an avenue of attack, and I'm not sure it's ruled out by the consistency check. We'll have to wait for security analysis to get more clarity here.\r\n\r\nI think our best bet is to leave this as an OPEN ISSUE. Something like:\r\n\r\n> [[OPEN ISSUE: Allowing the Leader to \"rewind\" the state of an aggregation job may be an avenue of attack on privacy. For instance, if the VDAF verification key changes, the preparation shares in the Helper's response would change even if the consistency check is made. Security analysis is required.]]\r\n\r\n> @branlwyd [1] Not caused by this PR, but it looks like neither the leader nor the helper checks that the request & response have the same reports in the same order during aggregation continuance. (The spec does dictate the helper's responses MUST be in the same order, but no one is required to check this.) Am I missing something or should this be fixed?\r\n\r\nHmm, I believe this is supposed to be enforced by the Helper. It should be fixed if not.\r\n\r\n",
              "createdAt": "2023-01-25T18:10:24Z",
              "updatedAt": "2023-01-25T18:10:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LsuVX",
          "commit": {
            "abbreviatedOid": "71ccb43"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-25T19:59:00Z",
          "updatedAt": "2023-01-25T19:59:01Z",
          "comments": [
            {
              "originalPosition": 190,
              "body": "Chris:\r\n> What triggers the consistency check? Is it noticing that for a given agg job, the current agg request has the same round number as the current one?\r\n\r\nYes. We want to make sure that the leader isn't doing exactly what you describe, rewinding state and re-running an aggregation round with different broadcast prep messages.\r\n\r\nI think I like the idea of making this a SHOULD so that implementations can defend against innocent bugs, and we'll leave an OPEN ISSUE about whether this needs to be upgraded to a MUST as an attack mitigation.\r\n\r\nBran:\r\n> [1] Not caused by this PR, but it looks like neither the leader nor the helper checks that the request & response have the same reports in the same order during aggregation continuance. (The spec does dictate the helper's responses MUST be in the same order, but no one is required to check this.) Am I missing something or should this be fixed?\r\n\r\nAt [the bottom of \"Helper Initialization\"](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/71ccb43c4356228c70219bf87f6c1ee4abadf8c2/draft-ietf-ppm-dap.md?plain=1#L1288) and [the bottom of \"Helper Continuation\"](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/71ccb43c4356228c70219bf87f6c1ee4abadf8c2/draft-ietf-ppm-dap.md?plain=1#L1560), we have text that states the leader has to apply the criteria in [`{{aggregation-job-validation}}`](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/blob/71ccb43c4356228c70219bf87f6c1ee4abadf8c2/draft-ietf-ppm-dap.md?plain=1#L1421). Admittedly it's confusing that this leader behavior appears in sections for the helper. The intent is that the text flows like this:\r\n\r\n- describe leader request\r\n- describe helper handling of request and how helper responds\r\n- describe leader handling of helper's response\r\n\r\nWhich follows the order in which this happens. We could move those paragraphs into \"Leader Initialization\" and \"Leader Continuation\", respectively, but then the flow of text would be:\r\n\r\n- describe leader request\r\n- describe leader handling of helper's response\r\n- describe helper handling of request and how helper responds\r\n\r\nI'll give that a try and see how it reads.\r\n\r\nThe other thing is that currently, the leader is allowed to change the order of reports across aggregation rounds. I believe that's already the case in DAP-03 (I don't see anything in [DAP-03 leader continuation](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-03#section-4.4.2.1) governing the order of `AggregateContinueReq.prepare_steps`. If we want to add ordering requirements on the leader side, I think we should discuss that in a separate PR.",
              "createdAt": "2023-01-25T19:59:01Z",
              "updatedAt": "2023-01-25T19:59:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LtXqG",
          "commit": {
            "abbreviatedOid": "c905045"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-25T21:55:21Z",
          "updatedAt": "2023-01-25T21:55:22Z",
          "comments": [
            {
              "originalPosition": 190,
              "body": "I put in a SHOULD and a TODO as discussed, and wrote up #401 to track the security analysis we discussed. Additionally I remixed the text on how the leader validates `AggregationJob` messages and I think it reads better now.",
              "createdAt": "2023-01-25T21:55:21Z",
              "updatedAt": "2023-01-25T21:55:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5LyZkD",
          "commit": {
            "abbreviatedOid": "3dc7454"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "DISMISSED",
          "body": "",
          "createdAt": "2023-01-26T17:42:00Z",
          "updatedAt": "2023-01-30T19:36:04Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5LyaNK",
          "commit": {
            "abbreviatedOid": "b1502b4"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-01-26T17:43:22Z",
          "updatedAt": "2023-01-26T17:43:23Z",
          "comments": [
            {
              "originalPosition": 190,
              "body": "Thanks -- IMO it's easier to follow with the leader checks in the Leader Initialization/Continuation sections, I like this change.",
              "createdAt": "2023-01-26T17:43:23Z",
              "updatedAt": "2023-01-26T17:43:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MLkso",
          "commit": {
            "abbreviatedOid": "3314abc"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-01-31T22:14:10Z",
          "updatedAt": "2023-01-31T22:32:34Z",
          "comments": [
            {
              "originalPosition": 42,
              "body": "```suggestion\r\n  Helper responds to the Leader's `AggregationInititalizationReq`.\r\n```",
              "createdAt": "2023-01-31T22:14:10Z",
              "updatedAt": "2023-01-31T22:32:34Z"
            },
            {
              "originalPosition": 28,
              "body": "nit: since `AggregateInitializeResp` is only used for the first round, `round` will always be `0`. We could drop the field entirely from `AggregateInitializeResp` and save a few bytes.",
              "createdAt": "2023-01-31T22:14:50Z",
              "updatedAt": "2023-01-31T22:32:35Z"
            },
            {
              "originalPosition": 13,
              "body": "```suggestion\r\nThe Helper's response will be an `AggregateInitializeResp` message (see\r\n```",
              "createdAt": "2023-01-31T22:15:30Z",
              "updatedAt": "2023-01-31T22:32:35Z"
            },
            {
              "originalPosition": 74,
              "body": "The `AggregationJob` message no longer exists, I think it was replaced by `Aggregate{Initialize,Continue}{Req,Resp}` based on context. (applies to various references to `AggregationJob` throughout this PR)",
              "createdAt": "2023-01-31T22:23:40Z",
              "updatedAt": "2023-01-31T22:32:35Z"
            },
            {
              "originalPosition": 236,
              "body": "nit: IMO, this sentence could be dropped -- we don't specify a solution to many implementation problems, and we don't need to call out every instance. (We _could_ also drop the preceding paragraph, as it does specify one possible implementation strategy for the round-skew recovery check.)",
              "createdAt": "2023-01-31T22:26:24Z",
              "updatedAt": "2023-01-31T22:32:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MMKqx",
          "commit": {
            "abbreviatedOid": "3314abc"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T01:11:14Z",
          "updatedAt": "2023-02-01T01:12:01Z",
          "comments": [
            {
              "originalPosition": 74,
              "body": "No, `AggregationJob` is what we want. This is also because I forgot to rebase on `main` after merging #398.",
              "createdAt": "2023-02-01T01:11:14Z",
              "updatedAt": "2023-02-01T01:12:01Z"
            },
            {
              "originalPosition": 42,
              "body": "This is confusing, but `AggregationJobInitReq` is correct. The diff looked funny when you reviewed because I forgot to rebase on `main` after merging #398.",
              "createdAt": "2023-02-01T01:11:17Z",
              "updatedAt": "2023-02-01T01:12:01Z"
            },
            {
              "originalPosition": 13,
              "body": "`AggregationJob` is correct, I forgot to rebase this on `main` after merging #398.",
              "createdAt": "2023-02-01T01:11:47Z",
              "updatedAt": "2023-02-01T01:12:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MMLJk",
          "commit": {
            "abbreviatedOid": "c45010a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T01:14:30Z",
          "updatedAt": "2023-02-01T01:14:30Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "You're right. And on subsequent rounds, we require the helper to echo back the round value that the leader sent in its request, so even there it carries no information and is just an opportunity for the helper to have a bug. So it seems like we'd want to stop using `struct AggregationJob` in three places and go back to `AggregateContinueReq` and `AggregateContinueResp`. I was trying to do a REST thing here -- the servers send a REpresentation of the STate of the aggregation job back and forth -- but I'm backing away from that particular ledge now.",
              "createdAt": "2023-02-01T01:14:30Z",
              "updatedAt": "2023-02-01T01:14:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MRHr4",
          "commit": {
            "abbreviatedOid": "3314abc"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T17:22:54Z",
          "updatedAt": "2023-02-01T17:22:54Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "I re-introduced `AggregationJobContinueReq` and `AggregationJobResp` messages. `AggregationJobContinueReq` contains the round field and is sent by the leader to helper.  `AggregationJobResp` omits the round field and is used as the response to both `AggregationJobInitReq` and `AggregationJobContinueReq`. I now wish I hadn't collapsed all the message types together back in #398, but so it goes.",
              "createdAt": "2023-02-01T17:22:54Z",
              "updatedAt": "2023-02-01T17:22:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MSBHM",
          "commit": {
            "abbreviatedOid": "cab9341"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T20:09:20Z",
          "updatedAt": "2023-02-01T20:13:22Z",
          "comments": [
            {
              "originalPosition": 156,
              "body": "Consider describing the expectation on the sequence of `round` values, i.e. that the first `AggregationJobContinueReq` will have a round of 1 and subsequent requests will increment the round by 1 (I think).\r\n\r\nTo make the later text around \"If the Leader's `round` is behind or more than one round ahead of the Helper's current round\" work out, I think we also need to describe the Helper is considered to be at round 0 after aggregation initialization.",
              "createdAt": "2023-02-01T20:09:20Z",
              "updatedAt": "2023-02-01T20:13:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MSc2g",
          "commit": {
            "abbreviatedOid": "cab9341"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T21:38:36Z",
          "updatedAt": "2023-02-01T21:38:36Z",
          "comments": [
            {
              "originalPosition": 156,
              "body": "Good point. I think I have made this clear in the latest commit.",
              "createdAt": "2023-02-01T21:38:36Z",
              "updatedAt": "2023-02-01T21:38:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MR-PH",
          "commit": {
            "abbreviatedOid": "e04cf34"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I think a bit of clarification around how round skew recovery works would be helpful.",
          "createdAt": "2023-02-01T19:59:56Z",
          "updatedAt": "2023-02-01T21:46:00Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "```suggestion\r\nmoves to the aggregation job continuation phase with the enclosed prepare\r\n```",
              "createdAt": "2023-02-01T19:59:57Z",
              "updatedAt": "2023-02-01T21:46:00Z"
            },
            {
              "originalPosition": 152,
              "body": "```suggestion\r\n  PrepareStep prepare_steps<1..2^32-1>;\r\n```",
              "createdAt": "2023-02-01T20:03:55Z",
              "updatedAt": "2023-02-01T21:46:00Z"
            },
            {
              "originalPosition": 190,
              "body": "```suggestion\r\none round ahead of the Helper, then the Helper combines the Leader's prepare\r\n```",
              "createdAt": "2023-02-01T20:05:21Z",
              "updatedAt": "2023-02-01T21:46:00Z"
            },
            {
              "originalPosition": 267,
              "body": "* What can go wrong if an Aggregator doesn't follow this SHOULD? Should this be a MUST?\r\n* What state is \"checkpointed\"? It would probably be helpful to be specific here: Do we need to put off committing nonces, or output shares, until a certain point?",
              "createdAt": "2023-02-01T21:41:19Z",
              "updatedAt": "2023-02-01T21:46:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MSlAM",
          "commit": {
            "abbreviatedOid": "f7867d9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T22:08:56Z",
          "updatedAt": "2023-02-01T22:08:56Z",
          "comments": [
            {
              "originalPosition": 267,
              "body": "> * What can go wrong if an Aggregator doesn't follow this SHOULD? Should this be a MUST?\r\n\r\nSuppose a Helper doesn't checkpoint any state for an aggregation job (i.e., all it ever stores is report shares and eventually output shares). Then if it crashes at any point during the aggregation protocol, it completely forgets that the aggregation job exists. If the Leader is on round 1 and then tries to send an `AggregationJobContinueReq` with `round = 1`, the Helper can't service that request and would have to return a `roundMismatch` error. \r\n\r\nI don't think this needs to be a MUST because what I just described doesn't leave the leader in an uncertain or invalid state: it just has to abandon that particular aggregation job and move on with other work. Leaders have to be ready to do this anyway, because there's other ways that an `AggregationJobContinueReq` failure forces the Leader to abandon the job.\r\n\r\n> * What state is \"checkpointed\"? It would probably be helpful to be specific here: Do we need to put off committing nonces, or output shares, until a certain point?\r\n\r\nI think the protocol should say as little as possible about what any server stores and when (storage is an implementation detail and none of DAP's business). Rather, I tried to phrase a requirement for implementations to satisfy: \"such that Leaders can re-construct continuation requests and Helpers can re-construct continuation responses as needed.\" I think we could write:\r\n\r\n> Aggregator implementations SHOULD checkpoint the most recent round's preparation state and messages to durable storage such that Leaders can re-construct continuation requests and Helpers can re-construct continuation responses as needed.",
              "createdAt": "2023-02-01T22:08:56Z",
              "updatedAt": "2023-02-01T22:08:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MaUk8",
          "commit": {
            "abbreviatedOid": "f7867d9"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-02T21:55:21Z",
          "updatedAt": "2023-02-02T21:55:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Mhvjs",
          "commit": {
            "abbreviatedOid": "052b1e9"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-03T23:54:57Z",
          "updatedAt": "2023-02-03T23:54:57Z",
          "comments": []
        }
      ]
    },
    {
      "number": 403,
      "id": "PR_kwDOFEJYQs5JCChd",
      "title": "Add `interval` to struct `Collection`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/403",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "The Leader now informs the Collector of what time interval is spanned by the reports aggregated into a collection, regardless of the query type. This is particularly useful for fixed size tasks as otherwise a Collector can only guess at the report times based on the timing of collection requests, but also useful in time interval tasks since the aggregated reports could span a *smaller* interval that what was requested (e.g., if the collector requested a one hour time interval, but reports only arrived during minutes 30-40 of that hour).\r\n\r\nResolves #397",
      "createdAt": "2023-02-01T18:03:29Z",
      "updatedAt": "2023-02-08T21:04:28Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "77cb75a6480a1777cf5ef7c1c6d1d9d12f3c08b2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/collection-interval",
      "headRefOid": "48e34d73ea8e3641688869b994cbfd66ceacdd48",
      "closedAt": "2023-02-08T21:04:28Z",
      "mergedAt": "2023-02-08T21:04:28Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "3eeb626bb7c00d02caff754a49fb915e5a947c89"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5MRZiq",
          "commit": {
            "abbreviatedOid": "6ab5e46"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-01T18:11:48Z",
          "updatedAt": "2023-02-01T18:13:25Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "I think we should specify that the interval be aligned to `time_precision`, for clarity.\r\n```suggestion\r\n* The smallest interval of time that contains the timestamps of all reports\r\n  included in the collection, such that the interval's start and duration are\r\n  both multiples of the task's `time_precision` parameter. Note that in the case of a `time_interval` type\r\n```",
              "createdAt": "2023-02-01T18:11:48Z",
              "updatedAt": "2023-02-01T18:13:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MRbfh",
          "commit": {
            "abbreviatedOid": "6ab5e46"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T18:17:47Z",
          "updatedAt": "2023-02-01T18:17:47Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "Won't this be the case already if the client rounded the timestamp before upload? But then I suppose there's no harm in the leader doing that again just in case the client didn't for some reason.",
              "createdAt": "2023-02-01T18:17:47Z",
              "updatedAt": "2023-02-01T18:17:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MRcec",
          "commit": {
            "abbreviatedOid": "6ab5e46"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T18:20:47Z",
          "updatedAt": "2023-02-01T18:20:47Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "The chief ambiguity I want to squash here is that, if every timestamp is equal, we should be reporting only `Interval { ts, time_precision }`, not, say, `Interval { ts, 1 }`.",
              "createdAt": "2023-02-01T18:20:47Z",
              "updatedAt": "2023-02-01T18:20:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MR9YK",
          "commit": {
            "abbreviatedOid": "31f20c4"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-01T19:57:17Z",
          "updatedAt": "2023-02-01T19:57:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5MSgqO",
          "commit": {
            "abbreviatedOid": "31f20c4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-01T21:52:47Z",
          "updatedAt": "2023-02-01T21:53:02Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "```suggestion\r\n  included in the batch, such that the interval's start and duration are\r\n```",
              "createdAt": "2023-02-01T21:52:47Z",
              "updatedAt": "2023-02-01T21:53:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5MSw02",
          "commit": {
            "abbreviatedOid": "b8efc64"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-01T22:43:23Z",
          "updatedAt": "2023-02-01T22:43:23Z",
          "comments": []
        }
      ]
    },
    {
      "number": 410,
      "id": "PR_kwDOFEJYQs5KmsLm",
      "title": "Update requirements for share validation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/410",
      "state": "MERGED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fixes #408 ",
      "createdAt": "2023-02-23T12:59:42Z",
      "updatedAt": "2023-02-27T22:11:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "89e5dc32be05ebd4eb032c7f6d0aaa7074c69b08",
      "headRepository": "simon-friedberger/draft-ietf-ppm-dap",
      "headRefName": "main",
      "headRefOid": "9104dc369832d99218542f7f3a9f2d37aeb9b85d",
      "closedAt": "2023-02-27T22:11:41Z",
      "mergedAt": "2023-02-27T22:11:41Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "210c1b9537d5d5c47d3979a8d8b9d96119f3f533"
      },
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Update with comments so far.",
          "createdAt": "2023-02-24T20:03:25Z",
          "updatedAt": "2023-02-24T20:03:25Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I have two more concerns:\r\n1. We have `report_too_early` do we want to add `report_too_late`? This would cover the \"validation state is no longer available\" which we currently use as an example of `report_dropped`. It would be clearer to define a lifetime for the state and emit `report_too_late` when it has expired.\r\n2. I think we should maybe change the names. `batch_redefined` would be more accurate than `batch_collected`. Or maybe `batch_overlap`. Same for `report_replayed`, this might be better as `agg_param_invalid` now.",
          "createdAt": "2023-02-27T19:58:16Z",
          "updatedAt": "2023-02-27T19:58:16Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": ">     1. We have `report_too_early` do we want to add `report_too_late`? This would cover the \"validation state is no longer available\" which we currently use as an example of `report_dropped`. It would be clearer to define a lifetime for the state and emit `report_too_late` when it has expired.\r\n>     2. I think we should maybe change the names. `batch_redefined` would be more accurate than `batch_collected`. Or maybe `batch_overlap`. Same for `report_replayed`, this might be better as `agg_param_invalid` now.\r\n\r\nI like these suggestions, but they should be discussed in a different PR. Note that  @tgeoghegan has in the past wanted to err on the side of not adding too many error variants.\r\n\r\n`batch_overlap` seems better to me :)\r\n\r\n",
          "createdAt": "2023-02-27T20:02:27Z",
          "updatedAt": "2023-02-27T20:02:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@simon-friedberger please squash these changes into a single commit, then I'll merge it.",
          "createdAt": "2023-02-27T21:15:54Z",
          "updatedAt": "2023-02-27T21:15:54Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> @simon-friedberger please squash these changes into a single commit, then I'll merge it.\r\n\r\nDidn't I already do that?",
          "createdAt": "2023-02-27T21:47:33Z",
          "updatedAt": "2023-02-27T21:47:33Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5OL6AY",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-23T16:58:34Z",
          "updatedAt": "2023-02-23T17:00:52Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "I'm not certain if we want to drop this requirement. IIRC the scenario we care about here is this:\r\n\r\n- _n_ reports are uploaded with timestamps in some interval _i_.\r\n- The collector makes a collect request for _i_ with aggregation parameter _p1_, and the aggregators service that request by aggregating over _n_ reports.\r\n- One more report with a timestamp inside _i_ is uploaded.\r\n- The collector makes another collect request for _i_ with aggregation parameter _p2_. The aggregators service that request, this time aggregating over _n + 1_ reports.\r\n\r\nWe want to prevent the collector from learning anything about the _n+1_-th report by comparing the two aggregations. Thus this requirement that the aggregators should reject any uploads for reports that fall into an already collected batch. Perhaps this requirement needs to be rephrased for clarity?",
              "createdAt": "2023-02-23T16:58:34Z",
              "updatedAt": "2023-02-23T17:00:52Z"
            },
            {
              "originalPosition": 56,
              "body": "I don't think a prescription like this suffices. This should be written in terms of some function exposed by VDAFs, so that DAP implementations can remain generic. So I believe we're blocked on a corresponding VDAF change here.",
              "createdAt": "2023-02-23T17:00:47Z",
              "updatedAt": "2023-02-23T17:00:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OOA6K",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-23T23:33:01Z",
          "updatedAt": "2023-02-23T23:33:01Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "Since the previous n which have been aggregated cannot be used anymore this is already covered. The only way to aggregate again is if there are another $threshold reports for the same time interval. In that case it's fine.\r\n\r\nMaybe the phrasing is indeed the problem. Afaiu \"is in a batch which has been collected\" and \"has never been aggregated\" are contradictory. I assumed it meant \"batch definition\" i.e. \"time interval\".",
              "createdAt": "2023-02-23T23:33:01Z",
              "updatedAt": "2023-02-23T23:33:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OOBCB",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-23T23:33:47Z",
          "updatedAt": "2023-02-23T23:33:48Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Agreed, this is supposed to be a bandaid. Until we have the VDAF change we have the only requirement that we know of covered here.",
              "createdAt": "2023-02-23T23:33:47Z",
              "updatedAt": "2023-02-23T23:33:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OOdaH",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T02:24:00Z",
          "updatedAt": "2023-02-24T02:30:25Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I actually think that @simon-friedberger is more or less on the right track here, however I agree we want to be generic. Here is where we have converged on the VDAF spec so far (https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/161, please feel free to offer feeedback there as well):\r\n\r\n> A VDAF *MAY* specify the \"validity scope\" of each aggregation parameter. Protocols using a VDAF (like DAP) *MUST* ensure that for each validity scope, a report is aggregated at most once with an aggregation parameter of that validity scope. If no validity scope is specified, a report *MUST NOT* be aggregated more than once with a given aggregation parameter.\r\n\r\nThis provides what we need for the current VDAFs:\r\n\r\n* For Poplar1 the validity scope is the IDPF tree level, which is included in the aggregation parameter; the language here seems sufficient to ensure we don't evaluate at the same level more than once.\r\n\r\n* For Prio3 we have no validity scope, however the only valid aggregation parameter is the empty string. The language implies we don't aggregate the same report more than once [1].\r\n\r\nSo how does this translate into normative text for input share validation? Maybe something like this:\r\n\r\n> Check if the input share was previously aggregated with the aggregation parameter's validity scope {{reference-to-definition-in-the-vdaf-draft}}. If so, the Aggregator *MUST* mark the input share as invalid with the error \"XXX\".\r\n\r\ncc/ @schoppmp\r\n\r\n[1] The attack we're considering here: We want to make sure the attacker can't query the same input/proof on multiple distinct query randomnesses. If we can ensure by some other means that both the nonce and verify_key can't change overtime, then this may not be strictly necessary. Since we haven't really settled the details here, making sure we aggregate each report at most once seems like a reasonable conservative choice.\r\n\r\n",
              "createdAt": "2023-02-24T02:24:01Z",
              "updatedAt": "2023-02-24T02:30:25Z"
            },
            {
              "originalPosition": 54,
              "body": "> Since the previous n which have been aggregated cannot be used anymore this is already covered. The only way to aggregate again is if there are another $threshold reports for the same time interval. In that case it's fine.\r\n\r\nHmm, I think I'm missing something important here. Can you clarify? Is there language elsewhere that covers this?\r\n\r\nSo the requirement is: \"once a batch has been defined, do not add any more reports to that batch\".\r\n\r\n",
              "createdAt": "2023-02-24T02:30:21Z",
              "updatedAt": "2023-02-24T02:30:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OPqzz",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T08:21:45Z",
          "updatedAt": "2023-02-24T08:21:46Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "Okay, so how about\r\n```\r\n1. A report must never be used in more than one batch. If a report with the same\r\n   report ID is contained in a batch for which the `AggregateShare` has been\r\n   sent the input share MUST be marked as invalid with the error\r\n   `batch_collected`.\r\n```\r\nThis makes the requirement that Chris stated explicit, and removes the somewhat contradictory statement about a batch which has been collected but contains a report which has not been aggregated.",
              "createdAt": "2023-02-24T08:21:45Z",
              "updatedAt": "2023-02-24T08:28:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OP_pm",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T08:57:33Z",
          "updatedAt": "2023-02-24T08:57:33Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I agree with Tim, since this is VDAF specific it should be implemented on the VDAF side.\r\nHow about:\r\n> Check if the input share was not previously aggregated with the aggregation parameter's validity scope by calling `VDAF.is_valid(report_id, agg_param)`. Otherwise, the Aggregator MUST mark the input share as invalid with the error \"invalid_aggregation_parameter\".",
              "createdAt": "2023-02-24T08:57:33Z",
              "updatedAt": "2023-02-24T08:57:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OTwLA",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T15:56:38Z",
          "updatedAt": "2023-02-24T15:56:38Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Something like that, yeah. However, `VDAF.is_valid()` would also need to take as input the the agg params that have been used for report_id in the past.",
              "createdAt": "2023-02-24T15:56:38Z",
              "updatedAt": "2023-02-24T15:56:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OT3pr",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "schoppmp",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T16:10:07Z",
          "updatedAt": "2023-02-24T16:10:08Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "If it helps, we could expose a function `VDAF.validity_scope(agg_param)` that maps aggregation parameters to validity scopes. Then DAP only needs to remember all scopes that a report has already been aggregated on, but doesn't have to pass them into VDAF.",
              "createdAt": "2023-02-24T16:10:08Z",
              "updatedAt": "2023-02-24T16:10:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OT5l8",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T16:14:14Z",
          "updatedAt": "2023-02-24T16:14:15Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "@schoppmp Yes, that sounds good, so `validity_scope = VDAF.validity_scope(agg_param)` would extract parts of the agg_param (or otherwise construct) an opaque bitstring and DAP ensures that (report_id, validity_scope) has never been aggregated before, right?",
              "createdAt": "2023-02-24T16:14:15Z",
              "updatedAt": "2023-02-24T16:14:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OT6fn",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T16:16:10Z",
          "updatedAt": "2023-02-24T16:16:11Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "Yup, that sounds right to me.",
              "createdAt": "2023-02-24T16:16:10Z",
              "updatedAt": "2023-02-24T16:16:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OUdbO",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T17:35:15Z",
          "updatedAt": "2023-02-24T17:35:16Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "I don't think that phrasing is right, because to me it doesn't capture that you can't add a report to a batch once it has been collected. How about this:\r\n\r\n```\r\n1. Some VDAFs allow a batch to be collected multiple times with different aggregation parameters, but the\r\n    aggregators must check that the same set of reports is being aggregated each time. A report may not be\r\n    aggregated into a collection unless it was included in all previous collections for the batch. If this check fails,\r\n    the input share MUST be marked as invalid with the error `batch_collected`. This prevents collectors from\r\n    learning anything about small numbers of reports that are uploaded between two collections of a batch.\r\n```",
              "createdAt": "2023-02-24T17:35:15Z",
              "updatedAt": "2023-02-24T18:01:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OUe50",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T17:38:01Z",
          "updatedAt": "2023-02-24T17:38:01Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "I was trying to clarify what \"once it has been collected\" means with \"for which the `AggregateShare` has been sent\". Since the helper cannot tell if collection has actually finished (and if there is pro-active aggregation maybe even if it has started) it can only check if it has sent the `AggregateShare` to the Leader.\r\n\r\n(Otherwise I like your longer version. It helps with clarity.)",
              "createdAt": "2023-02-24T17:38:01Z",
              "updatedAt": "2023-02-24T17:39:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OUz7T",
          "commit": {
            "abbreviatedOid": "abe0c96"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-24T18:35:44Z",
          "updatedAt": "2023-02-24T18:35:45Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "IIUC we want to write this section in terms of the interface being added in https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/161",
              "createdAt": "2023-02-24T18:35:45Z",
              "updatedAt": "2023-02-24T18:35:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OV5xX",
          "commit": {
            "abbreviatedOid": "24becac"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-02-24T23:13:16Z",
          "updatedAt": "2023-02-24T23:20:17Z",
          "comments": [
            {
              "originalPosition": 60,
              "body": "I would say the agg param validity scope check should be made for all VDAFs. Where we landed in 04 is to require that each VDAF specify a method for checking it: https://stackoverflow.com/questions/868568/what-do-the-terms-cpu-bound-and-i-o-bound-mean",
              "createdAt": "2023-02-24T23:13:16Z",
              "updatedAt": "2023-02-24T23:20:18Z"
            },
            {
              "originalPosition": 67,
              "body": "API is now `Vdaf.is_valid(agg_param: AggParam, previous_agg_params: Vec[AggParam]) -> Bool`. What we need to do is something like: \"Look up all of the aggregation parameters with which the report was previously aggregated; if `Vdaf.is_valid(agg_param, previous_agg_params)` is false, then the input share MUST be marked ....\"",
              "createdAt": "2023-02-24T23:16:21Z",
              "updatedAt": "2023-02-24T23:20:18Z"
            },
            {
              "originalPosition": 46,
              "body": "I think this check should go last, since it's kind of a \"catch all\": I.e., if the Aggregator did not have enough information to perform even one of the previous checks, then we have to invalidate the report.",
              "createdAt": "2023-02-24T23:18:21Z",
              "updatedAt": "2023-02-24T23:20:18Z"
            },
            {
              "originalPosition": 56,
              "body": "nit: Wrap paragraphs at 80 characters.",
              "createdAt": "2023-02-24T23:20:05Z",
              "updatedAt": "2023-02-24T23:20:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OWDya",
          "commit": {
            "abbreviatedOid": "24becac"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-02-25T00:17:41Z",
          "updatedAt": "2023-02-25T00:19:06Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "```suggestion\r\n   previous collections for the batch (checking that no reports are omitted\r\n   from a batch is covered in {{batch-validation}}).\r\n```",
              "createdAt": "2023-02-25T00:17:41Z",
              "updatedAt": "2023-02-25T00:19:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OY8Pi",
          "commit": {
            "abbreviatedOid": "24becac"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T09:08:36Z",
          "updatedAt": "2023-02-27T09:08:36Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "These seem like two sentences to me so I'm not sure why we would remove the split. I'm removing the parentheses instead.",
              "createdAt": "2023-02-27T09:08:36Z",
              "updatedAt": "2023-02-27T09:08:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OY9Zj",
          "commit": {
            "abbreviatedOid": "24becac"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T09:11:28Z",
          "updatedAt": "2023-02-27T09:11:29Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I am a fan of fuzzy 80 but happy to fix. These all show as 80 or less for me, though.",
              "createdAt": "2023-02-27T09:11:29Z",
              "updatedAt": "2023-02-27T09:11:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ObywD",
          "commit": {
            "abbreviatedOid": "15ccb3d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Looking good, just a few minor comments left.",
          "createdAt": "2023-02-27T16:07:33Z",
          "updatedAt": "2023-02-27T16:14:47Z",
          "comments": [
            {
              "originalPosition": 93,
              "body": "nit: This ought to minimize the size of the patch.\r\n```suggestion\r\n1. Finally, if an Aggregator cannot determine if an input share is valid, it\r\n```",
              "createdAt": "2023-02-27T16:07:33Z",
              "updatedAt": "2023-02-27T16:14:47Z"
            },
            {
              "originalPosition": 31,
              "body": "nit (grammar): Most of the items in this list follow the template: \"Check if <SOMETHING BAD HAPPNES>.\" Can we change this to something like \"Check if the aggregation parameter violates the validity scope of the report.\"",
              "createdAt": "2023-02-27T16:12:52Z",
              "updatedAt": "2023-02-27T16:14:47Z"
            },
            {
              "originalPosition": 56,
              "body": "```suggestion\r\n   previous collections for the batch. Checking that no reports are omitted\r\n```",
              "createdAt": "2023-02-27T16:13:37Z",
              "updatedAt": "2023-02-27T16:14:47Z"
            },
            {
              "originalPosition": 63,
              "body": "nit: IMO this doesn't need to be clarified here.\r\n```suggestion\r\n```",
              "createdAt": "2023-02-27T16:14:37Z",
              "updatedAt": "2023-02-27T16:14:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OcGVP",
          "commit": {
            "abbreviatedOid": "15ccb3d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T16:43:25Z",
          "updatedAt": "2023-02-27T16:43:25Z",
          "comments": [
            {
              "originalPosition": 63,
              "body": "I agree with Chris, but also agree with Simon that DAP should be more clear about the lifecycle or state machine of various objects in the protocol, because otherwise it's kinda hard for implementors to figure out stuff like when a helper aggregate share is considered to be collected. I'd like to have a section in DAP where we discuss all that, so that then other parts of the protocol like this could refer to it, and then it'd be clear just what we mean when we say that a report is aggregated or a batch is collected. We talked about that a bit in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/384. In any case, I certainly don't want to solve that problem in this PR. Maybe we could take Simon's text and include a TODO referencing #384?",
              "createdAt": "2023-02-27T16:43:25Z",
              "updatedAt": "2023-02-27T16:43:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OcGjV",
          "commit": {
            "abbreviatedOid": "15ccb3d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM, modulo Chris' observations.",
          "createdAt": "2023-02-27T16:43:54Z",
          "updatedAt": "2023-02-27T16:43:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5OdIma",
          "commit": {
            "abbreviatedOid": "15ccb3d"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T19:02:21Z",
          "updatedAt": "2023-02-27T19:02:22Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "I want to avoid saying \"validity scope\" because I don't think anybody would have an idea what that is supposed to be but I'll reword.",
              "createdAt": "2023-02-27T19:02:21Z",
              "updatedAt": "2023-02-27T19:02:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdMse",
          "commit": {
            "abbreviatedOid": "15ccb3d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T19:14:23Z",
          "updatedAt": "2023-02-27T19:14:24Z",
          "comments": [
            {
              "originalPosition": 31,
              "body": "Yeah make sense :)",
              "createdAt": "2023-02-27T19:14:23Z",
              "updatedAt": "2023-02-27T19:14:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Odd-o",
          "commit": {
            "abbreviatedOid": "9104dc3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-27T20:03:05Z",
          "updatedAt": "2023-02-27T20:03:05Z",
          "comments": []
        }
      ]
    },
    {
      "number": 411,
      "id": "PR_kwDOFEJYQs5KzutI",
      "title": "Add security requirements for verification key.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/411",
      "state": "MERGED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #407 ",
      "createdAt": "2023-02-27T08:46:54Z",
      "updatedAt": "2023-02-28T01:23:47Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "89e5dc32be05ebd4eb032c7f6d0aaa7074c69b08",
      "headRepository": "simon-friedberger/draft-ietf-ppm-dap",
      "headRefName": "407",
      "headRefOid": "ca072163c52daf175ce11c4b8dfeb9b4a952fc31",
      "closedAt": "2023-02-28T01:23:46Z",
      "mergedAt": "2023-02-28T01:23:46Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "52229942c6c1a2e4e264e75aa9a18d3d1feb9c97"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ObxLQ",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-02-27T16:04:12Z",
          "updatedAt": "2023-02-27T16:05:44Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "I think it's worth citing Prio3 as an example, but in the interest of being generic, I think we should add a little more context here.\r\n\r\n```suggestion\r\nThis consideration comes from current security analysis for existing VDAFs. For example,\r\nto ensure that the security proofs for Prio3 hold, the verification key MUST be\r\n```",
              "createdAt": "2023-02-27T16:04:12Z",
              "updatedAt": "2023-02-27T16:05:44Z"
            },
            {
              "originalPosition": 25,
              "body": "I don't think \"coin flipping protocol\" means much to most readers. Do we have a concrete suggestion here? If not, I think we should either cut this suggestion or add an OPEN ISSUE for tracking discussion on this.",
              "createdAt": "2023-02-27T16:05:39Z",
              "updatedAt": "2023-02-27T16:19:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OcIeO",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-02-27T16:48:14Z",
          "updatedAt": "2023-02-27T17:41:03Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "I think we want a MUST here, because the protocol breaks if either aggregator uses a different verification key during the lifetime of a task.\r\n```suggestion\r\nIt MUST be fixed for the lifetime of the task and cannot be rotated. One way\r\n```\r\n",
              "createdAt": "2023-02-27T16:48:14Z",
              "updatedAt": "2023-02-27T17:41:03Z"
            },
            {
              "originalPosition": 25,
              "body": "> if key rotation is necessary.\r\n\r\nWe say above that verification keys cannot be rotated, so it seems like this sentence can be deleted. And I agree with Chris: the specification is to be read by implementers who aren't necessarily cryptography experts and need concrete advice on how to do this. If I remember right, it's sufficient for the Leader to randomly generate a verification key with a CSPRNG and then transmit it to the Helper over a secure channel (like TLS). If we want to provide a concrete recommendation, does that suffice?",
              "createdAt": "2023-02-27T17:40:48Z",
              "updatedAt": "2023-02-27T17:41:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdHoG",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T18:59:33Z",
          "updatedAt": "2023-02-27T18:59:33Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Hm, so this was intended as a default but if both aggregators agree to use a coin flipping protocol instead that would also work. I'm not sure what the typical protocol text for such an OOB agreement would be.\r\n\r\nWe can make this a MUST but then key rotation really cannot be done anymore.",
              "createdAt": "2023-02-27T18:59:33Z",
              "updatedAt": "2023-02-27T18:59:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdLfC",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T19:11:12Z",
          "updatedAt": "2023-02-27T19:11:13Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "That would be sufficient for security, and I think it would be worth RECOMMENDING this. However this is kind of separate from what @simon-friedberger is suggesting here: If we specified some sort of protocol for exchanging the key whereby each Aggregator committed to and provided some entropy, then we might be able weaken the \"don't rotate while running a task\" requirement.",
              "createdAt": "2023-02-27T19:11:12Z",
              "updatedAt": "2023-02-27T19:11:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdL6U",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T19:12:06Z",
          "updatedAt": "2023-02-27T19:12:06Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Let's go with SHOULD for now. A nudge is sufficient for the time being. ",
              "createdAt": "2023-02-27T19:12:06Z",
              "updatedAt": "2023-02-27T19:12:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdM3X",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T19:14:54Z",
          "updatedAt": "2023-02-27T19:14:55Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "Yeah, I was under the impression that VDAF-04 had already forbidden verification key rotation, but that's not the case. Then I agree, we should leave the door open for coin-flipping schemes.",
              "createdAt": "2023-02-27T19:14:54Z",
              "updatedAt": "2023-02-27T19:14:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdNEB",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T19:15:28Z",
          "updatedAt": "2023-02-27T19:15:29Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "Yes, I was jumping the gun on forbidding verify key rotation. This is fine as is.",
              "createdAt": "2023-02-27T19:15:29Z",
              "updatedAt": "2023-02-27T19:15:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdeU7",
          "commit": {
            "abbreviatedOid": "6b253cd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T20:04:15Z",
          "updatedAt": "2023-02-27T20:04:25Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "```suggestion\r\nIt SHOULD be fixed for the lifetime of the task and not be rotated. One way\r\n```",
              "createdAt": "2023-02-27T20:04:15Z",
              "updatedAt": "2023-02-27T20:04:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdfZl",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T20:07:36Z",
          "updatedAt": "2023-02-27T20:07:36Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "Would a reference be sufficient clarification? Maybe this?\r\n> @inproceedings{Blum1981CoinFB,\r\n  title={Coin Flipping by Telephone.},\r\n  author={Manuel Blum},\r\n  booktitle={Annual International Cryptology Conference},\r\n  year={1981}\r\n}\r\n\r\nOr we could add a description of such a protocol, @cjpatton proposed one in #161.",
              "createdAt": "2023-02-27T20:07:36Z",
              "updatedAt": "2023-02-27T20:07:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OdgQS",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T20:10:20Z",
          "updatedAt": "2023-02-27T20:10:21Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "The Blum paper is the first crypto paper I ever read! Very cool, but very outdated. Moreover, I am not ready to endorse the solution we sketched in #161. I think what we would need here is a concrete proposal + security analysis.\r\n\r\nI really think we should do nothing here until we've had a chance to actually work it out.",
              "createdAt": "2023-02-27T20:10:20Z",
              "updatedAt": "2023-02-27T20:10:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OeB82",
          "commit": {
            "abbreviatedOid": "0f85819"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-02-27T21:56:22Z",
          "updatedAt": "2023-02-27T21:56:22Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "Removed.",
              "createdAt": "2023-02-27T21:56:22Z",
              "updatedAt": "2023-02-27T21:56:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5OeGaY",
          "commit": {
            "abbreviatedOid": "ca07216"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-27T22:12:17Z",
          "updatedAt": "2023-02-27T22:12:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Oei-O",
          "commit": {
            "abbreviatedOid": "ca07216"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-28T00:25:50Z",
          "updatedAt": "2023-02-28T00:25:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 412,
      "id": "PR_kwDOFEJYQs5K3ydg",
      "title": "Use the report ID as the nonce during sharding",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/412",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Catches us up with recent VDAF changes ([1], [2], [3]). We also require that VDAFs used in DAP have `NONCE_SIZE = 16`.\r\n\r\n[1]: https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/127\r\n[2]: https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/149\r\n[3]: https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/171\r\n\r\nResolves #394",
      "createdAt": "2023-02-27T21:00:49Z",
      "updatedAt": "2023-02-27T22:12:35Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "89e5dc32be05ebd4eb032c7f6d0aaa7074c69b08",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/client-nonce",
      "headRefOid": "13ac3ad521fe2de9f50685d21c14c78c6e25c856",
      "closedAt": "2023-02-27T22:12:34Z",
      "mergedAt": "2023-02-27T22:12:34Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "a541b41aed5f19bd5e8843b5850a7aff5c1a88e4"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5OdyVW",
          "commit": {
            "abbreviatedOid": "d2ceb32"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-27T21:04:06Z",
          "updatedAt": "2023-02-27T21:04:06Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Ody42",
          "commit": {
            "abbreviatedOid": "d2ceb32"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-02-27T21:05:52Z",
          "updatedAt": "2023-02-27T21:06:15Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "```suggestion\r\n`measurement_to_input_shares` and `prep_init` methods (see {{!VDAF, Section 5}}).\r\n```",
              "createdAt": "2023-02-27T21:05:52Z",
              "updatedAt": "2023-02-27T21:06:15Z"
            }
          ]
        }
      ]
    },
    {
      "number": 415,
      "id": "PR_kwDOFEJYQs5LK4hz",
      "title": "Add security consideration for DoS vector",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/415",
      "state": "MERGED",
      "author": "SulemanAhmadd",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Addresses issue #413 ",
      "createdAt": "2023-03-02T22:04:04Z",
      "updatedAt": "2023-05-15T18:51:45Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a41fa63c6f3c81f50f4f46848a28444bd695cde9",
      "headRepository": "SulemanAhmadd/draft-ietf-ppm-dap",
      "headRefName": "suleman/issue-413",
      "headRefOid": "de9c56e8d3c42584d1330d8877b7eda72e524292",
      "closedAt": "2023-05-15T18:51:44Z",
      "mergedAt": "2023-05-15T18:51:44Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "a6e6c2ea0a966475ee58c512575b81b728c13463"
      },
      "comments": [
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "I think this is too generic to be very useful. It would be better to include this in the VDAF spec next to Poplar1 and mention the specific optimization of early-abort for short URLs.",
          "createdAt": "2023-03-03T09:26:02Z",
          "updatedAt": "2023-03-03T09:26:02Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with @simon-friedberger: A more concrete example would help clarify.\r\n",
          "createdAt": "2023-03-06T18:04:27Z",
          "updatedAt": "2023-03-06T18:04:27Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Thank you folks for the suggestions. I have expanded the scope of the text by adding a representable example but keeping it generic enough to be broadly applicable. I believe, since DAP is the driver for the execution of the VDAF, this consideration seems more relevant to the DAP draft but I am open to redirections.",
          "createdAt": "2023-03-14T18:34:55Z",
          "updatedAt": "2023-03-14T18:34:55Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "CONTRIBUTOR",
          "body": "Thanks, comments have been addressed.",
          "createdAt": "2023-03-21T23:16:44Z",
          "updatedAt": "2023-03-21T23:16:44Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this has been hanging around long enough for any objections to have materialized, so I'm going to merge it. Thank you for your contribution @SulemanAhmadd!",
          "createdAt": "2023-05-15T18:51:40Z",
          "updatedAt": "2023-05-15T18:51:40Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5QjXAt",
          "commit": {
            "abbreviatedOid": "e2ee4b7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks good, just nits!",
          "createdAt": "2023-03-21T21:59:06Z",
          "updatedAt": "2023-03-21T22:02:09Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "The I-D reference need only be defined once.\r\n```suggestion\r\n   instance, the Poplar1 VDAF {{!VDAF}} when\r\n```",
              "createdAt": "2023-03-21T21:59:06Z",
              "updatedAt": "2023-03-21T22:02:09Z"
            },
            {
              "originalPosition": 10,
              "body": "nit: Add blank line between paragraphs.",
              "createdAt": "2023-03-21T21:59:32Z",
              "updatedAt": "2023-03-21T22:02:09Z"
            },
            {
              "originalPosition": 11,
              "body": "```suggestion\r\n   When dealing with variable length inputs (e.g domain names), it is\r\n```",
              "createdAt": "2023-03-21T21:59:48Z",
              "updatedAt": "2023-03-21T22:02:09Z"
            },
            {
              "originalPosition": 15,
              "body": "```suggestion\r\n   for a candidate measurement. For smaller length inputs, this significantly\r\n```",
              "createdAt": "2023-03-21T22:00:08Z",
              "updatedAt": "2023-03-21T22:02:09Z"
            },
            {
              "originalPosition": 16,
              "body": "```suggestion\r\n   reduces the cost of communication between Aggregators and the steps\r\n```",
              "createdAt": "2023-03-21T22:01:47Z",
              "updatedAt": "2023-03-21T22:02:10Z"
            },
            {
              "originalPosition": 17,
              "body": "```suggestion\r\n   required for the computation. However, malicious Clients can still generate\r\n```",
              "createdAt": "2023-03-21T22:01:55Z",
              "updatedAt": "2023-03-21T22:02:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5VEN_B",
          "commit": {
            "abbreviatedOid": "de9c56e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-15T18:50:53Z",
          "updatedAt": "2023-05-15T18:50:53Z",
          "comments": []
        }
      ]
    },
    {
      "number": 416,
      "id": "PR_kwDOFEJYQs5LZlUR",
      "title": "edit: intro: Define the DAP acronym immediately",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/416",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-03-06T18:34:55Z",
      "updatedAt": "2023-03-06T23:48:21Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "52229942c6c1a2e4e264e75aa9a18d3d1feb9c97",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/1",
      "headRefOid": "ce41d518c85864d091369cacb4ab1c2e23a83382",
      "closedAt": "2023-03-06T21:07:42Z",
      "mergedAt": "2023-03-06T21:07:41Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "ab5b5c0c3f644fcd8d02f4ef7a9addbf06726164"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PGwzc",
          "commit": {
            "abbreviatedOid": "ce41d51"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T20:51:39Z",
          "updatedAt": "2023-03-06T20:51:39Z",
          "comments": []
        }
      ]
    },
    {
      "number": 417,
      "id": "PR_kwDOFEJYQs5LZxQ3",
      "title": "edit: conventions: Update terminology to match VDAF",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/417",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #416.\r\n\r\nUpdate the definitions to align better with terminology in the VDAF draft. Also, add \"aggregation paramter\" and \"public \r\nshare\".",
      "createdAt": "2023-03-06T19:15:39Z",
      "updatedAt": "2023-03-06T23:48:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ab5b5c0c3f644fcd8d02f4ef7a9addbf06726164",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/2/terms",
      "headRefOid": "cb59c58c9008336af317f47e75690bb4d405c33b",
      "closedAt": "2023-03-06T21:51:03Z",
      "mergedAt": "2023-03-06T21:51:03Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "fc31fcb373a87be44d59333d38db54a74eca1771"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PGnpo",
          "commit": {
            "abbreviatedOid": "9612d81"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T20:23:48Z",
          "updatedAt": "2023-03-06T20:31:13Z",
          "comments": [
            {
              "originalPosition": 94,
              "body": "I think this is incorrect, because the actual times of reports doesn't affect the validity of a batch, only the definition of the batch interval. (In the most common case, batches will consist of a set of reports with the same client timestamp, due to client-side rounding. Such a batch would be valid, because the collector's query spanned the right amount of time, even though the time difference between the oldest and newest report is zero.) Moreover, this was renamed to \"time precision\" elsewhere in the document, so we should update and realphabetize this glossary entry to match.",
              "createdAt": "2023-03-06T20:23:48Z",
              "updatedAt": "2023-03-06T20:31:13Z"
            },
            {
              "originalPosition": 112,
              "body": "nit: this is out of alphabetical order",
              "createdAt": "2023-03-06T20:25:30Z",
              "updatedAt": "2023-03-06T20:31:13Z"
            },
            {
              "originalPosition": 120,
              "body": "nit: spelling\r\n```suggestion\r\n: A cryptographically protected measurement uploaded to the Leader by a Client.\r\n```",
              "createdAt": "2023-03-06T20:26:00Z",
              "updatedAt": "2023-03-06T20:31:13Z"
            },
            {
              "originalPosition": 87,
              "body": "nit: spelling\r\n```suggestion\r\n  use, multiple values may be grouped into a single measurement. As defined in\r\n```",
              "createdAt": "2023-03-06T20:30:39Z",
              "updatedAt": "2023-03-06T20:31:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PGs1N",
          "commit": {
            "abbreviatedOid": "9612d81"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T20:38:49Z",
          "updatedAt": "2023-03-06T20:38:50Z",
          "comments": [
            {
              "originalPosition": 94,
              "body": "Removed minimum batch duration, since it's out of date. I don't think we need to define time precision here, since it's pretty technical.",
              "createdAt": "2023-03-06T20:38:49Z",
              "updatedAt": "2023-03-06T20:38:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PGyr_",
          "commit": {
            "abbreviatedOid": "2e95509"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-03-06T20:57:53Z",
          "updatedAt": "2023-03-06T21:03:19Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "Should this be `draft-irtf-cfrg-vdaf-04`, or are we deliberately waiting for VDAF-05 to move past -03?",
              "createdAt": "2023-03-06T20:57:53Z",
              "updatedAt": "2023-03-06T21:03:19Z"
            },
            {
              "originalPosition": 50,
              "body": "This doesn't seem right. There's more to a DAP client than VDAF's definition of a client, so perhaps rephrase to explain that a DAP client is also a VDAF client.\r\n\r\nAdditionally, while you're here, referring to the client as an \"endpoint\" is confusing.",
              "createdAt": "2023-03-06T21:00:47Z",
              "updatedAt": "2023-03-06T21:03:19Z"
            },
            {
              "originalPosition": 32,
              "body": "There's more to a DAP aggregator than VDAF's definition of an aggregator, so perhaps rephrase to explain that a DAP aggregator is also a VDAF aggregator.",
              "createdAt": "2023-03-06T21:01:23Z",
              "updatedAt": "2023-03-06T21:03:19Z"
            },
            {
              "originalPosition": 55,
              "body": "There's more to a DAP collector than VDAF's definition of a collector, so perhaps rephrase to explain that a DAP collector is also a VDAF collector.",
              "createdAt": "2023-03-06T21:02:26Z",
              "updatedAt": "2023-03-06T21:03:19Z"
            },
            {
              "originalPosition": 110,
              "body": "```suggestion\r\n  Aggregators. As defined in {{!VDAF}}.\r\n```",
              "createdAt": "2023-03-06T21:03:00Z",
              "updatedAt": "2023-03-06T21:03:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PG2LW",
          "commit": {
            "abbreviatedOid": "2e95509"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T21:09:19Z",
          "updatedAt": "2023-03-06T21:09:20Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "I was thinking we'd wait to bump this until VDAF-05 is cut.",
              "createdAt": "2023-03-06T21:09:19Z",
              "updatedAt": "2023-03-06T21:09:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PG2rt",
          "commit": {
            "abbreviatedOid": "2e95509"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T21:10:59Z",
          "updatedAt": "2023-03-06T21:11:00Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "Are you alright with \"A party that uploads a report?\" I believe the \"endpoint\" language stems from the distinction we made early on between \"user\" and \"client\".",
              "createdAt": "2023-03-06T21:10:59Z",
              "updatedAt": "2023-03-06T21:11:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PG23Q",
          "commit": {
            "abbreviatedOid": "2e95509"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T21:11:37Z",
          "updatedAt": "2023-03-06T21:11:37Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "Done, I just removed the reference.",
              "createdAt": "2023-03-06T21:11:37Z",
              "updatedAt": "2023-03-06T21:11:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PG332",
          "commit": {
            "abbreviatedOid": "20ce90c"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T21:14:51Z",
          "updatedAt": "2023-03-06T21:14:51Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5PHAVv",
          "commit": {
            "abbreviatedOid": "2e95509"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T21:42:06Z",
          "updatedAt": "2023-03-06T21:42:06Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "My problem with \"endpoint\" is that I read it to mean \"a URL relative to which an API can be accessed\", which isn't right in the case of the client. \"A party that uploads a report\" sounds fine.",
              "createdAt": "2023-03-06T21:42:06Z",
              "updatedAt": "2023-03-06T21:42:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHAfW",
          "commit": {
            "abbreviatedOid": "20ce90c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T21:42:35Z",
          "updatedAt": "2023-03-06T21:42:35Z",
          "comments": []
        }
      ]
    },
    {
      "number": 418,
      "id": "PR_kwDOFEJYQs5LZ4td",
      "title": "edit: overview: nits and align with VDAF",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/418",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on #417.",
      "createdAt": "2023-03-06T19:44:17Z",
      "updatedAt": "2023-03-06T23:48:09Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "fc31fcb373a87be44d59333d38db54a74eca1771",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/3/overview",
      "headRefOid": "710cee8ec0faed01e2dc3ef4d413e144887fcf49",
      "closedAt": "2023-03-06T21:56:56Z",
      "mergedAt": "2023-03-06T21:56:56Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e2a289a578ea5b464f5973a86e2f39a13e7013f2"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@cjpatton can you please rebase? Some of the changes we improved on in #417 now appear here.",
          "createdAt": "2023-03-06T21:43:35Z",
          "updatedAt": "2023-03-06T21:43:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased.",
          "createdAt": "2023-03-06T21:53:35Z",
          "updatedAt": "2023-03-06T21:53:35Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PGq8X",
          "commit": {
            "abbreviatedOid": "264cf9f"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T20:34:02Z",
          "updatedAt": "2023-03-06T20:38:25Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "nit: spelling\r\n```suggestion\r\n* Given only a subset of the input shares, it is impossible to deduce the\r\n  plaintext measurement from which they were generated.\r\n```",
              "createdAt": "2023-03-06T20:34:03Z",
              "updatedAt": "2023-03-06T20:38:26Z"
            },
            {
              "originalPosition": 76,
              "body": "nit: typo\r\n```suggestion\r\n  aggregating up their input shares locally into \"aggregate shares\", then\r\n```",
              "createdAt": "2023-03-06T20:34:32Z",
              "updatedAt": "2023-03-06T20:38:26Z"
            },
            {
              "originalPosition": 103,
              "body": "```suggestion\r\n: The entity which wants to aggregate the measurements generated by the\r\n  Clients. Any given measurement task will have a single Collector.\r\n```",
              "createdAt": "2023-03-06T20:35:34Z",
              "updatedAt": "2023-03-06T20:38:26Z"
            },
            {
              "originalPosition": 190,
              "body": "```suggestion\r\nplaintext measurement.\r\n```",
              "createdAt": "2023-03-06T20:38:03Z",
              "updatedAt": "2023-03-06T20:38:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PG05Z",
          "commit": {
            "abbreviatedOid": "df7a219"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-03-06T21:05:11Z",
          "updatedAt": "2023-03-06T21:08:08Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "\"collection of Clients\" is awkward here. It's a collection of their inputs, right?",
              "createdAt": "2023-03-06T21:05:11Z",
              "updatedAt": "2023-03-06T21:08:08Z"
            },
            {
              "originalPosition": 75,
              "body": "```suggestion\r\n* It allows the Aggregators to compute the aggregation function by first\r\n```",
              "createdAt": "2023-03-06T21:05:48Z",
              "updatedAt": "2023-03-06T21:08:08Z"
            },
            {
              "originalPosition": 102,
              "body": "```suggestion\r\n: The entity which wants to obtain the aggregate of the measurements generated by the\r\n```\r\nIt'd be awkward to suggest that the collector does the aggregating, because that's the aggregators' job.",
              "createdAt": "2023-03-06T21:06:13Z",
              "updatedAt": "2023-03-06T21:08:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PG3tv",
          "commit": {
            "abbreviatedOid": "df7a219"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T21:14:22Z",
          "updatedAt": "2023-03-06T21:14:23Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "Yes, I was aiming for minimal change, but this seems like a sensible change. Changed to \"set of\".",
              "createdAt": "2023-03-06T21:14:22Z",
              "updatedAt": "2023-03-06T21:14:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHE1z",
          "commit": {
            "abbreviatedOid": "710cee8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T21:55:06Z",
          "updatedAt": "2023-03-06T21:55:06Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5PHFnR",
          "commit": {
            "abbreviatedOid": "710cee8"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T21:57:45Z",
          "updatedAt": "2023-03-06T21:57:45Z",
          "comments": []
        }
      ]
    },
    {
      "number": 419,
      "id": "PR_kwDOFEJYQs5LaXu3",
      "title": "edit: transport: nits",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/419",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-03-06T21:32:44Z",
      "updatedAt": "2023-03-06T23:48:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ab5b5c0c3f644fcd8d02f4ef7a9addbf06726164",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/4/transport",
      "headRefOid": "852cba1875fbcd7cbf013f1ff0bb323da0a80920",
      "closedAt": "2023-03-06T21:57:10Z",
      "mergedAt": "2023-03-06T21:57:10Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b4111aa9030514f6d85893a94a85ba281a0a0f26"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PG_0y",
          "commit": {
            "abbreviatedOid": "852cba1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T21:40:18Z",
          "updatedAt": "2023-03-06T21:40:18Z",
          "comments": []
        }
      ]
    },
    {
      "number": 420,
      "id": "PR_kwDOFEJYQs5Ladtr",
      "title": "edit: protocol: nits up to upload flow",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/420",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-03-06T21:56:18Z",
      "updatedAt": "2023-03-06T23:48:05Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b4111aa9030514f6d85893a94a85ba281a0a0f26",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/5/protocol/1",
      "headRefOid": "99a01889686ef6e8a9b2cc7c0d725852f2559a62",
      "closedAt": "2023-03-06T23:46:02Z",
      "mergedAt": "2023-03-06T23:46:02Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e1011ba22754a8f7a5a650698bf3394ff16d5b29"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PHkYo",
          "commit": {
            "abbreviatedOid": "3ea35b9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T23:36:28Z",
          "updatedAt": "2023-03-06T23:36:43Z",
          "comments": [
            {
              "originalPosition": 33,
              "body": "nit: put the section reference inside the `{{}}` as elsewhere in the draft.",
              "createdAt": "2023-03-06T23:36:28Z",
              "updatedAt": "2023-03-06T23:36:43Z"
            }
          ]
        }
      ]
    },
    {
      "number": 421,
      "id": "PR_kwDOFEJYQs5LamrZ",
      "title": "edit: protocol: nits up to aggregation flow",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/421",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-03-06T22:24:54Z",
      "updatedAt": "2023-03-06T23:48:09Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b4111aa9030514f6d85893a94a85ba281a0a0f26",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/6/protocol/2/upload",
      "headRefOid": "e10040d977af6c5d0b42e47501e9b918aef98a13",
      "closedAt": "2023-03-06T23:45:07Z",
      "mergedAt": "2023-03-06T23:45:06Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0360471f85006db59b827e4e4a2833dc4271c78b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PHOO_",
          "commit": {
            "abbreviatedOid": "e10040d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T22:25:36Z",
          "updatedAt": "2023-03-06T22:25:37Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "Reviewer note: At some point in the past we called this the \"key configuration\" and didn't pick up the change here.",
              "createdAt": "2023-03-06T22:25:37Z",
              "updatedAt": "2023-03-06T22:25:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHOq-",
          "commit": {
            "abbreviatedOid": "e10040d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T22:26:56Z",
          "updatedAt": "2023-03-06T22:27:00Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "Reviewer note: As of VDAF-04, Prio3 has a non-empty public share (the joint randomness parts).",
              "createdAt": "2023-03-06T22:26:56Z",
              "updatedAt": "2023-03-06T22:27:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHlVV",
          "commit": {
            "abbreviatedOid": "e10040d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-06T23:39:30Z",
          "updatedAt": "2023-03-06T23:39:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 422,
      "id": "PR_kwDOFEJYQs5La_d2",
      "title": "edit: aggregation nits and distinguish prep_msg from prep_share",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/422",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Continue editorial pass up to the collection flow. Along, clarify an\r\nambiguity in the mapping of VDAF preparation to DAP aggregation\r\ninitialization/continuation.\r\n\r\nIn particular, `VDAF.prep_next()` is meant to take in the \"preperation\r\nmessage\" from the previous round and output either (1) the Aggregator's\r\noutput share or (2) the Aggregator's next preparation sate and next\r\n\"preparation message-share\".\r\n\r\nWe are currently using the term \"inbound message\" for the former and\r\n\"outbound message\" for the latter. This seems to be causing us to\r\nconflate \"preparation message\" with \"preparation message-share\", which\r\nmay result in misunderstanding the protocol flow.",
      "createdAt": "2023-03-06T23:52:46Z",
      "updatedAt": "2023-10-26T15:44:13Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "e1011ba22754a8f7a5a650698bf3394ff16d5b29",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/edit/7/protocol/3/aggregate",
      "headRefOid": "136072ebfead699082d3b724f1ff9ee4ce6ccd04",
      "closedAt": "2023-03-07T20:41:02Z",
      "mergedAt": "2023-03-07T20:41:02Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "7764b10b7e16474ffcfec94857db8571d16f86ff"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Reviewer note: This change is meant to be strictly editorial, so I'm hoping it'll be straightforward to review. That said, it's quite big, so I'd be happy to split it up if desired.",
          "createdAt": "2023-03-07T00:12:27Z",
          "updatedAt": "2023-03-07T00:12:27Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PHqTp",
          "commit": {
            "abbreviatedOid": "081b960"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T23:54:10Z",
          "updatedAt": "2023-03-06T23:54:11Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "Reviewer note: I think we have already resolved this.",
              "createdAt": "2023-03-06T23:54:10Z",
              "updatedAt": "2023-03-07T00:13:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHq06",
          "commit": {
            "abbreviatedOid": "081b960"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T23:57:18Z",
          "updatedAt": "2023-03-06T23:57:18Z",
          "comments": [
            {
              "originalPosition": 189,
              "body": "Reviewer note: I believe this pertains to checking for distinctness of report IDs in the request: If so, this check does not go in {{input-share-validation}}, since that section is really about about per-report checks.",
              "createdAt": "2023-03-06T23:57:18Z",
              "updatedAt": "2023-03-06T23:57:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PHrKM",
          "commit": {
            "abbreviatedOid": "081b960"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-06T23:59:21Z",
          "updatedAt": "2023-03-06T23:59:21Z",
          "comments": [
            {
              "originalPosition": 275,
              "body": "Reviewer note: I have tried to rework with this chunk so that the actual thing to check is mentioned in the first sentence. @simon-friedberger please double check that I haven't inadvertently changed the meaning here.",
              "createdAt": "2023-03-06T23:59:21Z",
              "updatedAt": "2023-03-06T23:59:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PH1rp",
          "commit": {
            "abbreviatedOid": "1409f0f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-03-07T00:43:56Z",
          "updatedAt": "2023-03-07T00:57:26Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "```suggestion\r\nbegin the process of verifying and aggregating them with the Helpers. To enable the system to handle very large batches of reports, this process can\r\n```",
              "createdAt": "2023-03-07T00:43:57Z",
              "updatedAt": "2023-03-07T00:57:26Z"
            },
            {
              "originalPosition": 165,
              "body": "```suggestion\r\nconveyed by this message, the Helper attempts to initialize VDAF preparation\r\n```",
              "createdAt": "2023-03-07T00:49:48Z",
              "updatedAt": "2023-03-07T00:57:26Z"
            },
            {
              "originalPosition": 311,
              "body": "```suggestion\r\n`agg_param` is the opaque aggregation parameter distributed to the Aggregators\r\n```",
              "createdAt": "2023-03-07T00:51:48Z",
              "updatedAt": "2023-03-07T00:57:26Z"
            },
            {
              "originalPosition": 386,
              "body": "I don't like the \"interprets `out` as follows\" phrasing, and I deliberately changed it the last time I was in this paragraph. The reason is that the text you have suggests that the leader is supposed to check what round it currently is (\"[I]f this is the last round of the VDAF\") and then interpret `out` accordingly. That's backwards: the way that the Leader knows it's on the last round is if `VDAF.prep_next` returns an output share. It's not up to DAP to decide if we're on the last round of the VDAF or if `out` is an output share. VDAF makes that decision.",
              "createdAt": "2023-03-07T00:55:39Z",
              "updatedAt": "2023-03-07T00:57:26Z"
            },
            {
              "originalPosition": 415,
              "body": "```suggestion\r\nthe previous message round's prepare message (carried by the AggregationJobReq)\r\n```",
              "createdAt": "2023-03-07T00:56:11Z",
              "updatedAt": "2023-03-07T00:57:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PH5yj",
          "commit": {
            "abbreviatedOid": "9064c86"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-07T01:08:17Z",
          "updatedAt": "2023-03-07T01:08:17Z",
          "comments": [
            {
              "originalPosition": 386,
              "body": "Hmm. can you take another look? I haven't changes this phrasing AFAICT. In particular the following pattern is in both paragraphs:\r\n> If either of these operations fails, then ... . Otherwise, it interprets `out` as follows: If this is the last round of the VDAF, ... . Otherwise, `out` is the pair ...\r\n\r\nAlternatively, do you have a suggestion of how you think this should be worded?",
              "createdAt": "2023-03-07T01:08:17Z",
              "updatedAt": "2023-03-07T01:08:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5POnJG",
          "commit": {
            "abbreviatedOid": "9064c86"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-07T18:10:07Z",
          "updatedAt": "2023-03-07T18:10:08Z",
          "comments": [
            {
              "originalPosition": 386,
              "body": "Sorry, I think I got my wires crossed while reviewing this. You did indeed not change the wording here. I may or may not have a point about the interpretation thing, but we don't have to address that in this change.",
              "createdAt": "2023-03-07T18:10:07Z",
              "updatedAt": "2023-03-07T18:10:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5POnS8",
          "commit": {
            "abbreviatedOid": "9064c86"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-07T18:10:32Z",
          "updatedAt": "2023-03-07T18:10:32Z",
          "comments": []
        }
      ]
    },
    {
      "number": 423,
      "id": "PR_kwDOFEJYQs5LbNs1",
      "title": "edit: protocol: nits up to considerations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/423",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-03-07T00:56:40Z",
      "updatedAt": "2023-10-26T15:44:13Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "e1011ba22754a8f7a5a650698bf3394ff16d5b29",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/edit/9/protocol/4/collect",
      "headRefOid": "39a4de48c6ffb4db333f268f47de849668437940",
      "closedAt": "2023-03-07T20:40:45Z",
      "mergedAt": "2023-03-07T20:40:45Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "82d7530503a9b5ae1916d3f13dd1dd15aebd7441"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PH4FH",
          "commit": {
            "abbreviatedOid": "37d0a40"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2023-03-07T00:58:30Z",
          "updatedAt": "2023-03-07T01:04:17Z",
          "comments": [
            {
              "originalPosition": 51,
              "body": "```suggestion\r\nquery and be ready to return the aggregate shares right away. However, this is\r\n```",
              "createdAt": "2023-03-07T00:58:30Z",
              "updatedAt": "2023-03-07T01:04:17Z"
            },
            {
              "originalPosition": 57,
              "body": "```suggestion\r\ncollection job. This is because, in general, the aggregation parameter is not\r\nknown until this point. In certain situations it is possible to predict the\r\naggregation parameter in advance. For example, for Prio3 the only valid\r\naggregation parameter is the empty string. For these reasons, the collection\r\n```\r\n\r\nParentheticals, especially multi-sentence ones, are a text smell, IMO. Either these sentences are important enough for the reader to read them, and should be integrated into the text, or they're not important, and should be deleted.",
              "createdAt": "2023-03-07T00:58:54Z",
              "updatedAt": "2023-03-07T01:04:17Z"
            },
            {
              "originalPosition": 70,
              "body": "```suggestion\r\nThe Leader then begins working with the Helper to aggregate the reports\r\n```\r\nUnless you meant \"Leper\"",
              "createdAt": "2023-03-07T00:59:45Z",
              "updatedAt": "2023-03-07T01:04:17Z"
            },
            {
              "originalPosition": 90,
              "body": "```suggestion\r\nThe Leader obtains each Helper's aggregate share following the\r\naggregate-share request flow described in {{collect-aggregate}}. When all\r\n```\r\n\r\nI think?",
              "createdAt": "2023-03-07T01:00:33Z",
              "updatedAt": "2023-03-07T01:04:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5POnbC",
          "commit": {
            "abbreviatedOid": "e428e84"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-07T18:10:54Z",
          "updatedAt": "2023-03-07T18:10:54Z",
          "comments": []
        }
      ]
    },
    {
      "number": 424,
      "id": "PR_kwDOFEJYQs5Lg8LN",
      "title": "Bump version tag to \"dap-04\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/424",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Partially addresses #404.\r\n\r\nThe next draft will have breaking changes, so bump the version prefix.",
      "createdAt": "2023-03-07T20:45:58Z",
      "updatedAt": "2023-10-26T15:44:15Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7764b10b7e16474ffcfec94857db8571d16f86ff",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/1/version",
      "headRefOid": "65df4f42893f72f8d7ccd7b1d28922ae5bc9b804",
      "closedAt": "2023-03-07T22:58:11Z",
      "mergedAt": "2023-03-07T22:58:11Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "74b145c8fcd388c45136724c8877dbaf9170b663"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PQSil",
          "commit": {
            "abbreviatedOid": "65df4f4"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-07T22:44:19Z",
          "updatedAt": "2023-03-07T22:44:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 425,
      "id": "PR_kwDOFEJYQs5Lg-g3",
      "title": "Add random input to sharding algorithm",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/425",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "As of draft-irtf-cfrg-vdaf-05 (not yet published), the random coins used for sharding are an explicit parameter of\r\n`Vdaf.measurement_to_input_shares()`. Add this parameter and explasin how it is generated.",
      "createdAt": "2023-03-07T20:53:42Z",
      "updatedAt": "2023-10-26T15:44:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7764b10b7e16474ffcfec94857db8571d16f86ff",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-04/2/vdaf-05",
      "headRefOid": "be82880ab0dac35f16cbd0d7d3b9ed0bb5726c93",
      "closedAt": "2023-03-07T22:58:30Z",
      "mergedAt": "2023-03-07T22:58:30Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "021adec439b968051c002a58cb9eef2d5a3c2c49"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@divergentdave I believe this is the only API change we need to account for, correct?",
          "createdAt": "2023-03-07T20:54:08Z",
          "updatedAt": "2023-03-07T20:54:08Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PPvNu",
          "commit": {
            "abbreviatedOid": "7a3e48c"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-07T21:01:42Z",
          "updatedAt": "2023-03-07T21:02:47Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "```suggestion\r\nClient MUST generate this using a cryptographically secure random number\r\n```",
              "createdAt": "2023-03-07T21:01:42Z",
              "updatedAt": "2023-03-07T21:02:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PPwKK",
          "commit": {
            "abbreviatedOid": "7a3e48c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-07T21:04:09Z",
          "updatedAt": "2023-03-07T21:04:10Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "Fine if you wanna be all nit-picky about it :D",
              "createdAt": "2023-03-07T21:04:10Z",
              "updatedAt": "2023-03-07T21:04:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PQSyW",
          "commit": {
            "abbreviatedOid": "be82880"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-07T22:45:14Z",
          "updatedAt": "2023-03-07T22:45:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 427,
      "id": "PR_kwDOFEJYQs5Ls9gT",
      "title": "Harmonize terminology.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/427",
      "state": "MERGED",
      "author": "bhalleycf",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "All references to \"metadata\" are now to \"report_metadata\".\r\nReferences to \"CollectReq\" are now \"CollectionReq\".\r\nReferences to \"CollectResp\" are now \"Collection\".",
      "createdAt": "2023-03-09T18:02:14Z",
      "updatedAt": "2023-03-10T22:21:07Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "021adec439b968051c002a58cb9eef2d5a3c2c49",
      "headRepository": "bhalleycf/draft-ietf-ppm-dap",
      "headRefName": "main",
      "headRefOid": "1cf640ef70dc27bdf84af7a5d1676d4ab40f2529",
      "closedAt": "2023-03-10T16:22:04Z",
      "mergedAt": "2023-03-10T16:22:04Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b45d4821b541b383ef8350bd8d04b1b37921a812"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Pey-l",
          "commit": {
            "abbreviatedOid": "1cf640e"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-09T18:17:30Z",
          "updatedAt": "2023-03-09T18:17:30Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5PezqX",
          "commit": {
            "abbreviatedOid": "1cf640e"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-09T18:18:52Z",
          "updatedAt": "2023-03-09T18:18:52Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5PlQ2e",
          "commit": {
            "abbreviatedOid": "1cf640e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks a bunch @bhalleycf!",
          "createdAt": "2023-03-10T16:21:55Z",
          "updatedAt": "2023-03-10T16:21:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 428,
      "id": "PR_kwDOFEJYQs5LzRa6",
      "title": "Forbid `AggregationJobContinueReq.round = 0`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/428",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "During implementation of round skew recovery in Janus ([1]), we noticed that the specification is unclear about what the helper should do if `AggregationJobContinueReq.round = 0`. This commit clarifies that this value is illegal and should be rejected.\r\n\r\n[1]: https://github.com/divviup/janus/pull/1091\r\n\r\n===\r\n\r\nAfter handling `AggregationJobInitReq`, the helper's aggregation job will be on round 0, because the helper is waiting for the leader to send it the first-round broadcast prepare message, with which helper will advance to round 1. So before this commit, the applicable statement in the text was:\r\n\r\n> If the `round` in the Leader's request is equal to the Helper's current round\r\n> (i.e., this is not the first time the Leader has sent this request), then the\r\n> Helper responds with the current round's prepare messages. The Helper SHOULD\r\n> verify that the contents of the `AggregationJobContinueReq` are identical to the\r\n> previous message (see {{aggregation-round-skew-recovery}}).\r\n\r\nWhich means the helper would have to retransmit its first-round prepare shares, which it had previously sent in response to `AggregationJobInitReq`. This is implementable but weird: if the leader dropped the response to `AggregationJobInitReq`, then it should recover by re-sending it (I just wasted months of my life making sure that PUT to an aggregation job is idempotent). It seems simpler to me to just forbid round to be 0.",
      "createdAt": "2023-03-10T21:06:12Z",
      "updatedAt": "2023-03-13T16:09:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b45d4821b541b383ef8350bd8d04b1b37921a812",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-agg-job-continue-round-0",
      "headRefOid": "56a0309036a9af00c8e5d6bc0223d9f61f4c0849",
      "closedAt": "2023-03-13T16:09:58Z",
      "mergedAt": "2023-03-13T16:09:57Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "068f4a3b0d98a0c6f1844ab85391dc8a7c9437aa"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PnYQ7",
          "commit": {
            "abbreviatedOid": "56a0309"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-10T22:55:48Z",
          "updatedAt": "2023-03-10T22:55:48Z",
          "comments": []
        }
      ]
    },
    {
      "number": 429,
      "id": "PR_kwDOFEJYQs5LzdJp",
      "title": "Update VDAF reference to draft 05",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/429",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "And while we're at it, update an obsolete reference to Oblivious HTTP, which has long since been adopted by its WG.\r\n\r\n===\r\n\r\nVDAF-05 isn't expected until Monday 3/13, but we can get this change ready to go.",
      "createdAt": "2023-03-10T22:08:40Z",
      "updatedAt": "2023-03-13T16:10:10Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b45d4821b541b383ef8350bd8d04b1b37921a812",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-vdaf-05-sync",
      "headRefOid": "8249eb2af81b466332f10eb945f262a743ff0e0f",
      "closedAt": "2023-03-13T16:10:10Z",
      "mergedAt": "2023-03-13T16:10:10Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "04b909ab06e2646210e068fb4e128beffe3b6caa"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PnYZN",
          "commit": {
            "abbreviatedOid": "8249eb2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-10T22:56:44Z",
          "updatedAt": "2023-03-10T22:56:44Z",
          "comments": []
        }
      ]
    },
    {
      "number": 430,
      "id": "PR_kwDOFEJYQs5LzhlY",
      "title": "Add changelog for DAP-04",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/430",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "We need #429 for this change to make sense.",
      "createdAt": "2023-03-10T22:26:28Z",
      "updatedAt": "2023-03-13T16:11:55Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b45d4821b541b383ef8350bd8d04b1b37921a812",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-changelog",
      "headRefOid": "2af6a9359b72b31c22b1abc34692a70cbc13c82c",
      "closedAt": "2023-03-13T16:11:54Z",
      "mergedAt": "2023-03-13T16:11:54Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "a41fa63c6f3c81f50f4f46848a28444bd695cde9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PnYw8",
          "commit": {
            "abbreviatedOid": "93d5b33"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "For better alignment with previous diffs, could you make each sentence begin with a command, sort of like commit messages? E.g., instead of \"Resource oriented API\"  something like \"Change API endpoints to ...\" ",
          "createdAt": "2023-03-10T22:59:21Z",
          "updatedAt": "2023-03-10T22:59:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Pndgr",
          "commit": {
            "abbreviatedOid": "2af6a93"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-10T23:30:07Z",
          "updatedAt": "2023-03-10T23:30:11Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "I would add a sentence or explaining what \"resource oriented\" means and the motivation for this change. ",
              "createdAt": "2023-03-10T23:30:07Z",
              "updatedAt": "2023-03-10T23:30:11Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5PneD-",
          "commit": {
            "abbreviatedOid": "2af6a93"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-03-10T23:35:20Z",
          "updatedAt": "2023-03-10T23:35:20Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "There's ample discussion of that in the referenced issue and PRs. I don't think it's productive to re-hash it here.",
              "createdAt": "2023-03-10T23:35:20Z",
              "updatedAt": "2023-03-10T23:35:20Z"
            }
          ]
        }
      ]
    },
    {
      "number": 431,
      "id": "PR_kwDOFEJYQs5LzicC",
      "title": "correct spelling errors",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/431",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-04"
      ],
      "body": "",
      "createdAt": "2023-03-10T22:31:36Z",
      "updatedAt": "2023-03-10T23:28:18Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b45d4821b541b383ef8350bd8d04b1b37921a812",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/dap-04-spellcheck",
      "headRefOid": "15109cbea2a27291d661ed3c0e09493a9087a81a",
      "closedAt": "2023-03-10T23:28:18Z",
      "mergedAt": "2023-03-10T23:28:17Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "8ffbc9b589b029d4793ff10b0724910934dc39e9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5PnYIY",
          "commit": {
            "abbreviatedOid": "15109cb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-03-10T22:54:50Z",
          "updatedAt": "2023-03-10T22:54:50Z",
          "comments": []
        }
      ]
    },
    {
      "number": 434,
      "id": "PR_kwDOFEJYQs5N-a-m",
      "title": "experiments: Evaluate the network time for PR #393",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/434",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Create a tool for estimating the total network time for an aggregation job with and without PR #393.",
      "createdAt": "2023-04-10T23:41:56Z",
      "updatedAt": "2023-10-26T15:44:17Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a41fa63c6f3c81f50f4f46848a28444bd695cde9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/cost-of-fast-path",
      "headRefOid": "c9a9c64ee3c1596b81838712052889cf84f9a0cd",
      "closedAt": "2023-06-12T15:18:07Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [],
      "reviews": []
    },
    {
      "number": 439,
      "id": "PR_kwDOFEJYQs5PCdfD",
      "title": "Add an overview of the new aggregation flow",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/439",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "",
      "createdAt": "2023-04-24T20:00:54Z",
      "updatedAt": "2023-10-26T15:44:17Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a41fa63c6f3c81f50f4f46848a28444bd695cde9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/393-overview",
      "headRefOid": "e02064122d3cfcd386d932764bd79a529d7d2b63",
      "closedAt": "2023-04-27T03:01:10Z",
      "mergedAt": "2023-04-27T03:01:10Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "007b4e7a0fe00d1cc88302c64cd38b5f574aa72b"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "For consistency with the rest of the document (once PR#393 lands), you may wish to say \"preparation message/share\" rather than \"prep message/share\". (alternatively, I can fix this up in PR#393 to use \"prep\" instead of \"preparation\".)",
          "createdAt": "2023-04-24T22:37:50Z",
          "updatedAt": "2023-04-24T22:37:50Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> For consistency with the rest of the document (once PR#393 lands), you may wish to say \"preparation message/share\" rather than \"prep message/share\". (alternatively, I can fix this up in PR#393 to use \"prep\" instead of \"preparation\".)\r\n\r\nI think I would prefer the shortened version -- \"preparation\" is a bit of a mouthful when reading through the spec.",
          "createdAt": "2023-04-24T22:43:50Z",
          "updatedAt": "2023-04-24T22:43:50Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5TX3hF",
          "commit": {
            "abbreviatedOid": "2ffdbe6"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Seems sound to me. In particular I think the distinction between refinement and verification is helpful.",
          "createdAt": "2023-04-24T20:44:03Z",
          "updatedAt": "2023-04-24T20:49:48Z",
          "comments": [
            {
              "originalPosition": 89,
              "body": "nit:\r\n```suggestion\r\n   Poplar1, the output shares are required to sum up to a vector that is non-zero in at most one position.\r\n```\r\nWe don't need to define the term \"one-hot\" unless we're going to use it elsewhere in this document.",
              "createdAt": "2023-04-24T20:44:03Z",
              "updatedAt": "2023-04-24T20:49:48Z"
            },
            {
              "originalPosition": 94,
              "body": "```suggestion\r\nshares themselves: if preparation succeeds, then the resulting output shares are\r\n```",
              "createdAt": "2023-04-24T20:44:22Z",
              "updatedAt": "2023-04-24T20:49:48Z"
            },
            {
              "originalPosition": 179,
              "body": "```suggestion\r\n  prep share for the next round.\r\n```",
              "createdAt": "2023-04-24T20:47:00Z",
              "updatedAt": "2023-04-24T20:49:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5TYWbj",
          "commit": {
            "abbreviatedOid": "6ed22bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-04-24T22:33:56Z",
          "updatedAt": "2023-04-24T22:34:55Z",
          "comments": [
            {
              "originalPosition": 183,
              "body": "```suggestion\r\ntold there there are `ceil((ROUNDS+1)/2)` HTTP requests sent, where `ROUNDS` is\r\n```\r\n\r\nnit: typo",
              "createdAt": "2023-04-24T22:33:57Z",
              "updatedAt": "2023-04-24T22:34:55Z"
            }
          ]
        }
      ]
    },
    {
      "number": 440,
      "id": "PR_kwDOFEJYQs5PQbls",
      "title": "Example continuous, monotonic, equal batches",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/440",
      "state": "MERGED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "The originals were not (a value was repeated)",
      "createdAt": "2023-04-27T04:31:54Z",
      "updatedAt": "2023-04-27T16:33:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "007b4e7a0fe00d1cc88302c64cd38b5f574aa72b",
      "headRepository": "martinthomson/ppm-dap",
      "headRefName": "contiguous-intervals",
      "headRefOid": "21a1d71c59bf8ca31b169a54b0abd22b74420392",
      "closedAt": "2023-04-27T16:33:52Z",
      "mergedAt": "2023-04-27T16:33:52Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "d83a7d7448458abf6619005269c176a6722a08b8"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Ttm4o",
          "commit": {
            "abbreviatedOid": "21a1d71"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-04-27T16:32:07Z",
          "updatedAt": "2023-04-27T16:32:07Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5TtngQ",
          "commit": {
            "abbreviatedOid": "21a1d71"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-04-27T16:33:42Z",
          "updatedAt": "2023-04-27T16:33:42Z",
          "comments": []
        }
      ]
    },
    {
      "number": 447,
      "id": "PR_kwDOFEJYQs5PQ4kO",
      "title": "Only use helper endpoints",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/447",
      "state": "MERGED",
      "author": "martinthomson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Aggregator = leader + helpers; don't overuse it.",
      "createdAt": "2023-04-27T06:51:14Z",
      "updatedAt": "2023-04-28T19:47:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "007b4e7a0fe00d1cc88302c64cd38b5f574aa72b",
      "headRepository": "martinthomson/ppm-dap",
      "headRefName": "helper-endpoint",
      "headRefOid": "b5e2872faea2b24fb31d46f80b1a2c290fdab2ef",
      "closedAt": "2023-04-28T19:47:32Z",
      "mergedAt": "2023-04-28T19:47:31Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b662903de6997cce3bf811a3c4d618210ae0e0a6"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe all this language gets cleaned up in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/393",
          "createdAt": "2023-04-27T16:15:34Z",
          "updatedAt": "2023-04-27T16:15:34Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5T1Z_n",
          "commit": {
            "abbreviatedOid": "b5e2872"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udc4d ",
          "createdAt": "2023-04-28T19:47:20Z",
          "updatedAt": "2023-04-28T19:47:20Z",
          "comments": []
        }
      ]
    },
    {
      "number": 454,
      "id": "PR_kwDOFEJYQs5PaiWK",
      "title": "Clarify how resource URIs are constructed",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/454",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #441.",
      "createdAt": "2023-04-28T20:20:13Z",
      "updatedAt": "2023-10-26T15:44:18Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b662903de6997cce3bf811a3c4d618210ae0e0a6",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/441-uri-template",
      "headRefOid": "425db17ac4922389797d4d2800d1172afc961d5f",
      "closedAt": "2023-05-01T16:41:31Z",
      "mergedAt": "2023-05-01T16:41:31Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "a6f6b16ef5875a49e220fd7b84c8fd03ad0de3c5"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Please have a look @martinthomson!",
          "createdAt": "2023-04-28T20:20:24Z",
          "updatedAt": "2023-04-28T20:20:24Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed.",
          "createdAt": "2023-05-01T16:03:47Z",
          "updatedAt": "2023-05-01T16:03:47Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5T3nJP",
          "commit": {
            "abbreviatedOid": "b1ac630"
          },
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "Much improved, thanks.",
          "createdAt": "2023-04-30T10:12:51Z",
          "updatedAt": "2023-04-30T10:15:29Z",
          "comments": [
            {
              "originalPosition": 47,
              "body": "```suggestion\r\nhttps://example.com/tasks/8BY0RzZMzxvA46_8ymhzycOB9krN-QIGYvg_RsByGec/reports\r\n```",
              "createdAt": "2023-04-30T10:12:51Z",
              "updatedAt": "2023-04-30T10:15:29Z"
            },
            {
              "originalPosition": 65,
              "body": "It is a bit odd to have this structure.  I would have said global = `{aggregator}/hpke_config` and task-specific = `{aggregator}/tasks/{task-id}/hpke_config`.  Though I suspect that achieving consistency would be harder for the latter form, if you ever had to worry about that.",
              "createdAt": "2023-04-30T10:13:40Z",
              "updatedAt": "2023-04-30T10:15:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5T6Omv",
          "commit": {
            "abbreviatedOid": "b1ac630"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-01T16:01:33Z",
          "updatedAt": "2023-05-01T16:01:34Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "Nice idea! I'll create an issue to track.",
              "createdAt": "2023-05-01T16:01:33Z",
              "updatedAt": "2023-05-01T16:01:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5T6Pd0",
          "commit": {
            "abbreviatedOid": "b1ac630"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-01T16:04:30Z",
          "updatedAt": "2023-05-01T16:04:31Z",
          "comments": [
            {
              "originalPosition": 65,
              "body": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/459",
              "createdAt": "2023-05-01T16:04:30Z",
              "updatedAt": "2023-05-01T16:04:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5T6XOM",
          "commit": {
            "abbreviatedOid": "425db17"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-01T16:34:31Z",
          "updatedAt": "2023-05-01T16:34:31Z",
          "comments": []
        }
      ]
    },
    {
      "number": 457,
      "id": "PR_kwDOFEJYQs5PbDPE",
      "title": "Require unique HpkeConfigIds. Fixes #448",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/457",
      "state": "MERGED",
      "author": "ekr",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #448.\r\n",
      "createdAt": "2023-04-28T22:56:17Z",
      "updatedAt": "2023-05-01T19:01:31Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b662903de6997cce3bf811a3c4d618210ae0e0a6",
      "headRepository": "ekr/draft-ietf-ppm-dap",
      "headRefName": "issue448_unique_ids",
      "headRefOid": "ad17f4a4cd7748a6a3d003f53b90ecd2657ac56d",
      "closedAt": "2023-05-01T19:01:30Z",
      "mergedAt": "2023-05-01T19:01:30Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "d7437d52387cdfbb18a38e80503984dd3984bc15"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5T2IAo",
          "commit": {
            "abbreviatedOid": "2c2ade1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udc4d ",
          "createdAt": "2023-04-28T23:01:54Z",
          "updatedAt": "2023-04-28T23:01:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5T6xg2",
          "commit": {
            "abbreviatedOid": "2c2ade1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-01T18:16:59Z",
          "updatedAt": "2023-05-01T18:17:25Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "```suggestion\r\nAggregators MUST allocate distinct id values for each `HpkeConfig` in an\r\n```",
              "createdAt": "2023-05-01T18:16:59Z",
              "updatedAt": "2023-05-01T18:17:25Z"
            }
          ]
        }
      ]
    },
    {
      "number": 458,
      "id": "PR_kwDOFEJYQs5PbKjx",
      "title": "Don't RECOMMEND how to generate IDs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/458",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #452.\r\n\r\nThe current recommendation is to generate it using a \"cryptographically secure random number generator\", which is usually reserved for purposes that need a uniform random value. All we need is uniqueness; the purpose of this text is merely to suggest a convenient way of meeting the requirement.\r\n\r\nTo avoid confusion, just drop the recommendation altogether.",
      "createdAt": "2023-04-29T00:06:19Z",
      "updatedAt": "2023-10-26T15:44:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d7437d52387cdfbb18a38e80503984dd3984bc15",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/452-unique-ids",
      "headRefOid": "7d45da084cbc6561ce8686df9abea12222b7b534",
      "closedAt": "2023-05-01T19:54:50Z",
      "mergedAt": "2023-05-01T19:54:49Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "eeb6264c3ac4f5c3f8b2722061ed576e07e27011"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2023-05-01T19:03:18Z",
          "updatedAt": "2023-05-01T19:54:46Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5T60jd",
          "commit": {
            "abbreviatedOid": "3927905"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\"Scope\" feels more natural than \"context\" when discussing uniqueness of a name to me, but feel free to overrule me, especially if others disagree.",
          "createdAt": "2023-05-01T18:28:54Z",
          "updatedAt": "2023-05-01T18:29:54Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "```suggestion\r\n   scope of the corresponding DAP task.\r\n```",
              "createdAt": "2023-05-01T18:28:54Z",
              "updatedAt": "2023-05-01T18:29:54Z"
            },
            {
              "originalPosition": 31,
              "body": "```suggestion\r\nThis ID value MUST be unique within the scope of the corresponding DAP task.\r\n```",
              "createdAt": "2023-05-01T18:29:05Z",
              "updatedAt": "2023-05-01T18:29:54Z"
            }
          ]
        }
      ]
    },
    {
      "number": 461,
      "id": "PR_kwDOFEJYQs5PoL4I",
      "title": "Encapsulate details of VDAF preparation into \"prep steps\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/461",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Closes #451.\r\n\r\nThe VDAF spec distinguishes between two types of intermediate values computed and exchanged amongst the Aggregators: \"prep shares\" and \"prep messages\". This distinction is largely irrelevant in the current aggregation flow (i.e., that of draft04), since the Leader always sends the prep message to the Helper and the Helper always sends its prep share to the Leader.\r\n\r\nIn the proposed \"ping-pong\" flow (PR #393), the distinction is important because the Leader and Helper take turns sending these values. As a result, whether the host is processing a prep share or prep message depends on which step of preparation is being computed.\r\n\r\nIn PR #393 (by @branlwyd), these details are encapsulated by the `PrepareStep` message, which defines a variant for each combination of values encountered during the aggregation flow. In order to simplify the overview, this change hoists this encapsulation up to the overview section, thereby removing the low details that are irrelevant to the overall structure.\r\n\r\nWhile at it, this change defines two procedures, `DAPPrepInit()` and `DAPPrepNext()`, that specify explicitly the prep step processing rules. Once we merge this PR, we will rewrite #393 to use these functions. Something like this:\r\n1. During aggregation initialization, each Aggregator decrypts its input share, performs the early validation steps, then runs `DAPPrepInit()`. Then each Aggregator runs `DAPPrepNext(inbound)` where `inbound == None` in case of the Leader and, in case of the Helper, `inbound` is the the prep step sent by the Leader in the first request.\r\n2. During aggregation continuation, each Aggregator calls `DAPPrepNext(inbound)`, where `inbound` is the prep step sent from the peer. ",
      "createdAt": "2023-05-02T23:10:22Z",
      "updatedAt": "2023-10-26T15:44:20Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a6e6c2ea0a966475ee58c512575b81b728c13463",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/451/1/vdaf-prep-wrapper",
      "headRefOid": "8bb5abd4c8aabc2a442ecd296f453bd7746f203d",
      "closedAt": "2023-05-31T23:05:04Z",
      "mergedAt": "2023-05-31T23:05:04Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f755bce9555e43d58926235be0352d5ab7651150"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2023-05-11T15:17:01Z",
          "updatedAt": "2023-05-11T15:17:01Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased!",
          "createdAt": "2023-05-22T17:35:41Z",
          "updatedAt": "2023-05-22T17:35:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I have moved the wrapper code to a PR against VDAF: https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240",
          "createdAt": "2023-05-23T00:41:23Z",
          "updatedAt": "2023-05-23T00:41:23Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> [cfrg/draft-irtf-cfrg-vdaf#240](https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240) looks to be close to ready. I'd like to propose that we merge this PR as-is then start reworking #393 to use the new API there. (We'll need to update the references to VDAF once we cut a new version, but I think we can wait to do that until we finish #393.) What do you think @ekr, @branlwyd?\r\n\r\nMerging this PR sounds good to me.",
          "createdAt": "2023-05-26T02:07:41Z",
          "updatedAt": "2023-05-26T02:07:41Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm going to merge this today.",
          "createdAt": "2023-05-31T19:40:16Z",
          "updatedAt": "2023-05-31T19:40:16Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5UCt06",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-02T23:12:19Z",
          "updatedAt": "2023-05-02T23:12:19Z",
          "comments": [
            {
              "originalPosition": 185,
              "body": "@branlwyd: Please weigh in on weather there is a clean way to integrate this structure into #393. I think we definitely not avoiding repeating report IDs, either to avoid sending extra bits or having an extra validation step (pick your poison!).",
              "createdAt": "2023-05-02T23:12:19Z",
              "updatedAt": "2023-05-02T23:12:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5UCt8G",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-02T23:13:07Z",
          "updatedAt": "2023-05-02T23:13:08Z",
          "comments": [
            {
              "originalPosition": 255,
              "body": "@branlwyd: Please double check that both of these transition rules align with your understanding. Also let me know if you think this framing of things will actually help simplify the description of the aggregation flow, which is what it is intended to do.",
              "createdAt": "2023-05-02T23:13:07Z",
              "updatedAt": "2023-05-02T23:16:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5UCuIk",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-02T23:14:23Z",
          "updatedAt": "2023-05-02T23:14:23Z",
          "comments": [
            {
              "originalPosition": 353,
              "body": "Reviewer note: I'm not sure if we want to keep the figure below. It was helpful for working out the state machine details, but it has a lot of redundant information (cf. {{agg-flow}}).",
              "createdAt": "2023-05-02T23:14:23Z",
              "updatedAt": "2023-05-02T23:14:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5UH1tK",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I left a few comments. One meta-comment is that, if the original problem was confusion between prep messages & prep shares, we haven't actually resolved the issue -- DAP implementors still have to care about the distinction between the two opaque message types (and prep state), place the correct opaque messages in the correct fields of DAP messages, etc. Truly fixing DAP to just work on opaque VDAF messages would take a change to VDAF to e.g. introduce a 2-party interface & special messages for that interface, which is an undesirable level of complexity to add to VDAF.\r\n\r\nIf we can come up with a nice encapsulation that works as a simplification, that is separately valuable (but see my comments -- I think the encapsulation currently has a flaw, and I don't see a way to fix it without either making the encapsulation leaky or much more complicated).",
          "createdAt": "2023-05-03T16:53:24Z",
          "updatedAt": "2023-05-03T17:51:54Z",
          "comments": [
            {
              "originalPosition": 185,
              "body": "This is equivalent to the PrepareStep defined in #393, except that the `report_share` is removed from the `initialize` case & the `report_id` is removed from the remaining cases.\r\n\r\nThe non-`initialize` cases can be worked with: I would create a new intermediate message that contains both a `ReportId` and a `PrepareStep` and use that as the per-report message in an aggregation message. The `initialize` case can do the same, with an intermediate message containing a `ReportShare` and a `PrepareStep`; but given that this message would only be used for the initialize case, I would be tempted to just use a custom message for initialization & not use `PrepareStep` at all.\r\n\r\nBut I struggle to see the value added by the additional layer of indirection -- I think things would be easier to work with if we kept the `ReportShare`/`ReportId` in the `PrepareStep`, so that we don't have to define additional messages to carry these values.",
              "createdAt": "2023-05-03T16:53:25Z",
              "updatedAt": "2023-05-03T17:51:54Z"
            },
            {
              "originalPosition": 281,
              "body": "This method would then need to call `prep_next(host_prep_state, None)` as is currently done during initialization: https://www.ietf.org/archive/id/draft-ietf-ppm-dap-04.html#name-input-share-preparation",
              "createdAt": "2023-05-03T17:03:44Z",
              "updatedAt": "2023-05-03T17:51:54Z"
            },
            {
              "originalPosition": 281,
              "body": "I think the intent might be to move this `prep_next` call to https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/461/files#diff-8dbbcb0d121b441734a70cc867271480731a494b50eefdd758c249fdfb5ab84eR1347-R1350 -- if so, this works out for Helper initialization, but would leave the Leader needing to make an additional `prep_next` call after `VdafPrepInit` but before sending its aggregation-initialization message to the Helper.\r\n\r\nThat's not ideal since users of the rest of the document now need to care about both the \"wrapper\" interface and the normal VDAF interface; that is, the encapsulation of the VDAF interface is leaking.\r\n\r\nUnfortunately, if we do specify `VdafPrepInit` as including the initial `prep_next` call, this leaves the Helper initialization as requiring a `VdafPrepInit` followed by a function similar to `VdafPrepNext` except that the initial `prep_next` call is omitted. (that is, it would call `prep_shares_to_prep` followed by `prep_next`) I think having to specify another arbitrary wrapping function would probably tip this to being a complexity loss -- maybe there's some clever way to cleanly include this case into `VdafPrepNext`?",
              "createdAt": "2023-05-03T17:30:14Z",
              "updatedAt": "2023-05-03T17:51:54Z"
            },
            {
              "originalPosition": 312,
              "body": "Should this be an error? I'm not sure we want to pass a `finished` or `reject` PrepareStep to this method.",
              "createdAt": "2023-05-03T17:31:02Z",
              "updatedAt": "2023-05-03T17:51:54Z"
            },
            {
              "originalPosition": 345,
              "body": "As noted in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/451#issuecomment-1531741943, writing things this way effectively requires the Leader to store both the `prep_msg` and the `prep_share` to durable storage, barring implementors effectively splitting this function up to run part of it in one DAP round and part of it in another (which IMO may be a stretch to expect of implementors). #393 is written in a way that stores only `prep_msg`.\r\n\r\nMaybe we can include an implementation note that the final `prep_next` call can actually be made after storing `prep_msg` to storage, for implementations that \"checkpoint\" between rounds?",
              "createdAt": "2023-05-03T17:39:27Z",
              "updatedAt": "2023-05-03T17:51:54Z"
            },
            {
              "originalPosition": 353,
              "body": "I don't object to keeping this information around, but I think this ladder diagram should be merged with the one above (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/461/files#diff-8dbbcb0d121b441734a70cc867271480731a494b50eefdd758c249fdfb5ab84eR1107-R1138).",
              "createdAt": "2023-05-03T17:42:34Z",
              "updatedAt": "2023-05-03T17:51:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5UInIk",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-03T18:23:23Z",
          "updatedAt": "2023-05-03T18:23:23Z",
          "comments": [
            {
              "originalPosition": 281,
              "body": "Thinking about this a little more, another approach would to update `VdafPrepInit` to compute `prep_next` conditionally, based on whether it is called by the Leader or the Helper. This function would return different kinds of results based on the role of the caller, but this may still be a relatively-clean way to fix up this issue.",
              "createdAt": "2023-05-03T18:23:23Z",
              "updatedAt": "2023-05-03T18:23:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5UWucm",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-05T18:14:46Z",
          "updatedAt": "2023-05-05T18:27:24Z",
          "comments": [
            {
              "originalPosition": 353,
              "body": "Removed.",
              "createdAt": "2023-05-05T18:14:46Z",
              "updatedAt": "2023-05-05T18:27:24Z"
            },
            {
              "originalPosition": 185,
              "body": "The value would be the ability to describe the state machine without having to loop in the decryption and early validation steps.",
              "createdAt": "2023-05-05T18:16:28Z",
              "updatedAt": "2023-05-05T18:27:24Z"
            },
            {
              "originalPosition": 281,
              "body": "The intent is that the Leader can call `VdafPrepInit()`, then immediately call `VdafPrepNext(inbound=None)` to produce the first prep step. The Helper would call `VdafPrepInit()`, then immediately call `VdafPrepNext(inbound=peer_prep_step)`. Then in subsequent rounds, each just calls `VdafPrepNext()`.\r\n\r\nSo aggregation initialization becomes:\r\n1. Decrypt the report share\r\n1. Do early validation (report replay, etc.)\r\n1. Call `VdafPrepInit()`\r\n2. Call `VdafPrepNext(inbound=None)` if Leader else `VdafPrepNext(inbound=peer_prep_step)`.\r\n3. Send prep step produced from step (2.).\r\n\r\nAggregation continuation becomes:\r\n1. Call `VdafPrepNext(inbound=peer_prep_step)`.\r\n2. Send the prep step produced.",
              "createdAt": "2023-05-05T18:22:55Z",
              "updatedAt": "2023-05-05T18:27:24Z"
            },
            {
              "originalPosition": 312,
              "body": "The first time the Leader calls `VdafPrepNext()` it is immediately after calling `VdafPrepInit()`. It has not received a prep_msg or prep share from its peer, so there's nothing to do. The outbound message on this call is the leader's first prep share.",
              "createdAt": "2023-05-05T18:24:20Z",
              "updatedAt": "2023-05-05T18:27:24Z"
            },
            {
              "originalPosition": 345,
              "body": "> As noted in [#451 (comment)](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/451#issuecomment-1531741943), writing things this way effectively requires the Leader to store both the `prep_msg` and the `prep_share` to durable storage, barring implementors effectively splitting this function up to run part of it in one DAP round and part of it in another (which IMO may be a stretch to expect of implementors). #393 is written in a way that stores only `prep_msg`.\r\n\r\nI don't quite understand what you mean by durable storage. The intent of these methods is to describe the semantics of 393, i.e., I don't expect the protocol flow to be different.\r\n\r\n\r\n",
              "createdAt": "2023-05-05T18:27:02Z",
              "updatedAt": "2023-05-05T18:27:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Ums0V",
          "commit": {
            "abbreviatedOid": "82708b4"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-09T20:46:40Z",
          "updatedAt": "2023-05-09T20:46:40Z",
          "comments": [
            {
              "originalPosition": 281,
              "body": "Based on in-person discussion, this decomposition works:\r\n* Leader initialization is DapPrepInit -> DapPrepNext -> send/recv -> DapPrepNext.\r\n* Helper initialization is DapPrepInit -> DapPrepNext.\r\n* Leader continuation is send/recv -> DapPrepNext.\r\n* Helper continuation is DapPrepNext.",
              "createdAt": "2023-05-09T20:46:40Z",
              "updatedAt": "2023-05-09T20:46:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Um3wO",
          "commit": {
            "abbreviatedOid": "2ec360d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-09T21:22:10Z",
          "updatedAt": "2023-05-09T21:22:10Z",
          "comments": [
            {
              "originalPosition": 345,
              "body": "I think what is returned by `DapPrepNext` is effectively what a leader stores for retries -- `DapPrepNext` is the last step for leader initialization & continuation, and the result of that call is what is sent to the helper at the beginning of the next round of continuation.\r\n\r\nBut an implementation could effectively \"pause\" computation of `DapPrepNext` before the final `VDAF.prep_next` call, and only need to keep around the `prep_msg`, recomputing the `host_prep_share` if/when a retry occurs. This would help implementations that checkpoint each round to durable storage to e.g. survive unexpected process failures.\r\n\r\nThis is an implementation choice, so I don't think we need to make a change, but an implementation note would be helpful as I think otherwise it would be challenging for implementors to realize this optimization is possible.",
              "createdAt": "2023-05-09T21:22:10Z",
              "updatedAt": "2023-05-09T21:22:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5UnH7-",
          "commit": {
            "abbreviatedOid": "6ee3550"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-09T22:37:13Z",
          "updatedAt": "2023-05-09T22:37:13Z",
          "comments": [
            {
              "originalPosition": 345,
              "body": "(As discussed offline) The plan here will be to first try to make this point clear in the normative protocol text. Let's keep this section focused on the state machine.",
              "createdAt": "2023-05-09T22:37:13Z",
              "updatedAt": "2023-05-09T22:37:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5U2C8H",
          "commit": {
            "abbreviatedOid": "921dd45"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I traced through a 1- and 2-round VDAF and I think this works, both in terms of VDAF-message dataflow and round/state tracking.",
          "createdAt": "2023-05-11T21:20:37Z",
          "updatedAt": "2023-05-22T17:35:32Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5U9skk",
          "commit": {
            "abbreviatedOid": "11440dc"
          },
          "author": "ekr",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-13T20:50:01Z",
          "updatedAt": "2023-05-13T20:58:04Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "Is this saying:\r\n\"Send an AggregationJobInitReq consisting of[agg_param, report_share, and the output of prep_step()]\"\r\n\r\nIf so, maybe\r\n```suggestion\r\n   | AggregationJobInitReq:                           |\r\n   |    agg_param, report_share, prep_step(initialize)      |\r\n```\r\ne",
              "createdAt": "2023-05-13T20:50:01Z",
              "updatedAt": "2023-05-13T20:58:04Z"
            },
            {
              "originalPosition": 114,
              "body": "I don't understand what this is trying to say. These are arguments to some `prep_step()` function? How does the caller know what argument to provide? Why can't the encapsulated VDAF just do the right thing?\r\n\r\nI'm deliberately not referring to the VDAF spec here, because the idea here is that you should be able to read this bit and not know that.\r\n\r\n\r\n\r\n\r\n",
              "createdAt": "2023-05-13T20:55:34Z",
              "updatedAt": "2023-05-13T20:58:04Z"
            },
            {
              "originalPosition": 194,
              "body": "I still don't understand why this information has to be surfaced at this level.\r\n\r\nGiven that this message is self-contained, why can't this message be what's passed to the VDAF and let it figure out what to do. Please don't tell me that that's how the VDAF API works, because I am suggesting a different API.",
              "createdAt": "2023-05-13T20:56:41Z",
              "updatedAt": "2023-05-13T20:58:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5VFGyo",
          "commit": {
            "abbreviatedOid": "11440dc"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-15T21:35:02Z",
          "updatedAt": "2023-05-15T21:35:02Z",
          "comments": [
            {
              "originalPosition": 194,
              "body": "My understanding for the reason for this complexity is:\r\n* DAP is moving towards specifying to exactly two aggregators (this work is part of that effort).\r\n* VDAF is currently specified to an arbitrary number of aggregators.\r\n\r\n... so somewhere we need to specify how to adapt from the arbitrary-aggregator VDAF interface to the 2-aggregator DAP protocol, in a way that unlocks the performance benefit allowed by 2-aggregator DAP. This PR contains that complexity, and attempts to encapsulate it into the DAPPrepInit/DAPPrepNext functions (replacing/encapsulating the similar VDAF-interface VDAF.prep_init & VDAF.prep_next functions).\r\n\r\nIf we want to avoid this complexity entirely in DAP, the most likely change would be to the VDAF specification, either to specify VDAF to 2 aggregators or to specify a second VDAF interface which is equivalent to the 2-aggregator DAPPrepInit/DAPPrepNext interface described in this PR. I'm less confident to speak to changes to VDAF, but VDAF editors have expressed a desire to maintain that VDAF can work with an arbitrary number of aggregators. Perhaps we should consider specifying the functionality of this PR in VDAF as an additional interface? (I do not hold a strong opinion about where the better place for this complexity is, I'm just highlighting that that seems to be the tradeoff.)",
              "createdAt": "2023-05-15T21:35:02Z",
              "updatedAt": "2023-05-15T21:43:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5VqOx3",
          "commit": {
            "abbreviatedOid": "cc7775e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-22T17:37:55Z",
          "updatedAt": "2023-05-22T17:49:07Z",
          "comments": [
            {
              "originalPosition": 85,
              "body": "That's right. Done.",
              "createdAt": "2023-05-22T17:37:56Z",
              "updatedAt": "2023-05-22T17:49:07Z"
            },
            {
              "originalPosition": 114,
              "body": "You call `DAPPrepInit()` and `DAPPrepNext()` to do the right thing, which are defined in this document. These are actually wrapper functions around the VDAF API (i.e., `VDAF.prep_init()`, `VDAF.prep_next()`, and `VDAF.prep_shares_to_prep()`) that are specific to the 2-Aggregator, Leader-Helper structure of DAP. (VDAFs still support >2 Aggregators.)\r\n\r\nI think we have to specify these wrapper methods somewhere. I'd be fine moving them to VDAF if you think that would be better.",
              "createdAt": "2023-05-22T17:45:30Z",
              "updatedAt": "2023-05-22T17:49:07Z"
            },
            {
              "originalPosition": 194,
              "body": "It's easy enough to move the wrapper code to VDAF, I'll create a PR there today.",
              "createdAt": "2023-05-22T17:47:11Z",
              "updatedAt": "2023-05-22T17:49:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5VtY75",
          "commit": {
            "abbreviatedOid": "11440dc"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-05-23T00:42:36Z",
          "updatedAt": "2023-05-23T00:42:37Z",
          "comments": [
            {
              "originalPosition": 194,
              "body": "https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240",
              "createdAt": "2023-05-23T00:42:36Z",
              "updatedAt": "2023-05-23T00:42:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5WHVxp",
          "commit": {
            "abbreviatedOid": "8bb5abd"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-25T23:13:57Z",
          "updatedAt": "2023-05-25T23:13:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5WHh3L",
          "commit": {
            "abbreviatedOid": "8bb5abd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/240 looks to be close to ready. I'd like to propose that we merge this PR as-is then start reworking #393 to use the new API there. (We'll need to update the references to VDAF once we cut a new version, but I think we can wait to do that until we finish #393.) What do you think @ekr, @branlwyd?",
          "createdAt": "2023-05-26T00:47:52Z",
          "updatedAt": "2023-05-26T00:47:52Z",
          "comments": []
        }
      ]
    },
    {
      "number": 462,
      "id": "PR_kwDOFEJYQs5PtxCo",
      "title": "Provide request/response media types inline",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/462",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This adds mention of media types throughout the document where they were missing. Fixes #449.",
      "createdAt": "2023-05-03T21:15:52Z",
      "updatedAt": "2023-05-09T11:19:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "eeb6264c3ac4f5c3f8b2722061ed576e07e27011",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "david/media-types",
      "headRefOid": "b0c15131bef8c71028adbb55ccc16e3d083c2f9f",
      "closedAt": "2023-05-09T11:19:32Z",
      "mergedAt": "2023-05-09T11:19:32Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "4130afe23dc29eb2f88b9f908230d7acfb942b95"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5UKFon",
          "commit": {
            "abbreviatedOid": "b0c1513"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-03T21:43:15Z",
          "updatedAt": "2023-05-03T21:43:15Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Ui_zb",
          "commit": {
            "abbreviatedOid": "b0c1513"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-09T11:19:23Z",
          "updatedAt": "2023-05-09T11:19:23Z",
          "comments": []
        }
      ]
    },
    {
      "number": 463,
      "id": "PR_kwDOFEJYQs5P4OVI",
      "title": "Add batchOverlap to error table",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/463",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "This adds the batchOverlap error type, which is mentioned in section 4.6.6, to table 1.",
      "createdAt": "2023-05-05T16:47:01Z",
      "updatedAt": "2023-05-05T18:13:00Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "eeb6264c3ac4f5c3f8b2722061ed576e07e27011",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "david/error-table-batchOverlap",
      "headRefOid": "e0bc9402a95f7fe4c70f749b9ca00ef0127ba030",
      "closedAt": "2023-05-05T18:13:00Z",
      "mergedAt": "2023-05-05T18:13:00Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "48e3bcd51b55491800f101b22e2c84929f520069"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5UWS71",
          "commit": {
            "abbreviatedOid": "e0bc940"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-05T16:52:57Z",
          "updatedAt": "2023-05-05T16:52:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5UWUdt",
          "commit": {
            "abbreviatedOid": "e0bc940"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-05T16:57:22Z",
          "updatedAt": "2023-05-05T16:57:22Z",
          "comments": []
        }
      ]
    },
    {
      "number": 464,
      "id": "PR_kwDOFEJYQs5QittH",
      "title": "Use `invalidMessage` instead of `unrecognized`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/464",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The error type `unrecognizedMessage` and the `ReportShareError` variant `unrecognized_message` are changed to `invalidMessage` and `invalid_message`, respectively, as this better captures the range of conditions in which this error is used: some messages are recognizable in that a protocol participant can parse them, but invalid in that the values they contain are illegal.\r\n\r\nResolves #453",
      "createdAt": "2023-05-15T18:49:18Z",
      "updatedAt": "2023-05-22T22:08:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "4130afe23dc29eb2f88b9f908230d7acfb942b95",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/invalid-message",
      "headRefOid": "6fc24ebc25b9fb18856ecb695f707f8a4019d2ab",
      "closedAt": "2023-05-22T22:08:38Z",
      "mergedAt": "2023-05-22T22:08:38Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "a431ac9eaa2907f8977fcd7d028b3e77b0f44888"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This change has been up here for a week, the discussion about it was going on for two weeks prior to that in #453 and it has approval from two DAP editors, and nobody has objected, so I'm going to merge.",
          "createdAt": "2023-05-22T22:08:34Z",
          "updatedAt": "2023-05-22T22:08:34Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5VMLuN",
          "commit": {
            "abbreviatedOid": "6fc24eb"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-16T19:20:54Z",
          "updatedAt": "2023-05-16T19:20:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5VqZhe",
          "commit": {
            "abbreviatedOid": "6fc24eb"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-22T18:04:58Z",
          "updatedAt": "2023-05-22T18:04:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 465,
      "id": "PR_kwDOFEJYQs5REZxw",
      "title": "Remove queryMismatch, in favor of invalidMessage.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/465",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "The aggregator behavior on receiving this error is the same either way, and there are many other message-based errors that do not get their own code. This change effectively leaves it up to implementations to decide the level of detail they include in error reports.",
      "createdAt": "2023-05-22T21:54:15Z",
      "updatedAt": "2023-11-27T23:10:15Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "a431ac9eaa2907f8977fcd7d028b3e77b0f44888",
      "headRepository": null,
      "headRefName": "bran/rm-query-mismatch-error",
      "headRefOid": "924ac3d1f228ed57689c741214a76b5fbe2137c7",
      "closedAt": "2023-05-31T23:05:20Z",
      "mergedAt": "2023-05-31T23:05:20Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f006e450641aa1703c89f9c26cc3ddc6c9b2f9a8"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> I support this change, and agree that it'll be even better as \"invalidMessage\". I've just merged #464, so could you please rebase?\r\n\r\nRebased.",
          "createdAt": "2023-05-22T22:55:26Z",
          "updatedAt": "2023-05-22T22:55:26Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "No objection but one word of warning: There are now multiple reasons why the Helper would abort with \"invalidMessage\". It's a good idea to at least disambiguate the reason in the problem details document when aborting.",
          "createdAt": "2023-05-22T23:00:04Z",
          "updatedAt": "2023-05-22T23:00:04Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Vsp3V",
          "commit": {
            "abbreviatedOid": "fdbf023"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I support this change, and agree that it'll be even better as \"invalidMessage\". I've just merged #464, so could you please rebase?",
          "createdAt": "2023-05-22T22:09:05Z",
          "updatedAt": "2023-05-22T22:09:05Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Vs5Wu",
          "commit": {
            "abbreviatedOid": "924ac3d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-22T22:57:18Z",
          "updatedAt": "2023-05-22T22:57:18Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5V3tvV",
          "commit": {
            "abbreviatedOid": "924ac3d"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-05-23T22:47:17Z",
          "updatedAt": "2023-05-23T22:47:17Z",
          "comments": []
        }
      ]
    },
    {
      "number": 468,
      "id": "PR_kwDOFEJYQs5S1DMy",
      "title": "Idempotent aggregate shares endpoint",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/468",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Require that the helper's `/tasks/{task-id}/aggregate_shares` endpoint be idempotent.\r\n\r\nResolves #226",
      "createdAt": "2023-06-13T00:22:30Z",
      "updatedAt": "2023-06-16T21:51:33Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c5868ea9854cef7379cef6712aa2d702888f01c9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/idempotent-aggregate-share",
      "headRefOid": "cec73e093d85ad25f88ede61839d0763d50ec80e",
      "closedAt": "2023-06-16T21:51:33Z",
      "mergedAt": "2023-06-16T21:51:33Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "b48f13ff9131c4388a8c39e966c2ba84d248b7ed"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5X-fOB",
          "commit": {
            "abbreviatedOid": "1f9d562"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-13T00:25:12Z",
          "updatedAt": "2023-06-13T00:25:13Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I think a MUST (rather than a SHOULD) is appropriate here. If this is optional, then there's no way for the leader to recover from losing the first response to `POST /tasks/{task-id}/aggregate_shares`, because the helper can refuse to service subsequent aggregate share requests due to exhaustion of the max batch query count.",
              "createdAt": "2023-06-13T00:25:13Z",
              "updatedAt": "2023-06-13T00:25:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YFiaM",
          "commit": {
            "abbreviatedOid": "418c1ea"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-13T18:44:11Z",
          "updatedAt": "2023-06-15T22:37:57Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "I think this is roughly equivalent to saying that `max_batch_query_count` consumption is based on _distinct_ `AggregateShareReq`s, which is what we want, so \ud83d\udc4d\ud83c\udffb.",
              "createdAt": "2023-06-13T18:44:11Z",
              "updatedAt": "2023-06-13T18:44:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YFpBN",
          "commit": {
            "abbreviatedOid": "418c1ea"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-13T18:59:14Z",
          "updatedAt": "2023-06-15T22:37:56Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YXUmL",
          "commit": {
            "abbreviatedOid": "418c1ea"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-15T23:19:23Z",
          "updatedAt": "2023-06-15T23:24:33Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "```suggestion\r\nThe Helper's handling of this request MUST be idempotent. That is, if multiple\r\n```",
              "createdAt": "2023-06-15T23:19:23Z",
              "updatedAt": "2023-06-15T23:24:33Z"
            }
          ]
        }
      ]
    },
    {
      "number": 469,
      "id": "PR_kwDOFEJYQs5S1H8F",
      "title": "Be explicit about task param immutability",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/469",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "I've satisfied myself by reading the document that nothing implies that task params can be mutated or rotated, so all we need is a paragraph explicitly stating this.\r\n\r\nResolves #237",
      "createdAt": "2023-06-13T00:44:21Z",
      "updatedAt": "2023-06-16T21:51:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c5868ea9854cef7379cef6712aa2d702888f01c9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/immutable-task-params",
      "headRefOid": "e326eb4f28a859a066715fdf9f02048bbbbe16b5",
      "closedAt": "2023-06-16T21:51:18Z",
      "mergedAt": "2023-06-16T21:51:18Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "7c56ee33a4db36354d2c924a94170aa6a15f18cb"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5YFimA",
          "commit": {
            "abbreviatedOid": "e326eb4"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-13T18:44:42Z",
          "updatedAt": "2023-06-15T22:38:31Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YFlkv",
          "commit": {
            "abbreviatedOid": "e326eb4"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-13T18:50:14Z",
          "updatedAt": "2023-06-15T22:38:31Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YXVZs",
          "commit": {
            "abbreviatedOid": "e326eb4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Good idea.",
          "createdAt": "2023-06-15T23:25:14Z",
          "updatedAt": "2023-06-15T23:25:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 471,
      "id": "PR_kwDOFEJYQs5S777n",
      "title": "Explicitly forbid mutating agg jobs, collections",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/471",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Resolves #406",
      "createdAt": "2023-06-13T23:52:50Z",
      "updatedAt": "2023-06-21T16:45:17Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "d0d7ddd00fcdfa708b2f1fe7a04f5377023f87a7",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/mutating-the-immutable",
      "headRefOid": "bf7c5856a7d9d16674395b47889ab0df841d3d31",
      "closedAt": "2023-06-21T16:45:16Z",
      "mergedAt": "2023-06-21T16:45:16Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "ace9a83d202e0d425e998f635db783e335ba824b"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "After thinking about this further, and having re-read what RFC 9110 already says about idempotence, safety and the different HTTP verbs, I think this change isn't necessary. We already explicitly reference RFC 9110 and it's not DAP's job to re-iterate HTTP concepts. Further, standards that build on HTTP like ACME get by without being so explicit about immutability and idempotence, so I don't think we need to go farther than that protocol does.",
          "createdAt": "2023-06-16T21:41:57Z",
          "updatedAt": "2023-06-16T21:43:41Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I've refocused the change to be about the DAP level semantics and to avoid restating HTTP semantics, which is confusing to readers. RFC 9110 among others does a better job of explaining HTTP and its methods than DAP can.",
          "createdAt": "2023-06-20T16:31:35Z",
          "updatedAt": "2023-06-20T16:31:35Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5YNIxT",
          "commit": {
            "abbreviatedOid": "1df6370"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-14T16:06:25Z",
          "updatedAt": "2023-06-14T16:07:33Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "A few things to consider including:\r\n* Mention that \"further requests with a different body\" would be grouped together by aggregation job ID (I suppose).\r\n* We don't want to allow re-initialization -- so if a continuation step has occurred, we should no longer allow initialization.\r\n\r\n(& identical suggestions for the collection job case, with aggregation job ID -> collection job ID)",
              "createdAt": "2023-06-14T16:06:25Z",
              "updatedAt": "2023-06-14T16:07:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNm5B",
          "commit": {
            "abbreviatedOid": "1df6370"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T17:20:07Z",
          "updatedAt": "2023-06-14T17:20:07Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": ">Mention that \"further requests with a different body\" would be grouped together by aggregation job ID (I suppose).\r\n\r\nI think I know what you mean. Is this paragraph more clear?\r\n\r\n```\r\nAggregation job initialization MUST be idempotent. That is, further requests to\r\ninitialize some aggregation job whose body is identical to the first request\r\nMUST succeed. However, changing an aggregation job's parameters is illegal, so\r\nfurther requests to initialize an existing aggregation job with different parameters\r\nMUST fail with an HTTP client error status code.\r\n```\r\n\r\n>We don't want to allow re-initialization -- so if a continuation step has occurred, we should no longer allow initialization.\r\n\r\nSo we're talking about a sequence of requests like this:\r\n\r\n1. `PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}`\r\n2. `POST /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}`\r\n3. `PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}`\r\n\r\nWhen you say we should no longer allow initialization, do you mean that the request at (3) should fail with something like HTTP 400 Bad Request, or just that it shouldn't rewind or reset the state of the aggregation job? I think that the request at (3) should still get a 201 Created response, because in many cases, the leader can then follow up with a `POST` to continue the aggregation job and gracefully resume processing the job.",
              "createdAt": "2023-06-14T17:20:07Z",
              "updatedAt": "2023-06-14T17:20:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YNzMg",
          "commit": {
            "abbreviatedOid": "1df6370"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-14T17:53:08Z",
          "updatedAt": "2023-06-14T17:53:09Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "> Mention that \"further requests with a different body\" would be grouped together by aggregation job ID (I suppose).\r\n\r\nI'm suggesting that the text \"further requests to initialize an existing aggregation job with different parameters\" is ambiguous, because it does not define how to determine if a new request matches to an existing aggregation job. I think the right way to determine this is by aggregation job ID -- if the request includes an aggregation job ID matching to an existing aggregation job, it is a repeated request (possibly with different parameters); if the request includes a unique aggregation job ID, it is a request for a new aggregation job. My suggestion is that we state that explicitly.\r\n\r\n\r\n> When you say we should no longer allow initialization, do you mean that the request at (3) should fail with something like HTTP 400 Bad Request, or just that it shouldn't rewind or reset the state of the aggregation job?\r\n\r\nYes -- IMO request (3) should fail [and it shouldn't rewind/reset the state of the aggregation job]. I would suggest we don't allow re-initialization of existing aggregation jobs because we don't need to: if a further \"continuation\" request has been made, the Leader has definitely received the result of the prior \"initialization\" step. And if we allow aggregation jobs to be re-initialized, we will have to deal with \"weird\" issues like \"what if the Leader drives an agg job all the way to completion, then re-initializes it, then drives it to completion once again -- should the output shares recovered by the second completion contribute to the eventual aggregate 'again'?\" (probably not, but we'd need text to that effect, and implementations would have to add some complexity to deal with such issues)\r\n\r\nOr, framing things in terms of idempotency, request (2) changes the state of the aggregation job, so afterwards re-sending request (1) has no expectation of idempotency, as it is a request to rewind the aggregation job -- we would at that point expect request (2) to be idempotent rather than request (1).",
              "createdAt": "2023-06-14T17:53:08Z",
              "updatedAt": "2023-06-14T17:53:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YVmUP",
          "commit": {
            "abbreviatedOid": "1df6370"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T17:06:28Z",
          "updatedAt": "2023-06-15T17:06:29Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "> I'm suggesting that the text \"further requests to initialize an existing aggregation job with different parameters\" is ambiguous, because it does not define how to determine if a new request matches to an existing aggregation job.\r\n\r\nOK, I'll rewrite to be more explicit.\r\n\r\n> Yes -- IMO request (3) should fail [and it shouldn't rewind/reset the state of the aggregation job].\r\n\r\nI'm coming around to your point of view: reviewing the changes that will land in #393, it appears we don't allow for the `AggregationJobResp` a leader gets in response to initialization to ever contain `PrepareResp`s in the finished state, so as it stands, we can't allow `PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` to succeed regardless of the aggregation job's state. I was thinking that maybe an implementation could have multiple aggregation job workers racing to initialize and continue agg jobs, but the implementation would have to have a pretty serious state synchronization bug to have one worker step the job past round 1 and have the other still think the job needs to be initialized, so I don't think it's worth having the specification bend over backwards to accommodate this.\r\n\r\nIn short: requests to initialize an aggregation job that has been continued at least once are invalid.\r\n",
              "createdAt": "2023-06-15T17:06:29Z",
              "updatedAt": "2023-06-15T17:06:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YXXBt",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-15T23:37:09Z",
          "updatedAt": "2023-06-15T23:37:09Z",
          "comments": [
            {
              "originalPosition": 10,
              "body": "This seems to imply that if the Helper gets an agg request and marks the reports as processed, then in the second agg request it has to \"forget\" this previous state change and allow the reports to be processed. Is this intended?\r\n\r\nToday, Daphne won't even let you retry the second request:\r\n```rust\r\nif !self\r\n    .put_helper_state_if_not_exists(task_id, &agg_job_id, &state)\r\n    .await?\r\n{\r\n    // TODO spec: Consider an explicit abort for this case.\r\n    return Err(DapAbort::BadRequest(\r\n        \"unexpected message for aggregation job (already exists)\".into(),\r\n    ));\r\n}\r\n```\r\n\r\nAnother question: what would be the recommended mechanism for implementing this MUST? For multi-round VDAFs it would make sense to at least require the Helper to wait as long as possible before recording the reports that were processed (i.e., until the last request). But what if the aggregation job just takes one request/response cycle, e.g., if the VDAF is 1-round? \r\n\r\nFinally, how do we check that the message hasn't changed? We previously discussed keeping track of the hash of the request body. If we're going to require this then I think we need to spell it out.",
              "createdAt": "2023-06-15T23:37:09Z",
              "updatedAt": "2023-06-15T23:37:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YeJiT",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-16T20:09:03Z",
          "updatedAt": "2023-06-16T20:09:03Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "does it matter if the task has expired and leader deleted it's collection_jobs?",
              "createdAt": "2023-06-16T20:09:03Z",
              "updatedAt": "2023-06-16T20:09:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YeTNc",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-16T20:31:25Z",
          "updatedAt": "2023-06-16T20:31:25Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Does this conflict with max_batch_query_count? if a collection-job-id has been queried more than max_batch_query_count, the Leader is suppose to fail the CollectionReq with \"batchQueriedTooManyTimes\". ",
              "createdAt": "2023-06-16T20:31:25Z",
              "updatedAt": "2023-06-16T20:31:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YuWhw",
          "commit": {
            "abbreviatedOid": "da047fd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T16:26:50Z",
          "updatedAt": "2023-06-20T16:41:18Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "```suggestion\r\nAdditionally, it is illegal to rewind or reset the state of an aggregation\r\n```",
              "createdAt": "2023-06-20T16:26:50Z",
              "updatedAt": "2023-06-20T16:41:18Z"
            },
            {
              "originalPosition": 7,
              "body": "What requirement are you trying to meet here? To me the subtext reads like \"we should allow retries as long as retrying doesn't change the parameters\". If so, how about making it more explicit?\r\n\r\nSomething like this:\r\n\r\n> Changing an aggregation job's parameters is illegal. If another request to `PUT /tasks/{tasks}/aggregation_jobs/{aggregation-job-id}` for the same `aggregation-job-id` is made, the Helper MAY ignore the request if the message payload is the same as the previous request. Otherwise, if the message payload has changed or the Helper did not check, the Helper MUST abort the request.",
              "createdAt": "2023-06-20T16:37:37Z",
              "updatedAt": "2023-06-20T16:41:18Z"
            },
            {
              "originalPosition": 7,
              "body": "> ... MUST fail with an HTTP client error status code.\r\n\r\nI think we should be more prescriptive than this. It seems like a situation where the Leader would want to take a different action based on the feedback from the Helper.",
              "createdAt": "2023-06-20T16:39:11Z",
              "updatedAt": "2023-06-20T16:41:18Z"
            },
            {
              "originalPosition": 12,
              "body": "Can you check to make sure this doesn't contradict {{aggregation-step-skew-recovery}}?",
              "createdAt": "2023-06-20T16:39:54Z",
              "updatedAt": "2023-06-20T16:41:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Yuq1L",
          "commit": {
            "abbreviatedOid": "da047fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T17:13:49Z",
          "updatedAt": "2023-06-20T17:13:49Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "The requirement is that once an aggregation job has been initialized, you can't change its parameters, i.e. its aggregation parameter or the partial batch selector. We're not discussing retries anymore.",
              "createdAt": "2023-06-20T17:13:49Z",
              "updatedAt": "2023-06-20T17:13:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Yusu9",
          "commit": {
            "abbreviatedOid": "da047fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T17:18:45Z",
          "updatedAt": "2023-06-20T17:18:45Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I don't agree. What this paragraph says is that changing an aggregation job's parameters is illegal, undefined, outside of what this protocol discusses. So it does not make sense to design in-band mechanisms to recover from this scenario. If this ever happens, then one of the protocol participants has a bug and the way to recover is to make software changes.",
              "createdAt": "2023-06-20T17:18:45Z",
              "updatedAt": "2023-06-20T17:18:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YuyLu",
          "commit": {
            "abbreviatedOid": "da047fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T17:29:12Z",
          "updatedAt": "2023-06-20T17:29:13Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "It does not. That section concerns itself with the rounds of the continuation phase of the aggregation sub-protocol. This paragraph is saying you can't go back from the continuation phase to the initialization phase. See [discussion with @branlwyd](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/471#discussion_r1229862877) for context.",
              "createdAt": "2023-06-20T17:29:12Z",
              "updatedAt": "2023-06-20T17:29:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Yu5fn",
          "commit": {
            "abbreviatedOid": "da047fd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T17:46:54Z",
          "updatedAt": "2023-06-20T17:46:54Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "Ah Ok, then it that case, I wouldn't mention the payload explicitly: Just abort if the PUT is repeated for a particular aggregation job.",
              "createdAt": "2023-06-20T17:46:54Z",
              "updatedAt": "2023-06-20T17:46:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YvL7o",
          "commit": {
            "abbreviatedOid": "da047fd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T18:32:53Z",
          "updatedAt": "2023-06-20T18:32:53Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "That doesn't work, because it would make it impossible to retry an aggregation job initialization request, which would make DAP basically impossible to operate.",
              "createdAt": "2023-06-20T18:32:53Z",
              "updatedAt": "2023-06-20T18:32:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YwCQ6",
          "commit": {
            "abbreviatedOid": "37182c8"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-20T20:43:38Z",
          "updatedAt": "2023-06-20T20:43:38Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Ywniv",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T21:37:09Z",
          "updatedAt": "2023-06-20T21:37:10Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "Bump",
              "createdAt": "2023-06-20T21:37:09Z",
              "updatedAt": "2023-06-20T21:37:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Ywnka",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T21:37:16Z",
          "updatedAt": "2023-06-20T21:37:16Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "Bump",
              "createdAt": "2023-06-20T21:37:16Z",
              "updatedAt": "2023-06-20T21:37:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Ywotx",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T21:41:48Z",
          "updatedAt": "2023-06-20T21:41:48Z",
          "comments": [
            {
              "originalPosition": 27,
              "body": "This does not conflict with max batch query count. Each of the queries against a given batch is a distinct collection with a distinct collection ID.",
              "createdAt": "2023-06-20T21:41:48Z",
              "updatedAt": "2023-06-20T21:41:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5YwqTO",
          "commit": {
            "abbreviatedOid": "37182c8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T21:47:24Z",
          "updatedAt": "2023-06-20T21:47:25Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "We discussed this offline. The intention is to require idempotence here without spelling out how it's supposed to be achieved. There are a few options (quoting @tgeoghegan):\r\n> 1. You could store the entire aggregation job init req that led to the creation of the aggregation job, and then compare the next request.\r\n> 2. You could store a hash of the aggregation job init req and check it against hashes of next request.\r\n> 3. You could reevaluate the aggregation job init req and check if its results match what you have stored from the last time you initialized the request.\r\n\r\nI'm a little uncomfortable about leaving this up to implementations, especially since (3.) is a bit fiddly, but it's probably less fiddly to try to spell it out.",
              "createdAt": "2023-06-20T21:47:24Z",
              "updatedAt": "2023-06-20T21:47:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Ywrut",
          "commit": {
            "abbreviatedOid": "a354bfe"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-20T21:51:56Z",
          "updatedAt": "2023-06-20T21:51:56Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "I don't understand how task expiration complicates this. We already have this text:\r\n\r\n> Upon receipt of a `CollectionReq`, the Leader begins by checking that it\r\n> recognizes the task ID in the request path. If not, it MUST abort with error\r\n> `unrecognizedTask`.\r\n\r\nSo if task expires, then the leader should refuse to allow new collection jobs to be created in it, regardless of whether the request has been repeated.",
              "createdAt": "2023-06-20T21:51:56Z",
              "updatedAt": "2023-06-20T21:51:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Ywu7B",
          "commit": {
            "abbreviatedOid": "37182c8"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-20T21:59:49Z",
          "updatedAt": "2023-06-20T21:59:49Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Yw-Dn",
          "commit": {
            "abbreviatedOid": "37182c8"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-20T23:05:29Z",
          "updatedAt": "2023-06-20T23:05:29Z",
          "comments": []
        }
      ]
    },
    {
      "number": 474,
      "id": "PR_kwDOFEJYQs5TI_NX",
      "title": "Use `Empty` for empty variants",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/474",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Define `struct {} Empty` and use it wherever we have an enum variant with no bytes following it.\r\n\r\nThis was removed in c5868ea9854cef7379cef6712aa2d702888f01c9. Our thinking was that unlisted variants implicitly have no bytes following them. This seems like a logical interpretation based on the following example (from RFC8446, Appendix B.3):\r\n\r\n```\r\n      struct {\r\n          HandshakeType msg_type;    /* handshake type */\r\n          uint24 length;             /* bytes in message */\r\n          select (Handshake.msg_type) {\r\n              case client_hello:          ClientHello;\r\n              case server_hello:          ServerHello;\r\n              case end_of_early_data:     EndOfEarlyData;\r\n              case encrypted_extensions:  EncryptedExtensions;\r\n              case certificate_request:   CertificateRequest;\r\n              case certificate:           Certificate;\r\n              case certificate_verify:    CertificateVerify;\r\n              case finished:              Finished;\r\n              case new_session_ticket:    NewSessionTicket;\r\n              case key_update:            KeyUpdate;\r\n          };\r\n      } Handshake;\r\n```\r\n\r\n`HandshakeType` defines variants that are not listed here; presumably it is because there are no more bytes to read following the `select` statement in these variants.\r\n\r\nHowever, as explained at the top of Appendix B, those variants suffixed by `RESERVED` are defined for backwards compatibiliby only and are not meant to be sent on the wire in TLS 1.3. Thus if the peer sends one of these variants, the host is supposed to treat it as a fatal error.\r\n\r\nSimilarly for the `message_hash` variant: This is used to construct an input to a hash function, but is never used in a message written to the wire.",
      "createdAt": "2023-06-15T23:15:36Z",
      "updatedAt": "2023-10-26T15:44:21Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b0a323c60ccf438aad8b7f249a92ad12a890e139",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/add-empty-back",
      "headRefOid": "41ffb309ee61b200b58f5381f777ae955e895439",
      "closedAt": "2023-06-16T17:25:40Z",
      "mergedAt": "2023-06-16T17:25:40Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "364c0e73b5554b7f0f708c78d212162da583cc83"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Hmm -- while what you say is true about the `Handshake` message, see e.g. the `PreSharedKeyExtension` in [RFC 8446 B.3.1](https://datatracker.ietf.org/doc/html/rfc8446#appendix-B.3.1) -- this message omits all variants it does not care about, including many that are not `RESERVED`. So I think omitting \"empty\" cases is supported by the examples of RFC 8446. (whether or not this is clearer is up to y'all, I suppose)",
          "createdAt": "2023-06-16T02:07:03Z",
          "updatedAt": "2023-06-16T02:07:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Hmm -- while what you say is true about the `Handshake` message, see e.g. the `PreSharedKeyExtension` in [RFC 8446 B.3.1](https://datatracker.ietf.org/doc/html/rfc8446#appendix-B.3.1) -- this message omits all variants it does not care about, including many that are not `RESERVED`. So I think omitting \"empty\" cases is supported by the examples of RFC 8446. (whether or not this is clearer is up to y'all, I suppose)\r\n\r\nI'm happy to look at more examples, but eventually we ought to take this to the TLS mailing list :) (Incidentally there is a somewhat lively discussion happening there right now about the presentation language.)\r\n\r\nIn this particular case, I think the correct interpretation is that any missing variant is treated as an error. (Similarly to Appendix B.3.) Here's the definition:\r\n```\r\n      struct {\r\n          select (Handshake.msg_type) {\r\n              case client_hello: OfferedPsks;\r\n              case server_hello: uint16 selected_identity;\r\n          };\r\n      } PreSharedKeyExtension;\r\n```\r\n\r\nWe can see here that the payload of this message depends on which handshake message the extension is included in. The message format is only defined for `ClientHello` and `ServerHello`; it is undefined for any other handshake message.\r\n\r\nBut is it really an error? Well, I happen to know that this is the extension of a payload that MUST NOT appear in any handshake message other than `ClientHello` and `ServerHello`. From Section 4.2: \r\n\r\n>   The table below [\"pre_shared_key\" is \"CH, SH\" ] indicates the messages where a given extension may\r\n>   appear, using the following notation: CH (ClientHello),\r\n>   SH (ServerHello), EE (EncryptedExtensions), CT (Certificate),\r\n>   CR (CertificateRequest), NST (NewSessionTicket), and\r\n>   HRR (HelloRetryRequest).  If an implementation receives an extension\r\n>   which it recognizes and which is not specified for the message in\r\n>   which it appears, it MUST abort the handshake with an\r\n>   \"illegal_parameter\" alert.\r\n\r\nPerhaps more generally we can interpret a missing variant as \"unspecified behavior\" that the rest of the spec must fill in somehow?",
          "createdAt": "2023-06-16T02:25:33Z",
          "updatedAt": "2023-06-16T02:25:33Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5YXen3",
          "commit": {
            "abbreviatedOid": "85974e9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Strict adherence to RFC 8446 aside, I think this is simply more clear than omitting the variant.",
          "createdAt": "2023-06-16T00:21:51Z",
          "updatedAt": "2023-06-16T00:21:51Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Yb1D3",
          "commit": {
            "abbreviatedOid": "41ffb30"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-16T14:49:58Z",
          "updatedAt": "2023-06-16T14:49:58Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YcXOZ",
          "commit": {
            "abbreviatedOid": "41ffb30"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-16T16:06:13Z",
          "updatedAt": "2023-06-16T16:06:13Z",
          "comments": []
        }
      ]
    },
    {
      "number": 475,
      "id": "PR_kwDOFEJYQs5TJCyk",
      "title": "Rename ReportShareError to PrepareError.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/475",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "The old name is confusing to me because almost none of the errors represented by this type are directly related to the report share. I chose the name PrepareError because this error is used during the report preparation (aka aggregation) process, matching other types similarly used during the report preparation process.",
      "createdAt": "2023-06-15T23:41:21Z",
      "updatedAt": "2023-11-27T23:10:13Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c5868ea9854cef7379cef6712aa2d702888f01c9",
      "headRepository": null,
      "headRefName": "bran/report-share-error",
      "headRefOid": "8c97257bf307aab8364551a728065af10ae43227",
      "closedAt": "2023-06-16T00:43:21Z",
      "mergedAt": "2023-06-16T00:43:21Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b0a323c60ccf438aad8b7f249a92ad12a890e139"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "(a nitpicky rename I noticed during #393, happy to take or leave this PR based on how folks feel about the rename.)",
          "createdAt": "2023-06-15T23:41:53Z",
          "updatedAt": "2023-06-15T23:41:53Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5YXYOT",
          "commit": {
            "abbreviatedOid": "8c97257"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Good idea.",
          "createdAt": "2023-06-15T23:44:17Z",
          "updatedAt": "2023-06-15T23:44:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YXfhB",
          "commit": {
            "abbreviatedOid": "8c97257"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-16T00:29:06Z",
          "updatedAt": "2023-06-16T00:29:06Z",
          "comments": []
        }
      ]
    },
    {
      "number": 476,
      "id": "PR_kwDOFEJYQs5TMV_s",
      "title": "Add missing FixedSizeQuery qualifier in select",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/476",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "See [PR](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/341) for rationale",
      "createdAt": "2023-06-16T13:39:38Z",
      "updatedAt": "2023-06-16T17:25:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b0a323c60ccf438aad8b7f249a92ad12a890e139",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "self-describing",
      "headRefOid": "72079884f82e4cedaeff9da4b5b3e228eaa5c96a",
      "closedAt": "2023-06-16T17:25:18Z",
      "mergedAt": "2023-06-16T17:25:18Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "a146ecd946f970d465025ce4cc6ca9ada902f21d"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5YdAvD",
          "commit": {
            "abbreviatedOid": "7207988"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-16T17:25:08Z",
          "updatedAt": "2023-06-16T17:25:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 477,
      "id": "PR_kwDOFEJYQs5TN_-I",
      "title": "Correct spelling of ReportID",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/477",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-06-16T18:56:24Z",
      "updatedAt": "2023-06-16T19:41:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "364c0e73b5554b7f0f708c78d212162da583cc83",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "correct-report-id",
      "headRefOid": "e49218ca171cb180163e453d3c09580b8e61885b",
      "closedAt": "2023-06-16T19:41:51Z",
      "mergedAt": "2023-06-16T19:41:51Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "4670759c2d1230aba92920fa08a09e58785ce669"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5YdjqP",
          "commit": {
            "abbreviatedOid": "e49218c"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-16T18:58:57Z",
          "updatedAt": "2023-06-16T18:58:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Ydj4G",
          "commit": {
            "abbreviatedOid": "e49218c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Ah yes I think this is correct, but cc/ @branlwyd to check for consistency.",
          "createdAt": "2023-06-16T18:59:21Z",
          "updatedAt": "2023-06-16T18:59:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Ydzza",
          "commit": {
            "abbreviatedOid": "e49218c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-16T19:30:28Z",
          "updatedAt": "2023-06-16T19:30:28Z",
          "comments": []
        }
      ]
    },
    {
      "number": 480,
      "id": "PR_kwDOFEJYQs5Te2Xx",
      "title": "Move draft-irtf-cfrg-vdaf-05 to 06",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/480",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Closes #470. As far as I can tell the only thing that's changed that's relevant to DAP is the new interface.",
      "createdAt": "2023-06-20T22:06:55Z",
      "updatedAt": "2023-10-26T15:44:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b48f13ff9131c4388a8c39e966c2ba84d248b7ed",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/470/1",
      "headRefOid": "ec776a96b6bc20e2c8ba686d6bf1645dec2f0da9",
      "closedAt": "2023-06-20T23:38:42Z",
      "mergedAt": "2023-06-20T23:38:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "d0d7ddd00fcdfa708b2f1fe7a04f5377023f87a7"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5Yw0uC",
          "commit": {
            "abbreviatedOid": "ec776a9"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-20T22:23:26Z",
          "updatedAt": "2023-06-20T22:23:26Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Yw36k",
          "commit": {
            "abbreviatedOid": "ec776a9"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-20T22:39:43Z",
          "updatedAt": "2023-06-20T22:39:43Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5YxBUj",
          "commit": {
            "abbreviatedOid": "ec776a9"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-20T23:24:59Z",
          "updatedAt": "2023-06-20T23:24:59Z",
          "comments": []
        }
      ]
    },
    {
      "number": 481,
      "id": "PR_kwDOFEJYQs5T-JfD",
      "title": "Fix type of `AggregationJobContinueReq.step`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/481",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Use the RFC 8446 `uint16`, not Rust `u16`\r\n\r\nResolves #433",
      "createdAt": "2023-06-26T22:01:41Z",
      "updatedAt": "2023-07-03T18:08:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ace9a83d202e0d425e998f635db783e335ba824b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/agg-job-continue-round-type",
      "headRefOid": "958ad56b84eaa2a350aa2d33ed2df6cbcfa7fcaf",
      "closedAt": "2023-07-03T18:08:33Z",
      "mergedAt": "2023-07-03T18:08:33Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f90e904c50261ce358a645a4c772db06a094a7c2"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> (nit that the PR title refers to the wrong field name, but the change itself is \ud83d\udc4d\ud83c\udffb)\r\n\r\nGood catch; the name of the field changed since #433 was written.",
          "createdAt": "2023-06-26T23:03:49Z",
          "updatedAt": "2023-06-26T23:03:49Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ZYaNC",
          "commit": {
            "abbreviatedOid": "958ad56"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-26T22:11:35Z",
          "updatedAt": "2023-06-26T22:11:35Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5ZYyS7",
          "commit": {
            "abbreviatedOid": "958ad56"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "(nit that the PR title refers to the wrong field name, but the change itself is \ud83d\udc4d\ud83c\udffb)",
          "createdAt": "2023-06-26T23:01:55Z",
          "updatedAt": "2023-06-26T23:01:55Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5aGTzB",
          "commit": {
            "abbreviatedOid": "958ad56"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-03T18:08:28Z",
          "updatedAt": "2023-07-03T18:08:28Z",
          "comments": []
        }
      ]
    },
    {
      "number": 482,
      "id": "PR_kwDOFEJYQs5T-frs",
      "title": "Fix TODO to reference correct issue",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/482",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Related to #438",
      "createdAt": "2023-06-26T23:23:54Z",
      "updatedAt": "2023-07-03T18:08:06Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ace9a83d202e0d425e998f635db783e335ba824b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/prepare-step-state-todo",
      "headRefOid": "2dfcfd0d08a396aea7dcd5a30488fafc723a332f",
      "closedAt": "2023-07-03T18:08:06Z",
      "mergedAt": "2023-07-03T18:08:06Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e0aaf2eb3ab26015ce44f84b5a6b6c3bf9861302"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ZZSP2",
          "commit": {
            "abbreviatedOid": "2dfcfd0"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-27T01:36:27Z",
          "updatedAt": "2023-06-27T01:36:27Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5aGTsR",
          "commit": {
            "abbreviatedOid": "2dfcfd0"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-03T18:08:01Z",
          "updatedAt": "2023-07-03T18:08:01Z",
          "comments": []
        }
      ]
    },
    {
      "number": 483,
      "id": "PR_kwDOFEJYQs5T-g8n",
      "title": "Only valid reports count for min/max batch size",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/483",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Resolves #456",
      "createdAt": "2023-06-26T23:34:35Z",
      "updatedAt": "2023-07-03T18:11:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ace9a83d202e0d425e998f635db783e335ba824b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/valid-reports-batch-size",
      "headRefOid": "99c16b25d4b45513e3173d5f9058f55fd6462aef",
      "closedAt": "2023-07-03T18:11:38Z",
      "mergedAt": "2023-07-03T18:11:38Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "bfb3e485ba5e87b9afea27601a7915569859aa83"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> I think saying \"valid\" isn't quite specific enough -- even if a given report is \"valid\", that doesn't necessarily mean it has completed the aggregation process. (One might argue that we must complete the aggregation process to determine if a report is valid, but several of the initial checks pre-aggregation determine \"validity\" of the report share.)\r\n> \r\n> To avoid any wiggle room here, I think these checks should specifically check the number of reports which contributed to the values in the `CollectionReq`/`AggregateShareReq`, i.e. the number of reports in the batch which successfully completed aggregation (I believe).\r\n\r\nFair enough. I changed the language from \"valid reports\" to \"successfully aggregated reports\".",
          "createdAt": "2023-06-27T21:23:11Z",
          "updatedAt": "2023-06-27T21:23:11Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ZY7Yl",
          "commit": {
            "abbreviatedOid": "f5f3778"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-26T23:35:49Z",
          "updatedAt": "2023-06-26T23:35:49Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I considered throwing in a reference to either `{{aggregate-flow}}` or `{{validating-inputs}}` to clarify just what we mean by \"valid\", but I think it's clear enough: the document overall discusses report validity all over the place.",
              "createdAt": "2023-06-26T23:35:49Z",
              "updatedAt": "2023-06-26T23:35:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Zeetp",
          "commit": {
            "abbreviatedOid": "f5f3778"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-06-27T15:06:13Z",
          "updatedAt": "2023-06-27T15:06:14Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "We should do the same under `{#fixed-size-batch-validation}` as well below.",
              "createdAt": "2023-06-27T15:06:13Z",
              "updatedAt": "2023-06-27T15:06:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5Zf51f",
          "commit": {
            "abbreviatedOid": "e96fb65"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-27T17:46:30Z",
          "updatedAt": "2023-06-27T17:46:30Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Zg4bF",
          "commit": {
            "abbreviatedOid": "e96fb65"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I think saying \"valid\" isn't quite specific enough -- even if a given report is \"valid\", that doesn't necessarily mean it has completed the aggregation process. (One might argue that we must complete the aggregation process to determine if a report is valid, but several of the initial checks pre-aggregation determine \"validity\" of the report share.)\r\n\r\nTo avoid any wiggle room here, I think these checks should specifically check the number of reports which contributed to the values in the `CollectionReq`/`AggregateShareReq`, i.e. the number of reports in the batch which successfully completed aggregation (I believe).",
          "createdAt": "2023-06-27T20:27:24Z",
          "updatedAt": "2023-06-27T20:27:24Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5Zhbwu",
          "commit": {
            "abbreviatedOid": "99c16b2"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-06-27T22:20:28Z",
          "updatedAt": "2023-06-27T22:20:28Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5aGUlJ",
          "commit": {
            "abbreviatedOid": "99c16b2"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-03T18:11:32Z",
          "updatedAt": "2023-07-03T18:11:32Z",
          "comments": []
        }
      ]
    },
    {
      "number": 484,
      "id": "PR_kwDOFEJYQs5T-q4R",
      "title": "Defer leader batch validation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/484",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "Move references to `{{batch-validation}}` in the leader's text to just before it requests an aggregate share from the helper, since validating the batch at the time of creating a collection job is premature. We do however allow for leader implementations to implement batch validation checks earlier if they want to.\r\n\r\nRefine the text in `{{batch-validation}}` to capture some error handling nuances: the helper should always abort and do so with a synchronous response to the `AggregateShareReq` it got, but the leader's errors will be transmitted in subsequent responses to `POST` on a collection, and also too few valid reports isn't a fatal error.\r\n\r\nResolves #466",
      "createdAt": "2023-06-27T00:25:37Z",
      "updatedAt": "2023-07-05T23:09:03Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ace9a83d202e0d425e998f635db783e335ba824b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/leader-batch-validation",
      "headRefOid": "8a0e2ef6c51f095ab799311fc92f0f69e7125862",
      "closedAt": "2023-07-05T23:09:02Z",
      "mergedAt": "2023-07-05T23:09:02Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "9f73a17500c437c04300bd53aafb321ffd95f0a9"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5aVKRr",
          "commit": {
            "abbreviatedOid": "8a0e2ef"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-05T23:08:56Z",
          "updatedAt": "2023-07-05T23:08:56Z",
          "comments": []
        }
      ]
    },
    {
      "number": 485,
      "id": "PR_kwDOFEJYQs5UEJoY",
      "title": "inline aggregation job resp validation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/485",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "The section describing how the Leader validates an `AggregationJobResp` only contained a single criteria, and referring to it from multiple places made it unnecessarily complicated. We now simply restate the singular requirement in two sections, which should make this more clear for implementers.\r\n\r\nResolves #473",
      "createdAt": "2023-06-27T18:19:16Z",
      "updatedAt": "2023-07-04T01:05:02Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "ace9a83d202e0d425e998f635db783e335ba824b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/inline-agg-job-validation",
      "headRefOid": "8639b59cd3c9414b00269d8fe35095b8d6d8da2f",
      "closedAt": "2023-07-04T01:05:02Z",
      "mergedAt": "2023-07-04T01:05:02Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b8bf08f9851cf6da56dd0324dcd167a65abefb86"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5aHHtu",
          "commit": {
            "abbreviatedOid": "8639b59"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-03T22:11:14Z",
          "updatedAt": "2023-07-03T22:11:14Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5aHbiu",
          "commit": {
            "abbreviatedOid": "8639b59"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-04T01:04:57Z",
          "updatedAt": "2023-07-04T01:04:57Z",
          "comments": []
        }
      ]
    },
    {
      "number": 486,
      "id": "PR_kwDOFEJYQs5Uwlth",
      "title": "Prepare to cut draft 05",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/486",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "draft-05"
      ],
      "body": "",
      "createdAt": "2023-07-06T00:41:48Z",
      "updatedAt": "2023-10-26T15:44:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9f73a17500c437c04300bd53aafb321ffd95f0a9",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/dap-05",
      "headRefOid": "905a58533e14758ccea7f00344afc839a29c6847",
      "closedAt": "2023-07-10T19:59:26Z",
      "mergedAt": "2023-07-10T19:59:26Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "8fe9e265ea08a14a8aa1aab1d7dcf05498430b15"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5aa8YT",
          "commit": {
            "abbreviatedOid": "e5aba77"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-07-06T17:29:23Z",
          "updatedAt": "2023-07-06T17:58:59Z",
          "comments": [
            {
              "originalPosition": 294,
              "body": "```suggestion\r\nThe reason a POST is used to poll the state of a collection job instead of a\r\n```",
              "createdAt": "2023-07-06T17:29:23Z",
              "updatedAt": "2023-07-06T17:58:59Z"
            },
            {
              "originalPosition": 96,
              "body": "```suggestion\r\nprocess (though potentially aggregating multiple batches of measurements). The\r\n```",
              "createdAt": "2023-07-06T17:38:30Z",
              "updatedAt": "2023-07-06T17:58:59Z"
            },
            {
              "originalPosition": 35,
              "body": "```suggestion\r\n  and an aggregation parameter. As defined in {{!VDAF}}.\r\n```",
              "createdAt": "2023-07-06T17:45:57Z",
              "updatedAt": "2023-07-06T17:58:59Z"
            },
            {
              "originalPosition": 14,
              "body": "I don't see that collection job creation is required to be idempotent. (agg job creation & agg shares are explicitly requried to be so)",
              "createdAt": "2023-07-06T17:58:55Z",
              "updatedAt": "2023-07-06T17:58:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5abHz5",
          "commit": {
            "abbreviatedOid": "e5aba77"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-07-06T18:01:09Z",
          "updatedAt": "2023-07-06T18:01:10Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "Taking another look, I think agg job creation is also not required to be idempotent; the only related text I can find is:\r\n\r\n> Changing an aggregation job's parameters is illegal, so further requests to PUT /tasks/{tasks}/aggregation_jobs/{aggregation-job-id} for the same aggregation-job-id but with a different AggregationJobInitReq in the body MUST fail with an HTTP client error status code.\r\n\r\n... and this text allows, but does not require, idempotency.",
              "createdAt": "2023-07-06T18:01:10Z",
              "updatedAt": "2023-07-06T18:01:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5abIFl",
          "commit": {
            "abbreviatedOid": "746e1b1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-07-06T18:01:54Z",
          "updatedAt": "2023-07-06T18:02:29Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "Why doesn't this require idempotency?",
              "createdAt": "2023-07-06T18:01:54Z",
              "updatedAt": "2023-07-06T18:02:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5aeGY3",
          "commit": {
            "abbreviatedOid": "e5aba77"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-07-07T05:02:57Z",
          "updatedAt": "2023-07-07T05:02:58Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "It says that additional `PUT` requests to the same aggregation job ID but otherwise different parameters `MUST` fail (which is sensible & good). For idempotency to be required, we would need a different condition: that repeated identical requests (same aggregation job ID, same additional parameters) `MUST` succeed.",
              "createdAt": "2023-07-07T05:02:57Z",
              "updatedAt": "2023-07-07T05:02:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ajrlj",
          "commit": {
            "abbreviatedOid": "746e1b1"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-07-07T16:54:59Z",
          "updatedAt": "2023-07-07T16:55:00Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "Fixed, PTAL and make sure it's correct. (Also feel free to suggest a clearer description.)",
              "createdAt": "2023-07-07T16:55:00Z",
              "updatedAt": "2023-07-07T16:55:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5akDnW",
          "commit": {
            "abbreviatedOid": "31ca3b9"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-07T17:59:08Z",
          "updatedAt": "2023-07-07T17:59:21Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "```suggestion\r\n- Allow the following actions to be safely retried: aggregation job creation,\r\n```\r\n\r\n(one last editorial nit)",
              "createdAt": "2023-07-07T17:59:08Z",
              "updatedAt": "2023-07-07T17:59:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5axekO",
          "commit": {
            "abbreviatedOid": "e980f0f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-07-10T19:54:20Z",
          "updatedAt": "2023-07-10T19:56:09Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "```suggestion\r\n  pseudorandom number generator wherever pseudorandomness is not required.\r\n```",
              "createdAt": "2023-07-10T19:54:20Z",
              "updatedAt": "2023-07-10T19:56:09Z"
            }
          ]
        }
      ]
    },
    {
      "number": 488,
      "id": "PR_kwDOFEJYQs5WmFln",
      "title": "Overhaul security considerations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/488",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "- Concentrates discussion of Sybil attacks and client auth as a mitigation thereto in security considerations\r\n- Refocus section enumerating risks/attacks NOT addressed by DAP\r\n- Remix discussion of capabilities and mitigations so that mitigations are discussed immediately after a capability/attack\r\n- Discuss clients uploading multiple times (#443)\r\n- Reorganize some scattered security considerations notes into a section on task configuration\r\n- Remix the \"[Upload, Aggregate, Collect] Message Security\" sections into security considerations\r\n  - Drop most of \"Upload Message Security\"\r\n  - Drop most of \"Aggregate Message Security\"\r\n  - Move discussion of leader forging collect sub-protocol messages to sec considerations\r\n\r\nResolves #460",
      "createdAt": "2023-07-27T22:10:16Z",
      "updatedAt": "2023-08-17T20:01:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8fe9e265ea08a14a8aa1aab1d7dcf05498430b15",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/overhaul-security-considerations",
      "headRefOid": "ef67e88cb2f1a9746dd03d57548d21027f8f8608",
      "closedAt": "2023-08-17T20:01:56Z",
      "mergedAt": "2023-08-17T20:01:56Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "2a605cca8b46816721876329da1eb3fe4b05a5c2"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I promised on the PPM list to merge this on August 11, and it's well past that, so let's land this!",
          "createdAt": "2023-08-17T20:01:50Z",
          "updatedAt": "2023-08-17T20:01:50Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ccRQ0",
          "commit": {
            "abbreviatedOid": "fdf9dad"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-07-27T22:12:22Z",
          "updatedAt": "2023-07-27T22:17:00Z",
          "comments": [
            {
              "originalPosition": 201,
              "body": "I deleted this because it's no longer true under the current collect protocol, which if anything might err too far on the side of protecting privacy at the cost of utility.",
              "createdAt": "2023-07-27T22:12:23Z",
              "updatedAt": "2023-07-27T22:17:00Z"
            },
            {
              "originalPosition": 238,
              "body": "This discussion is now down in \"Task parameters\", below, to keep this list focused on problems that DAP doesn't address",
              "createdAt": "2023-07-27T22:13:04Z",
              "updatedAt": "2023-07-27T22:17:01Z"
            },
            {
              "originalPosition": 613,
              "body": "I deleted this because distribution of task parameters, including DP parameters, is out of scope for DAP.",
              "createdAt": "2023-07-27T22:15:57Z",
              "updatedAt": "2023-07-27T22:17:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5c1e55",
          "commit": {
            "abbreviatedOid": "fdf9dad"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "LGTM. Lots of suggestions, none intended to be blocking. Overall I would recommend harmonizing the language with the VDAF vocabulary (i.e., write \"measurement\" / \"report\" instead of \"input\", \"aggregate result\" instead of \"output\", \"batch\" instead of \"aggregation\", and so on.)",
          "createdAt": "2023-08-01T17:48:16Z",
          "updatedAt": "2023-08-01T19:01:38Z",
          "comments": [
            {
              "originalPosition": 191,
              "body": "There is still an numbered list of \"out-of-scope\" threats below, so we need some transition text.\r\n```suggestion\r\nsection. VDAFs, on their own, do not defend against all threats:\r\n```",
              "createdAt": "2023-08-01T17:48:16Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 241,
              "body": "This bullet is really about \"you may need DP\". In that spirit I'd add another example for Prio3. For instance, the aggregation function computed by Prio3Sum may leak information about outlier values (recall the \"tall person\" example from IETF 117).",
              "createdAt": "2023-08-01T17:50:13Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 258,
              "body": "Also, I might use the \"DAP\" acronym here.\r\n```suggestion\r\nAggregation Protocol deployment, enumerate their assets (secrets that are\r\n```",
              "createdAt": "2023-08-01T17:51:42Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 272,
              "body": "```suggestion\r\n1. Individual users can reveal their own measurement and compromise their own privacy.\r\n```",
              "createdAt": "2023-08-01T17:52:46Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 298,
              "body": "What do you mean by \"aggregator output\" here? We should be specific (\"aggregate share\", \"aggregate result\", both?)",
              "createdAt": "2023-08-01T17:54:22Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 311,
              "body": "In this particular case, it's possible to design a VDAF that checks a particular range of heights (say, 1-2 meters). In fact you might be able to use Prio3Sum as it is, but scale the measurement accordingly.\r\n\r\nI like the idea of mentioning heuristics here, but I wonder if we really need a per-measurement example. Perhaps we could just say \"(i.e., it may be apparent from the aggregate result that some fraction of clients are producing incorrect measurements)\".\r\n\r\nAlso, let's avoid the word \"bogus\" :)\r\n\r\n",
              "createdAt": "2023-08-01T17:55:13Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 308,
              "body": "It's an output share that's bogus, it's the sum of the output shares. This is what we call in the VDAF paper the \"refined measurement\". You could also just say \"measurement\" here, even though it's less precise.\r\n\r\n```suggestion\r\n     * There is no way to detect bogus measurement except by applying\r\n```",
              "createdAt": "2023-08-01T17:57:56Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 323,
              "body": "```suggestion\r\n1. Violate robustness. Any Aggregator can collude with a malicious\r\n```",
              "createdAt": "2023-08-01T18:09:32Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 327,
              "body": "Yes it's still true. We should emphasize though that it's outside the threat model for VDAF robustness.",
              "createdAt": "2023-08-01T18:10:28Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 330,
              "body": "This one is a bit confusing. The collection result now encodes the report count, so this is revealed to all parties as a matter of course. Can we update this one? Perhaps\r\n> The Aggregators (and the Collector) learn the number of reports in each batch, which may violate differential privacy {{dp}}.",
              "createdAt": "2023-08-01T18:13:01Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 358,
              "body": "It would be really nice to have a draft to reference here \ud83d\ude09 \r\n",
              "createdAt": "2023-08-01T18:14:08Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 376,
              "body": "What do we mean by \"output pats\" and \"inputs\" here? (I'm genuinely confused.) Can we update to match the terminology in the VDAF draft?",
              "createdAt": "2023-08-01T18:14:48Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 433,
              "body": "```suggestion\r\nMost passive network attacks are mitigated by DAP's requirement of HTTPS for all\r\n```",
              "createdAt": "2023-08-01T18:15:44Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 455,
              "body": "```suggestion\r\n   1. The time of transmission of reports by Clients could reveal\r\n```",
              "createdAt": "2023-08-01T18:27:19Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 458,
              "body": "Should we mention IP address here?",
              "createdAt": "2023-08-01T18:27:37Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 465,
              "body": "```suggestion\r\n   * These attacks can be mitigated by requiring Clients to submit reports at\r\n```",
              "createdAt": "2023-08-01T18:27:51Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 499,
              "body": "```suggestion\r\nvalid under the chosen VDAF but incorrect. For example, a DAP deployment might be\r\n```",
              "createdAt": "2023-08-01T18:28:08Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 509,
              "body": "```suggestion\r\nsubtracting the known measurements from the aggregate result. The result may reveal\r\n```",
              "createdAt": "2023-08-01T18:29:36Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 526,
              "body": "It may be worth mentioning that only the Leader can authenticate the Client; the Helper cannot. At least in the absence of an extension that make it in-band.",
              "createdAt": "2023-08-01T18:30:57Z",
              "updatedAt": "2023-08-01T19:01:38Z"
            },
            {
              "originalPosition": 549,
              "body": "```suggestion\r\nminimum batch size of 1 report).\r\n```",
              "createdAt": "2023-08-01T18:58:15Z",
              "updatedAt": "2023-08-01T19:01:39Z"
            },
            {
              "originalPosition": 567,
              "body": "```suggestion\r\nbatch includes too few reports, then the aggregate result can reveal information\r\n```",
              "createdAt": "2023-08-01T18:58:57Z",
              "updatedAt": "2023-08-01T19:01:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5dFafT",
          "commit": {
            "abbreviatedOid": "fdf9dad"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-03T19:06:42Z",
          "updatedAt": "2023-08-03T19:26:58Z",
          "comments": [
            {
              "originalPosition": 311,
              "body": "Bogus is a perfectly cromulent word! But fine, I'll swap in some synonyms out of deference to your fragile, ivory tower, academic sensibilities.",
              "createdAt": "2023-08-03T19:06:42Z",
              "updatedAt": "2023-08-03T19:26:58Z"
            },
            {
              "originalPosition": 308,
              "body": "I rewrote this to discuss aggregate shares instead of output shares, which is the actual observable output of an aggregator.",
              "createdAt": "2023-08-03T19:06:57Z",
              "updatedAt": "2023-08-03T19:26:58Z"
            },
            {
              "originalPosition": 358,
              "body": "I was going to refer to the DP draft that is getting started, but AFAICT no such I-D exists on the Datatracker. I put in a TODO to link that doc once it's published.",
              "createdAt": "2023-08-03T19:11:13Z",
              "updatedAt": "2023-08-03T19:26:58Z"
            },
            {
              "originalPosition": 376,
              "body": "This was old text that I remixed into this section. I fixed the terminology.",
              "createdAt": "2023-08-03T19:13:04Z",
              "updatedAt": "2023-08-03T19:26:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5dzvNs",
          "commit": {
            "abbreviatedOid": "af65719"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-11T14:59:52Z",
          "updatedAt": "2023-08-11T15:44:36Z",
          "comments": [
            {
              "originalPosition": 327,
              "body": "for parallelism with the client section:\r\n```suggestion\r\n#### Capabilities and mitigations\r\n```",
              "createdAt": "2023-08-11T14:59:52Z",
              "updatedAt": "2023-08-11T15:44:36Z"
            },
            {
              "originalPosition": 406,
              "body": "```suggestion\r\n#### Capabilities and mitigations\r\n```",
              "createdAt": "2023-08-11T15:00:36Z",
              "updatedAt": "2023-08-11T15:44:36Z"
            },
            {
              "originalPosition": 454,
              "body": "This is partly mitigated by the encryption of the aggregate share including the `BatchSelector` in its AAD. The aggregation parameter could still be rewritten and go undetected, but as noted below this isn't in the scope of the security model.",
              "createdAt": "2023-08-11T15:10:34Z",
              "updatedAt": "2023-08-11T15:44:36Z"
            },
            {
              "originalPosition": 649,
              "body": "This is an existing paragraph, moved around by this PR, but could you add an open issue note pointing to cfrg/draft-irtf-cfrg-vdaf#273? In short, optimized Poplar1 tree walking algorithms should skip to the leaf level and run one last aggregation, rather than stopping after any inner level, in order to maintain robustness.",
              "createdAt": "2023-08-11T15:33:44Z",
              "updatedAt": "2023-08-11T15:44:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5d087p",
          "commit": {
            "abbreviatedOid": "af65719"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-11T18:03:14Z",
          "updatedAt": "2023-08-11T18:03:14Z",
          "comments": [
            {
              "originalPosition": 454,
              "body": "Good catch! I also now wonder why the aggregate share isn't included in `AggregateShareAad` (#493).",
              "createdAt": "2023-08-11T18:03:14Z",
              "updatedAt": "2023-08-11T18:05:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5eXl8f",
          "commit": {
            "abbreviatedOid": "ef67e88"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-17T18:34:22Z",
          "updatedAt": "2023-08-17T19:03:51Z",
          "comments": [
            {
              "originalPosition": 139,
              "body": "Are we sure it's wise to use a `MUST` for server-to-server HTTPS client authentication? My interpretation of the existing text is that we are requiring client authentication here, via e.g. Ouath2/client certificates/`DAP-Auth-Token`; what about a deployment scenario where e.g. the Leader & Helper are on a private network/VPN and the operators are happy enough using IP- or hostname-based routability for \"authentication\"? That would likely be a step down in terms of security, but I'm not sure that's worthy of declaring such a deployment as \"not really DAP\".",
              "createdAt": "2023-08-17T18:34:22Z",
              "updatedAt": "2023-08-17T19:06:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5eXx9q",
          "commit": {
            "abbreviatedOid": "ef67e88"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-17T19:08:06Z",
          "updatedAt": "2023-08-17T19:08:06Z",
          "comments": [
            {
              "originalPosition": 139,
              "body": "(along those lines, now that I'm thinking about VPN-based deployments, such a deployment might even decide to drop TLS entirely for servers communicating within the VPN. that would be an even bigger change, of course.)",
              "createdAt": "2023-08-17T19:08:06Z",
              "updatedAt": "2023-08-17T19:08:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5eYGXa",
          "commit": {
            "abbreviatedOid": "ef67e88"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-17T20:01:14Z",
          "updatedAt": "2023-08-17T20:01:14Z",
          "comments": [
            {
              "originalPosition": 139,
              "body": "This MUST points at `{{request-authentication}]`, which really just says:\r\n\r\n> In other cases, DAP requires HTTPS client authentication as well as server authentication. Any authentication scheme that is composable with HTTP is allowed.\r\n\r\nI think a private network or VPN is composable with HTTP and is not forbidden by this text. Whether we want to relax DAP's long-standing requirement to use HTTPS is beyond the scope of this issue.",
              "createdAt": "2023-08-17T20:01:14Z",
              "updatedAt": "2023-08-17T20:01:14Z"
            }
          ]
        }
      ]
    },
    {
      "number": 491,
      "id": "PR_kwDOFEJYQs5XCnx7",
      "title": "fix formatting of expanded URL",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/491",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This piece of Markdown is awkward when rendered into text (see [DAP-05](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-05#section-4.3-3)). With this change, the resulting rendered text is:\r\n\r\n```\r\n   For example, resource URI {leader}/tasks/{task-id}/reports might be\r\n   expanded into https://example.com/tasks/8BY0RzZMzxvA46_8ymhzycOB9krN-\r\n   QIGYvg_RsByGec/reports\r\n```",
      "createdAt": "2023-08-02T19:12:00Z",
      "updatedAt": "2023-08-02T19:38:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8fe9e265ea08a14a8aa1aab1d7dcf05498430b15",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/fix-formatting",
      "headRefOid": "0780bf63fb9568df15dee83cea3ef018df126a75",
      "closedAt": "2023-08-02T19:38:56Z",
      "mergedAt": "2023-08-02T19:38:56Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "72e1e8c1b63a3b4720072de6e70758a5ebc8a896"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5c94KU",
          "commit": {
            "abbreviatedOid": "0780bf6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-02T19:38:51Z",
          "updatedAt": "2023-08-02T19:38:51Z",
          "comments": []
        }
      ]
    },
    {
      "number": 492,
      "id": "PR_kwDOFEJYQs5XIV9W",
      "title": "Fix typo",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/492",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-08-03T17:18:21Z",
      "updatedAt": "2023-08-03T17:25:36Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "72e1e8c1b63a3b4720072de6e70758a5ebc8a896",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/preapre-typo",
      "headRefOid": "06052dd42cb99622a4982fb18d7d537956bf70d2",
      "closedAt": "2023-08-03T17:25:35Z",
      "mergedAt": "2023-08-03T17:25:35Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0b289d044f6283004cc27f700c765a09b7301b8e"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5dE26J",
          "commit": {
            "abbreviatedOid": "06052dd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-03T17:25:30Z",
          "updatedAt": "2023-08-03T17:25:30Z",
          "comments": []
        }
      ]
    },
    {
      "number": 494,
      "id": "PR_kwDOFEJYQs5YR1Kl",
      "title": "Square up with new VDAF ping-pong functions",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/494",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Recent VDAF changes replaced the ping-pong family of functions from VDAF-06. We now use the new ones which will appear in VDAF-07.\r\n\r\nOne consequence of this is that various aggregation protocol sub-messages now have fields of type `Message` (which is defined in TLS syntax in VDAF and thus can be included in DAP's message definitions) instead of being opaque byte buffers.\r\n\r\nAdditionally, we use the form `Vdaf.function_name()` instead of `VDAF.function_name()` to harmonize with the VDAF document.",
      "createdAt": "2023-08-18T20:43:32Z",
      "updatedAt": "2023-08-31T17:01:32Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2a605cca8b46816721876329da1eb3fe4b05a5c2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/update-ping-pong",
      "headRefOid": "da262ccf3f11ae154b01d7f33c4d6d2ef3b4214f",
      "closedAt": "2023-08-31T17:01:32Z",
      "mergedAt": "2023-08-31T17:01:32Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "644a31bc137161e73ac8a9fea7b65a5f1a7cb1fe"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "The corresponding VDAF changes: https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/281\r\n\r\nIdeally we'll wait for a VDAF including that PR to be published before we take this change to DAP.",
          "createdAt": "2023-08-18T20:44:13Z",
          "updatedAt": "2023-08-18T20:44:13Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5efK_C",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I propose we continue to handle the outputs of `Vdaf` as opaque bytes strings, including the length prefix. It's somewhat wasteful in terms of number of bytes on the wire, but it's useful to keep a clean separation between messages defined here and messages defined in VDAF.",
          "createdAt": "2023-08-18T21:14:27Z",
          "updatedAt": "2023-08-18T21:21:30Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "I'm pretty sure the `make` winds if \"art\" (lines terminated by \"~~~\") is longer than 70 columns.",
              "createdAt": "2023-08-18T21:14:27Z",
              "updatedAt": "2023-08-18T21:21:30Z"
            },
            {
              "originalPosition": 52,
              "body": "Hmm, we need to pick a more distinctive name than `Message`. Another option is to continue to handle this as an opaque byte string, including the length prefix. In fact, I think I'd prefer no to reference messages defined in another document.",
              "createdAt": "2023-08-18T21:15:24Z",
              "updatedAt": "2023-08-18T21:21:30Z"
            },
            {
              "originalPosition": 67,
              "body": "IMO this doesn't need to be said.",
              "createdAt": "2023-08-18T21:16:06Z",
              "updatedAt": "2023-08-18T21:21:30Z"
            },
            {
              "originalPosition": 84,
              "body": "```suggestion\r\n                                                True, # is_leader\r\n```",
              "createdAt": "2023-08-18T21:18:41Z",
              "updatedAt": "2023-08-18T21:21:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5efbFt",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-18T23:03:05Z",
          "updatedAt": "2023-08-18T23:03:06Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "If we do that, then we should change the function definitions in VDAF so that instead of accepting and returning objects of type `Message`, they take opaque byte buffers. So instead of:\r\n\r\n```\r\ndef ping_pong_continued(\r\n            Vdaf,\r\n            is_leader: bool,\r\n            agg_param: bytes,\r\n            state: State,\r\n            inbound: Message,\r\n        ) -> (State, Optional[Message])\r\n```\r\n\r\nWe would have\r\n\r\n```\r\ndef ping_pong_continued(\r\n            Vdaf,\r\n            is_leader: bool,\r\n            agg_param: bytes,\r\n            state: State,\r\n            inbound: bytes,\r\n        ) -> (State, Optional[bytes])\r\n```\r\n\r\nWhich is less expressive, but keeps anything to do with `Message` inside of VDAF. I don't mind doing that because I think the layering concern you bring up is valid. I'm curious what others think.",
              "createdAt": "2023-08-18T23:03:05Z",
              "updatedAt": "2023-08-18T23:03:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5emxEs",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-21T15:08:01Z",
          "updatedAt": "2023-08-21T15:08:01Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "Sounds like a reasonable solution to me.",
              "createdAt": "2023-08-21T15:08:01Z",
              "updatedAt": "2023-08-21T15:08:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5engE9",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-21T16:25:01Z",
          "updatedAt": "2023-08-21T16:25:02Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "We end up with this:\r\n\r\n```\r\n   Next, for each report the Leader executes the following procedure:\r\n\r\n(state, outbound) = Vdaf.ping_pong_leader_init(vdaf_verify_key,\r\n                                               agg_param,\r\n                                               report_id,\r\n                                               public_share,\r\n                                               plaintext_input_share.payload)\r\n\r\n   where:\r\n```\r\n\r\nwhich didn't make the renderer bug out, but it is weird how the centered text goes past the left margin. I re-indented and now we get:\r\n\r\n```\r\n   Next, for each report the Leader executes the following procedure:\r\n\r\n   (state, outbound) = Vdaf.ping_pong_leader_init(\r\n       vdaf_verify_key,\r\n       agg_param,\r\n       report_id,\r\n       public_share,\r\n       plaintext_input_share.payload)\r\n\r\n   where:\r\n```\r\n\r\nwhich I think is better.",
              "createdAt": "2023-08-21T16:25:01Z",
              "updatedAt": "2023-08-21T16:25:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5enhyM",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-21T16:29:25Z",
          "updatedAt": "2023-08-21T16:29:26Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "I dunno, we're telling DAP authors here to go look at the `State` enum in VDAF and how to deal with its `Continued` and `Rejected` variants, so they'll quite naturally wonder what they should do about `Finished`, won't they?",
              "createdAt": "2023-08-21T16:29:25Z",
              "updatedAt": "2023-08-21T16:29:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5enivX",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-21T16:31:51Z",
          "updatedAt": "2023-08-21T16:31:52Z",
          "comments": [
            {
              "originalPosition": 67,
              "body": "Yeah fair enough.",
              "createdAt": "2023-08-21T16:31:52Z",
              "updatedAt": "2023-08-21T16:31:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5enooN",
          "commit": {
            "abbreviatedOid": "a2f340a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-21T16:46:56Z",
          "updatedAt": "2023-08-21T16:46:56Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "OK, I made this change, with a corresponding change on my VDAF PR (commit [2066205](https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/281/commits/2066205c572de1eb625ece322b9437d677948509) https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/281) to change the ping-pong interfaces.",
              "createdAt": "2023-08-21T16:46:56Z",
              "updatedAt": "2023-08-21T16:46:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5fcCi8",
          "commit": {
            "abbreviatedOid": "6293920"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-29T20:00:32Z",
          "updatedAt": "2023-08-29T20:00:55Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "Suggestion: instead of using a bit to convey the role, we could split this into `leader_contineud` and `helper_continued` in https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/281.",
              "createdAt": "2023-08-29T20:00:32Z",
              "updatedAt": "2023-08-29T20:00:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5fcMYy",
          "commit": {
            "abbreviatedOid": "6293920"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-29T20:28:16Z",
          "updatedAt": "2023-08-29T20:28:17Z",
          "comments": [
            {
              "originalPosition": 71,
              "body": "Sure, that seems more expressive than a boolean. I'll make that change in VDAF and sync up here.",
              "createdAt": "2023-08-29T20:28:16Z",
              "updatedAt": "2023-08-29T20:28:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5fdB3A",
          "commit": {
            "abbreviatedOid": "dddc112"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-29T22:47:48Z",
          "updatedAt": "2023-08-29T23:24:11Z",
          "comments": [
            {
              "originalPosition": 86,
              "body": "Fixing a typo and generalizing for arbitrarily many rounds:\r\n```suggestion\r\n   * `prev_state` is the state computed earlier by calling\r\n     `Vdaf.ping_pong_leader_init` or `Vdaf.ping_pong_leader_continued`\r\n```",
              "createdAt": "2023-08-29T22:47:48Z",
              "updatedAt": "2023-08-29T23:24:11Z"
            },
            {
              "originalPosition": 146,
              "body": "This should use `ping_pong_leader_continued`.",
              "createdAt": "2023-08-29T23:23:15Z",
              "updatedAt": "2023-08-29T23:24:11Z"
            }
          ]
        }
      ]
    },
    {
      "number": 495,
      "id": "PR_kwDOFEJYQs5YvXG5",
      "title": "Clarify max_batch_query_count is counted by distinct aggregation params.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/495",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "I believe this is the correct check, as re-collecting a batch against an already-collected aggregation parameter will not reveal additional information -- it is the number of distinct aggregation parameters that determines the number of different collections that have been made against a given batch.\r\n\r\nSee https://github.com/divviup/janus/pull/1799 for context.",
      "createdAt": "2023-08-24T23:22:23Z",
      "updatedAt": "2023-11-27T23:10:12Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2a605cca8b46816721876329da1eb3fe4b05a5c2",
      "headRepository": null,
      "headRefName": "bran/clarify-max-batch-query-count",
      "headRefOid": "115dedf1c8d98e1979ed38375ee8cfe705cdc32a",
      "closedAt": "2023-08-31T15:28:33Z",
      "mergedAt": "2023-08-31T15:28:33Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "62730130ce6347cb3e01512e664265044fca14ac"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Hmm -- it seems like the larger change that is needed is for DAP to drop `max_batch_query_count` entirely, replacing it with a call to a new VDAF-specific function that captures the requirements for aggregation parameter validity.\r\n\r\nIn any case, I think we want to capture the idea that repeating the same collection request (same batch, same aggregation parameter) is valid. I think we'd need to tweak the `is_valid` definitions in VDAF to achieve this. Prio3's validity check currently checks that there are no previous aggregation parameters; the corrected check would take the union of `agg_param` into `previous_agg_params` (as a set), and check that the size of the resulting set is at most 1. I think Poplar1's check would be to partition the aggregation parameters (current & previous) by level, then check that each \"level\" has at most 1 distinct aggregation parameter.",
          "createdAt": "2023-08-29T23:43:30Z",
          "updatedAt": "2023-08-29T23:43:30Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> In any case, I think we want to capture the idea that repeating the same collection request (same batch, same aggregation parameter) is valid. I think we'd need to tweak the `is_valid` definitions in VDAF to achieve this.\r\n\r\nWhy do we want to capture this? To make collection requests idempotent?\r\n\r\nIf so, I'm not sure if it makes sense to try to handle this at the VDAF level. `Vdaf.is_valid()` is meant to specify requirements for securely evaluating the VDAF. If a collection request is repeated, presumably we don't re-run the VDAF, but just return the aggregate result that was computed on the first run. So it sounds like all we want to say is that if the collect request with a given ID has the same content as the original, then we return the same result.\r\n\r\n\r\n> Prio3's validity check currently checks that there are no previous aggregation parameters; the corrected check would take the union of `agg_param` into `previous_agg_params` (as a set), and check that the size of the resulting set is at most 1.\r\n\r\nBut the size of the union would only be 1 if `previous_agg_params` is empty, right?\r\n\r\n I think Poplar1's check would be to partition the aggregation parameters (current & previous) by level, then check that each \"level\" has at most 1 distinct aggregation parameter.\r\n\r\nSimilarly here: this would only be the case if `previous_agg_param` does not include the level of `agg_param`.\r\n\r\n\r\n",
          "createdAt": "2023-08-30T20:40:21Z",
          "updatedAt": "2023-08-30T20:40:21Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Why do we want to capture this? To make collection requests idempotent?\r\n\r\nIndeed. I suppose another way to carve this out would be to explicitly state that (some?) request validation does not happen for repeated collection requests, and the expectation is that repeating a collection request (by batch ID & aggregation parameter) will simply receive the same results as last time.\r\n\r\n> But the size of the union would only be 1 if `previous_agg_params` is empty, right?\r\n\r\nOr if `previous_agg_params` has one element, and `agg_param` is equal to the element in `previous_agg_params`. Similarly for Poplar1: we could accept `agg_param` at a level that is already represented in `previous_agg_params` iff `agg_param` is equal to the previous agg param at the matching level.",
          "createdAt": "2023-08-30T21:29:29Z",
          "updatedAt": "2023-08-30T21:29:29Z"
        },
        {
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "body": "I think it would be safer to phrase this in terms of re-sending previously computed aggregate shares. Not only do we want the query and aggregation parameter to be the same, the set of reports that contributed to the aggregation must be the same as well. (this would be easier to get wrong for TimeInterval, though I suppose it could also be done wrong with FixedSize as well)",
          "createdAt": "2023-08-30T21:34:58Z",
          "updatedAt": "2023-08-30T21:34:58Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Indeed. I suppose another way to carve this out would be to explicitly state that (some?) request validation does not happen for repeated collection requests, and the expectation is that repeating a collection request (by batch ID & aggregation parameter) will simply receive the same results as last time.\r\n\r\nYeah, that sounds to me to be the right shape.\r\n\r\n\r\n> > But the size of the union would only be 1 if `previous_agg_params` is empty, right?\r\n> \r\n> Or if `previous_agg_params` has one element, and `agg_param` is equal to the element in `previous_agg_params`. \r\n\r\nAh! But the intent though is to prevent an attacker from querying a single Prio3 proof more than once, so for Prio3 we really do want there to be one and only one acceptable agg param (the empty string). I guess the aggregation parameter is an awkward way to express this :/\r\n",
          "createdAt": "2023-08-30T21:35:28Z",
          "updatedAt": "2023-08-30T21:35:42Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "There's further work to be done here that might lead to removing `max_batch_query_count` (#436) but this change is good to go on its own.",
          "createdAt": "2023-08-31T15:29:16Z",
          "updatedAt": "2023-08-31T15:29:16Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5fJjKk",
          "commit": {
            "abbreviatedOid": "115dedf"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-25T18:57:47Z",
          "updatedAt": "2023-08-25T18:57:47Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5fcBwQ",
          "commit": {
            "abbreviatedOid": "115dedf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "This is definitely an improvement, but the actual requirement is o enforce aggregation parameter validation, as required by the specific VDAF. e.g.: https://www.ietf.org/archive/id/draft-irtf-cfrg-vdaf-06.html#section-8.2.3\r\n\r\nFor Poplar1, and even for Prio3, if the randomness generated by the Client is used more than once by the Aggregators during preparation, then there is a risk of exposing information about the measurement. See Remark 6 in https://eprint.iacr.org/2023/130.pdf. ",
          "createdAt": "2023-08-29T19:58:38Z",
          "updatedAt": "2023-08-29T19:58:38Z",
          "comments": []
        }
      ]
    },
    {
      "number": 496,
      "id": "PR_kwDOFEJYQs5ZL5Od",
      "title": "Prepare to publish draft-ietf-ppm-dap-06",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/496",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": " - Bump version tags in domain separation strings\r\n - Update VDAF reference\r\n - Update changelog",
      "createdAt": "2023-08-30T23:57:24Z",
      "updatedAt": "2023-08-31T19:01:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "644a31bc137161e73ac8a9fea7b65a5f1a7cb1fe",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/prepare-dap-06",
      "headRefOid": "79e5b11421e8dd9f621dc5ffa066535119acc6dd",
      "closedAt": "2023-08-31T19:01:44Z",
      "mergedAt": "2023-08-31T19:01:44Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "9965d785da29f72d88b7c5dfe053d50a7bdba4e2"
      },
      "comments": [
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "just pointing out the title of this PR should be ppm-dap-06",
          "createdAt": "2023-08-31T10:54:39Z",
          "updatedAt": "2023-08-31T10:54:39Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> just pointing out the title of this PR should be ppm-dap-06\r\n\r\nThank you, you are correct. I got mixed up with VDAF-07. I fixed the PR title and the commit.",
          "createdAt": "2023-08-31T15:58:24Z",
          "updatedAt": "2023-08-31T15:58:24Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm waiting for draft-irtf-cfrg-vdaf-07 to be published so we can verify that this document's references to that revision are OK, and then I will merge this and cut draft-ietf-ppm-dap-06.",
          "createdAt": "2023-08-31T17:33:20Z",
          "updatedAt": "2023-08-31T17:33:20Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5flcT-",
          "commit": {
            "abbreviatedOid": "df64475"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-30T23:57:55Z",
          "updatedAt": "2023-08-30T23:57:56Z",
          "comments": [
            {
              "originalPosition": 19,
              "body": "We might also take #495, in which case we shall add an item here.",
              "createdAt": "2023-08-30T23:57:55Z",
              "updatedAt": "2023-08-30T23:58:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5fqxU1",
          "commit": {
            "abbreviatedOid": "e65dc18"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-31T15:30:08Z",
          "updatedAt": "2023-08-31T15:30:45Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "We should not be incrementing this field, as it is a RFCXML version number. This is the source of the warning message `*** unsupported RFCXML version 4` when building the document. See https://github.com/cabo/kramdown-rfc/blob/930935b2c42f5d453ed29baa474502bb3a58d39a/lib/kramdown-rfc/command.rb#L276-L287\r\n```suggestion\r\nv: 3\r\n```",
              "createdAt": "2023-08-31T15:30:08Z",
              "updatedAt": "2023-08-31T15:30:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5fq9nQ",
          "commit": {
            "abbreviatedOid": "e65dc18"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-08-31T15:55:38Z",
          "updatedAt": "2023-08-31T15:55:38Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Dang it, I've made this mistake before! Thanks, David.",
              "createdAt": "2023-08-31T15:55:38Z",
              "updatedAt": "2023-08-31T15:55:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5frx2T",
          "commit": {
            "abbreviatedOid": "79e5b11"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-31T17:38:57Z",
          "updatedAt": "2023-08-31T19:00:15Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "nit: It may be worth noting this change in the commit message.",
              "createdAt": "2023-08-31T17:38:57Z",
              "updatedAt": "2023-08-31T17:39:37Z"
            }
          ]
        }
      ]
    },
    {
      "number": 498,
      "id": "PR_kwDOFEJYQs5ZPxVk",
      "title": "Include aggregation parameter in `AggregateShareAad`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/498",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Resolves #493",
      "createdAt": "2023-08-31T14:40:53Z",
      "updatedAt": "2023-08-31T19:09:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "62730130ce6347cb3e01512e664265044fca14ac",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/agg-share-aad-agg-param",
      "headRefOid": "f939205820d708659e5af90e4615a4483e8139af",
      "closedAt": "2023-08-31T17:01:12Z",
      "mergedAt": "2023-08-31T17:01:12Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "5141522593b98581c53e035996ace1c35ef2a905"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Don't forget to update the change log.\r\n\r\nYup, already done over in #496. I saw that you approved, but then you put another \"commented\" review so GitHub stripped the green checkmark off the PR. Could you please leave another approved review, @cjpatton?",
          "createdAt": "2023-08-31T16:26:12Z",
          "updatedAt": "2023-08-31T16:26:12Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Whoops, :stamp: :D",
          "createdAt": "2023-08-31T19:09:15Z",
          "updatedAt": "2023-08-31T19:09:15Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5fq8Vo",
          "commit": {
            "abbreviatedOid": "f939205"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "nit: the PR title should reference `AggregateShareAad`",
          "createdAt": "2023-08-31T15:53:54Z",
          "updatedAt": "2023-08-31T16:04:16Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5frJf9",
          "commit": {
            "abbreviatedOid": "f939205"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-31T16:15:09Z",
          "updatedAt": "2023-08-31T16:15:09Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5frMfj",
          "commit": {
            "abbreviatedOid": "f939205"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-08-31T16:21:34Z",
          "updatedAt": "2023-08-31T16:21:34Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5frMne",
          "commit": {
            "abbreviatedOid": "f939205"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Don't forget to update the change log.",
          "createdAt": "2023-08-31T16:21:51Z",
          "updatedAt": "2023-08-31T16:21:51Z",
          "comments": []
        }
      ]
    },
    {
      "number": 501,
      "id": "PR_kwDOFEJYQs5aYMnU",
      "title": "Publish draft-ietf-ppm-dap-07",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/501",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We missed some changes in the previous draft, so bump the version tags and cut a new draft.\r\n\r\nThe wrong commit was tagged. We tagged:\r\n\r\n644a31bc137161e73ac8a9fea7b65a5f1a7cb1fe\r\n\r\nbut intended to tag:\r\n\r\n9965d785da29f72d88b7c5dfe053d50a7bdba4e2",
      "createdAt": "2023-09-14T20:08:42Z",
      "updatedAt": "2023-10-26T15:44:05Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9965d785da29f72d88b7c5dfe053d50a7bdba4e2",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/bump",
      "headRefOid": "c2ae6eafd34e32fbc003c0ac9916556f7d6a5234",
      "closedAt": "2023-09-14T20:14:02Z",
      "mergedAt": "2023-09-14T20:14:02Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "e84f2a3284cdeff528e7176e49bbb4e1a7ff33ac"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5hBEZ7",
          "commit": {
            "abbreviatedOid": "c2ae6ea"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-09-14T20:09:51Z",
          "updatedAt": "2023-09-14T20:09:51Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5hBEm7",
          "commit": {
            "abbreviatedOid": "c2ae6ea"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-09-14T20:10:26Z",
          "updatedAt": "2023-09-14T20:10:26Z",
          "comments": []
        }
      ]
    },
    {
      "number": 503,
      "id": "PR_kwDOFEJYQs5ckjvg",
      "title": "Align VDAF interface with vdaf-07",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/503",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Fix issue https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/502",
      "createdAt": "2023-10-11T22:03:23Z",
      "updatedAt": "2023-10-13T20:04:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "e84f2a3284cdeff528e7176e49bbb4e1a7ff33ac",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "wangshan/align-with-vdaf07-verbs",
      "headRefOid": "4dffdf6bea2220ffcf315ec59fab225cca2ca4da",
      "closedAt": "2023-10-13T20:04:22Z",
      "mergedAt": "2023-10-13T20:04:22Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "726af7a77204303f9da5e28c9b3b6459a2517d70"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5j-S1A",
          "commit": {
            "abbreviatedOid": "4dffdf6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks!",
          "createdAt": "2023-10-13T20:04:17Z",
          "updatedAt": "2023-10-13T20:04:17Z",
          "comments": []
        }
      ]
    },
    {
      "number": 504,
      "id": "PR_kwDOFEJYQs5cxbGD",
      "title": "editorial: De-duplicate a forward reference",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/504",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #332.",
      "createdAt": "2023-10-13T20:33:38Z",
      "updatedAt": "2023-10-26T15:44:04Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "726af7a77204303f9da5e28c9b3b6459a2517d70",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/332",
      "headRefOid": "0ea28c396e1d749084e4e81884a205ca30801461",
      "closedAt": "2023-10-17T14:55:56Z",
      "mergedAt": "2023-10-17T14:55:56Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "228bbf5df0f1c89d32533750a3b68cd8bf24d0f5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kSltU",
          "commit": {
            "abbreviatedOid": "0ea28c3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-17T14:54:09Z",
          "updatedAt": "2023-10-17T14:54:09Z",
          "comments": []
        }
      ]
    },
    {
      "number": 506,
      "id": "PR_kwDOFEJYQs5dKAug",
      "title": "editorial: `outbound` doesnt't look right in .txt",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/506",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-10-18T15:25:43Z",
      "updatedAt": "2023-10-26T15:43:59Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "228bbf5df0f1c89d32533750a3b68cd8bf24d0f5",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/typo",
      "headRefOid": "cc855abf90a9ffb8af1cabb0051ab2e146bd0f3f",
      "closedAt": "2023-10-18T21:15:27Z",
      "mergedAt": "2023-10-18T21:15:27Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "5a2c1ea21c0a10b0aa09d56b9a498d34972cf756"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kfdGS",
          "commit": {
            "abbreviatedOid": "fb6cc8e"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-18T19:31:05Z",
          "updatedAt": "2023-10-18T19:31:21Z",
          "comments": [
            {
              "originalPosition": 3,
              "body": "I think this change should be rolled back because it delimits the YAML document for front matter",
              "createdAt": "2023-10-18T19:31:05Z",
              "updatedAt": "2023-10-18T19:31:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5kgMBS",
          "commit": {
            "abbreviatedOid": "fb6cc8e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-18T21:13:03Z",
          "updatedAt": "2023-10-18T21:13:03Z",
          "comments": [
            {
              "originalPosition": 3,
              "body": "whoops, fat fingers!",
              "createdAt": "2023-10-18T21:13:03Z",
              "updatedAt": "2023-10-18T21:13:04Z"
            }
          ]
        }
      ]
    },
    {
      "number": 507,
      "id": "PR_kwDOFEJYQs5dUMnk",
      "title": "Clarify Sybil attack mitigations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/507",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Closes #499 ",
      "createdAt": "2023-10-19T20:49:15Z",
      "updatedAt": "2023-10-23T20:18:28Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5a2c1ea21c0a10b0aa09d56b9a498d34972cf756",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/sybil-clarification",
      "headRefOid": "4b697fa6afe0ace175c4e06c3eb8d6fcab3e1b33",
      "closedAt": "2023-10-23T20:18:28Z",
      "mergedAt": "2023-10-23T20:18:28Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "db02684b03465430e275faa9bab53d33d31c2afe"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqary",
          "commit": {
            "abbreviatedOid": "4b697fa"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-19T23:26:21Z",
          "updatedAt": "2023-10-19T23:26:21Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5kqbG7",
          "commit": {
            "abbreviatedOid": "4b697fa"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-19T23:28:20Z",
          "updatedAt": "2023-10-19T23:28:26Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "We should link to https://github.com/cpriebe/draft-priebe-ppm-dap-reportauth if it ever makes it to the Datatracker.",
              "createdAt": "2023-10-19T23:28:20Z",
              "updatedAt": "2023-10-19T23:28:26Z"
            }
          ]
        }
      ]
    },
    {
      "number": 508,
      "id": "PR_kwDOFEJYQs5dUNVs",
      "title": "Update problem details reference",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/508",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #490",
      "createdAt": "2023-10-19T20:51:47Z",
      "updatedAt": "2023-10-23T20:17:48Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5a2c1ea21c0a10b0aa09d56b9a498d34972cf756",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/problem-details",
      "headRefOid": "2a9964d48eefb28522b0e4989729b4c49c405f27",
      "closedAt": "2023-10-23T20:17:48Z",
      "mergedAt": "2023-10-23T20:17:48Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "7f462a3c71d77b0b9214d68f9cad0664b7c983d1"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "My cursory glance at 9457 suggests that updating the reference is sufficient. ",
          "createdAt": "2023-10-19T23:48:44Z",
          "updatedAt": "2023-10-19T23:48:44Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqaYA",
          "commit": {
            "abbreviatedOid": "2a9964d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Are we sure that updating the reference suffices? Are there any \"breaking\" changes between RFC 7807 and RFC 9457 that would require updates to text elsewhere in DAP?",
          "createdAt": "2023-10-19T23:24:24Z",
          "updatedAt": "2023-10-19T23:24:24Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5k6q6h",
          "commit": {
            "abbreviatedOid": "2a9964d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-23T17:22:34Z",
          "updatedAt": "2023-10-23T17:22:34Z",
          "comments": []
        }
      ]
    },
    {
      "number": 509,
      "id": "PR_kwDOFEJYQs5dUQBo",
      "title": "Add a PrepareError registry",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/509",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Closes #487",
      "createdAt": "2023-10-19T21:00:53Z",
      "updatedAt": "2023-10-26T15:43:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "359b8a6964100902b735407ddb84a83d5a5dae20",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/prepare-error-registry",
      "headRefOid": "468579ddcad5611af6cf4707ebff0b9ba5c7cdfb",
      "closedAt": "2023-10-23T22:32:43Z",
      "mergedAt": "2023-10-23T22:32:43Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "97b9d0bbc1ccc73c34149c12efe630c71756be74"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan @cjpatton the size increase is to account for random selection of new error types for experimentation. But we can do that separately, if at all, so I'll revert it from this PR.",
          "createdAt": "2023-10-20T00:07:06Z",
          "updatedAt": "2023-10-20T00:07:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood can you squash?",
          "createdAt": "2023-10-23T20:20:22Z",
          "updatedAt": "2023-10-23T20:20:22Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2023-10-23T22:28:10Z",
          "updatedAt": "2023-10-23T22:28:10Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqaNO",
          "commit": {
            "abbreviatedOid": "7e813e6"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-19T23:23:23Z",
          "updatedAt": "2023-10-19T23:23:36Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Can you explain the choice to widen this error type? Are we ever going to define this many distinct error codes? Admittedly errors should be the minority of messages so this extra byte won't cost extra bandwidth in the common case of the protocol.",
              "createdAt": "2023-10-19T23:23:23Z",
              "updatedAt": "2023-10-19T23:23:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5kqabZ",
          "commit": {
            "abbreviatedOid": "7e813e6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-19T23:24:45Z",
          "updatedAt": "2023-10-19T23:24:45Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Why bump 1-byte to 2-byte?",
              "createdAt": "2023-10-19T23:24:45Z",
              "updatedAt": "2023-10-19T23:24:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5kqoBv",
          "commit": {
            "abbreviatedOid": "7e813e6"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-20T00:07:17Z",
          "updatedAt": "2023-10-20T00:07:50Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "```suggestion\n  (255)\n```\n",
              "createdAt": "2023-10-20T00:07:17Z",
              "updatedAt": "2023-10-20T00:07:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5k6rbH",
          "commit": {
            "abbreviatedOid": "67627da"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-23T17:23:48Z",
          "updatedAt": "2023-10-23T17:23:52Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "```suggestion\r\n: The 1-byte value of the PrepareError value\r\n```",
              "createdAt": "2023-10-23T17:23:49Z",
              "updatedAt": "2023-10-23T17:23:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5k8Biu",
          "commit": {
            "abbreviatedOid": "468579d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-23T20:19:20Z",
          "updatedAt": "2023-10-23T22:27:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 510,
      "id": "PR_kwDOFEJYQs5dUUa7",
      "title": "Make HPKE config fetching API consistent with other API endpoints",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/510",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "breaking"
      ],
      "body": "Closes #459",
      "createdAt": "2023-10-19T21:17:06Z",
      "updatedAt": "2023-11-07T13:50:55Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5a2c1ea21c0a10b0aa09d56b9a498d34972cf756",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/consistent-hpke-config",
      "headRefOid": "733bf1306cd71c7c76218bcda966c6cdd5f069e0",
      "closedAt": "2023-11-07T13:50:55Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "How does this break global configs? Aggregators can just put the same config behind each per-task resource, no?",
          "createdAt": "2023-10-19T23:47:18Z",
          "updatedAt": "2023-10-19T23:47:18Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "That would work for most cases, but it breaks taskprov, specifically: in that case, the aggregators may not yet know what the task ID is. The task configuration blob that lets them derive a task ID isn't provided until an upload request, but the client can't do an upload until it gets HPKE configs to encrypt report shares to. ",
          "createdAt": "2023-10-19T23:58:19Z",
          "updatedAt": "2023-10-19T23:58:28Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "That's a taskprov problem to solve and shouldn't force the hand of the core protocol. That draft can reserve a special taskid for the global case, if it chooses to do it that way. Or it can introduce the global API endpoint that this particular change removes. Both would be valid.",
          "createdAt": "2023-10-20T00:01:01Z",
          "updatedAt": "2023-10-20T00:01:01Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree, but I think we should defer this change past the draft we'll cut today(?) for IETF 118 so we can coordinate this with the taskprov authors.",
          "createdAt": "2023-10-23T17:22:10Z",
          "updatedAt": "2023-10-23T17:22:10Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "This is related to #505, the gist of which is \"task-specific configs leak the IPs of clients interested in the task to the Helper.\" With this change, client IPs would always be leaked to the Helper, even when the HPKE configs being returned are actually global -- deployments would have no way to avoid this downside.\r\n\r\nWe may want to decide if #505 is something deployments will care to address before taking this PR.",
          "createdAt": "2023-10-24T22:04:00Z",
          "updatedAt": "2023-10-24T22:04:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Good call @branlwyd, though I think preserving the semantics of global mode should be possible. (@chris-wood  suggested a couple of strategies.)",
          "createdAt": "2023-10-24T22:45:24Z",
          "updatedAt": "2023-10-24T22:45:24Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> This is related to #505, the gist of which is \"task-specific configs leak the IPs of clients interested in the task to the Helper.\" With this change, client IPs would always be leaked to the Helper, even when the HPKE configs being returned are actually global -- deployments would have no way to avoid this downside.\r\n> \r\n> We may want to decide if #505 is something deployments will care to address before taking this PR.\r\n\r\nThis doesn't really make sense to me. Whether or not the client IP is revealed to the Aggregator is orthogonal to how the API is structured. If that's something deployments care about, they should take steps to protect their IP address via proxies. ",
          "createdAt": "2023-10-24T22:47:44Z",
          "updatedAt": "2023-10-24T22:47:44Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> This doesn't really make sense to me. Whether or not the client IP is revealed to the Aggregator is orthogonal to how the API is structured. If that's something deployments care about, they should take steps to protect their IP address via proxies.\r\n\r\nNo matter how the HPKE config request is structured, the client IP is revealed to the Helper. If the request to the Helper for the HPKE config also includes the task ID, the Helper can infer that the requesting IP is interested in that specific task (which #505 considers a privacy leak).\r\n\r\nI agree this might be better-mitigated by proxies, as it is challenging for me to envision a scenario where the client IP can be freely shared with the Leader but must not be shared with the Helper.",
          "createdAt": "2023-10-24T23:31:55Z",
          "updatedAt": "2023-10-24T23:31:55Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd you seem to be agreeing with me. This PR has no effect on whether or not client IPs are revealed. That's a totally orthogonal problem.",
          "createdAt": "2023-10-25T13:32:21Z",
          "updatedAt": "2023-10-25T13:32:21Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood to clarify:\r\n\r\n* #505 points out that task-based HPKE config requests allow the Helper to build a map from client IP to tasks of interest for that IP, by the task ID included in the HPKE config request.\r\n* With this PR, the issue pointed out by #505 would also be true for deployments using global HPKE configs, since with this PR all requests for an HPKE config include a task ID.\r\n\r\n(that is, the risk isn't that client IPs are leaked, it is that the helper can determine which tasks each client IP is interested in.)\r\n\r\nSeparately, as you pointed out, an alternate strategy to mitigate this risk would be to deploy a proxy in front of the DAP aggregators which hides client IPs.\r\n\r\nMy suggested course of action would be:\r\n* If a proxy is an acceptable mitigation, close #505 and take this PR.\r\n* If a proxy is not an acceptable mitigation, don't take this PR.",
          "createdAt": "2023-10-25T16:19:43Z",
          "updatedAt": "2023-10-25T16:19:43Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd again, this PR doesn't change the situation here, since the spec _currently_ allows clients to fetch per-task configs. This PR is not a functionality change. It's an aesthetic one. ",
          "createdAt": "2023-10-25T16:21:31Z",
          "updatedAt": "2023-10-25T16:21:31Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood With this PR, *every* request for an HPKE config now includes a task ID. Prior to this PR, only task-specific HPKE config requests include a task ID. So this PR expands the issue raised by #505 to affect deployments using global HPKE configs, too.",
          "createdAt": "2023-10-25T16:24:35Z",
          "updatedAt": "2023-10-25T16:24:35Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> @chris-wood With this PR, _every_ request for an HPKE config now includes a task ID. Prior to this PR, only task-specific HPKE config requests include a task ID. So this PR expands the issue raised by #505 to affect deployments using global HPKE configs, too.\r\n\r\nAgain, this is an aesthetic change, not a functional one. IP address exposure is a distraction here.",
          "createdAt": "2023-10-25T16:30:33Z",
          "updatedAt": "2023-10-25T16:30:33Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Forget the IP address for a minute. Before this change, a helper would expose global HPKE configs at `/hpke_config`. A request to that path reveals nothing about what task(s) clients are interested in. After this change, it's at `/tasks/{task-id}/hpke_config`, and even if the helper serves up the exact same list of HPKE configs for any `task-id` value, it now can learn how many clients are interested in one task versus another. That's the information leak Brandon is pointing to.",
          "createdAt": "2023-10-25T17:00:12Z",
          "updatedAt": "2023-10-25T17:00:12Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "> Forget the IP address for a minute. Before this change, a helper would expose global HPKE configs at `/hpke_config`. A request to that path reveals nothing about what task(s) clients are interested in. After this change, it's at `/tasks/{task-id}/hpke_config`, and even if the helper serves up the exact same list of HPKE configs for any `task-id` value, it now can learn how many clients are interested in one task versus another. That's the information leak Brandon is pointing to.\r\n\r\nYeah, I agree with this, but what I'm trying to communicate is that, to me, this doesn't seem like a meaningful change because the _leader_ will already learn what task a client is interested in via the upload request. (I know that this particular PR changes what the _helper_ sees, but to me this doesn't feel like a notable distinction.) In any case, the solution to this general problem and #505 in particular seems to be the same -- toss in a proxy if you care about this leakage to any aggregator. ",
          "createdAt": "2023-10-25T19:05:26Z",
          "updatedAt": "2023-10-25T19:05:26Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqZ6O",
          "commit": {
            "abbreviatedOid": "733bf13"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "This breaks global HPKE configs, which are not bound to any specific task, and are necessary for taskprov to work. Previously we considered exposing both `GET {aggregator}/tasks/{task-id}/hpke_config` and `GET {aggregator}/hpke_config`, but I think we stuck with the query param layout because it was good enough.",
          "createdAt": "2023-10-19T23:21:35Z",
          "updatedAt": "2023-10-19T23:21:35Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5lFiZH",
          "commit": {
            "abbreviatedOid": "733bf13"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks good to me.\r\n\r\nAnother way taskprov could address this is by adding the taskprov HTTP header to the GET request so that the Aggregator can derive the task ID. (cc/ @wangshan)",
          "createdAt": "2023-10-24T21:55:55Z",
          "updatedAt": "2023-10-24T21:55:55Z",
          "comments": []
        }
      ]
    },
    {
      "number": 511,
      "id": "PR_kwDOFEJYQs5dUbVj",
      "title": "Make the checksum optional",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/511",
      "state": "CLOSED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "breaking"
      ],
      "body": "Closes #446 ",
      "createdAt": "2023-10-19T21:40:10Z",
      "updatedAt": "2023-11-07T13:03:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5a2c1ea21c0a10b0aa09d56b9a498d34972cf756",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/optional-checksum",
      "headRefOid": "8092353cdc0e4d0e16b26d90476b67143e565398",
      "closedAt": "2023-11-07T13:03:14Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Putting that signal in the task config seems like it'd work!",
          "createdAt": "2023-10-20T00:17:25Z",
          "updatedAt": "2023-10-20T00:17:25Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Closing per discussion at IETF 118.",
          "createdAt": "2023-11-07T13:03:22Z",
          "updatedAt": "2023-11-07T13:03:22Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqYXs",
          "commit": {
            "abbreviatedOid": "8092353"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "In principle I have no objection to the checksum being optional, but I think we should figure out a way to signal to the Helper at _aggregation time_ that the the checksum will be requested later at collection time. I explain below.\r\n\r\nThe checksum is designed so that the order of reports doesn't matter: if both Aggregators have the same _set_ of reports in the batch, then the checksums will match and collection will succeed. This means that we can either compute it as we go at aggregation time or wait until collection time and compute it all at once. The former is strictly better because we already have the report IDs in memory during aggregation time, whereas if we wait until collection time, we'd have to make a bunch of database queries to look up the report IDs in the batch.\r\n\r\nComputing it aggregation time is always better I think. If the Helper does not know whether the checksum will be requested, I suspect it'll always want to compute it ahead of time just in case.\r\n\r\nWe can avoid this by signaling ahead of time that the checksum will be requested. One way to do this would be to add the signal to the task config.\r\n",
          "createdAt": "2023-10-19T23:13:36Z",
          "updatedAt": "2023-10-19T23:13:36Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5lFqli",
          "commit": {
            "abbreviatedOid": "8092353"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "Moved to \"request changes\" while we wait on discussion. (This PR needs work as-is, but it could work with minor changes.)",
          "createdAt": "2023-10-24T22:23:08Z",
          "updatedAt": "2023-10-24T22:23:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 512,
      "id": "PR_kwDOFEJYQs5dUgWg",
      "title": "Add a mutex around batches for aggregation and collection",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/512",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Closes #445 ",
      "createdAt": "2023-10-19T21:59:34Z",
      "updatedAt": "2023-10-26T15:43:59Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "db02684b03465430e275faa9bab53d33d31c2afe",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/batch-overlap-race",
      "headRefOid": "155ef91343f1f35efa08fb417246487ab778b136",
      "closedAt": "2023-10-23T22:26:38Z",
      "mergedAt": "2023-10-23T22:26:38Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "359b8a6964100902b735407ddb84a83d5a5dae20"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Took @tgeoghegan's suggestion, rebased, and squahsed.",
          "createdAt": "2023-10-23T22:25:40Z",
          "updatedAt": "2023-10-23T22:25:40Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqTal",
          "commit": {
            "abbreviatedOid": "5ed145d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-19T22:47:28Z",
          "updatedAt": "2023-10-19T22:47:28Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5kqYKg",
          "commit": {
            "abbreviatedOid": "5ed145d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-19T23:12:33Z",
          "updatedAt": "2023-10-19T23:12:58Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "Should there be text stating that the leader MUST NOT schedule new aggregation jobs that overlap with the batch once it has requested an aggregate share from the helper?",
              "createdAt": "2023-10-19T23:12:33Z",
              "updatedAt": "2023-10-19T23:12:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5k8A2a",
          "commit": {
            "abbreviatedOid": "5ed145d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-23T20:17:21Z",
          "updatedAt": "2023-10-23T20:17:21Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "feel free to tack on a suggestion to this commit and I'll review.",
              "createdAt": "2023-10-23T20:17:21Z",
              "updatedAt": "2023-10-23T20:17:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5k8JcG",
          "commit": {
            "abbreviatedOid": "5ed145d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-23T20:42:32Z",
          "updatedAt": "2023-10-23T20:42:32Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "```suggestion\r\n{{batch-validation}} was not met. If the Leader has a pending aggregation job\r\nthat overlaps with the batch for the collection job, the Leader MUST first\r\ncomplete the aggregation job before proceeding and requesting an aggregate share\r\nfrom the Helper. After the collection job has been created, the Leader MUST NOT\r\nschedule new aggregation jobs that overlap with the batch for the collection\r\njob. This avoids a race condition between aggregation and collection jobs that\r\ncan yield trivial batch mismatch errors.\r\n```",
              "createdAt": "2023-10-23T20:42:32Z",
              "updatedAt": "2023-10-23T20:42:32Z"
            }
          ]
        }
      ]
    },
    {
      "number": 513,
      "id": "PR_kwDOFEJYQs5dUpZY",
      "title": "Rewrite verification key example",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/513",
      "state": "MERGED",
      "author": "chris-wood",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Closes #414",
      "createdAt": "2023-10-19T22:38:15Z",
      "updatedAt": "2023-10-23T20:16:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "5a2c1ea21c0a10b0aa09d56b9a498d34972cf756",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "caw/verify-key-derive",
      "headRefOid": "ce684e7e555f0a3357f42ee01555ede26b758ba5",
      "closedAt": "2023-10-23T20:16:41Z",
      "mergedAt": "2023-10-23T20:16:41Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "11a9ba39e97b6b1c735d56c5478191c22b6f16b5"
      },
      "comments": [
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan that's fine to include as an example. The point here is to not be prescriptive. ",
          "createdAt": "2023-10-19T23:50:02Z",
          "updatedAt": "2023-10-19T23:50:02Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5kqSB4",
          "commit": {
            "abbreviatedOid": "ce684e7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83e\udd86 - very nice to have a clear example of this. For the record, this is what we do in https://github.com/wangshan/draft-wang-ppm-dap-taskprov. If and when that gets adopted by the WG, we should add a reference to it here.",
          "createdAt": "2023-10-19T22:40:58Z",
          "updatedAt": "2023-10-19T22:40:58Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5kqZLA",
          "commit": {
            "abbreviatedOid": "ce684e7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I think gesturing at the out of band agreement to a verify key seed is unnecessarily complicated for illustrating this. taskprov requires multiple independent actors to independently derive the same task ID, so you need a previously agreed upon verify key seed. But in the task provisioning protocol we worked out for Divvi Up, we have a single entity responsible for choosing the task ID and then distributing it to participating aggregators, so we just randomly generate the verify key and then take a SHA-256 of that. ",
          "createdAt": "2023-10-19T23:17:57Z",
          "updatedAt": "2023-10-19T23:17:57Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5k6w43",
          "commit": {
            "abbreviatedOid": "ce684e7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-23T17:34:19Z",
          "updatedAt": "2023-10-23T17:34:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 515,
      "id": "PR_kwDOFEJYQs5dlUEp",
      "title": "Update Change Log",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/515",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-10-23T22:50:30Z",
      "updatedAt": "2023-10-26T15:43:57Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "97b9d0bbc1ccc73c34149c12efe630c71756be74",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/draft08",
      "headRefOid": "449b3976d6d70347f23df7e0dda9ce3037c53931",
      "closedAt": "2023-10-23T22:56:55Z",
      "mergedAt": "2023-10-23T22:56:54Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f3b02c10bb1f3efcd82e2f9106a64dc3c5ea8301"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5k8vbY",
          "commit": {
            "abbreviatedOid": "a0d4373"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-23T22:51:54Z",
          "updatedAt": "2023-10-23T22:51:54Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5k8veV",
          "commit": {
            "abbreviatedOid": "a0d4373"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-23T22:52:09Z",
          "updatedAt": "2023-10-23T22:52:09Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "```suggestion\r\n- Add an error type registry for the aggregation sub-protocol.\r\n```",
              "createdAt": "2023-10-23T22:52:09Z",
              "updatedAt": "2023-10-23T22:52:09Z"
            }
          ]
        }
      ]
    },
    {
      "number": 516,
      "id": "PR_kwDOFEJYQs5dqcCN",
      "title": "Re-allow creation of aggregation jobs after collection job created.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/516",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "This wording was introduced by #512, please see that PR for context. Removing this sentence is desirable for a few reasons:\r\n\r\n* For any VDAFs which require an aggregation parameter before aggregation can begin, or which require multiple aggregations with differing aggregation parameters, a collection job must be received before aggregation jobs can be created.\r\n* Even for VDAFs which do not use an aggregation parameter, implementations may wish to use a more flexible job-scheduling strategy. For example, Janus will still create new aggregation jobs while awaiting completion of existing aggregation jobs, even if a collection job has been received for the relevant batch.\r\n* I believe the technical requirement here is captured by the preceding sentence, i.e. aggregation jobs must be completed before an aggregate share request is made -- so it is safe to remove this sentence.\r\n\r\nThis PR also clarifies that, for an aggregation job & collection job to overlap, the aggregation parameter must match between the two jobs.",
      "createdAt": "2023-10-24T16:47:51Z",
      "updatedAt": "2023-11-27T23:10:11Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f3b02c10bb1f3efcd82e2f9106a64dc3c5ea8301",
      "headRepository": null,
      "headRefName": "bran/relax-mutex",
      "headRefOid": "067ab5273645d89f6c9e6c9260bad732ffdb6b30",
      "closedAt": "2023-10-26T15:46:14Z",
      "mergedAt": "2023-10-26T15:46:14Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "27c6bec59dcdbc46b86c4d951b9248831ffc77e1"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Nitpick around: \"If the Leader has a pending aggregation job that overlaps with the batch for the collection job, ...\"\r\n\r\nI think the technical requirement here is that the aggregation job would need to overlap the batch *and aggregation parameter* of an existing collection job. Thoughts on updating text to capture that the aggregation parameter matters too?",
          "createdAt": "2023-10-24T16:49:25Z",
          "updatedAt": "2023-10-24T16:49:25Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Yes, this should reckon with `(batch, aggregation parameter)` tuples. It might be helpful to introduce into the glossary some noun that refers to that tuple and then use it in various places, but I can't think of a suitable noun right now. Regardless, we should take this change to address the regression introduces in the previous PR.\r\n\r\n100% agreed on coming up with better terminology (though it doesn't need to happen here, of course). I have seen \"batch\" variably refer to:\r\n\r\n* A minimal unit of collection.\r\n* A contiguous-in-time sequence of minimal units of collection, in the time-interval query type.\r\n* Either of the above, along with an aggregation parameter.\r\n\r\nThis can get pretty confusing!",
          "createdAt": "2023-10-24T17:48:04Z",
          "updatedAt": "2023-10-24T17:48:04Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5lDu1Z",
          "commit": {
            "abbreviatedOid": "81b56b6"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Yes, this should reckon with `(batch, aggregation parameter)` tuples. It might be helpful to introduce into the glossary some noun that refers to that tuple and then use it in various places, but I can't think of a suitable noun right now. Regardless, we should take this change to address the regression introduces in the previous PR.",
          "createdAt": "2023-10-24T17:20:48Z",
          "updatedAt": "2023-10-24T17:20:48Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5lEIgt",
          "commit": {
            "abbreviatedOid": "067ab52"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-24T18:11:44Z",
          "updatedAt": "2023-10-24T18:11:44Z",
          "comments": []
        }
      ]
    },
    {
      "number": 517,
      "id": "PR_kwDOFEJYQs5dsDQG",
      "title": "Explain constant role bytes in HPKE info",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/517",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "This adds explanation to each HPKE operation of what the byte literals in each info string represent. This addresses #478.",
      "createdAt": "2023-10-24T22:09:08Z",
      "updatedAt": "2023-10-26T15:45:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f3b02c10bb1f3efcd82e2f9106a64dc3c5ea8301",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "david/explain-roles-in-info",
      "headRefOid": "f7de97ffd42b3f0fcb47f7928e67d3562c9e0326",
      "closedAt": "2023-10-26T15:45:58Z",
      "mergedAt": "2023-10-26T15:45:58Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b0f74c9bf21df18df07425066bb710fe478be261"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5lFp2S",
          "commit": {
            "abbreviatedOid": "f7de97f"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-24T22:19:33Z",
          "updatedAt": "2023-10-24T22:19:33Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5lL09d",
          "commit": {
            "abbreviatedOid": "f7de97f"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-25T14:55:59Z",
          "updatedAt": "2023-10-25T14:55:59Z",
          "comments": []
        }
      ]
    },
    {
      "number": 518,
      "id": "PR_kwDOFEJYQs5dsFLw",
      "title": "Provide guidance for abandoning aggregation jobs",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/518",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "enhancement"
      ],
      "body": "Closes #241.",
      "createdAt": "2023-10-24T22:17:55Z",
      "updatedAt": "2023-10-26T15:43:56Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "f3b02c10bb1f3efcd82e2f9106a64dc3c5ea8301",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/241",
      "headRefOid": "dbd7c0db3b42eb7e53fcc69a78360c2f893054e4",
      "closedAt": "2023-10-26T15:43:30Z",
      "mergedAt": "2023-10-26T15:43:30Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "cc86ddb2f86a5933ffa4f9f8e4e38afde71c69d2"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5lFp8Y",
          "commit": {
            "abbreviatedOid": "be8f100"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-24T22:20:01Z",
          "updatedAt": "2023-10-24T22:20:01Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5lL0n5",
          "commit": {
            "abbreviatedOid": "be8f100"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-25T14:55:27Z",
          "updatedAt": "2023-10-25T14:55:30Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Since this recommendation applies to the leader, it should be in the \"Leader Continuation\" section, below.",
              "createdAt": "2023-10-25T14:55:27Z",
              "updatedAt": "2023-10-25T14:55:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5lO0_z",
          "commit": {
            "abbreviatedOid": "be8f100"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-25T23:11:02Z",
          "updatedAt": "2023-10-25T23:11:03Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Done!",
              "createdAt": "2023-10-25T23:11:02Z",
              "updatedAt": "2023-10-25T23:11:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5lPT_o",
          "commit": {
            "abbreviatedOid": "6c67c70"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-26T00:24:33Z",
          "updatedAt": "2023-10-26T00:24:37Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Now I'm being difficult: I think it makes more sense to put this paragraph at the very end of \"Leader Continuation\" rather than the very beginning.",
              "createdAt": "2023-10-26T00:24:33Z",
              "updatedAt": "2023-10-26T00:24:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5lPU7I",
          "commit": {
            "abbreviatedOid": "be8f100"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-26T00:26:55Z",
          "updatedAt": "2023-10-26T00:26:55Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "I don't fully agree given that it's a branch point that goes more naturally before constructing the next request. However I don't think it makes much of a practical difference, so I'm happy to follow you here.",
              "createdAt": "2023-10-26T00:26:55Z",
              "updatedAt": "2023-10-26T00:26:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5lPVA0",
          "commit": {
            "abbreviatedOid": "be8f100"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-10-26T00:27:09Z",
          "updatedAt": "2023-10-26T00:27:09Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Moved!",
              "createdAt": "2023-10-26T00:27:09Z",
              "updatedAt": "2023-10-26T00:27:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5lUGH4",
          "commit": {
            "abbreviatedOid": "dbd7c0d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-26T14:09:52Z",
          "updatedAt": "2023-10-26T14:09:52Z",
          "comments": []
        }
      ]
    },
    {
      "number": 521,
      "id": "PR_kwDOFEJYQs5d8nNb",
      "title": "Clarify access requirements of verification key.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/521",
      "state": "MERGED",
      "author": "simon-friedberger",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Partially addresses #505.",
      "createdAt": "2023-10-27T08:35:22Z",
      "updatedAt": "2023-10-27T15:30:25Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "27c6bec59dcdbc46b86c4d951b9248831ffc77e1",
      "headRepository": "simon-friedberger/draft-ietf-ppm-dap",
      "headRefName": "main",
      "headRefOid": "a04865473343df15679579982daea3bf9bfb31cc",
      "closedAt": "2023-10-27T15:30:24Z",
      "mergedAt": "2023-10-27T15:30:24Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "79a8dcd893eac9f310f76c779a0ffe3769e16adf"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5lcoXe",
          "commit": {
            "abbreviatedOid": "a048654"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-27T15:04:17Z",
          "updatedAt": "2023-10-27T15:04:17Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5lc1mV",
          "commit": {
            "abbreviatedOid": "a048654"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-10-27T15:30:19Z",
          "updatedAt": "2023-10-27T15:30:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 522,
      "id": "PR_kwDOFEJYQs5ezo7q",
      "title": "Clarify that implementations can ignore query types",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/522",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "Closes #519.",
      "createdAt": "2023-11-07T13:26:37Z",
      "updatedAt": "2023-11-08T08:16:43Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "79a8dcd893eac9f310f76c779a0ffe3769e16adf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/519-close",
      "headRefOid": "663cda327f46370c00a72d0778137d67a459957f",
      "closedAt": "2023-11-08T08:16:43Z",
      "mergedAt": "2023-11-08T08:16:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "834367599ad9ede57ccd898e4d1017973cd9db87"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5mZ86b",
          "commit": {
            "abbreviatedOid": "663cda3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-07T15:56:57Z",
          "updatedAt": "2023-11-07T15:56:57Z",
          "comments": []
        }
      ]
    },
    {
      "number": 523,
      "id": "PR_kwDOFEJYQs5e1BRW",
      "title": "Editorial things",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/523",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "",
      "createdAt": "2023-11-07T16:32:59Z",
      "updatedAt": "2023-11-08T15:08:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "79a8dcd893eac9f310f76c779a0ffe3769e16adf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/editorial-1",
      "headRefOid": "5c7674c26fa2bb64149c49a58dd27daabd5183f2",
      "closedAt": "2023-11-08T15:08:42Z",
      "mergedAt": "2023-11-08T15:08:42Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0a077dd83e42e9526c9546d9b61fdc3d70f9ff32"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5mjhXS",
          "commit": {
            "abbreviatedOid": "5c7674c"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-08T15:05:35Z",
          "updatedAt": "2023-11-08T15:05:35Z",
          "comments": []
        }
      ]
    },
    {
      "number": 524,
      "id": "PR_kwDOFEJYQs5e1XxR",
      "title": "Recommend that task configuration is transparent",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/524",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #500.",
      "createdAt": "2023-11-07T17:26:23Z",
      "updatedAt": "2023-11-08T16:18:57Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0a077dd83e42e9526c9546d9b61fdc3d70f9ff32",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/500-add-requirement",
      "headRefOid": "e28b5889b92a531f8b65c3aec91e7d54c00f0cca",
      "closedAt": "2023-11-08T16:18:57Z",
      "mergedAt": "2023-11-08T16:18:57Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "66efd2707bd1d2f6758370aea712b2bdab7bdfb5"
      },
      "comments": [
        {
          "author": "npdoty",
          "authorAssociation": "NONE",
          "body": "I think it would be helpful to set that expectation of transparency, even if the implementation will depend on an extension or an out of band configuration. I suggested a small change to the language to make the choice clear (rather than \"opt out\").",
          "createdAt": "2023-11-07T18:58:35Z",
          "updatedAt": "2023-11-07T18:58:35Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I think it would be helpful to set that expectation of transparency, even if the implementation will depend on an extension or an out of band configuration. I suggested a small change to the language to make the choice clear (rather than \"opt out\").\r\n\r\nHmm, I don't see your suggestion. Did you create a comment and forget to publish it in the review?",
          "createdAt": "2023-11-07T19:12:42Z",
          "updatedAt": "2023-11-07T19:12:42Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Task parameter negotiation and distribution is outside of DAP's scope. I don't know that it's wise for this document to contain SHOULDs about something it explicitly declares out of scope, especially since the definition of transparency is not especially strong and the recommendation for how to achieve it is not concrete. How could it be, when we deliberately don't define a representation of task parameters?\r\n\r\nI think we should at most keep the security considerations item. I'm happy to be overruled by the other editors and the WG though.",
          "createdAt": "2023-11-07T21:08:10Z",
          "updatedAt": "2023-11-07T21:08:10Z"
        },
        {
          "author": "npdoty",
          "authorAssociation": "NONE",
          "body": "The security considerations piece seems like the most significant to me. That section already includes other security-related considerations regarding task parameters, even with a note that while it's not defined in the spec, there are security implications.",
          "createdAt": "2023-11-07T23:58:00Z",
          "updatedAt": "2023-11-07T23:58:00Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I've removed the paragraph from the task config section and took @npdoty's suggested wording change. Please take another look!",
          "createdAt": "2023-11-08T08:16:10Z",
          "updatedAt": "2023-11-08T08:16:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased and squashed.",
          "createdAt": "2023-11-08T15:03:08Z",
          "updatedAt": "2023-11-08T15:03:08Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Squashed and rebased",
          "createdAt": "2023-11-08T15:22:31Z",
          "updatedAt": "2023-11-08T15:22:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Took @tgeoghegan's suggestion and squashed.",
          "createdAt": "2023-11-08T16:13:06Z",
          "updatedAt": "2023-11-08T16:13:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@chris-wood can you take a last look?",
          "createdAt": "2023-11-08T16:16:41Z",
          "updatedAt": "2023-11-08T16:16:41Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5maxpx",
          "commit": {
            "abbreviatedOid": "ff89021"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-07T17:27:28Z",
          "updatedAt": "2023-11-07T17:27:28Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "Reviewer note: I considered refining this to \"the value of any _privacy-relevant_ parameter\", but I thought it would be a bit bike-sheddy to work out which parameters are privacy-relevant and which aren't. The downside is that just making things like the min batch size transparent is not sufficient for this requirement.\r\n\r\nI'd be happy to refine if folks think it would be useful.",
              "createdAt": "2023-11-07T17:27:28Z",
              "updatedAt": "2023-11-07T17:28:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5mbg8y",
          "commit": {
            "abbreviatedOid": "ff89021"
          },
          "author": "npdoty",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-07T18:54:34Z",
          "updatedAt": "2023-11-07T19:34:18Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "```suggestion\r\ntask parameters. Such a mechanism SHOULD be transparent so that each party can\r\nchoose whether to participate based on the value of any parameter. (See\r\n```",
              "createdAt": "2023-11-07T18:54:34Z",
              "updatedAt": "2023-11-07T19:34:18Z"
            },
            {
              "originalPosition": 6,
              "body": "I'm not sure \"opt out\" makes sense, given that the client can always choose and there is no defined default. And I think the goal should be that clients freely choose to participate in private measurements.",
              "createdAt": "2023-11-07T18:55:35Z",
              "updatedAt": "2023-11-07T19:34:18Z"
            },
            {
              "originalPosition": 6,
              "body": "I actually think \"privacy-relevant parameter\" would be fine too. Since this is just a requirement, not an enumeration of the parameters, it would be up to the creator of any mechanism \r\n(in band or out of band) for transparency to make sure it includes transparency for the client. ",
              "createdAt": "2023-11-07T18:56:58Z",
              "updatedAt": "2023-11-07T19:34:18Z"
            },
            {
              "originalPosition": 28,
              "body": "```suggestion\r\nvisible to all parties such that each party can choose whether to participate in\r\nthe task based on the value of any parameter. This includes the parameters\r\n```",
              "createdAt": "2023-11-07T18:57:32Z",
              "updatedAt": "2023-11-07T19:34:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5mjeQg",
          "commit": {
            "abbreviatedOid": "172dfa9"
          },
          "author": "npdoty",
          "authorAssociation": "NONE",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-08T15:00:31Z",
          "updatedAt": "2023-11-08T15:02:51Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5mjhI0",
          "commit": {
            "abbreviatedOid": "172dfa9"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "I think we ought to talk not just about agreement, but also about consistency of tasks amongst clients. DAP with a single client uploading reports is not great!",
          "createdAt": "2023-11-08T15:05:11Z",
          "updatedAt": "2023-11-08T15:14:42Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "```suggestion\r\n## Task Configuration Agreement and Consistency\r\n```",
              "createdAt": "2023-11-08T15:05:11Z",
              "updatedAt": "2023-11-08T15:14:42Z"
            },
            {
              "originalPosition": 4,
              "body": "Transparency is likely to confuse people who may understand it to have the CT or KT-like meaning, which isn't what we're talking about here. Rather, I think what we're talking about is simply agreement.",
              "createdAt": "2023-11-08T15:06:20Z",
              "updatedAt": "2023-11-08T15:14:42Z"
            },
            {
              "originalPosition": 18,
              "body": "```suggestion\r\nDepending on the deployment model, agreement can require that task parameters\r\nare visible to all parties such that each party can choose whether to participate\r\nbased on the value of any parameter. This includes the parameters enumerated\r\nin {{task-configuration}} and any additional parameters implied by upload\r\nextensions {{upload-extensions}} used by the task. It also includes the task itself:\r\nmeaningful privacy requires that multiple clients contribute to a task and therefore\r\nshare a consistent view of the task configuration.\r\n```",
              "createdAt": "2023-11-08T15:12:57Z",
              "updatedAt": "2023-11-08T15:14:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5mjorf",
          "commit": {
            "abbreviatedOid": "f35c425"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-08T15:18:34Z",
          "updatedAt": "2023-11-08T15:18:34Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "This is confusing: what is a task but the parameters that constitute it? I think the text you had before was better, though I like that this version removes the all-caps SHOULD.",
              "createdAt": "2023-11-08T15:18:34Z",
              "updatedAt": "2023-11-08T15:18:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5mjpRP",
          "commit": {
            "abbreviatedOid": "f35c425"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-08T15:19:39Z",
          "updatedAt": "2023-11-08T15:19:39Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "It's the set of all parameters. Can you offer suggestions?",
              "createdAt": "2023-11-08T15:19:39Z",
              "updatedAt": "2023-11-08T15:19:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5mkA_C",
          "commit": {
            "abbreviatedOid": "f5d6304"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-08T15:58:09Z",
          "updatedAt": "2023-11-08T15:58:09Z",
          "comments": [
            {
              "originalPosition": 20,
              "body": "```suggestion\r\nbased on the value of any parameter. This includes the parameters enumerated\r\nin {{task-configuration}} and any additional parameters implied by upload\r\nextensions {{upload-extensions}} used by the task. Since meaningful privacy\r\nrequires that multiple clients contribute to a task, they should also share a\r\nconsistent view of the task configuration.\r\n```",
              "createdAt": "2023-11-08T15:58:09Z",
              "updatedAt": "2023-11-08T15:58:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5mkLMK",
          "commit": {
            "abbreviatedOid": "e28b588"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-08T16:16:01Z",
          "updatedAt": "2023-11-08T16:16:01Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5mkMQn",
          "commit": {
            "abbreviatedOid": "e28b588"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "\ud83d\udea2 \ud83d\udea2 \ud83d\udea2 ",
          "createdAt": "2023-11-08T16:18:08Z",
          "updatedAt": "2023-11-08T16:18:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 525,
      "id": "PR_kwDOFEJYQs5fr0h5",
      "title": "Clarify mapping of input shares to leader/helper, encoding requirements.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/525",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Specifically, call out that the sharding algorithm will return two input shares, and the first one is the Leader's.\r\n\r\nAlso describe that DAP only works with VDAFs which have defined encodings for the various VDAF types which DAP embeds into its own types as opaque byte strings. I purposefully leave the set of messages which require a defined encoding ambiguous in hopes that this avoids the possibility of forgetting to update the list as the specification is changed.",
      "createdAt": "2023-11-16T22:05:56Z",
      "updatedAt": "2023-12-18T23:27:24Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "15527a8c92552cf55c63611f8406902ad14c6fbf",
      "headRepository": null,
      "headRefName": "bran/editorial",
      "headRefOid": "b275a39f879b4268573eccc681a7df11fa871f23",
      "closedAt": "2023-11-29T16:51:26Z",
      "mergedAt": "2023-11-29T16:51:26Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "0ac3b865d8bb91371d3eb1807f6dbe2801a3e131"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "A few notes:\r\n* These changes fell out of some implementation questions asked by @jbr.\r\n* This point only matters if we decide we want to explicitly list the set of VDAF types which must have an encoding specified. Most of the VDAF types which require a specified encoding are clear -- they're the VDAF types which are embedded in some DAP type as an opaque byte string. The one VDAF type for which the encodability requirement isn't clear is the VDAF state: this type is never sent on the wire, and indeed an in-memory-only DAP implementation could get away without an encoding for the VDAF state at all. However, a DAP implementation which durably stores state between requests will need _some_ encoding for the VDAF state. IMO, if we decide to list VDAF types explicitly, we should call out the VDAF state as being only recommended for encoding.\r\n* AFAICT, the DAP specification doesn't mention that VDAFs should be instantiated for 2 aggregators. IMO, this is OK to leave ambiguous.",
          "createdAt": "2023-11-16T22:13:08Z",
          "updatedAt": "2023-11-16T22:13:08Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5nc-Wz",
          "commit": {
            "abbreviatedOid": "02cc516"
          },
          "author": "jbr",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-16T22:24:00Z",
          "updatedAt": "2023-11-16T22:24:00Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "\"\u2026must specify encodings all relevant\u2026\" probably is missing a word, but I'm not sure what word was intended",
              "createdAt": "2023-11-16T22:24:00Z",
              "updatedAt": "2023-11-16T22:24:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nkniq",
          "commit": {
            "abbreviatedOid": "c42eb33"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-17T18:30:49Z",
          "updatedAt": "2023-11-17T18:30:55Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Can we enumerate what these messages are? It's input shares, prepare shares and aggregate shares, right? ",
              "createdAt": "2023-11-17T18:30:50Z",
              "updatedAt": "2023-11-17T18:30:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nl0fS",
          "commit": {
            "abbreviatedOid": "c42eb33"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-17T21:34:43Z",
          "updatedAt": "2023-11-17T21:34:43Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "We can, with the downside that we're now on the hook to keep this list up-to-date. IMO, we might be better served by leaving the list ambiguous.\r\n\r\nNote that the last two types (`PrepShare` and `PrepMessage`) require an encoding because of their inclusion in ping-pong messages, the serialization of which is specified in VDAF (which itself assumes that these types have a defined encoding). I think it's less confusing to list the specific VDAF types, since that is what someone trying to implement DAP would be able to reference when looking at any given VDAF.",
              "createdAt": "2023-11-17T21:34:43Z",
              "updatedAt": "2023-11-17T21:34:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nl1sK",
          "commit": {
            "abbreviatedOid": "c42eb33"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-17T21:38:02Z",
          "updatedAt": "2023-11-17T21:38:02Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Any thoughts on mentioning encodability of `PrepState`, either in this list or separately, given the considerations I mention above?",
              "createdAt": "2023-11-17T21:38:02Z",
              "updatedAt": "2023-11-17T21:38:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nmAd-",
          "commit": {
            "abbreviatedOid": "c42eb33"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-17T22:09:49Z",
          "updatedAt": "2023-11-17T22:09:49Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "> I think it's less confusing to list the specific VDAF types, since that is what someone trying to implement DAP would be able to reference when looking at any given VDAF.\r\n\r\nYep, I think this is the right trade-off.\r\n\r\n> Any thoughts on mentioning encodability of `PrepState`, either in this list or separately, given the considerations I mention above?\r\n\r\nI don't think we need to discuss it in this protocol. I agree that any practical DAP implementation will need to be able to somehow encode and store `PrepState`, but since that representation doesn't have to be agreed upon by more than one implementation, this document doesn't need to be prescriptive about it.",
              "createdAt": "2023-11-17T22:09:49Z",
              "updatedAt": "2023-11-17T22:09:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nmAjw",
          "commit": {
            "abbreviatedOid": "85468d1"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-17T22:10:02Z",
          "updatedAt": "2023-11-17T22:10:02Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5nmCs1",
          "commit": {
            "abbreviatedOid": "85468d1"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-17T22:15:52Z",
          "updatedAt": "2023-11-17T22:15:52Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Note that we need an encoding for the aggregation parameter as well, in collection requests and responses.",
              "createdAt": "2023-11-17T22:15:52Z",
              "updatedAt": "2023-11-17T22:15:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nmDmE",
          "commit": {
            "abbreviatedOid": "c42eb33"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-17T22:20:08Z",
          "updatedAt": "2023-11-17T22:20:09Z",
          "comments": [
            {
              "originalPosition": 12,
              "body": "Gah, thanks, I mistakenly conflated AggParam with AggShare when I was generating the list.",
              "createdAt": "2023-11-17T22:20:08Z",
              "updatedAt": "2023-11-17T22:20:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5nsl3k",
          "commit": {
            "abbreviatedOid": "81a7230"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-20T12:54:24Z",
          "updatedAt": "2023-11-20T12:54:34Z",
          "comments": [
            {
              "originalPosition": 11,
              "body": "```suggestion\r\nspecify a `NONCE_SIZE` of 16 bytes, and MUST specify encodings for the following\r\n```",
              "createdAt": "2023-11-20T12:54:24Z",
              "updatedAt": "2023-11-20T12:54:34Z"
            }
          ]
        }
      ]
    },
    {
      "number": 527,
      "id": "PR_kwDOFEJYQs5f6Foy",
      "title": "Bump VDAF-07 to 08",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/527",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "breaking"
      ],
      "body": "This is a breaking change, so bump the DAP version tag as well. (The next version tag is \"dap-09\".)",
      "createdAt": "2023-11-20T13:00:35Z",
      "updatedAt": "2023-11-29T16:22:40Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "66efd2707bd1d2f6758370aea712b2bdab7bdfb5",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/bump-vdaf",
      "headRefOid": "e911db718ff42f5965c19f732c5a6fdad58f57dd",
      "closedAt": "2023-11-29T16:22:39Z",
      "mergedAt": "2023-11-29T16:22:39Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "a317d73dedc337e02cafafb381e2e4d62c333191"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5nwtef",
          "commit": {
            "abbreviatedOid": "e911db7"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-20T22:39:49Z",
          "updatedAt": "2023-11-20T22:39:49Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5oo-4-",
          "commit": {
            "abbreviatedOid": "e911db7"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-29T15:34:46Z",
          "updatedAt": "2023-11-29T15:34:46Z",
          "comments": []
        }
      ]
    },
    {
      "number": 528,
      "id": "PR_kwDOFEJYQs5f7p99",
      "title": "Define FixedSizeQueryType enum max value",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/528",
      "state": "MERGED",
      "author": "junyechen1996",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Otherwise it's unclear what the encoded length of the type is.",
      "createdAt": "2023-11-20T16:47:08Z",
      "updatedAt": "2023-11-20T16:48:46Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "66efd2707bd1d2f6758370aea712b2bdab7bdfb5",
      "headRepository": "junyechen1996/draft-ietf-ppm-dap",
      "headRefName": "junyec/fixed-size-query-type-length",
      "headRefOid": "a401966f801b79c7ed4ad8db8468084c3014ca8c",
      "closedAt": "2023-11-20T16:48:46Z",
      "mergedAt": "2023-11-20T16:48:46Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "67d23efe7e3ac293e118883bef20f8653200be56"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5nuoHv",
          "commit": {
            "abbreviatedOid": "a401966"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-20T16:48:35Z",
          "updatedAt": "2023-11-20T16:48:35Z",
          "comments": []
        }
      ]
    },
    {
      "number": 529,
      "id": "PR_kwDOFEJYQs5gCVF8",
      "title": "Fix a typo",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/529",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "",
      "createdAt": "2023-11-21T15:37:12Z",
      "updatedAt": "2023-11-29T16:23:01Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "67d23efe7e3ac293e118883bef20f8653200be56",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/typo-2",
      "headRefOid": "ff12d3c1944d3c014000d2355ca26579659095a4",
      "closedAt": "2023-11-29T16:23:01Z",
      "mergedAt": "2023-11-29T16:23:01Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "f3faba7350cad4200fcc2079c4404aa44ab18de5"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5n2slv",
          "commit": {
            "abbreviatedOid": "ff12d3c"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-21T16:03:09Z",
          "updatedAt": "2023-11-21T16:03:09Z",
          "comments": []
        }
      ]
    },
    {
      "number": 530,
      "id": "PR_kwDOFEJYQs5gE85-",
      "title": "Make max batch size optional",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/530",
      "state": "MERGED",
      "author": "wangshan",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "As discussed in ietf118.\r\n\r\nNot all applications require batch size to be tightly controlled within [min, max] batch size. For some applications, satisfy min batch size is sufficient. Making max batch size optional also allows `fixed_size` query to mimic `time_interval` query type.\r\n\r\nNote the name `fixed_size` isn't strictly true anymore without max batch size, but I think it still conveys the right information: The aggregator should check one or two fixed batch size before outputting, therefore I opt to keep the current name.",
      "createdAt": "2023-11-21T23:57:23Z",
      "updatedAt": "2023-11-28T18:41:07Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "67d23efe7e3ac293e118883bef20f8653200be56",
      "headRepository": "wangshan/draft-ietf-ppm-dap",
      "headRefName": "optional-max-batch-size",
      "headRefOid": "52636f4ab48a22a1f7a6f7fd151eadc58a710204",
      "closedAt": "2023-11-28T18:41:07Z",
      "mergedAt": "2023-11-28T18:41:07Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "9dd5fedd8653abde45e8cdcfbbbbc6779f41e611"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5n_LfP",
          "commit": {
            "abbreviatedOid": "0d00954"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks good to me.",
          "createdAt": "2023-11-22T14:57:02Z",
          "updatedAt": "2023-11-22T14:59:18Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "```suggestion\r\nconfiguration MAY include a parameter `max_batch_size` that determines maximum\r\n```",
              "createdAt": "2023-11-22T14:57:02Z",
              "updatedAt": "2023-11-22T14:58:46Z"
            },
            {
              "originalPosition": 58,
              "body": "```suggestion\r\noptionally the maximum batch size, `max_batch_size`. The Aggregator checks that\r\n```",
              "createdAt": "2023-11-22T14:58:10Z",
              "updatedAt": "2023-11-22T14:58:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ofUc6",
          "commit": {
            "abbreviatedOid": "52636f4"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-11-28T14:30:42Z",
          "updatedAt": "2023-11-28T18:24:39Z",
          "comments": []
        }
      ]
    },
    {
      "number": 531,
      "id": "PR_kwDOFEJYQs5gL-zq",
      "title": "Use proper syntax for `select` in `FixedSizeQuery`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/531",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "",
      "createdAt": "2023-11-23T02:58:21Z",
      "updatedAt": "2023-11-29T16:23:22Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "67d23efe7e3ac293e118883bef20f8653200be56",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/typo-3",
      "headRefOid": "e9af4365c05d029c9831f12f8b7f17d9b116abe5",
      "closedAt": "2023-11-29T16:23:21Z",
      "mergedAt": "2023-11-29T16:23:21Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "15527a8c92552cf55c63611f8406902ad14c6fbf"
      },
      "comments": [],
      "reviews": []
    },
    {
      "number": 532,
      "id": "PR_kwDOFEJYQs5gfjsv",
      "title": "Require current-batch queries to return distinct batches.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/532",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Previously, the Leader could assign the same batch to an arbitrary number of current-batch queries, which required the Collector to dedupe. Now, each batch is assigned to exactly one current-batch collection request.\r\n\r\nCloses https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/526.",
      "createdAt": "2023-11-27T23:12:27Z",
      "updatedAt": "2023-12-15T00:04:50Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "b015359e95daaa79f7a527876d6b0fb74e873ecd",
      "headRepository": null,
      "headRefName": "bran/current-batch",
      "headRefOid": "5b0ba59068325aeea51c10957dcf81be4bd7aeb5",
      "closedAt": "2023-12-15T00:04:42Z",
      "mergedAt": "2023-12-15T00:04:42Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "571988ca27bc65dad3cd97544c340623330333b8"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> IMO: this idea is nice in that it potentially removes statefulness in Collector implementations, at the cost of adding a fair bit of complexity (& request traffic) to Aggregator implementations. One downside is that it's a lot of additional API surface to specify, and unless it's a MUST a generic DAP Collector can't make use of it & therefore must still be stateful.\r\n\r\nYeah, I agree that specifying such an endpoint would be complicated, and I'm not sure DAP wants to do it, which is why I think gesturing at it in operational considerations would suffice. In any case we don't need to solve that here.\r\n\r\n",
          "createdAt": "2023-12-06T16:45:36Z",
          "updatedAt": "2023-12-06T16:45:36Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5ogJVa",
          "commit": {
            "abbreviatedOid": "e3bb499"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I'd be OK with changing the POST to a GET, but we might err on the side of not messing with HTTP stuff until we've had a chance to do a thorough review (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/450#issuecomment-1815711315)",
          "createdAt": "2023-11-28T15:49:52Z",
          "updatedAt": "2023-11-28T15:54:31Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "```suggestion\r\nIt is RECOMMENDED that Collectors which use `current_batch` queries generate and\r\n```",
              "createdAt": "2023-11-28T15:49:52Z",
              "updatedAt": "2023-11-28T15:54:32Z"
            },
            {
              "originalPosition": 8,
              "body": "\"as durably\"?",
              "createdAt": "2023-11-28T15:50:11Z",
              "updatedAt": "2023-11-28T15:54:32Z"
            },
            {
              "originalPosition": 10,
              "body": "```suggestion\r\nis that a Collector that issues a `current_batch` request and then loses the\r\n```",
              "createdAt": "2023-11-28T15:50:25Z",
              "updatedAt": "2023-11-28T15:54:32Z"
            },
            {
              "originalPosition": 6,
              "body": "Why RECOMMEND and not MUST? What are you trying to get implementations to do here?",
              "createdAt": "2023-11-28T15:52:05Z",
              "updatedAt": "2023-11-28T15:54:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ohM5G",
          "commit": {
            "abbreviatedOid": "e3bb499"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "> I'd be OK with changing the POST to a GET, but we might err on the side of not messing with HTTP stuff until we've had a chance to do a thorough review ([#450 (comment)](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/450#issuecomment-1815711315))\r\n\r\nAck, I've switched back to POST for now. I'll note the GET/POST question on that issue.\r\n\r\nedit: I still remove the \"note on idempotency\" section, as it is no longer accurate & it's not clear what we'd replace it with.",
          "createdAt": "2023-11-28T17:50:49Z",
          "updatedAt": "2023-11-28T18:44:32Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "I am trying to suggest that Collectors mustn't lose collection job IDs for `current-batch` requests, otherwise they will lose the batch assigned to that collection job. To avoid losing the collection job ID, the Collector needs to write it down _before_ making the `current-batch` request. That is, the collection job ID is just as important as the eventual collection result in terms of storage requirements to avoid data loss. (that's what I'm trying to get at with the \"as durably\" comment)\r\n\r\nI used RECOMMEND rather than MUST because this isn't required for interoperability, only to avoid data loss -- if a Collector implementation is OK losing data if they lose an ID for an in-process collection job, they don't need to follow this advice.",
              "createdAt": "2023-11-28T17:50:49Z",
              "updatedAt": "2023-11-28T18:41:32Z"
            },
            {
              "originalPosition": 8,
              "body": "See the RECOMMEND vs MUST thread for more detail: what I'm trying to get at is that a collection job ID, even for an in-process collection job, is equally \"important\" as the eventual results from that collection job. Specifically, \"important\" in the sense that losing either would lose the results for the batch selected by the query, since in a `current-batch` query each batch is (now) assigned to exactly one collection job.\r\n\r\nI use the term \"durably\" here in the sense of \"durable storage\", e.g. to suggest that the collection job ID be stored to the same datastore as the eventual collection result (as opposed to being stored in memory only, I suppose). Maybe there's a better way to phrase this?",
              "createdAt": "2023-11-28T18:39:26Z",
              "updatedAt": "2023-11-28T18:41:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5ozbPy",
          "commit": {
            "abbreviatedOid": "47e1341"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Thanks for taking out the breaking change. I have no objection to merging this as-is.",
          "createdAt": "2023-11-30T19:39:49Z",
          "updatedAt": "2023-11-30T19:48:26Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "My chief complaint is that it's grammatically awkward. (When I get the tokens \"as durably as\", the next token I predict is \"possible\".)",
              "createdAt": "2023-11-30T19:39:49Z",
              "updatedAt": "2023-11-30T19:48:26Z"
            },
            {
              "originalPosition": 6,
              "body": "I see. My gut feeling is that this paragraph is not really necessary: since a collection job is a resource associated with an ID, it seems obvious that you would have to remember that ID in order to complete the job later on.\r\n\r\nAm I missing anything? Perhaps you encountered a bug in Janus that made you feel this text was needed?",
              "createdAt": "2023-11-30T19:47:34Z",
              "updatedAt": "2023-11-30T19:48:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o0X1Q",
          "commit": {
            "abbreviatedOid": "e3bb499"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-30T22:19:11Z",
          "updatedAt": "2023-11-30T22:19:11Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "That's fair. This didn't fall out of a specific bug in Janus or the like; instead, it's a difference between the old & new semantics (the old semantics _did_ allow collectors to generate collection IDs and send them without recording them first, with no possibility of data loss).\r\n\r\nThinking through it again, I think it's plausible to expect Collector implementors to get to the correct semantics on their own. I've removed this paragraph.",
              "createdAt": "2023-11-30T22:19:11Z",
              "updatedAt": "2023-12-01T02:10:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o0Yae",
          "commit": {
            "abbreviatedOid": "e3bb499"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-30T22:21:22Z",
          "updatedAt": "2023-11-30T22:21:23Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "\ud83d\udc4d\ud83c\udffb, but the relevant paragraph has been removed so this is now moot.",
              "createdAt": "2023-11-30T22:21:23Z",
              "updatedAt": "2023-11-30T22:21:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5pF37q",
          "commit": {
            "abbreviatedOid": "b8a0eb3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "In response to the earlier discussion on what happens if the collector loses collection job IDs: another solution besides requiring the collector to remember collection job IDs would be a leader endpoint `GET /tasks/{task-id}/collection_jobs` that would return a (paginated?) list of collection jobs for the task. I'm not sure if we want to go so far as to make this a MUST for implementations, but maybe it could go in `{{operational-capabilities}}`? Actually, discussion of collectors durably storing collection job IDs could go in \"Operational Considerations/Collector capabilities\".",
          "createdAt": "2023-12-04T18:41:21Z",
          "updatedAt": "2023-12-04T18:44:24Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "Should this be \"begun\" instead of \"began\"?\r\n\r\nMore substantialy: are we being sufficiently clear about what it means for a batch to have begun collection? I think that once the leader has successfully handled `PUT /tasks/{task-id}/collection_jobs/{collection-job-id}` (regardless of whether the collector ever saw the response), the batch has \"beg[u]n collection\", but I'm not sure if we say that. Maybe it'd be better to say \"a batch for which a collection job exists\", because I think that's easier to understand in terms of the relation between a batch and a collection job resource?",
              "createdAt": "2023-12-04T18:41:22Z",
              "updatedAt": "2023-12-04T18:44:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5pRlnn",
          "commit": {
            "abbreviatedOid": "b8a0eb3"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "> In response to the earlier discussion on what happens if the collector loses collection job IDs: another solution besides requiring the collector to remember collection job IDs would be a leader endpoint `GET /tasks/{task-id}/collection_jobs` that would return a (paginated?) list of collection jobs for the task. I'm not sure if we want to go so far as to make this a MUST for implementations, but maybe it could go in `{{operational-capabilities}}`?\r\n\r\nI think this API is \"a list of _uncollected_ collection jobs which are ready for collection\", right? Otherwise we end up with unbounded growth in the result set (up to GC).\r\n\r\nIMO: this idea is nice in that it potentially removes statefulness in Collector implementations, at the cost of adding a fair bit of complexity (& request traffic) to Aggregator implementations. One downside is that it's a lot of additional API surface to specify, and unless it's a MUST a generic DAP Collector can't make use of it & therefore must still be stateful.\r\n\r\nIf we wanted to permit stateless Collector implementations, this would be one way to do so. OTOH, it's plausible that Collector implementations will need to store per-collection-job state anyway for implementation-specific reasons. (For example, even with the proposed list-of-collection-jobs API, if there are several independent Collector processes running they will need some form of coordination to avoid repeatedly collecting the same batch; without coordination, there is a race condition s.t. two different Collector processes list collection jobs & decide to collect the same job.)\r\n\r\nBut ultimately, I think this idea is orthogonal to this PR. Let's either file an issue or just write a separate PR for this, if we think it's a good idea?",
          "createdAt": "2023-12-05T22:23:30Z",
          "updatedAt": "2023-12-05T22:43:03Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I think you're right that it's \"begun\" rather than \"began\".\r\n\r\nI updated the text to clarify that the selected batch \"has not yet been associated with a collection job\", which I think clarifies any ambiguity. I think that an aggregator implementation can choose when to associate a given `current-batch` collection request to a specific batch -- e.g. it might immediately do so as part of the `PUT` request handling, or it might choose to do so later -- but this text handles both of those choices.\r\n\r\nOn a side note: I think it's worth calling out that this leaves open whether we need to care about aggregation parameters here. That is, should the semantics be \"the aggregator must select a batch which has not yet begun collection _for any aggregation parameter_\" (which are the semantics we have as-written) or \"not yet begun collection for the aggregation parameter in the collection request\". IMO: I'm OK with going with any aggregation parameter, as this works just fine for all existing uses of DAP & the other choice is harder to implement and harder to understand.",
              "createdAt": "2023-12-05T22:23:30Z",
              "updatedAt": "2023-12-05T22:34:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5pYwh0",
          "commit": {
            "abbreviatedOid": "b8a0eb3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-12-06T16:44:13Z",
          "updatedAt": "2023-12-06T16:44:13Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "I think it's got to be \"for any aggregation parameter\", because otherwise that allows a leader to return the same batch forever.",
              "createdAt": "2023-12-06T16:44:13Z",
              "updatedAt": "2023-12-06T16:44:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5pYxW-",
          "commit": {
            "abbreviatedOid": "5b0ba59"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-06T16:45:58Z",
          "updatedAt": "2023-12-15T00:01:59Z",
          "comments": []
        }
      ]
    },
    {
      "number": 533,
      "id": "PR_kwDOFEJYQs5gns4r",
      "title": "Revise security considerations section.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/533",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Mostly, I condense the information in the threat model section into the top-level security considerations section. I also moved a few sections around, eliminated a couple of no-longer-relevant sections, and gave the entire security considerations section a basic editorial pass.",
      "createdAt": "2023-11-29T02:16:04Z",
      "updatedAt": "2023-12-15T00:09:48Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "571988ca27bc65dad3cd97544c340623330333b8",
      "headRepository": null,
      "headRefName": "bran/threat-model",
      "headRefOid": "e3229a8047e99646cb9a6f6d4bc72dfce4161754",
      "closedAt": "2023-12-15T00:09:41Z",
      "mergedAt": "2023-12-15T00:09:41Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "390bd712c2c5e21fbf52f8478b2860344ad26e35"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5okH1F",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-29T02:20:21Z",
          "updatedAt": "2023-11-29T02:37:55Z",
          "comments": [
            {
              "originalPosition": 119,
              "body": "Please note the change here -- I'm not sure it's useful to define an adversary who can arbitrarily control all of the parties. For example, under the old scenario, the attacker knows the Leader's TLS private key as well as both aggregators' HPKE private keys (since it knows the \"secret state of any party\"), and is in control of the network, and as such can MitM the Client<->Leader connections then decrypt each measurement as it arrives.\r\n\r\nWe could call this out as another item outside of our threat model, but I think it makes more sense to restrict attacker control to at most one aggregator -- DAP & VDAF provide no guarantees if the aggregators collude.",
              "createdAt": "2023-11-29T02:20:21Z",
              "updatedAt": "2023-11-29T02:37:55Z"
            },
            {
              "originalPosition": 325,
              "body": "Is this sentence accurate/not overstating things? If I know a single measurement-with-noise value, and the distribution used for noise, I can determine a probability distribution for the measurement-without-noise value. My understanding is that noise values are typically \"small\", so this distribution would determine a small likely range for the true measurement value.",
              "createdAt": "2023-11-29T02:30:34Z",
              "updatedAt": "2023-11-29T02:37:55Z"
            },
            {
              "originalPosition": 444,
              "body": "I removed this section as the relevant issue is closed, and also because we now specify to two aggregators so aggregator subset selection no longer makes sense. (I think something like this idea could work if we had 3+ independent DAP aggregator deployments, sent each report to all deployments, and collected from all of them -- but IMO that's outside of the scope of what needs to be mentioned in the DAP spec.)",
              "createdAt": "2023-11-29T02:35:15Z",
              "updatedAt": "2023-11-29T02:37:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5opTSW",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-29T16:09:23Z",
          "updatedAt": "2023-11-29T16:09:23Z",
          "comments": [
            {
              "originalPosition": 41,
              "body": "Note to self: move assets to an appendix (unless I figure out a clever way to keep it in the main text).",
              "createdAt": "2023-11-29T16:09:23Z",
              "updatedAt": "2023-11-29T16:09:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5oz2dk",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-11-30T20:44:33Z",
          "updatedAt": "2023-11-30T21:42:15Z",
          "comments": [
            {
              "originalPosition": 119,
              "body": "Agreed, DAP can't stand up to an attacker that knows everything. This threat model can be addressed to some extent with randomized response, a la https://github.com/wangshan/draft-wang-ppm-differential-privacy/, but not with core DAP. I don't think it's worth mentioning.",
              "createdAt": "2023-11-30T20:44:33Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 127,
              "body": "I'd suggest splitting this into three lists: privacy attacks, robustness attacks, and other attacks.\r\n\r\nI think the fingerprinting issues fall into the \"other\" category. They are attacks on privacy in an intuitive sense, but they are technically outside the threat model for VDAF privacy.\r\n",
              "createdAt": "2023-11-30T20:54:40Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 169,
              "body": "Worth pointing out this may also happen accidently (imagine the Aggregators share service logs with one another and input share accidently makes its way into the logs).",
              "createdAt": "2023-11-30T20:56:09Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 173,
              "body": "```suggestion\r\n   omitting reports from the aggregation process, or by manipulating the VDAF preparation process for a single report.\r\n```",
              "createdAt": "2023-11-30T20:58:11Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 189,
              "body": "Not specific to any VDAF. I think Prio3Sum is meant only as an example, but calling it out specifically may make a casual reader think it's more vulnerable than other VDAFs.\r\n```suggestion\r\n   heavy hitters themselves. Or the result could leak\r\n```",
              "createdAt": "2023-11-30T21:00:33Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 194,
              "body": "This issue has been closed, so let's delete this paragraph.",
              "createdAt": "2023-11-30T21:01:25Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 236,
              "body": "This open issue is redundant and can be removed. We also plan to call out this threat model in the VDAF draft: https://github.com/cfrg/draft-irtf-cfrg-vdaf/issues/304",
              "createdAt": "2023-11-30T21:06:22Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 254,
              "body": "```suggestion\r\ndeployment might be measuring the heights of a human population and configure a\r\nvariant of Prio3 to prove that measurements are values in the range of 80-250 cm. A\r\n```",
              "createdAt": "2023-11-30T21:07:52Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 252,
              "body": "```suggestion\r\nSeveral attacks on robustness or privacy involve malicious Clients uploading\r\nreports that are valid under the chosen VDAF but incorrect.\r\n\r\nFor example, a DAP\r\n```",
              "createdAt": "2023-11-30T21:09:46Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 275,
              "body": "We should reference the privacy pass draft: https://datatracker.ietf.org/doc/draft-ietf-privacypass-architecture/\r\n\r\nIt's in WGLC and shouldn't be a blocker for us.",
              "createdAt": "2023-11-30T21:21:41Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 321,
              "body": "```suggestion\r\ndifferential privacy {{Vad16}}. A simple approach would require the Aggregators to\r\n```",
              "createdAt": "2023-11-30T21:22:35Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 331,
              "body": "I would probably not add this dependency because it creates a cyclic dependency. (One of these documents will become an RFC before the other.)",
              "createdAt": "2023-11-30T21:23:23Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 279,
              "body": "We don't want to over-promise here. Whether/how DP is used to beat Sybil attacks is not straightforward.\r\n```suggestion\r\n1. Differential privacy ({{dp}}) can help mitigate Sybil attacks to some extent.\r\n```",
              "createdAt": "2023-11-30T21:25:30Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 314,
              "body": "```suggestion\r\n{{!I-D.draft-ietf-ohai-ohttp-10}} to forward reports to the DAP Leader. In this\r\n```",
              "createdAt": "2023-11-30T21:26:56Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 341,
              "body": "```suggestion\r\ninvolving crafted DAP task parameters can be mitigated by having the\r\n```",
              "createdAt": "2023-11-30T21:28:17Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 299,
              "body": "In the previous sections we have been capitalizing subsection titles. (Here and below.)\r\n```suggestion\r\n## Anonymizing Proxies {#anon-proxy}\r\n```",
              "createdAt": "2023-11-30T21:29:39Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 345,
              "body": "```suggestion\r\n### VDAF Verification key requirements {#verification-key}\r\n```",
              "createdAt": "2023-11-30T21:31:21Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 361,
              "body": "We don't know this for sure: the important thing is that our threat model assumes the verification key does not depend on the me measurements.\r\n\r\n```suggestion\r\nFurthermore, for a given report, it may be possible to craft a verification key\r\n```",
              "createdAt": "2023-11-30T21:31:43Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 384,
              "body": "Perhaps note that the batch size may be determined by DP parameters.",
              "createdAt": "2023-11-30T21:33:49Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 396,
              "body": "```suggestion\r\nmultiple rounds of collection are needed for each bit of the measurement value,\r\n```",
              "createdAt": "2023-11-30T21:34:25Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 423,
              "body": "The attack angle here feels thin. Previously we had an \"operational considerations\" section (I forget what it was called): I think this subsubsection fits more naturally there.",
              "createdAt": "2023-11-30T21:38:01Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            },
            {
              "originalPosition": 415,
              "body": "Let's resolve this here as follows: If the padding is reached, then add the padded measurement into the last set of candidate prefixes.",
              "createdAt": "2023-11-30T21:40:34Z",
              "updatedAt": "2023-11-30T21:42:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o1X8k",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-12-01T01:01:07Z",
          "updatedAt": "2023-12-01T01:01:08Z",
          "comments": [
            {
              "originalPosition": 423,
              "body": "Hmm -- while availability/denial-of-service is often considered a security concern, I agree this section doesn't fit in well since all the other sections are more related to the robustness/privacy properties. Moved.\r\n\r\nThere was already a subsection (\"Data Resolution Limitations\") in the Operational Considerations section which touches on basically the same ideas; I pulled a little bit of information in from that subsection & eliminated it.",
              "createdAt": "2023-12-01T01:01:08Z",
              "updatedAt": "2023-12-01T01:01:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o1eXk",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-12-01T01:29:20Z",
          "updatedAt": "2023-12-01T01:29:20Z",
          "comments": [
            {
              "originalPosition": 127,
              "body": "Sure -- I include \"the client can lie, or submit multiple reports, to skew the results\" in attacks on robustness. I think this is technically correct; VDAF defines robustness as \"An attacker that controls the network and a subset of Clients cannot cause the Collector to compute anything other than the aggregate of the measurements of honest Clients.\" DAP will cause the Collector to compute the aggregate of the measurements of honest _and dishonest_ Clients.",
              "createdAt": "2023-12-01T01:29:20Z",
              "updatedAt": "2023-12-01T01:29:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o1kEm",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-12-01T01:53:34Z",
          "updatedAt": "2023-12-01T01:53:34Z",
          "comments": [
            {
              "originalPosition": 325,
              "body": "IIUC, I think what I say is true for \"Client DP\" (noise added by client to original measurement), but not \"Aggregator DP\" (noise added by aggregator to aggregate share), looking to https://www.ietf.org/archive/id/draft-wang-ppm-differential-privacy-00.html. I am definitely not a DP expert, but I'm happy leaving this text as-is.",
              "createdAt": "2023-12-01T01:53:34Z",
              "updatedAt": "2023-12-01T01:53:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o1kuz",
          "commit": {
            "abbreviatedOid": "a7a2d5b"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2023-12-01T01:57:05Z",
          "updatedAt": "2023-12-01T01:57:06Z",
          "comments": [
            {
              "originalPosition": 41,
              "body": "Looking back to this list of assets, all of the assets are enumerated in the list of attacks. I think it's more useful to specify the assets with the context of how they can be used/compromised in an attack.",
              "createdAt": "2023-12-01T01:57:06Z",
              "updatedAt": "2023-12-01T01:57:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5o7Let",
          "commit": {
            "abbreviatedOid": "f5be2bf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-01T18:15:51Z",
          "updatedAt": "2023-12-01T18:15:51Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5pF8ZG",
          "commit": {
            "abbreviatedOid": "e3229a8"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-04T18:53:17Z",
          "updatedAt": "2023-12-15T00:08:20Z",
          "comments": []
        }
      ]
    },
    {
      "number": 534,
      "id": "PR_kwDOFEJYQs5g1Hq-",
      "title": "Update Change Log for draft09",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/534",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "cc/ @branlwyd for review\r\n* Please update the change log (in a new PR) after merging:\r\n   * https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/532\r\n   * https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/533 (note any non-editorial changes, i.e., any security considerations that have been removed, modified, or added)\r\n\r\ncc/ @wangshan for review",
      "createdAt": "2023-11-30T22:05:11Z",
      "updatedAt": "2023-12-05T17:12:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "2225230fcbf662280031e6d527d9c720814b1ba8",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/next-dap-cl",
      "headRefOid": "00a6bde0ad41e4208814928648ce31c769e9593f",
      "closedAt": "2023-12-05T17:12:43Z",
      "mergedAt": "2023-12-05T17:12:43Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "b015359e95daaa79f7a527876d6b0fb74e873ecd"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5pPcph",
          "commit": {
            "abbreviatedOid": "c4cb669"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-05T16:58:12Z",
          "updatedAt": "2023-12-05T16:58:12Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5pPjZG",
          "commit": {
            "abbreviatedOid": "c4cb669"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-05T17:10:05Z",
          "updatedAt": "2023-12-05T17:10:23Z",
          "comments": [
            {
              "originalPosition": 6,
              "body": "```suggestion\r\n- Fixed-size queries: make the maximum batch size optional.\r\n```",
              "createdAt": "2023-12-05T17:10:06Z",
              "updatedAt": "2023-12-05T17:10:23Z"
            }
          ]
        }
      ]
    },
    {
      "number": 535,
      "id": "PR_kwDOFEJYQs5g1PO0",
      "title": "Add contributors section",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/535",
      "state": "MERGED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2023-11-30T22:34:06Z",
      "updatedAt": "2023-12-04T16:30:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "0ac3b865d8bb91371d3eb1807f6dbe2801a3e131",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/next-dap-ack",
      "headRefOid": "60597c7e49cbe134690eadb2c13bb271bd0e1045",
      "closedAt": "2023-12-04T16:30:52Z",
      "mergedAt": "2023-12-04T16:30:52Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "cb2c4baacedd0b89cb83c63b183fd5ea22ff833a"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5o1f-s",
          "commit": {
            "abbreviatedOid": "60597c7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-01T01:34:58Z",
          "updatedAt": "2023-12-01T01:34:58Z",
          "comments": []
        }
      ]
    },
    {
      "number": 536,
      "id": "PR_kwDOFEJYQs5hE-Y3",
      "title": "Add Brandon Pitman as a DAP author",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/536",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Per consensus of the existing DAP authors and the PPM chairs, this commit adds Brandon Pitman as an author of the Distributed Aggregation Protocol.",
      "createdAt": "2023-12-04T16:46:52Z",
      "updatedAt": "2023-12-04T17:30:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "cb2c4baacedd0b89cb83c63b183fd5ea22ff833a",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/add-bran-author",
      "headRefOid": "b678dafec1410423a89d472d3fc2617eb8852dcd",
      "closedAt": "2023-12-04T16:47:29Z",
      "mergedAt": "2023-12-04T16:47:29Z",
      "mergedBy": "chris-wood",
      "mergeCommit": {
        "oid": "c0cfab7c566409283b20d8fb3316f93891ce4547"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd Would you please double check that I've put in your information correctly?",
          "createdAt": "2023-12-04T16:47:15Z",
          "updatedAt": "2023-12-04T16:47:15Z"
        },
        {
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "body": "Oops, Tim's comment crossed the air as I merged. ",
          "createdAt": "2023-12-04T16:47:56Z",
          "updatedAt": "2023-12-04T16:47:56Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> Oops, Tim's comment crossed the air as I merged.\r\n\r\n\ud83e\udd37\ud83c\udffb We can make another change to correct errors.",
          "createdAt": "2023-12-04T16:52:14Z",
          "updatedAt": "2023-12-04T16:52:14Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "No worries, y'all got it right.",
          "createdAt": "2023-12-04T16:54:59Z",
          "updatedAt": "2023-12-04T16:54:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "We also need to remove him from the contributor's list.",
          "createdAt": "2023-12-04T16:55:02Z",
          "updatedAt": "2023-12-04T16:55:02Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> We also need to remove him from the contributor's list.\r\n\r\nhttps://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/537",
          "createdAt": "2023-12-04T17:30:39Z",
          "updatedAt": "2023-12-04T17:30:39Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5pFBUP",
          "commit": {
            "abbreviatedOid": "b678daf"
          },
          "author": "chris-wood",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-04T16:47:25Z",
          "updatedAt": "2023-12-04T16:47:25Z",
          "comments": []
        }
      ]
    },
    {
      "number": 537,
      "id": "PR_kwDOFEJYQs5hFQAL",
      "title": "Remove Brandon (now author) from contributors",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/537",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Add them to CODEOWNERS, too, so they get tagged on PRs.",
      "createdAt": "2023-12-04T17:30:31Z",
      "updatedAt": "2023-12-05T01:20:34Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c0cfab7c566409283b20d8fb3316f93891ce4547",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/remove-bran-contributor",
      "headRefOid": "abd48fb2178376693b4d363478eaef5e19c4ba16",
      "closedAt": "2023-12-05T01:20:34Z",
      "mergedAt": "2023-12-05T01:20:34Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "2225230fcbf662280031e6d527d9c720814b1ba8"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5pFY8q",
          "commit": {
            "abbreviatedOid": "17ef2de"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-04T17:35:03Z",
          "updatedAt": "2023-12-04T17:35:03Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5pFcuB",
          "commit": {
            "abbreviatedOid": "abd48fb"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-04T17:43:40Z",
          "updatedAt": "2023-12-04T17:43:40Z",
          "comments": []
        }
      ]
    },
    {
      "number": 538,
      "id": "PR_kwDOFEJYQs5iDld8",
      "title": "Further update changelog for DAP-09.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/538",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "The added lines correspond to changes #532 & #533.",
      "createdAt": "2023-12-15T00:28:56Z",
      "updatedAt": "2023-12-15T00:43:02Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "390bd712c2c5e21fbf52f8478b2860344ad26e35",
      "headRepository": null,
      "headRefName": "bran/changelog",
      "headRefOid": "490a67acc5067cc5da90d49a4bf3f90e75167ef3",
      "closedAt": "2023-12-15T00:42:59Z",
      "mergedAt": "2023-12-15T00:42:59Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "1d40f97e89a5147acc6706776dd6785a34682391"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5qRfMd",
          "commit": {
            "abbreviatedOid": "490a67a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2023-12-15T00:42:14Z",
          "updatedAt": "2023-12-15T00:42:14Z",
          "comments": []
        }
      ]
    },
    {
      "number": 539,
      "id": "PR_kwDOFEJYQs5jEmW-",
      "title": "Fix a couple typos",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/539",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "See [\"confusables note For bear\"](https://www.dictionary.com/browse/bear) to justify \"borne\" vs. \"born\".",
      "createdAt": "2024-01-02T18:22:29Z",
      "updatedAt": "2024-01-08T15:50:46Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1d40f97e89a5147acc6706776dd6785a34682391",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/typos",
      "headRefOid": "40a0997c063bdabdc769b55626b50238e4e56479",
      "closedAt": "2024-01-08T15:50:46Z",
      "mergedAt": "2024-01-08T15:50:46Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "99011a0746db28410e1292006ab7f19ee0dc21b1"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5rVoxd",
          "commit": {
            "abbreviatedOid": "40a0997"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-02T18:23:19Z",
          "updatedAt": "2024-01-02T18:23:19Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5r2duH",
          "commit": {
            "abbreviatedOid": "40a0997"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-08T15:50:41Z",
          "updatedAt": "2024-01-08T15:50:41Z",
          "comments": []
        }
      ]
    },
    {
      "number": 540,
      "id": "PR_kwDOFEJYQs5jEsHI",
      "title": "Address some items from httpdir early review",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/540",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Mark Nottingham (@mnot) was kind enough to share early review feedback on DAP on behalf of httpdir ([1]). This PR addresses several of the items there. See individual commit messages for discussion.\r\n\r\n[1]: https://datatracker.ietf.org/doc/review-ietf-ppm-dap-09-httpdir-early-nottingham-2023-12-29/",
      "createdAt": "2024-01-02T18:49:38Z",
      "updatedAt": "2024-01-17T02:20:04Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1d40f97e89a5147acc6706776dd6785a34682391",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/endpoint",
      "headRefOid": "b47c136ff201b9b82ff9018c3b4a4b0a0369312f",
      "closedAt": "2024-01-17T02:20:04Z",
      "mergedAt": "2024-01-17T02:20:04Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "76237517fbd8e8e78de43dba1f01729dba22eb89"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "Draft for now -- I think I want to stack some related commits on this.",
          "createdAt": "2024-01-02T18:53:07Z",
          "updatedAt": "2024-01-02T21:08:25Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5rdp7u",
          "commit": {
            "abbreviatedOid": "4bcd0a7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-03T19:16:12Z",
          "updatedAt": "2024-01-03T19:47:38Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "I am not sure `server` is appropriate here (or for `helper_aggregator_server` below) -- I think either `{leader,helper}_aggregator_endpoint` or perhaps `{leader,helper}_aggregator_url` would be clearer. I'd expect a server to indicate a hostname, while this is a full URL including a protocol & path.",
              "createdAt": "2024-01-03T19:16:13Z",
              "updatedAt": "2024-01-03T19:47:39Z"
            },
            {
              "originalPosition": 33,
              "body": "```suggestion\r\n: A DAP protocol role identifying a party that uploads a report. Note the\r\n```\r\n\r\n(editorial, matching structure of other items in this list)",
              "createdAt": "2024-01-03T19:25:22Z",
              "updatedAt": "2024-01-03T19:47:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5rdzs5",
          "commit": {
            "abbreviatedOid": "5dba50d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-03T19:48:41Z",
          "updatedAt": "2024-01-03T19:48:41Z",
          "comments": [
            {
              "originalPosition": 56,
              "body": "(I'd go with \"endpoint\", but I think that an endpoint might also indicate a particular HTTP verb; if that's correct, I'd go with \"URL.\")",
              "createdAt": "2024-01-03T19:48:41Z",
              "updatedAt": "2024-01-03T19:48:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5reMEP",
          "commit": {
            "abbreviatedOid": "4bcd0a7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-03T21:12:49Z",
          "updatedAt": "2024-01-03T21:12:49Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "(This comment follows from off-PR discussion with/suggestion by @jbr.)\r\n\r\n\"small set of aggregator servers\" is correct but perhaps imprecise -- nowadays, there are exactly two aggregator servers, the Leader & the Helper. Maybe \"two aggregator servers, called the Leader & the Helper\"?",
              "createdAt": "2024-01-03T21:12:49Z",
              "updatedAt": "2024-01-03T21:12:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5reM8L",
          "commit": {
            "abbreviatedOid": "4bcd0a7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-03T21:16:04Z",
          "updatedAt": "2024-01-03T21:19:23Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "Makes sense! I left out the discussion of leader and helper because those terms aren't used again in the introduction and they get introduced properly in the glossary.",
              "createdAt": "2024-01-03T21:16:04Z",
              "updatedAt": "2024-01-03T21:19:23Z"
            },
            {
              "originalPosition": 33,
              "body": "Sure. \"The\" feels a bit more natural than \"A\" to me.",
              "createdAt": "2024-01-03T21:16:50Z",
              "updatedAt": "2024-01-03T21:19:23Z"
            },
            {
              "originalPosition": 56,
              "body": "`_url` seems better since that's what the text on the right of the colon says. I want to avoid \"endpoint\" as the httpdir review flagged that word.",
              "createdAt": "2024-01-03T21:18:59Z",
              "updatedAt": "2024-01-03T21:19:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5s19S3",
          "commit": {
            "abbreviatedOid": "9c26647"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-17T00:58:18Z",
          "updatedAt": "2024-01-17T00:59:07Z",
          "comments": [
            {
              "originalPosition": 76,
              "body": "Is this the correct reference? RFC8446 defines TLS, not HTTPS.",
              "createdAt": "2024-01-17T00:58:19Z",
              "updatedAt": "2024-01-17T00:59:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5s2HTp",
          "commit": {
            "abbreviatedOid": "9c26647"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-17T01:08:27Z",
          "updatedAt": "2024-01-17T01:08:27Z",
          "comments": [
            {
              "originalPosition": 76,
              "body": "Hmm, good point. RFC 9110 obsoletes RFC 2818 \"HTTP Over TLS\", so maybe the reference to 9110 suffices?",
              "createdAt": "2024-01-17T01:08:27Z",
              "updatedAt": "2024-01-17T01:08:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5s2I8i",
          "commit": {
            "abbreviatedOid": "9c26647"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-17T01:10:07Z",
          "updatedAt": "2024-01-17T01:10:07Z",
          "comments": [
            {
              "originalPosition": 76,
              "body": "That makes sense to me.",
              "createdAt": "2024-01-17T01:10:07Z",
              "updatedAt": "2024-01-17T01:10:07Z"
            }
          ]
        }
      ]
    },
    {
      "number": 542,
      "id": "PR_kwDOFEJYQs5jnFa4",
      "title": "require error responses to be consistent",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/542",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "# Stacked on #540 \r\n\r\nBased on httpdir early review of DAP ([1]), amend the text on error handling:\r\n\r\n- fix non-sequitur reference to iana-considerations and \"challenge objects\"\r\n- require that responses containing problem documents use an error HTTP status\r\n\r\n[1]: https://datatracker.ietf.org/doc/review-ietf-ppm-dap-09-httpdir-early-nottingham-2023-12-29/",
      "createdAt": "2024-01-09T19:00:56Z",
      "updatedAt": "2024-01-17T02:20:53Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "timg/endpoint",
      "baseRefOid": "9c26647a6e15e8fcac51ba9d793bbcbf022341de",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/error-consistency",
      "headRefOid": "1b1bff31d865fbb7a78c9396a29f808b1a929957",
      "closedAt": "2024-01-17T02:20:53Z",
      "mergedAt": "2024-01-17T02:20:53Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "bc0c0b999fb33af7705b8432f229095a3a03c6e4"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5sGqRQ",
          "commit": {
            "abbreviatedOid": "1b1bff3"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-10T16:56:23Z",
          "updatedAt": "2024-01-10T16:56:23Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5sG7L_",
          "commit": {
            "abbreviatedOid": "1b1bff3"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-10T17:31:12Z",
          "updatedAt": "2024-01-10T17:31:12Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5s1_Ce",
          "commit": {
            "abbreviatedOid": "1b1bff3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-17T01:00:02Z",
          "updatedAt": "2024-01-17T01:00:02Z",
          "comments": []
        }
      ]
    },
    {
      "number": 543,
      "id": "PR_kwDOFEJYQs5joElK",
      "title": "Use HTTP POST for report uploads",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/543",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Per httpdir early review of DAP ([1]), it's inappropriate to use PUT for a resource that cannot later be GET. Change the HTTP method for report uploads to POST, and add a note making it clear that uploads are idempotent and safe to retry.\r\n\r\n[1]: https://datatracker.ietf.org/doc/review-ietf-ppm-dap-09-httpdir-early-nottingham-2023-12-29/\r\n\r\nPrevious description of this PR:\r\n~For that reason, and also consistency with the aggregation job and collection job request paths, hoist the report ID out of the upload request body and into the request path. We don't want to repeat the report ID twice in one request, so `struct Report` no longer includes a `struct Metadata` and instead just puts the `time` inline in `Report`. `struct ReportMetadata` is still used in `InputShareAad` and elsewhere in DAP, so its definition is not changed, just moved farther down the document, and we explain how to construct it from the report request path and body.~\r\n\r\n~With this change, there is now a unique URI for each report uploaded to a DAP leader. However we do not add any requirement that an Aggregator support GET requests on that path, because not all DAP implementations will necessarily store enough per-report information to be able to respond to such a request with a complete `struct Report`. Implementations are free to do that, but can also respond with HTTP 204, 404, 410 or some other error if they wish. As these are well-established HTTP semantics, DAP doesn't need to spend any more ink on explaining them.~",
      "createdAt": "2024-01-09T22:19:37Z",
      "updatedAt": "2024-01-25T17:48:14Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "cc7e5f4ae10f3ff5f28a23d330105f3dd41fdb35",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/report-id-request-path",
      "headRefOid": "806d329e4512399d1f5ef26e0c7d0b98ef36175e",
      "closedAt": "2024-01-25T17:48:14Z",
      "mergedAt": "2024-01-25T17:48:14Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "8f1f29daf32e3bdc769ccc65938d421be923e56c"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I lean toward using PUT and changing the request path because:\r\n\r\n1. It makes the report upload request path consistent with the aggregation job and collection job request paths, which include their respective unique job IDs.\r\n1. Using PUT elegantly makes it clear that this request is idempotent and safe to retry (of course if we go to a POST we can just explain in the DAP text that it's an idempotent POST).\r\n1. While this forces a wire-breaking change, I think we're going to do draft 10 of DAP anyway, and that will mean a wire-breaking change to update the HPKE domain separation strings. Making this change to request paths and message formats isn't zero risk, but I think it's a marginal lift for implementations if they're breaking wire compatibility anyway.",
          "createdAt": "2024-01-10T17:59:31Z",
          "updatedAt": "2024-01-10T17:59:31Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@tgeoghegan I read through the mailing list but don't get why this change is necessary? it seems specifying idempotent POST will do the same? Will we ever have a good reason to support GET a report? ",
          "createdAt": "2024-01-15T18:45:34Z",
          "updatedAt": "2024-01-15T18:45:34Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> @tgeoghegan I read through the mailing list but don't get why this change is necessary? it seems specifying idempotent POST will do the same? Will we ever have a good reason to support GET a report?\r\n\r\nI agree that this change is not necessary, in that the protocol will work fine if we do nothing, or if we switch to a POST request and otherwise change nothing. I laid out my arguments for this change [above](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/543#issuecomment-1885351964), and concede that my arguments are essentially esthetic.\r\n\r\nFWIW I don't intend to move forward with this change until and unless we establish WG rough consensus for it. I'll be following up on the mailing list with some concise discussion points to move this and other topics forward.",
          "createdAt": "2024-01-16T18:48:01Z",
          "updatedAt": "2024-01-16T18:48:01Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "For what it's worth, I think a `POST` is ultimately a smaller change that arrives at the desired behavior.\r\n\r\nI don't see much use for a `GET` of uploaded reports, and implementations will plausibly delete report content as soon as possible to reduce storage costs (which would make `GET` not so useful).",
          "createdAt": "2024-01-16T19:32:24Z",
          "updatedAt": "2024-01-16T19:32:24Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan please rebase when you have a moment",
          "createdAt": "2024-01-17T16:04:54Z",
          "updatedAt": "2024-01-17T16:04:54Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This change has been up for quite a while and it seems to me that the most popular option, by dint of its simplicity, is to change the request upload to using POST without changing the request path or the request body. Accordingly I'm repurposing this PR to make that change.",
          "createdAt": "2024-01-24T21:38:45Z",
          "updatedAt": "2024-01-24T21:38:45Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5sHB7m",
          "commit": {
            "abbreviatedOid": "e758193"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I think this change is fine. However, as discussed off-PR (pointed out by @simon-friedberger, I think), switching from `PUT` to `POST` might be a smaller change. Are there downsides to using `POST`?",
          "createdAt": "2024-01-10T17:45:44Z",
          "updatedAt": "2024-01-10T17:45:44Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5sHKAw",
          "commit": {
            "abbreviatedOid": "e758193"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-10T18:06:14Z",
          "updatedAt": "2024-01-10T18:06:14Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5sm77A",
          "commit": {
            "abbreviatedOid": "e758193"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-15T18:46:28Z",
          "updatedAt": "2024-01-15T18:46:28Z",
          "comments": [
            {
              "originalPosition": 86,
              "body": "Does ReportMetadata still have meaning? would it be simpler if we simply put reportID and time in InputShareAad?",
              "createdAt": "2024-01-15T18:46:28Z",
              "updatedAt": "2024-01-15T18:46:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5s9IJ4",
          "commit": {
            "abbreviatedOid": "e758193"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-01-17T18:47:23Z",
          "updatedAt": "2024-01-17T18:47:23Z",
          "comments": [
            {
              "originalPosition": 86,
              "body": "`struct ReportMetadata` ends up being useful in the aggregation interaction (e.g. it goes into a `ReportShare`) so I think it's useful. ",
              "createdAt": "2024-01-17T18:47:23Z",
              "updatedAt": "2024-01-17T18:47:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5trH9A",
          "commit": {
            "abbreviatedOid": "571e7c6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "No objection to this change!",
          "createdAt": "2024-01-23T22:02:14Z",
          "updatedAt": "2024-01-23T22:02:14Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5t1FNZ",
          "commit": {
            "abbreviatedOid": "72f5b95"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Works for me. I think this is slightly better because it's clearer that reports aren't necessarily GETable once they are PUT (or POSTed, now).",
          "createdAt": "2024-01-25T00:17:45Z",
          "updatedAt": "2024-01-25T00:17:45Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5t1KsG",
          "commit": {
            "abbreviatedOid": "72f5b95"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-25T00:44:15Z",
          "updatedAt": "2024-01-25T00:44:29Z",
          "comments": [
            {
              "originalPosition": 21,
              "body": "```suggestion\r\n`{helper}/tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` would be expanded into\r\n```\r\n\r\n(editorial nit)",
              "createdAt": "2024-01-25T00:44:15Z",
              "updatedAt": "2024-01-25T00:44:29Z"
            }
          ]
        }
      ]
    },
    {
      "number": 544,
      "id": "PR_kwDOFEJYQs5jt2Yw",
      "title": "Use GET rather than POST when polling collection jobs.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/544",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This was first attempted as part of\r\n571988ca27bc65dad3cd97544c340623330333b8, since the original reason for using POST was removed; but the switch to GET was reverted to avoid including (too many?) breaking wire changes.\r\n\r\nNow that we are applying feedback from the HTTP directorate, we'll have a number of similar breaking changes in the next DAP draft. I think this is a good time to include this change.",
      "createdAt": "2024-01-10T18:43:16Z",
      "updatedAt": "2024-01-22T22:08:44Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "99011a0746db28410e1292006ab7f19ee0dc21b1",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "bran/get-for-collection-polling",
      "headRefOid": "2975cc4f20d61fb10f348c9371ceeef497b5bdd2",
      "closedAt": "2024-01-22T22:08:40Z",
      "mergedAt": "2024-01-22T22:08:40Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "cc7e5f4ae10f3ff5f28a23d330105f3dd41fdb35"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd I think we have enough consensus to take this change now. Anything else you want to do before we merge?",
          "createdAt": "2024-01-17T02:21:25Z",
          "updatedAt": "2024-01-17T02:21:25Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5sHdur",
          "commit": {
            "abbreviatedOid": "2975cc4"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Yes, I think this is the right time for this change. Thanks for remembering it.",
          "createdAt": "2024-01-10T18:52:58Z",
          "updatedAt": "2024-01-10T18:52:58Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5s16Pq",
          "commit": {
            "abbreviatedOid": "2975cc4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-17T00:55:19Z",
          "updatedAt": "2024-01-17T00:55:19Z",
          "comments": []
        }
      ]
    },
    {
      "number": 545,
      "id": "PR_kwDOFEJYQs5kQ7_H",
      "title": "require error responses to be consistent, again",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/545",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Based on httpdir early review of DAP ([1]), amend the text on error handling:\r\n\r\n- fix non-sequitur reference to iana-considerations and \"challenge objects\"\r\n- require that responses containing problem documents use an error HTTP status\r\n\r\n[1]: https://datatracker.ietf.org/doc/review-ietf-ppm-dap-09-httpdir-early-nottingham-2023-12-29/\r\n\r\nThis is exactly the same as #542, but I accidentally merged that change into the wrong branch.",
      "createdAt": "2024-01-17T02:25:38Z",
      "updatedAt": "2024-01-17T14:54:10Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "76237517fbd8e8e78de43dba1f01729dba22eb89",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/error-consistency",
      "headRefOid": "4e1593d7ebc2077df1925615eb56038911588334",
      "closedAt": "2024-01-17T14:54:09Z",
      "mergedAt": "2024-01-17T14:54:09Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "3aee3f8ba87c73aacadf03143e6a3e6d5d91ff9f"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5s29nI",
          "commit": {
            "abbreviatedOid": "4e1593d"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-17T02:27:26Z",
          "updatedAt": "2024-01-17T02:27:26Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5s7K71",
          "commit": {
            "abbreviatedOid": "4e1593d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-01-17T14:36:51Z",
          "updatedAt": "2024-01-17T14:36:51Z",
          "comments": []
        }
      ]
    },
    {
      "number": 547,
      "id": "PR_kwDOFEJYQs5nJjJ2",
      "title": "DAP Version info in mime types",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/547",
      "state": "CLOSED",
      "author": "suman-ganta",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "This adds support for expressing dap versions in the mime types. This addresses the issue #541 ",
      "createdAt": "2024-02-17T02:22:27Z",
      "updatedAt": "2024-07-08T23:17:55Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8f1f29daf32e3bdc769ccc65938d421be923e56c",
      "headRepository": "suman-ganta/draft-ietf-ppm-dap",
      "headRefName": "dap-version",
      "headRefOid": "6236ff0d53a38456d539aa151fd171a2407991f9",
      "closedAt": "2024-07-08T23:17:55Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5wwxdH",
          "commit": {
            "abbreviatedOid": "e9409be"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "The idea here is to version media types following \"semver2\", presumably as described in https://semver.org/. This seems like a reasonable way to address the issue, but we'll need two bits of information:\r\n\r\n1. How do we construct media types with versions? Describe how the string is formatted.\r\n2. What is the version of the current draft?\r\n\r\nAlso, are there side-effects of making the version optional that we need to account for? Suppose a versioned client talks to an unversioned server.",
          "createdAt": "2024-02-21T00:40:21Z",
          "updatedAt": "2024-02-21T00:46:21Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "(Showing my lack of experience with HTTP here :/) What does it mean for \"version\" to be an optional parameter of a media type? Is this a string I am supposed to append to the media type string? If so, how do I construct this string?\r\n\r\nIf you're following a pattern in another document, could drop a link here?",
              "createdAt": "2024-02-21T00:40:21Z",
              "updatedAt": "2024-02-21T00:45:15Z"
            }
          ]
        }
      ]
    },
    {
      "number": 549,
      "id": "PR_kwDOFEJYQs5nynLq",
      "title": "Update reference to OHTTP as RFC 9458",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/549",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2024-02-23T20:22:45Z",
      "updatedAt": "2024-02-23T22:51:19Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "8f1f29daf32e3bdc769ccc65938d421be923e56c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/update-ohttp-reference",
      "headRefOid": "2030cea36d0b039a90152bad9cf45772446e2f1a",
      "closedAt": "2024-02-23T22:51:19Z",
      "mergedAt": "2024-02-23T22:51:19Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "3448ba400a628b18a785e069d544314c1bbff8c6"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5xLtN7",
          "commit": {
            "abbreviatedOid": "2030cea"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-02-23T20:38:16Z",
          "updatedAt": "2024-02-23T20:38:16Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs5xL0J3",
          "commit": {
            "abbreviatedOid": "2030cea"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-02-23T21:00:21Z",
          "updatedAt": "2024-02-23T21:00:21Z",
          "comments": []
        }
      ]
    },
    {
      "number": 550,
      "id": "PR_kwDOFEJYQs5oLvSS",
      "title": "Recommend batches above minimum size for Poplar1",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/550",
      "state": "MERGED",
      "author": "divergentdave",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This PR addresses the possibility that reports could be successfully prepared with a first aggregation parameter, but fail preparation with a subsequent aggregation parameter. A suggestion is added to increase actual batch size above the minimum batch size. The description of the fixed size query type is loosened -- the goal is no longer to create batches of the minimum batch size, but to create batches of a deployment-specific target batch size.\r\n\r\nThis closes #548.",
      "createdAt": "2024-02-28T15:02:00Z",
      "updatedAt": "2024-02-28T23:26:51Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3448ba400a628b18a785e069d544314c1bbff8c6",
      "headRepository": "divergentdave/ppm-specification",
      "headRefName": "david/max-batch-size-recommendation",
      "headRefOid": "545dd66360fd6002bf11019785d0cdb7d952f398",
      "closedAt": "2024-02-28T23:26:51Z",
      "mergedAt": "2024-02-28T23:26:51Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "d80ccbee79014ca06135b51cc9eedf6ae0bc727b"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5xo8f3",
          "commit": {
            "abbreviatedOid": "fcc9c31"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Looks good. Remember to use \"SHOULD\" instead of \"should\" when you want to clearly specify recommended behavior. Likewise for \"may\" and \"must\".",
          "createdAt": "2024-02-28T15:23:25Z",
          "updatedAt": "2024-02-28T15:29:23Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "```suggestion\r\nsize is in part intended to allow room for rejecting some number of invalid reports, and allow a range of target\r\n```",
              "createdAt": "2024-02-28T15:23:25Z",
              "updatedAt": "2024-02-28T15:29:23Z"
            },
            {
              "originalPosition": 47,
              "body": "Instead of \"pass/fail preparation\", I prefer the verbiage \"accept/reject\".\r\n```suggestion\r\nthat some reports may be accepted with the first\r\naggregation parameter, but be rejected with a subsequent aggregation parameter. Once a\r\n```",
              "createdAt": "2024-02-28T15:26:54Z",
              "updatedAt": "2024-02-28T15:29:23Z"
            },
            {
              "originalPosition": 52,
              "body": "```suggestion\r\nif enough reports fail validation such that fewer than `min_batch_size` output shares\r\n```",
              "createdAt": "2024-02-28T15:27:24Z",
              "updatedAt": "2024-02-28T15:29:23Z"
            },
            {
              "originalPosition": 43,
              "body": "This paragraph is a little long ... see if there is a way to split it up into more digestable chunks.",
              "createdAt": "2024-02-28T15:28:23Z",
              "updatedAt": "2024-02-28T15:29:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5xqdrD",
          "commit": {
            "abbreviatedOid": "fcc9c31"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-02-28T18:18:31Z",
          "updatedAt": "2024-02-28T18:20:35Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "Rejecting invalid reports isn't the only reason an aggregator could deviate from the target batch size, so I think David's more general text is better.",
              "createdAt": "2024-02-28T18:18:31Z",
              "updatedAt": "2024-02-28T18:20:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5xqfSK",
          "commit": {
            "abbreviatedOid": "fcc9c31"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-02-28T18:21:38Z",
          "updatedAt": "2024-02-28T18:21:38Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "Hmm, what else might happen?",
              "createdAt": "2024-02-28T18:21:38Z",
              "updatedAt": "2024-02-28T18:21:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5xqlCh",
          "commit": {
            "abbreviatedOid": "fcc9c31"
          },
          "author": "divergentdave",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-02-28T18:33:25Z",
          "updatedAt": "2024-02-28T18:33:25Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "In this sentence, \"room for error\" was previously referring to variance in batch size that may occur depending on how the aggregation jobs are ran by a distributed system. (depending on implementation decisions, etc.) I added the \"in part\" caveat in this PR because we now also need to allow for rejecting invalid reports. I can rephrase \"error\" to be more clear here.",
              "createdAt": "2024-02-28T18:33:25Z",
              "updatedAt": "2024-02-28T18:33:25Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5xqn0Y",
          "commit": {
            "abbreviatedOid": "fcc9c31"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-02-28T18:38:03Z",
          "updatedAt": "2024-02-28T18:38:03Z",
          "comments": [
            {
              "originalPosition": 35,
              "body": "Nah that's fine, thanks.",
              "createdAt": "2024-02-28T18:38:03Z",
              "updatedAt": "2024-02-28T18:38:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5xrB9Y",
          "commit": {
            "abbreviatedOid": "545dd66"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-02-28T19:30:00Z",
          "updatedAt": "2024-02-28T19:30:00Z",
          "comments": []
        }
      ]
    },
    {
      "number": 552,
      "id": "PR_kwDOFEJYQs5oNH4q",
      "title": "Prepare `draft-ietf-ppm-dap-10`",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/552",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "There's no PPM session at IETF 119, but we nonetheless want to publish a new draft to keep DAP current.\r\n\r\nCloses #551 ",
      "createdAt": "2024-02-28T18:26:24Z",
      "updatedAt": "2024-02-29T21:44:39Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "3448ba400a628b18a785e069d544314c1bbff8c6",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/draft-10",
      "headRefOid": "778194ade3c5fdadab220d48ec52982272da6b77",
      "closedAt": "2024-02-29T21:44:39Z",
      "mergedAt": "2024-02-29T21:44:39Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "08b67b9afd470bc12c7f13f87449879e5d26ed1c"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> There's no PPM session at IETF 119, but we nonetheless want to publish a new draft to keep DAP current.\r\n> \r\n> Closes #551\r\n\r\nThere will (hopefully!) be an interim!",
          "createdAt": "2024-02-28T18:29:20Z",
          "updatedAt": "2024-02-28T18:29:20Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "> > There's no PPM session at IETF 119, but we nonetheless want to publish a new draft to keep DAP current.\r\n> > Closes #551\r\n> \r\n> There will (hopefully!) be an interim!\r\n\r\nYes, definitely. What I meant to say is that we want to get a draft in by the IETF 119 submission deadline despite not actually planning to discuss `draft-ietf-ppm-dap-10` at the plenary.",
          "createdAt": "2024-02-28T18:35:19Z",
          "updatedAt": "2024-02-28T18:35:19Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5xqhuo",
          "commit": {
            "abbreviatedOid": "778194a"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-02-28T18:27:30Z",
          "updatedAt": "2024-02-28T18:27:30Z",
          "comments": [
            {
              "originalPosition": 15,
              "body": "This refers to #550, which we plan to merge before cutting `draft-ietf-ppm-dap-10`.",
              "createdAt": "2024-02-28T18:27:30Z",
              "updatedAt": "2024-02-28T18:27:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5xqiv_",
          "commit": {
            "abbreviatedOid": "778194a"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-02-28T18:29:56Z",
          "updatedAt": "2024-02-28T18:29:56Z",
          "comments": []
        }
      ]
    },
    {
      "number": 554,
      "id": "PR_kwDOFEJYQs5th_h9",
      "title": "Remove multi-collection of batches.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/554",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This is a prospective change to remove support for multi-collection of batches from DAP, as this was only required for the Heavy Hitters application which will no longer be supported by DAP.\r\n\r\nSpecifically:\r\n * Remove the `max_batch_query_count` task parameter, replacing uses of this parameter with semantics which are equivalent to a `max_batch_query_count` of 1.\r\n * Remove the fixed-size query type's `by_batch_id` query (as well as the `FixedSizeQueryType` message, since there is now only one possibility).\r\n * Remove the `PartialBatchSelector` from the `Collection` message, since this was used only to convey the `BatchId` to the `Collector` for later use of the `by_batch_id` query type, which is now gone. (With this change, Collectors no longer need to be aware of the concept of batch IDs.)\r\n\r\nCloses #259.\r\nCloses #316.\r\nCloses #405.\r\nCloses #409.\r\nCloses #436.",
      "createdAt": "2024-04-23T23:03:03Z",
      "updatedAt": "2024-05-21T20:58:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "919fe97674dae0be68b52f40892883aafc5d77bf",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "bran/remove-multi-collection",
      "headRefOid": "25fb1cb293186fb28e37652e5b8a05d7cf049ddb",
      "closedAt": "2024-05-21T20:58:52Z",
      "mergedAt": "2024-05-21T20:58:52Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "1d80bf9eadafe47f38391809c1f02d2611c8ad45"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Other than the changes suggested by review, I realized we can drop the partial batch selector from the `Collection` message, since this was used to convey the `BatchId` to the Collector for use in `by_batch_id` queries (which have now been removed). Other than making this message a little smaller, this also means that `BatchId`s are now private to the aggregators, which is a nice conceptual simplification for the Collector I suppose.\r\n\r\nThis simplification sounds good to me.\r\n",
          "createdAt": "2024-04-24T23:55:49Z",
          "updatedAt": "2024-04-24T23:55:49Z"
        },
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "I think this change is in good shape, but we need to get a consensus call in the WG before we take it.",
          "createdAt": "2024-04-26T00:11:43Z",
          "updatedAt": "2024-04-26T00:11:43Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Rebased & squashed. I'm going to hold off on merging this for a few days to allow for any final comments.",
          "createdAt": "2024-05-14T23:48:52Z",
          "updatedAt": "2024-05-14T23:48:52Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs54T3vZ",
          "commit": {
            "abbreviatedOid": "e6d57ed"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-04-23T23:06:51Z",
          "updatedAt": "2024-04-23T23:17:53Z",
          "comments": [
            {
              "originalPosition": 70,
              "body": "It is OK to remove `by_batch_id` because this fixed-size query type was used only for repeated collection of the same batch with differing aggregation parameters. (Recovering from transient failures in collecting a given batch is enabled via re-querying the same collection job ID.)",
              "createdAt": "2024-04-23T23:06:51Z",
              "updatedAt": "2024-04-23T23:17:53Z"
            },
            {
              "originalPosition": 132,
              "body": "I removed this section because:\r\n * The second & third paragraphs only apply to multiply-collected VDAFs.\r\n * The first paragraph boils down to \"you should probably set `min_batch_size` and `max_batch_size` to the same value, and if you don't the Leader can set batches freely to any size between these two values.\"\r\n\r\nShould we remove the concept of `max_batch_size`?",
              "createdAt": "2024-04-23T23:08:33Z",
              "updatedAt": "2024-04-23T23:17:53Z"
            },
            {
              "originalPosition": 240,
              "body": "Is there a need for `Vdaf.is_valid` anymore? It _might_ be used to validate that an aggregation parameter itself is valid (irrespective of other aggregation parameters), but I think this might be better done at some point in evaluating the VDAF rather than via a separate method, for simplicity's sake.",
              "createdAt": "2024-04-23T23:10:12Z",
              "updatedAt": "2024-04-23T23:17:53Z"
            },
            {
              "originalPosition": 303,
              "body": "I'm not sure there is a concrete example to cite here anymore, though I suppose there's nothing stopping a singly-collected VDAF with a nontrivial aggregation parameter from leaking information via the aggregation parameter.",
              "createdAt": "2024-04-23T23:15:44Z",
              "updatedAt": "2024-04-23T23:17:53Z"
            },
            {
              "originalPosition": 26,
              "body": "I dropped references to Poplar1 throughout; I'm not sure if there is anything reasonable to replace it with. (Both PINE & Mastic, IIUC, are not yet to the point where we might cite them.)",
              "createdAt": "2024-04-23T23:17:18Z",
              "updatedAt": "2024-04-23T23:17:53Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs54bOjg",
          "commit": {
            "abbreviatedOid": "e6d57ed"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Thanks for taking this on, Bran. Here's a first pass of comments, but I want to go back over the document looking for other things that may need to change.",
          "createdAt": "2024-04-24T16:37:54Z",
          "updatedAt": "2024-04-24T17:36:06Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "I agree that referencing PINE or Mastic is premature. However I think that here we should replace this list of candidate VDAFs (which now only includes Prio3) with a characterization of the class of VDAFs that DAP supports, which is ones that only admit a single collection of the batch.",
              "createdAt": "2024-04-24T16:37:54Z",
              "updatedAt": "2024-04-24T17:36:06Z"
            },
            {
              "originalPosition": 59,
              "body": "We could move the declaration of `BatchID` closer to where it's first used, in the description of `AggregationJobInitReq`.",
              "createdAt": "2024-04-24T16:42:36Z",
              "updatedAt": "2024-04-24T17:36:06Z"
            },
            {
              "originalPosition": 182,
              "body": "Let's rephrase in terms of concepts defined by the protocol to make this more clear. I'm also concerned that the notion of \"linearity\" discussed in this paragraph isn't introduced anywhere in this document. I assume it's linear in the sense of linear queries as discussed in [BBCGI22](https://eprint.iacr.org/2019/188.pdf) but we should either explain that or remove the unhelpful mention of linearity. Sorry, this suggestion won't apply cleanly, but GitHub won't let me make a suggestion that spans diff and non-diff text.\r\n\r\n```suggestion\r\n   aggregatable form. For some VDAFs, like Prio3, the mapping from input to\r\n   output shares is a fixed operation depending only on the input share, but\r\n   in general the mapping involves an aggregation parameter, chosen\r\n   dynamically by the Collector.\r\n```\r\n\r\n@cjpatton, do you think it's important to define and use the notion of linearity here?",
              "createdAt": "2024-04-24T16:52:55Z",
              "updatedAt": "2024-04-24T17:36:06Z"
            },
            {
              "originalPosition": 190,
              "body": "```suggestion\r\n   that the output shares sum up to an integer in a specific range, while the\r\n   Prio3Histogram variant ({{Section 7.4.4 of !VDAF}}) proves that output\r\n   shares sum up to a one-hot vector representing a contribution to a single\r\n   bucket of the histogram.\r\n```",
              "createdAt": "2024-04-24T16:56:06Z",
              "updatedAt": "2024-04-24T17:36:06Z"
            },
            {
              "originalPosition": 240,
              "body": "Yes, I agree. The check for whether a report has been previously aggregated at all should suffice, and VDAF methods like `prep_init` or `prep_next` should safely reject invalid aggregation parameters. It's still useful for `Vdaf.is_valid` to be defined, because DAP-HH would use it, but DAP doesn't need to call it anymore.",
              "createdAt": "2024-04-24T17:07:17Z",
              "updatedAt": "2024-04-24T17:36:06Z"
            },
            {
              "originalPosition": 303,
              "body": "I think we should leave this in, even without the explicit reference to Poplar1. Even when restricted to collecting batches once, DAP admits many future, unknown VDAFs, and pointing out that VDAFs may leak information (and not just to the collector!) and may need to be composed with DP is a legitimate security consideration. However I don't like the phrasing \"Collection requests may leak information beyond the collection results\" because it seems tautological. Let's try:\r\n\r\n```suggestion\r\n1. Some VDAFs could leak information to either Aggregator or the Collector\r\n   beyond what the protocol intended to learn. It may be possible to mitigate\r\n   such leakages using differential privacy ({{dp}}). \r\n```",
              "createdAt": "2024-04-24T17:16:11Z",
              "updatedAt": "2024-04-24T17:36:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs54c-_O",
          "commit": {
            "abbreviatedOid": "e6d57ed"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-04-24T20:16:21Z",
          "updatedAt": "2024-04-24T20:28:36Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "We could keep the \"counting the number of Clients that hold a given string\" use case for Poplar1. This is basically a limited form of attributed based metrics (i.e., Prio3Count with attributes).",
              "createdAt": "2024-04-24T20:16:21Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            },
            {
              "originalPosition": 70,
              "body": "SGTM.",
              "createdAt": "2024-04-24T20:17:26Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            },
            {
              "originalPosition": 132,
              "body": "Let's consider removing `max_batch_size` in a separate issue.",
              "createdAt": "2024-04-24T20:19:40Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            },
            {
              "originalPosition": 182,
              "body": "No I don't think linearity is important. The suggestion looks good to me.",
              "createdAt": "2024-04-24T20:21:46Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            },
            {
              "originalPosition": 240,
              "body": "I think of validity of the agg param as being delegated to decoding on the wire.",
              "createdAt": "2024-04-24T20:23:56Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            },
            {
              "originalPosition": 225,
              "body": "This note is still relevant I think.",
              "createdAt": "2024-04-24T20:24:37Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            },
            {
              "originalPosition": 268,
              "body": "nit: Consider merging the text after the only remaining bullet into this paragraph.",
              "createdAt": "2024-04-24T20:26:24Z",
              "updatedAt": "2024-04-24T20:28:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs54dEz-",
          "commit": {
            "abbreviatedOid": "e6d57ed"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "We should delete [this paragraph](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-10#section-4.5.1.1-16) at L1345:\r\n\r\n> [[OPEN ISSUE: Consider sending report shares separately (in parallel) to the aggregate instructions. Right now, aggregation parameters and the corresponding\r\nreport shares are sent at the same time, but this may not be strictly\r\nnecessary.]]\r\n\r\nThis isn't a concern anymore since we don't support multiple collection, and thus a report will be used exactly once.\r\n\r\nWe should revisit [VDAFs and compute requirements](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-10#name-vdafs-and-compute-requireme) so it doesn't discuss Poplar1 (or delete that and carry it over to the eventual DAP-HH draft).",
          "createdAt": "2024-04-24T20:31:03Z",
          "updatedAt": "2024-04-24T21:10:33Z",
          "comments": [
            {
              "originalPosition": 272,
              "body": "I feel like there should be a reference here to [4.1.2](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-10#name-fixed-size-queries) which discusses how the Leader deals with \"current-batch\" queries. However we can deal with that separately from this PR.",
              "createdAt": "2024-04-24T20:31:03Z",
              "updatedAt": "2024-04-24T21:10:33Z"
            },
            {
              "originalPosition": 262,
              "body": "I think this could be stated more clearly in terms of the batch already having been collected.\r\n```suggestion\r\nNext, the Aggregator checks that the batch has not already been aggregated. If it\r\nhas, then the Aggregator MUST abort with error of type \"batchAlreadyAggregated\".\r\n```\r\n\r\nNote also the error type changes `batchQueriedMultipleTimes => batchAlreadyAggregated`.",
              "createdAt": "2024-04-24T21:00:43Z",
              "updatedAt": "2024-04-24T21:10:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs54dpUR",
          "commit": {
            "abbreviatedOid": "7457073"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "> We should revisit [VDAFs and compute requirements](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-10#name-vdafs-and-compute-requireme) so it doesn't discuss Poplar1 (or delete that and carry it over to the eventual DAP-HH draft).\r\n\r\nDone. I used a Prio3 example since that's the only concrete VDAF we have in hand at the moment, but some of the points regarding superlinearity of processing don't apply to any of the Prio3 variants (AFAIK).\r\n\r\nOther than the changes suggested by review, I realized we can drop the partial batch selector from the `Collection` message, since this was used to convey the `BatchId` to the Collector for use in `by_batch_id` queries (which have now been removed). Other than making this message a little smaller, this also means that `BatchId`s are now private to the aggregators, which is a nice conceptual simplification for the Collector I suppose.",
          "createdAt": "2024-04-24T22:08:28Z",
          "updatedAt": "2024-04-24T23:31:44Z",
          "comments": [
            {
              "originalPosition": 26,
              "body": "I like providing the property.",
              "createdAt": "2024-04-24T22:08:28Z",
              "updatedAt": "2024-04-24T23:31:44Z"
            },
            {
              "originalPosition": 132,
              "body": "Filed https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/555.",
              "createdAt": "2024-04-24T22:22:38Z",
              "updatedAt": "2024-04-24T23:31:44Z"
            },
            {
              "originalPosition": 262,
              "body": "I think the notion of _distinct_ aggregation parameters is still useful. I'd like to maintain the property that the same batch can be collected multiple times with the same aggregation parameter. (Though I'm not sure we really need this property, with or without multiply-collected batches -- but I think if we want to drop that property, we should do so as a separate change.)\r\n\r\nSeparately, I'd like to keep the notion of \"aggregating\" the batch separate from \"querying\"/\"collecting\" the batch, due to the common optimizaton of aggregating before a collection query arrives if the aggregation parameter can be known in advance (e.g. if it's the empty string). An overly-pedantic reader would take the suggested edit as-written to mean that such a pre-aggregated batch can never be collected.",
              "createdAt": "2024-04-24T22:49:15Z",
              "updatedAt": "2024-04-24T23:31:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs54tzUf",
          "commit": {
            "abbreviatedOid": "7457073"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-04-26T15:05:31Z",
          "updatedAt": "2024-04-26T15:05:31Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs541v4F",
          "commit": {
            "abbreviatedOid": "7457073"
          },
          "author": "kenluck2001",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-04-28T21:01:00Z",
          "updatedAt": "2024-04-28T21:01:01Z",
          "comments": [
            {
              "originalPosition": 307,
              "body": "Is it possible to effect this repetitive query of the same batch by adopting a rate-limiting scheme that is not tightly bundled with the the underlying DAP. Could we make it loosely coupled?",
              "createdAt": "2024-04-28T21:01:01Z",
              "updatedAt": "2024-04-28T21:01:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs546_Gb",
          "commit": {
            "abbreviatedOid": "7457073"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-04-29T14:46:30Z",
          "updatedAt": "2024-04-29T14:46:30Z",
          "comments": [
            {
              "originalPosition": 307,
              "body": "What would be the goal of rate limiting?\r\n\r\nThe purpose of querying a batch multiple times is to allow a batch to be aggregated with different aggregation parameters. This enables the tree traversal at the heart of Poplar1's solution to heavy hitters: https://datatracker.ietf.org/meeting/interim-2024-ppm-01/materials/slides-interim-2024-ppm-01-sessa-supporting-heavy-hitters-00",
              "createdAt": "2024-04-29T14:46:30Z",
              "updatedAt": "2024-04-29T14:46:30Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs548Avo",
          "commit": {
            "abbreviatedOid": "7457073"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-04-29T16:27:51Z",
          "updatedAt": "2024-04-29T16:27:52Z",
          "comments": [
            {
              "originalPosition": 307,
              "body": "This PR removes the ability to query the same batch multiple times. The purpose of the check this comment was made on is to ensure that we do not collect the same batch multiple times, since doing so may violate the privacy of the reports included in the batch. Thus, we need a solution which permanently blocks the undesirable queries, rather than temporarily rate-limiting them.\r\n\r\nAs Chris notes, before we removed multi-collection in this PR, each query of the same batch used a distinct aggregation parameter. For the Heavy Hitters application, there was a nontrivial rule to check whether a given aggregation parameter could be used given the aggregation parameters already used. Thus, any given solution to the multiple-queries problem would need to be aware of the aggregation parameters in use.",
              "createdAt": "2024-04-29T16:27:52Z",
              "updatedAt": "2024-04-29T16:27:52Z"
            }
          ]
        }
      ]
    },
    {
      "number": 559,
      "id": "PR_kwDOFEJYQs5uY_mV",
      "title": "Clarify purpose of report ID uniqueness",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/559",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Discuss explicitly the attack prevented by enforcing unique report IDs, which is to stop honest Client reports from being replayed. This would also be necessary to satisfy VDAF's requirement of nonce uniqueness, but it's not yet clear VDAF will impose that exact requirement (see [1]).\r\n\r\nThere's no functional change here, but hopefully being explicit can short-circuit future discussion of why we have this expensive requirement.\r\n\r\nSee #558 for motivating discussion.\r\n\r\n[1]: https://github.com/cfrg/draft-irtf-cfrg-vdaf/pull/340",
      "createdAt": "2024-05-02T17:34:57Z",
      "updatedAt": "2024-05-08T16:23:42Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "08b67b9afd470bc12c7f13f87449879e5d26ed1c",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/clarify-replay",
      "headRefOid": "67e863175755cac71b56f409e10c15fcf6c7e5e9",
      "closedAt": "2024-05-08T16:23:41Z",
      "mergedAt": "2024-05-08T16:23:41Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "919fe97674dae0be68b52f40892883aafc5d77bf"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "This should merge cleanly before or after #554, despite touching nearby text.",
          "createdAt": "2024-05-02T17:35:23Z",
          "updatedAt": "2024-05-02T17:35:23Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs55ynMW",
          "commit": {
            "abbreviatedOid": "67e8631"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-05-07T14:10:20Z",
          "updatedAt": "2024-05-07T14:10:20Z",
          "comments": []
        }
      ]
    },
    {
      "number": 562,
      "id": "PR_kwDOFEJYQs5wILRS",
      "title": "Update changelog & version tags for DAP-11.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/562",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2024-05-21T21:10:28Z",
      "updatedAt": "2024-05-21T22:23:55Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "1d80bf9eadafe47f38391809c1f02d2611c8ad45",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "bran/changelog",
      "headRefOid": "eb64f280913ee10fc302b1de8723b388cb38f023",
      "closedAt": "2024-05-21T22:23:54Z",
      "mergedAt": "2024-05-21T22:23:54Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "35f1274853c1c06a5132612b111debefe4fd17ac"
      },
      "comments": [],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs57XRjX",
          "commit": {
            "abbreviatedOid": "eb64f28"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-05-21T21:57:07Z",
          "updatedAt": "2024-05-21T21:57:07Z",
          "comments": []
        }
      ]
    },
    {
      "number": 563,
      "id": "PR_kwDOFEJYQs5ybBGu",
      "title": "Rename \"query type\" to \"batch mode\", \"fixed-size\" to \"leader-selected\", remove max_batch_size.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/563",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "wire breaking"
      ],
      "body": "Batch mode is a more descriptive name than query type. And leader-selected is a more accurate name than fixed-size, since the batch sizes are not fixed -- any size above min_batch_size (and, if specified, below max_batch_size) is acceptable.\r\n\r\nThe max_batch_size parameter of the leader-selected query type is removed. This parameter does not need to be enforced by both aggregators, so it can be made implementation-specific.\r\n\r\nAlso, restore the partial batch selector to the Collection message; the included batch ID is required for the Collector to decrypt the aggregate shares, as it is included in the AAD. This bug was introduced in #554 & effectively breaks collection for the fixed-size query type in DAP-11.\r\n\r\nCloses #555.",
      "createdAt": "2024-06-13T23:56:57Z",
      "updatedAt": "2024-07-26T23:51:41Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "7b5eeca998d7871089249a7efc8224d9a3651df0",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "bran/leader-selected",
      "headRefOid": "6d7fd1b8c84e9b40cb1d3f40c8d5e6a646e321d4",
      "closedAt": "2024-07-26T23:51:35Z",
      "mergedAt": "2024-07-26T23:51:35Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "83f1ea77fb9ff1f304779e775bb15efcf51a9269"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Closes #555.",
          "createdAt": "2024-06-14T00:21:02Z",
          "updatedAt": "2024-06-14T00:21:02Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@branlwyd I feel `leader-selected` is a bit misleading, given that the batch size is decided by task creator based on some privacy requirements, for eg differential privacy. `leader-selected query` sounds odd to me.\r\nI suggest we change it to something that reflects the batch releasing criteria, how about `batch-threshold`? ",
          "createdAt": "2024-06-26T11:06:19Z",
          "updatedAt": "2024-06-26T11:06:19Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @branlwyd I feel `leader-selected` is a bit misleading, given that the batch size is decided by task creator based on some privacy requirements, for eg differential privacy. `leader-selected query` sounds odd to me. I suggest we change it to something that reflects the batch releasing criteria, how about `batch-threshold`?\r\n\r\nYou're correct that the batch size is not chosen by the Leader, but it is true that the Leader decides which reports go in which batch. On the other hand, that may not be the intent of the deployment.\r\n\r\nThat said, `batch-threshold` sounds generic enough to apply to any query type, no? E.g., isn't the \"batch releasing criteria\" now the same for time_interval as for fixed_size?\r\n\r\nWhat about something that emphasizes that there are no batch boundary criteria, as there are for time_interval? Perhaps any_batch or something?",
          "createdAt": "2024-06-27T17:57:40Z",
          "updatedAt": "2024-06-27T17:57:40Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "> You're correct that the batch size is not chosen by the Leader, but it is true that the Leader decides which reports go in which batch. On the other hand, that may not be the intent of the deployment.\r\n> \r\n> That said, `batch-threshold` sounds generic enough to apply to any query type, no? E.g., isn't the \"batch releasing criteria\" now the same for time_interval as for fixed_size?\r\n> \r\n> What about something that emphasizes that there are no batch boundary criteria, as there are for time_interval? Perhaps any_batch or something?\r\n\r\nThe batch releasing criteria is still different, `fixed_size` only requires minimum batch size to be met, `time_interval` requires size and time. How about `size_conditioned`?\r\nI feel the query type as is only defines the batch creation criteria, not the releasing criteria. If that's what we want query type to do, then something like `any_batch` would work too.\r\n",
          "createdAt": "2024-06-28T12:00:07Z",
          "updatedAt": "2024-06-28T12:00:07Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> > You're correct that the batch size is not chosen by the Leader, but it is true that the Leader decides which reports go in which batch. On the other hand, that may not be the intent of the deployment.\r\n> > That said, `batch-threshold` sounds generic enough to apply to any query type, no? E.g., isn't the \"batch releasing criteria\" now the same for time_interval as for fixed_size?\r\n> > What about something that emphasizes that there are no batch boundary criteria, as there are for time_interval? Perhaps any_batch or something?\r\n> \r\n> The batch releasing criteria is still different, `fixed_size` only requires minimum batch size to be met, `time_interval` requires size and time. How about `size_conditioned`? I feel the query type as is only defines the batch creation criteria, not the releasing criteria. If that's what we want query type to do, then something like `any_batch` would work too.\r\n\r\nMy interpretation is also that the query type determines how batches are determined:\r\n\r\n* `time_interval` determines batches by intervals of time.\r\n* `leader_selected` determines batches by the mostly-arbitrary selection of the Leader. \r\n\r\n`any_batch`, or even `unconstrained`/`arbitrary` would also work for `leader_selected`; but IMO these are not strongly better than `leader_selected` -- they might even be considered worse, in the sense that `leader_selected` indicates which protocol participant is doing the batch selection.",
          "createdAt": "2024-07-02T23:05:27Z",
          "updatedAt": "2024-07-02T23:05:27Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@wangshan's interpretation of \"release criteria\" sounds reasonable to me. If we take this suggestion, then we should clarify in the query type what the names mean. Previously it was \"how a batch is constructed\"; \"when a batch can be queried\" is subtley different.",
          "createdAt": "2024-07-03T14:38:03Z",
          "updatedAt": "2024-07-03T14:38:03Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I think a query type effectively:\r\n\r\n* determines how reports can be grouped together\r\n* determines how batches are addressed (i.e. what is the identifier for a batch)\r\n* determines the semantics of how aggregate results can be queried by the collector\r\n\r\nI added some text to this effect -- I'm not totally sure I understand what is meant by \"release criteria\". If this doesn't capture what we hoped, please push back.",
          "createdAt": "2024-07-03T18:29:16Z",
          "updatedAt": "2024-07-03T18:29:16Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@cjpatton @branlwyd I don't have a strong opinion on what it should be called. From the discussion so far, I think we agree the purpose of this `query_type` is how the batch is created, or partitioned/selected. In this context, `leader_selected` makes sense, as long as we explain this in the text. I would also preferred to change `query_type` to `batch_select_type`, but I don't have strong opinion on that either.",
          "createdAt": "2024-07-16T00:14:13Z",
          "updatedAt": "2024-07-16T00:14:13Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @cjpatton @branlwyd I don't have a strong opinion on what it should be called. From the discussion so far, I think we agree the purpose of this `query_type` is how the batch is created, or partitioned/selected. In this context, `leader_selected` makes sense, as long as we explain this in the text. I would also preferred to change `query_type` to \r\n`batch_select_type`, but I don't have strong opinion on that either.\r\n\r\nIt's a little broader than that, but I think I might know what's missing: a notion of \"bucket\" (#560). Namely, I've understood \"query\" to refer to both:\r\n\r\n1. how reports may be mapped to batch buckets (e.g., a time window)\r\n2. how batch buckets are merged into batches (e.g., a sequence of time windows, i.e., a time interval)",
          "createdAt": "2024-07-16T01:26:04Z",
          "updatedAt": "2024-07-16T01:26:04Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> I would also preferred to change `query_type` to `batch_select_type`, but I don't have strong opinion on that either.\r\n\r\nWhat about `batching_mode` (or `batch_mode`)?",
          "createdAt": "2024-07-18T23:42:08Z",
          "updatedAt": "2024-07-18T23:42:08Z"
        },
        {
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "body": "\r\n> \r\n> What about `batching_mode` (or `batch_mode`)?\r\n\r\n`batch_mode` sounds nice.",
          "createdAt": "2024-07-19T12:29:56Z",
          "updatedAt": "2024-07-19T12:29:56Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks -- I've updated this PR to rename \"query type\" to \"batch mode\" & I like the change. @cjpatton interested in your thoughts here, I can revert (or rename to something else) if this isn't so good.",
          "createdAt": "2024-07-19T17:35:10Z",
          "updatedAt": "2024-07-19T17:35:10Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> Thanks -- I've updated this PR to rename \"query type\" to \"batch mode\" & I like the change. @cjpatton interested in your thoughts here, I can revert (or rename to something else) if this isn't so good.\r\n\r\nNo objection. ",
          "createdAt": "2024-07-19T18:40:42Z",
          "updatedAt": "2024-07-19T18:40:42Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@branlwyd please squash and merge at will.",
          "createdAt": "2024-07-26T23:18:36Z",
          "updatedAt": "2024-07-26T23:18:36Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5-MhnW",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-14T00:19:19Z",
          "updatedAt": "2024-06-14T00:19:20Z",
          "comments": [
            {
              "originalPosition": 235,
              "body": "The aggregate share AAD includes the batch ID, so the batch ID needs to be communicated to the Collector to allow decryption, alas. I restored the batch ID to the `Collection` message.\r\n\r\nHowever, I think maybe this isn't the right fix (just the least controversial one to unbreak things for now). DAP aims to provide robustness only if both aggregators are honest (and robustness is already trivally breakable by a single dishonest aggregator). If both aggregators are honest, there is no need for the aggregate share AAD to include the batch ID, since there is no need for the Collector to e.g. detect the aggregator attempting to repeatedly transmit an aggregate share. So I think the \"right\" fix would be to drop batch ID from both the `Collection` and `AggregateShareAad` messages.\r\n\r\nHowever, this argument applies just as well to every other field in `AggregateShareAad` -- so what is the purpose of the AAD here? (best-effort protection against bugs, perhaps?)\r\n\r\nIf this isn't trivially resolvable, this might be worthy of an issue to allow discussion on what the right fix is.",
              "createdAt": "2024-06-14T00:19:19Z",
              "updatedAt": "2024-06-14T00:35:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5-Mi9d",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-14T00:28:05Z",
          "updatedAt": "2024-06-14T00:28:05Z",
          "comments": [
            {
              "originalPosition": 235,
              "body": "(I think the same argument doesn't apply to `InputShareAad`, since this AAD is used to allow the Helper to ensure the encrypted input share it received from the Leader matches the rest of the report share. Failing to perform this check would allow a malicious Leader to break a report's privacy even in the face of an honest Helper, and DAP's aim is to protect report privacy as long as at least one aggregator is honest.)",
              "createdAt": "2024-06-14T00:28:05Z",
              "updatedAt": "2024-06-14T00:36:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_j4yM",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-25T23:18:16Z",
          "updatedAt": "2024-06-25T23:49:04Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "nit: I'd re-order these, since you need the second as context for the first.",
              "createdAt": "2024-06-25T23:18:16Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 79,
              "body": "```suggestion\r\nacceptable for reports to be batched by the Leader in an arbitrary fashion. Each batch of\r\n```",
              "createdAt": "2024-06-25T23:24:57Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 103,
              "body": "This text now lacks important. There is only a minimum batch size, hence no application of DAP is allowed to be \"concerned with the sample size\".\r\n\r\nI think what we want to do now instead is say something generic about how privacy guarantees of DAP depend on the batch size, perhaps when composed with some DP policy.\r\n\r\nHowever I wonder if we still think this is accurate. My intuition is that choosing a larger batch size strictly increases privacy, regardless of how DP is implemented. (Though utility may be negatively impacted.) Is this correct?\r\n\r\ncc/ @wangshan since according to `git blame`, you wrote this text originally.",
              "createdAt": "2024-06-25T23:33:29Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 112,
              "body": "I don't object, but why change \"deployment\" to \"implementation\" here?",
              "createdAt": "2024-06-25T23:33:54Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 120,
              "body": "here too, @wangshan (see above comment)",
              "createdAt": "2024-06-25T23:34:34Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 154,
              "body": "This doesn't appear to be desirable from a privacy perspective. I'll note here that having the timestamp in the metadata seems to be helpful for replay protection, since it allows you to shard by batch interval.\r\n\r\nI suggest we just delete this text.",
              "createdAt": "2024-06-25T23:35:58Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 168,
              "body": "This is a wire-breaking change. We can avoid it by just deleting `batch_saturated` and not renumbering the remaining variants. However this would leave a gap in the variants defined in our draft, which is a bit odd.\r\n\r\nI think we should avoid breaking changes if we can. An alternative would be to leave a note here that if we want a new variant before RFC, then the variant should use `6`.\r\n\r\nWe could take this one step further by being very explicit about what we want in the end:\r\n\r\n```\r\nenum {\r\n  batch_collected(0),\r\n  report_replayed(1),\r\n  report_dropped(2),\r\n  hpke_unknown_config_id(3),\r\n  hpke_decrypt_error(4),\r\n  vdaf_prep_error(5),\r\n  /* [TODO Remove or replace this variant.] */\r\n  todo_remove_me(6),\r\n  task_expired(7),\r\n  invalid_message(8),\r\n  report_too_early(9),\r\n  (256)\r\n} PrepareError;\r\n```\r\n",
              "createdAt": "2024-06-25T23:42:54Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 206,
              "body": "```suggestion\r\n  query. For leader_selected tasks, this includes the batch ID assigned to the batch\r\n```",
              "createdAt": "2024-06-25T23:43:56Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            },
            {
              "originalPosition": 235,
              "body": "I think we should include the batch ID in the AAD. It's not strictly necessary in our threat model, so think of it as defense in depth: successful execution implies agreement on the batch ID between the aggregators and collector; this may have benefits we don't see right now.\r\n\r\nAs a general rule, binding protocol execution to crypto operations is a good idea whenever feasible. There are tons of examples of this, the most recent of which might be https://eprint.iacr.org/2023/691.\r\n",
              "createdAt": "2024-06-25T23:48:27Z",
              "updatedAt": "2024-06-25T23:49:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_pCmE",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T10:58:15Z",
          "updatedAt": "2024-06-26T10:58:15Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "generally speaking larger batch size gives better DP guarantee, however, there can be a case where the DP guarantee is based on a certain sampling rate and allowing too big a batch may break that condition. But in practice, I don't think sampling would be implemented in a way that make this a problem. If we decide to remove max_batch_size, I agree it doesn't make much sense to mention sampling here.  ",
              "createdAt": "2024-06-26T10:58:15Z",
              "updatedAt": "2024-06-26T10:58:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_pDXZ",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T10:59:55Z",
          "updatedAt": "2024-06-26T10:59:55Z",
          "comments": [
            {
              "originalPosition": 168,
              "body": "+1",
              "createdAt": "2024-06-26T10:59:55Z",
              "updatedAt": "2024-06-26T10:59:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_pEQb",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T11:01:50Z",
          "updatedAt": "2024-06-26T11:01:51Z",
          "comments": [
            {
              "originalPosition": 120,
              "body": "Similar to above, shall we just say whether to control the size of the batch is an implementation specific choice?",
              "createdAt": "2024-06-26T11:01:50Z",
              "updatedAt": "2024-06-26T11:01:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AAWz8",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-28T10:22:07Z",
          "updatedAt": "2024-06-28T10:22:07Z",
          "comments": [
            {
              "originalPosition": 154,
              "body": "I'm sure we've discussed this before so maybe I am missing something but there is an attack where a Leader separately collects reports from a single client and then only aggregates those.\r\n\r\nFor time intervals the leader is strongly constrained here and that's exactly why we prohibit overlapping time-intervals. We might however have lost this meaning in the wording over time...\r\n\r\n> If the report pertains to a batch that was previously collected, then the input share MUST be marked as invalid with error batch_collected.\r\n\r\n> Finally, the Aggregator checks that the batch does not contain a report that was included in any previous batch. If this batch overlap check fails, then the Aggregator MUST abort with error of type \"batchOverlap\". For time_interval tasks, it is sufficient (but not necessary) to check that the batch interval does not overlap with the batch interval of any previous query. If this batch interval check fails, then the Aggregator MAY abort with error of type \"batchOverlap\".\r\n\r\n> For each report in the batch, the time at which that report was generated (see [Section 4.4](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#upload-flow)) MUST fall within the batch interval specified by the Collector.\r\n\r\nAccording to my probably incorrect memory this is supposed to be prevented by the collector deciding which reports to aggregate and the other aggregator checking as well.\r\nWe should be explicit about this attack in https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#name-batch-parameters",
              "createdAt": "2024-06-28T10:22:07Z",
              "updatedAt": "2024-06-28T10:22:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AcihG",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-02T22:19:06Z",
          "updatedAt": "2024-07-02T22:19:06Z",
          "comments": [
            {
              "originalPosition": 235,
              "body": "Fair enough. Even if this AAD doesn't support the explicit threat model, at the very least it will help avoid accidental bugs which e.g. mismatch aggregate shares. (I'm not sure it can protect against any form of malice, as the components of the AAD are public to both aggregators; and each aggregator can already falsify results of any aggregate shares it can control -- i.e. the Helper can falsify the Helper's aggregate share, the Leader can falsify the Leader or Helper's aggregate share.)",
              "createdAt": "2024-07-02T22:19:06Z",
              "updatedAt": "2024-07-02T22:19:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Acixf",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-02T22:20:22Z",
          "updatedAt": "2024-07-02T22:20:22Z",
          "comments": [
            {
              "originalPosition": 9,
              "body": "Done, though now we're referencing \"fixed-size\" directly before we mention that it is renamed.",
              "createdAt": "2024-07-02T22:20:22Z",
              "updatedAt": "2024-07-02T22:20:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AcpWE",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-02T22:41:39Z",
          "updatedAt": "2024-07-02T22:41:39Z",
          "comments": [
            {
              "originalPosition": 103,
              "body": "I got rid of this text and merged the next paragraph into it, since that paragraph is also concerned with the range of possible batch sizes. The following paragraph is concerned with the batch sizes that might be generated by an implementation, but IMO the existing text makes it clear the difference between specification & implementation advice (and this section is not normative).",
              "createdAt": "2024-07-02T22:41:39Z",
              "updatedAt": "2024-07-02T22:41:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Acp2d",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-02T22:42:47Z",
          "updatedAt": "2024-07-02T22:42:47Z",
          "comments": [
            {
              "originalPosition": 112,
              "body": "Saying something is \"implementation-specific\" is (in my experience) much more common than \"deployment-specific\". Prior to this document, I am not sure I have seen the use of \"deployment\" in this fashion; instead, typically \"implementation\" would be used.",
              "createdAt": "2024-07-02T22:42:47Z",
              "updatedAt": "2024-07-02T22:42:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Acutr",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-02T22:53:11Z",
          "updatedAt": "2024-07-02T22:53:12Z",
          "comments": [
            {
              "originalPosition": 154,
              "body": "Deleting the text SGTM because I think `time_interval`'s association of report to batch via timestamp is (plausibly) desirable from a privacy perspective.\r\n\r\n",
              "createdAt": "2024-07-02T22:53:12Z",
              "updatedAt": "2024-07-02T22:53:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AcwSk",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-02T22:57:54Z",
          "updatedAt": "2024-07-02T22:57:55Z",
          "comments": [
            {
              "originalPosition": 168,
              "body": "I suppose -- every new draft is a breaking change because it will, at least, change the domain-separation strings. I suppose it could be argued that shifting the numbers around could lead to bugs in implementations that implement multiple DAP drafts & use the wrong \"version\" of `PrepareError`; but we could run into similar issues if/when we reuse the number with a new semantic meaning.\r\n\r\nThat said, I'm happy enough kicking the can down the road, as long as we clean things up for the final RFC.",
              "createdAt": "2024-07-02T22:57:55Z",
              "updatedAt": "2024-07-02T22:57:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ai4Ld",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T14:32:47Z",
          "updatedAt": "2024-07-03T14:32:47Z",
          "comments": [
            {
              "originalPosition": 154,
              "body": "@simon-friedberger I agree that time interval queries are much more constrained, and this definitely has benefits. But I don't think fixed size queries are unconstrained in a way that leads to overlapping batches. The aggregators are supposed to assign reports to batches and make sure each report is mapped to a single batch. At a high level, any query type needs to ensure that queries _partition_ the set of reports for a task.",
              "createdAt": "2024-07-03T14:32:47Z",
              "updatedAt": "2024-07-03T14:32:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ai5Mz",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T14:34:24Z",
          "updatedAt": "2024-07-03T14:34:24Z",
          "comments": [
            {
              "originalPosition": 168,
              "body": "We don't actually need to update the domain separation tags with each version. In fact, in the latest version of the VDAF draft, we didn't update the version tag because there were no breaking changes. ",
              "createdAt": "2024-07-03T14:34:24Z",
              "updatedAt": "2024-07-03T14:34:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Akuy1",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T18:29:31Z",
          "updatedAt": "2024-07-03T18:29:31Z",
          "comments": [
            {
              "originalPosition": 154,
              "body": "I added some text to the security considerations section capturing this concern.",
              "createdAt": "2024-07-03T18:29:31Z",
              "updatedAt": "2024-07-03T18:29:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Akwdd",
          "commit": {
            "abbreviatedOid": "302c7bf"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T18:34:17Z",
          "updatedAt": "2024-07-03T18:34:17Z",
          "comments": [
            {
              "originalPosition": 112,
              "body": "Thinking about this a little more, at least in my head:\r\n\r\n* An \"implementation\" is a specific software implementation of some DAP protocol actor. Examples would include Janus or Daphne.\r\n* A \"deployment\" is a practical deployment of DAP; it would use one or more implementations of the DAP protocol actors, as well as defining tasks (or deciding how tasks can be dynamically configured, e.g. taskprov or some other task-provisioning scheme), defining other configuration parameters, deciding how the software implementations are run on machines, and everything else that comes with deploying software. Examples would include Divvi Up.",
              "createdAt": "2024-07-03T18:34:17Z",
              "updatedAt": "2024-07-03T18:34:17Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Am2OD",
          "commit": {
            "abbreviatedOid": "55d42f4"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "Looks good to me. I lean towards \"leader_selected\", but I don't much care what we call it at the end of the day.",
          "createdAt": "2024-07-03T23:59:34Z",
          "updatedAt": "2024-07-04T00:03:52Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "```suggestion\r\nEach measurement task has a preconfigured \"query type\". This query type defines\r\n```",
              "createdAt": "2024-07-03T23:59:34Z",
              "updatedAt": "2024-07-04T00:03:52Z"
            },
            {
              "originalPosition": 27,
              "body": "```suggestion\r\nboth how reports may be partitions into batches, as well as how these batches are\r\naddressed and the semantics of the query used for collection.\r\n```",
              "createdAt": "2024-07-04T00:00:00Z",
              "updatedAt": "2024-07-04T00:03:52Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Az7iR",
          "commit": {
            "abbreviatedOid": "55d42f4"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-05T18:05:15Z",
          "updatedAt": "2024-07-05T18:05:15Z",
          "comments": [
            {
              "originalPosition": 25,
              "body": "(made this change throughout.)",
              "createdAt": "2024-07-05T18:05:15Z",
              "updatedAt": "2024-07-05T18:05:21Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6A0ypn",
          "commit": {
            "abbreviatedOid": "bb404b7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-05T23:31:13Z",
          "updatedAt": "2024-07-05T23:31:13Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs6B30wS",
          "commit": {
            "abbreviatedOid": "bb404b7"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T00:10:00Z",
          "updatedAt": "2024-07-16T00:10:01Z",
          "comments": [
            {
              "originalPosition": 236,
              "body": "names with _ are usually backquoted. Consider adding `` to  this and time_interval on above row",
              "createdAt": "2024-07-16T00:10:00Z",
              "updatedAt": "2024-07-16T00:10:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CsMaj",
          "commit": {
            "abbreviatedOid": "73053f5"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-22T22:03:00Z",
          "updatedAt": "2024-07-22T22:09:40Z",
          "comments": [
            {
              "originalPosition": 213,
              "body": "While at it: Do we still like the term \"partial batch selector\"?",
              "createdAt": "2024-07-22T22:03:01Z",
              "updatedAt": "2024-07-22T22:09:40Z"
            },
            {
              "originalPosition": 314,
              "body": "Reviewer note: [This was removed in draft 11](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-10#section-4.6.1-18), but [we decided to add it back during review of this PR](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/563#discussion_r1639068652). @branlwyd perhaps the Change Log should be updated to reflect this. This is a breaking change, btw.",
              "createdAt": "2024-07-22T22:06:35Z",
              "updatedAt": "2024-07-22T22:09:40Z"
            },
            {
              "originalPosition": 469,
              "body": "For consistency with above?\r\n```suggestion\r\ninfringed upon by selection of the batch. For example, in the leader_selected\r\n```",
              "createdAt": "2024-07-22T22:07:21Z",
              "updatedAt": "2024-07-22T22:09:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DKkox",
          "commit": {
            "abbreviatedOid": "73053f5"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-25T22:56:35Z",
          "updatedAt": "2024-07-25T22:56:35Z",
          "comments": [
            {
              "originalPosition": 469,
              "body": "I went with a different change: everywhere we are referring to the modes themselves, as opposed to referring to a specific variant of the `BatchMode` enum, I use time-interval/leader-selected instead of `time_interval`/`leader_selected`. I think this reads a little better.",
              "createdAt": "2024-07-25T22:56:35Z",
              "updatedAt": "2024-07-25T23:04:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DKk0M",
          "commit": {
            "abbreviatedOid": "73053f5"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-25T22:57:33Z",
          "updatedAt": "2024-07-25T22:57:33Z",
          "comments": [
            {
              "originalPosition": 213,
              "body": "I do not like \"partial batch selector\" at all, but I have struggled to come up with a better name (e.g. more descriptive, without being too long). Suggestions for a better name are very welcome IMO",
              "createdAt": "2024-07-25T22:57:33Z",
              "updatedAt": "2024-07-25T22:57:33Z"
            }
          ]
        }
      ]
    },
    {
      "number": 564,
      "id": "PR_kwDOFEJYQs5ziDTd",
      "title": "Helper-side aggregation job init/continue asynchronous processing",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564",
      "state": "OPEN",
      "author": "inahga",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [
        "wire breaking"
      ],
      "body": "Closes https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/557.\r\n\r\nAllows helper-side aggregate job preparation to be handled asynchronously,\r\nsimilarly to how collection jobs work. Briefly, after calls with\r\n`AggregationJobInitReq` and `AggregationJobContinueReq`, the leader polls\r\nthe helper at `GET /tasks/{task-id]/aggregation_jobs/{aggregation-job-id}`\r\nuntil receiving an `AggregationJobResp`.\r\n\r\nIn discussion in #577 we had soft consensus on making this the only option,\r\ni.e. for spec simplicity we don't need to also allow the old synchronous\r\nbehavior.",
      "createdAt": "2024-06-25T18:08:22Z",
      "updatedAt": "2024-08-06T21:47:06Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "35f1274853c1c06a5132612b111debefe4fd17ac",
      "headRepository": "inahga/draft-ietf-ppm-dap",
      "headRefName": "inahga/async-helper-agg-jobs",
      "headRefOid": "d62e7e08993edcf437a9624e4663d10e4e5f5c93",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "I think I like the async approach proposed here. With this model, job as a computation unit does not have any significance. This async model could work at batch level between leader and helper.\r\n\r\n## Abstract Approach\r\nLeader submits to helper one or more reports belonging to a batch -  Ex: `PUT /tasks/taskId/batches/batchId <list of PrepareInit>` - returns 201 with no body\r\nLeader can query batch status - Ex `GET /tasks/taskId/batches/batchId?status=completed&begin=10&pageSize=20`  and receives paginated list of aggregates along with their status`\r\nLeader performs aggregation of successfully completed aggregates it received and runs the above query until it sees all the entries of the batch.\r\n\r\n#### Benefits\r\n* This enables concurrent processing on both leader and helper ends without blocking for whole job completion on either end.\r\n* This also gives flexibility to let leader and helper choose independent parallelism.\r\n* This is slightly different from the above approach even if we say jobCount=1. This can be loosely equivalent to jobCount=1 with ability to poll for multiple jobs at once.\r\n* Since job as an entity does not have any DAP meaning to it, we can drop it and have more granular parallelism.\r\n\r\n## Communication Patterns\r\nThis model also enables us to various communication patterns between Leader and helper.\r\n\r\n#### Request - Response with polling\r\nThis is essentially what was described above\r\n\r\n#### BiDirectional Streaming\r\nfor a given batch, bidirectional streaming RPC could allow sending report shares as and when they are available on leader and helper can respond the responses as a stream of messages. This is essentially gRPC bi-directional streaming\r\n\r\nSo, I think, instead of keeping the spec prescriptive about one pattern vs the other, it could be best to keep these patterns open to the implementation.\r\n\r\nThoughts?",
          "createdAt": "2024-06-27T04:47:20Z",
          "updatedAt": "2024-06-27T04:47:20Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@suman-ganta,\r\n\r\n> Leader submits to helper one or more reports belonging to a batch - Ex: `PUT /tasks/taskId/batches/batchId <list of PrepareInit>` - returns 201 with no body Leader can query batch status - Ex `GET /tasks/taskId/batches/batchId?status=completed&begin=10&pageSize=20` and receives paginated list of aggregates along with their status` Leader performs aggregation of successfully completed aggregates it received and runs the above query until it sees all the entries of the batch.\r\n\r\nJust to clarify, are you suggesting this as a change to the current PR? \r\n\r\nIn the current PR, when the Leader sends `GET {helper}/tasks/{task-id}/aggregation_jobs/{aggregation-job-id}`, the Helper is supposed to send the next prep step for each report in the batch. (For Prio3, this is the last prep step, i.e., the Leader learns whether each report was aggregated or not.) Is the idea to add some parameters to the request path that tell the Helper which reports to return?",
          "createdAt": "2024-06-27T17:53:08Z",
          "updatedAt": "2024-06-27T17:53:18Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "> @suman-ganta,\r\n> \r\n> > Leader submits to helper one or more reports belonging to a batch - Ex: `PUT /tasks/taskId/batches/batchId <list of PrepareInit>` - returns 201 with no body Leader can query batch status - Ex `GET /tasks/taskId/batches/batchId?status=completed&begin=10&pageSize=20` and receives paginated list of aggregates along with their status` Leader performs aggregation of successfully completed aggregates it received and runs the above query until it sees all the entries of the batch.\r\n> \r\n> Just to clarify, are you suggesting this as a change to the current PR?\r\n> \r\n> In the current PR, when the Leader sends `GET {helper}/tasks/{task-id}/aggregation_jobs/{aggregation-job-id}`, the Helper is supposed to send the next prep step for each report in the batch. (For Prio3, this is the last prep step, i.e., the Leader learns whether each report was aggregated or not.) Is the idea to add some parameters to the request path that tell the Helper which reports to return?\r\n\r\nYeah, at a high level, packaging these list of reports with a fixed job and computing that job as a whole, waiting for the whole job to finish on either end seem to introduce blocking processing. Instead of that, for a batch, we can break away from job notion and submit/process individual reports, query for completed (init/continue) reports (instead of blocking on one job completion). This could allow concurrently process these reports on both sides.",
          "createdAt": "2024-06-28T14:18:05Z",
          "updatedAt": "2024-06-28T14:18:05Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @suman-ganta Yeah, at a high level, packaging these list of reports with a fixed job and computing that job as a whole, waiting for the whole job to finish on either end seem to introduce blocking processing. Instead of that, for a batch, we can break away from job notion and submit/process individual reports, query for completed (init/continue) reports (instead of blocking on one job completion). This could allow concurrently process these reports on both sides.\r\n\r\nI see. Couldn't this be achieved at the level of aggregation jobs? I.e., instead of splitting processing of aggregation jobs across 4 threads, just create 4 aggregation jobs. Does that solve your problem?",
          "createdAt": "2024-06-28T20:29:00Z",
          "updatedAt": "2024-06-28T20:29:00Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "> A few thoughts on Suman's proposal:\r\n> \r\n> * I interpret this proposal as removing the concept of \"aggregation jobs\" entirely, and allowing reports to be grouped arbitrarily (and even grouped differently across different aggregation steps?).\r\n> * This is similar to the suggestion to drop batched requests entirely, sending each report in a separate request (likely requiring connection reuse/pipelining for efficiency). However, by batching we avoid that suggestion's issue with needing to repeatedly transmit batch ID/aggregation parameters on a per-report basis.\r\n> * However, even with Suman's suggestion, we would still need to retransmit the aggregation parameter on a per-step basis. This would be most impactful for VDAFs with a large aggregation parameter.\r\n> * The proposed URL scheme does not quite work out for the time-interval query type, where (a) the batch associated with each report is implicit (based on the timestamp) and (b) reports in a single aggregation job can currently be spread across multiple batches. I think (a) is more important than (b), but likely we can fix the URL scheme so this is not a fatal concern to the idea.\r\n> * This would also increase the complexity of handling aggregation jobs, since certain per-batch errors would become per-report errors.\r\n> * Implementations can also currently store the state for an aggregation job & its reports \"together\" (whatever that might mean for the underlying datastore); with this change, a Leader could potentially send a different grouping with each step, which might lead to less-optimal data access patterns. (More concretely, an implementation currently could store a single \"object\" for each aggregation job; I think with Suman's suggestion, implementations would be forced to store a single \"object\" for each report share, since the Helper has no guarantee that the grouping of reports per request will be stable.)\r\n> \r\n> I'm most concerned about retransmission of aggregation parameters, as well as the increase in complexity.\r\n\r\nFew clarifications:\r\n* My proposal is not really eliminating the grouping of reports in a request. instead, making the grouping optional and dynamic. The only purpose, as I understand, job is serving is to perform batch computation on both ends. This can be pretty dynamic based on available resources on either end. So, I'm seeing that as implementation detail as opposed to keep it as part of spec.\r\n* In this PR, the proposal is to do blocking poll on each job during collection phase. This can introduce blocking wait on leader and chattiness between leader and helper. An improvement could be - ability to ask for all completed jobs so far, which, in implementation can translate to ability to express completion status, page size etc as parameters.\r\n\r\nOther than that, my proposal shouldn't change anything in terms of additional overhead. If this sounds reasonable I'm happy to detail these further to make more sense.",
          "createdAt": "2024-07-03T01:32:06Z",
          "updatedAt": "2024-07-03T01:32:06Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @suman-ganta: Other than that, my proposal shouldn't change anything in terms of additional overhead. If this sounds reasonable I'm happy to detail these further to make more sense.\r\n\r\nI suggest moving this conversation to a different thread so that we can focus this PR on the move from sync to async aggregation. Would you follow up on the issue (#557) with a description of the problem you see and your proposal for solving it?\r\n\r\n",
          "createdAt": "2024-07-03T14:46:25Z",
          "updatedAt": "2024-07-03T14:46:25Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "I've made the editorial changes requested. I'd like some time to hack together a POC of this in Janus before merging this, to reveal any edge cases we haven't considered.\r\n\r\n> I suggest moving this conversation to a different thread so that we can focus this PR on the move from sync to async aggregation.\r\n\r\nAgreed, let's discuss there.",
          "createdAt": "2024-07-03T21:35:32Z",
          "updatedAt": "2024-07-03T21:35:32Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "One concern that fell out of discussion with @inahga today: I think any proposal which makes aggregation job processing asynchronous will significantly increase durable storage usage for the Helper.\r\n\r\nSpecifically, in today's \"sync\" world, the Helper can receive an aggregation job request, then handle processing of it in-memory. In the common case of a 1-step VDAF, this means that the only thing that needs to be durably stored is (a) a marker for each report ID that was aggregated, for replay protection and (b) the aggregate shares derived from the aggregation process, for collection. (This is what Janus implements today.)\r\n\r\nIn an \"async\" world, to allow the request to be reliably processed in an asynchronous fashion, it must be durably stored. This means that the Helper will need to durably store the entirety of the request, which includes both the (encrypted) Helper input shares as well as the Leader's first-step preparation messages. For many VDAFs, or for aggregation jobs containing many report shares, this will be significantly larger than what is stored in the \"sync\" approach.",
          "createdAt": "2024-07-16T19:59:27Z",
          "updatedAt": "2024-07-16T20:07:35Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "> I think any proposal which makes aggregation job processing asynchronous will significantly increase durable storage usage for the Helper.\r\n\r\nThat additional storage should be ephemeral, as you can delete the persisted request once the response is computed. So the only additional \"long-term\" persisted data should be an extra request digest (for retry and caching purposes).",
          "createdAt": "2024-07-16T20:13:00Z",
          "updatedAt": "2024-07-16T20:13:07Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> That additional storage should be ephemeral, as you can delete the persisted request once the response is computed. So the only additional \"long-term\" persisted data should be an extra request digest (for retry and caching purposes).\r\n\r\nCorrect, this is additional IO rate rather than additional long-term storage.\r\n\r\nFor multi-step VDAFs, the increased storage occurs on a per-step basis: the first step will need to store (encrypted) Helper input shares & preparation messages; later steps will need to store preparation messages.",
          "createdAt": "2024-07-16T20:40:58Z",
          "updatedAt": "2024-07-16T20:40:58Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I did a little napkin math to roughly estimate the increase in Helper storage IO for an async approach.\r\n\r\nConcretely, I used `Prio3SumVec` with `BITS=1`, `LEN=100000`, `CHUNK_LENGTH=316`. This is a large but realistic VDAF. In this configuration, the Helper input share has size 48, the Leader message has size 10165, and aggregate shares have size 1600000. I am assuming a fixed-size query mode, i.e. a single aggregation job touches a single batch & therefore a single aggregate share, and aggregation job sizes of 250.\r\n\r\nIn the \"sync\" model, the Helper receives the aggregation request from the Leader, performs VDAF computations, and stores an aggregate share. The total read/written is 1.6 megabytes. (This ignores whatever needs to be written for anti-replay purposes.)\r\n\r\nIn the \"async\" model, the Helper receives the aggregation request from the Leader & stores it (2.55 megabytes written); reads it, performs VDAF computations, and stores an aggregate share (2.55 megabytes read, 1.6 megabytes written). The total read/written is 6.7 megabytes, a slightly-more-than-fourfold increase in storage IO. (This ignores whatever needs to be written for anti-replay purposes, as well as ignoring the need to write/read the Helper messages to construct the eventual reply to the Leader -- N.B. the helper message is only 21 bytes.)\r\n\r\nIMO, this is a large enough increase to database IO that we should consider whether the async model justifies itself, especially since the current \"sync\" model would allow for background processing as an implementation detail -- with the downside that retries require re-transmitting the aggregation job. Perhaps we can get away with synchronous messages by tuning aggregation job sizes & timeouts appropriately on a per-deployment basis?",
          "createdAt": "2024-07-16T23:37:09Z",
          "updatedAt": "2024-07-16T23:54:15Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "> especially since the current \"sync\" model would allow for background processing as an implementation detail\r\n\r\nTo elaborate on this:\r\n\r\nConcretely, in Janus, we have a problem with CPU-intensive work being done inside an HTTP request when large histograms are being prepared. This is difficult to load balance and autoscale. One problem, out of a few: if requests come in a burst while the system is scaled-down, then those requests could overwhelm a single aggregator replica.\r\n\r\nThe natural response to this is to move CPU intensive work into a pool of workers that can intelligently schedule work, and have the original request used for enqueuing the work.\r\n\r\nIt occurred to me after writing this PR that this can be accomplished without spec changes. Inside the aggregate init/continue request, we can enqueue work and internally poll (or whatever signaling mechanism makes sense) until the internal job is complete, then return the request. All the while we hold the HTTP connection open.\r\n\r\nThis approach does have some disadvantages:\r\n1. The implementation still incurs DB IO disadvantages, noted by @branlwyd above.\r\n2. Repeated requests, e.g. due to timeout, require retransmitting the request body, which is potentially large.\r\n3. We still have a long running HTTP connection to deal with.\r\n\r\nI'm not sure that shifting to the async model in this PR meaningfully ameliorates these disadvantages. For point 1, this is a fact of life, but by not taking this PR, we've at least not imposed this problem on all implementers of DAP. For point 2, this is not a new disadvantage, but maybe there's a better way to fix this problem (e.g. idempotency key?). For point 3, I'm less concerned about long HTTP connections if they aren't also doing lots of CPU work--that is I think these connections can be made cheap.\r\n\r\nIn short: through some further thought and good points raised by folks in this issue, I'm starting to think that we shouldn't take this change right now.\r\n\r\nI'm curious about other operators of helpers, whether there's still use cases for async aggregate init/continue that are not satisfied by the workaround I've described.",
          "createdAt": "2024-07-17T16:42:12Z",
          "updatedAt": "2024-07-17T16:42:12Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "n.b. the \"internal async\" strategy (which is what I'm calling the alternative proposal I described above) still may be aided by the \"job cancellation\" change originally proposed in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/557#issue-2274105720. That is, we don't want 504 Gateway Timeout to be a normal part of operations, we'd rather internally timeout a long-running job/connection with 429.",
          "createdAt": "2024-07-17T17:03:03Z",
          "updatedAt": "2024-07-17T17:03:03Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @branlwyd In an \"async\" world, to allow the request to be reliably processed in an asynchronous fashion, it must be durably stored. \r\n\r\nWhy is this true?\r\n\r\n",
          "createdAt": "2024-07-17T17:34:30Z",
          "updatedAt": "2024-07-17T17:34:30Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> > @branlwyd In an \"async\" world, to allow the request to be reliably processed in an asynchronous fashion, it must be durably stored.\r\n> \r\n> Why is this true?\r\n\r\nI think anything we'd call \"async\" conceptually follows this model:\r\n\r\n1. Leader sends request to Helper; Helper responds OK once it has done what it needs to do to begin background processing, then begins processing in the background.\r\n2. Leader polls Helper until the Helper's processing is complete, then receives result.\r\n\r\nIf step 1's \"Helper responds OK once it has done what it needs to do to perform background processing\" doesn't involve storing the request durably, we are just as open to \"losing\" requests due to transient failures as we are in the \"sync\" world. (Or, to look at it another way: if we don't store the request durably in step 1, and the request is lost, recovery would involve the Leader receiving an appropriate error on its poll attempt in step 2 & retransmitting the aggregation job -- requiring retransmission just as in the \"sync\" world, with the additional complexity & network transit of polling.)",
          "createdAt": "2024-07-17T18:23:51Z",
          "updatedAt": "2024-07-17T18:23:51Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "The internal async approach does not solve the replay check issue. The replay checks are problematic so long as they need to be done in the critical path.",
          "createdAt": "2024-07-17T18:26:37Z",
          "updatedAt": "2024-07-17T18:26:37Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "@branlwyd what I like about the async proposal is that it gives far more flexibility to implementations for how they store data and handle things.\r\n\r\nThe synchronous approach requires a lot of strongly consistent operations to occur in the critical path, and this has significant limitations on the scalability of the protocol and the availability that implementations can potentially offer. In the handling of an aggregate job request, the processing and replay checks both must be conducted. Internal async does not address this - we are still bound to perform all actions successfully within whatever timeouts are set with the leader. This is something we have demonstrated internally to be a fairly significant operational challenge for the protocol.\r\n\r\nIf the critical path on the other hand is potentially just pushing this to a queue, that is a substantial improvement. Handling the replay checks in particular out-of-band temporarily with the request is a huge improvement for us, and gives us a lot more flexibility and allows us to construct a far more robust and reliable system.\r\n\r\nOn the subject of IO costs, costs of reads and writes to eventually consistent systems are generally significantly cheaper than e.g a relational database. Async gives implementations access to a far better toolkit here.",
          "createdAt": "2024-07-17T18:46:18Z",
          "updatedAt": "2024-07-17T18:46:18Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "> I did a little napkin math to roughly estimate the increase in Helper storage IO for an async approach.\r\n> \r\n> Concretely, I used `Prio3SumVec` with `BITS=1`, `LEN=100000`, `CHUNK_LENGTH=316`. This is a large but realistic VDAF. In this configuration, the Helper input share has size 48, the Leader message has size 10165, and aggregate shares have size 1600000. I am assuming a fixed-size query mode, i.e. a single aggregation job touches a single batch & therefore a single aggregate share, and aggregation job sizes of 250.\r\n> \r\n> In the \"sync\" model, the Helper receives the aggregation request from the Leader, performs VDAF computations, and stores an aggregate share. The total read/written is 1.6 megabytes. (This ignores whatever needs to be written for anti-replay purposes.)\r\n> \r\n> In the \"async\" model, the Helper receives the aggregation request from the Leader & stores it (2.55 megabytes written); reads it, performs VDAF computations, and stores an aggregate share (2.55 megabytes read, 1.6 megabytes written). The total read/written is 6.7 megabytes, a slightly-more-than-fourfold increase in storage IO. (This ignores whatever needs to be written for anti-replay purposes, as well as ignoring the need to write/read the Helper messages to construct the eventual reply to the Leader -- N.B. the helper message is only 21 bytes.)\r\n> \r\n> IMO, this is a large enough increase to database IO that we should consider whether the async model justifies itself, especially since the current \"sync\" model would allow for background processing as an implementation detail -- with the downside that retries require re-transmitting the aggregation job. Perhaps we can get away with synchronous messages by tuning aggregation job sizes & timeouts appropriately on a per-deployment basis?\r\n\r\n@branlwyd I'm also confused here about why the costs are being measured in bytes - generally it's the transactions and consistency checks that are the expensive part of writes, not the raw bytes.",
          "createdAt": "2024-07-17T19:52:45Z",
          "updatedAt": "2024-07-17T19:52:45Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "> Internal async does not address this - we are still bound to perform all actions successfully within whatever timeouts are set with the leader.\r\n\r\nI think this depends on how the helper operates. Timeouts should be normal(-ish) and tolerable. With internal async, if the leader repeats a request due to a timeout, the helper should await the existing background job.\r\n\r\nLet me write some pseudocode for an internal async helper's aggregate init/continue handler to be clear.\r\n\r\n```rust\r\nlet existing_job = database.get_queued_aggregation_job(aggregation_job_id);\r\n\r\nlet job = match existing_job {\r\n    Some(job) => job,\r\n    None => {\r\n        // Do some basic, cheap, checks on the aggregation job request, i.e. make sure it's well-formed.\r\n        if !is_valid(aggregation_job_request) {\r\n            return Err(InvalidRequest);\r\n        } else {\r\n            database.enqueue_new_aggregation_job(aggregation_job_request);\r\n        }\r\n    }\r\n}\r\n\r\n// Simple polling strategy, but implementations *could* do their own non-polling design, e.g. postgres LISTEN/NOTIFY.\r\nlet deadline = Time::now().add(SOME_REASONABLE_TIMEOUT);\r\nloop {\r\n    let job_response = database.check_if_aggregation_job_is_ready(job);\r\n\r\n    if job_response.is_ready() {\r\n        return Ok(job_response)\r\n    }\r\n\r\n    if Time::now() > deadline {\r\n        return Err(Timeout);\r\n    }\r\n    sleep(SOME_POLLING_INTERVAL);\r\n}\r\n```\r\n\r\nThis handwaves some checks into `is_valid()`, but I think the only things we want/need to check at this stage are:\r\n- Task ID recognized.\r\n- All reports IDs within request are distinct.\r\n\r\nNone of which require consistency guarantees since they're all in the context of the single request.\r\n\r\nEdit: Or does checking whether an aggregation job is already enqueued represent something unworkable for non-relational datastores? It does require a strong consistency guarantee. Edit again: [Although, I think even in true async we're going to need strong consistency guarantees about aggregation jobs.](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1681344788)\r\n\r\n(Note I haven't convinced myself yet one way or the other which is best, so I appreciate talking it out with me \ud83d\ude04)\r\n",
          "createdAt": "2024-07-17T20:52:01Z",
          "updatedAt": "2024-07-17T22:05:08Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "I've taken another editorial pass.\r\n\r\nI believe the only open discussions are:\r\n- https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1659280137 (what to do about duplicated POST/PUT?)\r\n- https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#issuecomment-2231990580 (is the IO cost too high?)\r\n- https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#issuecomment-2233743979 (should we even take this change?)\r\n- https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1674725591 (editorial).\r\n\r\nPlease let me know if I've missed any feedback, the PR comments are getting quite large :smile:.",
          "createdAt": "2024-07-17T22:39:57Z",
          "updatedAt": "2024-07-17T22:39:57Z"
        },
        {
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "body": "> > Internal async does not address this - we are still bound to perform all actions successfully within whatever timeouts are set with the leader.\r\n> \r\n> I think this depends on how the helper operates. Timeouts should be normal(-ish) and tolerable. With internal async, if the leader repeats a request due to a timeout, the helper should await the existing background job.\r\n> \r\n> Let me write some pseudocode for an internal async helper's aggregate init/continue handler to be clear.\r\n> \r\n> ```rust\r\n> let existing_job = database.get_queued_aggregation_job(aggregation_job_id);\r\n> \r\n> let job = match existing_job {\r\n>     Some(job) => job,\r\n>     None => {\r\n>         // Do some basic, cheap, checks on the aggregation job request, i.e. make sure it's well-formed.\r\n>         if !is_valid(aggregation_job_request) {\r\n>             return Err(InvalidRequest);\r\n>         } else {\r\n>             database.enqueue_new_aggregation_job(aggregation_job_request);\r\n>         }\r\n>     }\r\n> }\r\n> \r\n> // Simple polling strategy, but implementations *could* do their own non-polling design, e.g. postgres LISTEN/NOTIFY.\r\n> let deadline = Time::now().add(SOME_REASONABLE_TIMEOUT);\r\n> loop {\r\n>     let job_response = database.check_if_aggregation_job_is_ready(job);\r\n> \r\n>     if job_response.is_ready() {\r\n>         return Ok(job_response)\r\n>     }\r\n> \r\n>     if Time::now() > deadline {\r\n>         return Err(Timeout);\r\n>     }\r\n>     sleep(SOME_POLLING_INTERVAL);\r\n> }\r\n> ```\r\n> \r\n> This handwaves some checks into `is_valid()`, but I think the only things we want/need to check at this stage are:\r\n> \r\n>     * Task ID recognized.\r\n> \r\n>     * All reports IDs within request are distinct.\r\n> \r\n> \r\n> None of which require consistency guarantees since they're all in the context of the single request.\r\n> \r\n> Edit: Or does checking whether an aggregation job is already enqueued represent something unworkable for non-relational datastores? It does require a strong consistency guarantee. Edit again: [Although, I think even in true async we're going to need strong consistency guarantees about aggregation jobs.](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1681344788)\r\n> \r\n> (Note I haven't convinced myself yet one way or the other which is best, so I appreciate talking it out with me \ud83d\ude04)\r\n\r\nSo, our helper runs globally distributed across many sites, and when receiving traffic from leader nodes we generally expect to see this traffic split across multiple sites, even if the leader is only in a single source DC.\r\n\r\nWe can do strong consistency, it just reduces our availability, especially when doing things like sharding replay checks to improve throughput. These compromises to availability are extremely OK so long as they don't impact the critical paths of getting a job from a leader or serving a finished job to a leader.\r\n\r\nFor helpers which run in a single site, async might not be as helpful, but it is essential for helpers running in a multi-site configuration with high availability.",
          "createdAt": "2024-07-17T23:42:51Z",
          "updatedAt": "2024-07-17T23:42:51Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "> So, our helper runs globally distributed across many sites, and when receiving traffic from leader nodes we generally expect to see this traffic split across multiple sites, even if the leader is only in a single source DC.\r\n> \r\n> We can do strong consistency, it just reduces our availability, especially when doing things like sharding replay checks to improve throughput. These compromises to availability are extremely OK so long as they don't impact the critical paths of getting a job from a leader or serving a finished job to a leader.\r\n> \r\n> For helpers which run in a single site, async might not be as helpful, but it is essential for helpers running in a multi-site configuration with high availability.\r\n\r\nDo you have rough sketches of your system architecture and how sharding takes place? Ideally too, what the system architecture might look like with async as described in this PR? I totally understand if that's proprietary/NDA-applicable though.\r\n\r\nI'm trying to reason about where the internal async breaks down in an eventually consistent system, but am having trouble without a visualization of the system I'm reasoning against.\r\n\r\n(I do, however, understand how internal async would be exceptionally inelegant in an eventually consistent system, so I am relaxing on that being the solution).",
          "createdAt": "2024-07-19T02:18:43Z",
          "updatedAt": "2024-07-19T02:18:43Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "Does synchronous `/aggregate_shares` also present consistency problems in your system? I've dismissed making it async because it is not computationally expensive--it's more or less selecting some rows and summing them up.\r\n\r\n(Not that I particularly want to change more protocol text, but just asking for completeness sake).",
          "createdAt": "2024-07-19T02:20:59Z",
          "updatedAt": "2024-07-19T02:21:35Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> The internal async approach does not solve the replay check issue. The replay checks are problematic so long as they need to be done in the critical path.\r\n\r\n> If the critical path on the other hand is potentially just pushing this to a queue, that is a substantial improvement. Handling the replay checks in particular out-of-band temporarily with the request is a huge improvement for us, and gives us a lot more flexibility and allows us to construct a far more robust and reliable system.\r\n\r\nCan you add some detail here about how pushing these replay checks into a background job will help? Ultimately, the replay checks must occur, and they effectively must be \"strongly consistent\"/\"serializable\" (lest two concurrent requests permit a replay). Pushing the work into a background worker may make individual HTTP requests resolve more quickly, but it is not clear to me how the overall work to process an aggregation step is reduced or simplified.\r\n\r\n\r\n> On the subject of IO costs, costs of reads and writes to eventually consistent systems are generally significantly cheaper than e.g a relational database. Async gives implementations access to a far better toolkit here.\r\n\r\nI think \"async\" vs \"sync\" implementations are orthogonal to the durable storage technology in use, that is, either async or sync can use either a \"strongly consistent\" or an \"eventually consistent\" storage technology. Can you expand on your concern here?\r\n\r\n\r\n> The synchronous approach requires a lot of strongly consistent operations to occur in the critical path, and this has significant limitations on the scalability of the protocol and the availability that implementations can potentially offer. In the handling of an aggregate job request, the processing and replay checks both must be conducted. Internal async does not address this - we are still bound to perform all actions successfully within whatever timeouts are set with the leader. This is something we have demonstrated internally to be a fairly significant operational challenge for the protocol.\r\n\r\nDAP currently requires a lot of strong consistency/serializability of requests, not all of which are addressed by fixing aggregation job stepping:\r\n* Report replay checks\r\n* Collection/aggregation job semantics (the Leader must complete a collection job only after all reports in the relevant batch have been aggregated; the Leader & Helper must not permit additional aggregation into a batch once a collection job exists)\r\n* Probably more?\r\n\r\nI'm amenable to improving things here, but IMO we should identify the current locations where semantics requiring strong consistency/serializability might be relaxed to semantics requiring only eventual consistency, and address them.\r\n\r\n\r\n> We can do strong consistency, it just reduces our availability, especially when doing things like sharding replay checks to improve throughput. These compromises to availability are extremely OK so long as they don't impact the critical paths of getting a job from a leader or serving a finished job to a leader.\r\n\r\nThis is helpful, thank you. IIUC, your implementation has intermittent/transient(?) availability issues when performing some strongly-consistent DB operations; if these operations are part of the synchronous aggregation step, they will bubble up to the Leader as errors. But async allows these errors to be hidden, and (I suppose) the availability failures still permit the system to process aggregations with appropriate throughput.\r\n\r\n\r\n>  I'm also confused here about why the costs are being measured in bytes - generally it's the transactions and consistency checks that are the expensive part of writes, not the raw bytes.\r\n\r\nFair enough, the numbers are similar:\r\n\r\nIn the \"sync\" model, the Helper receives the aggregation request from the Leader, performs VDAF computations, and stores an aggregate share (1 tx). The total number of transactions is 1. (This ignores none of the required work -- the replay check would be done as part of the transaction.)\r\n\r\nIn the \"async\" model, the Helper receives the aggregation request from the Leader & stores it (1 tx); reads it (1 tx), performs VDAF computations, and stores an aggregate share (1 tx). At least one poll from the Leader is then required, each poll taking 1 tx. The total number of transactions is 4+, a fourfold-or-more increase in transactions. (This also ignores nothing -- I think the replay check could be done as part of the Helper processing write tx. Possibly the \"processing\" step could be done in a single transaction that both reads & writes, but this would require keeping the transaction open while time-consuming VDAF computations are ongoing -- which may or may not be a good idea depending on the underlying DB in use.)\r\n",
          "createdAt": "2024-07-19T03:12:07Z",
          "updatedAt": "2024-07-19T04:16:36Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "> We should add some discussion about asynchronocity to the overview at the top of {{aggregate-flow}}: Mention that aggregation jobs are handled asynchronicity, and why.\r\n\r\nSorry, I overlooked this change. I have added a short summary that explains the rough what and why.\r\n\r\nNon-critical atm: Is it worth drawing the new flow in the ladder diagram `#agg-flow`? On one hand it's more accurate, on the other hand it may make the diagram too busy.",
          "createdAt": "2024-07-23T20:19:47Z",
          "updatedAt": "2024-07-23T20:19:47Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "We discussed in meeting to prepare for possibly taking both async and sync HTTP request models for this PR. I have written it up in commit https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564/commits/39cfd4eb42f0ee010a2a11e54e03fe126c0d87f4.",
          "createdAt": "2024-07-24T17:54:06Z",
          "updatedAt": "2024-07-24T17:54:06Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "n.b. to focus this PR I've backed out any (editorial) changes to collection. I will save those for another PR.",
          "createdAt": "2024-07-26T15:51:18Z",
          "updatedAt": "2024-07-26T15:51:18Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@inahga we will hold off on merging this for a couple of weeks in order to give folks a little time to think about the design space.",
          "createdAt": "2024-07-26T23:39:06Z",
          "updatedAt": "2024-07-26T23:39:06Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "Perfectly acceptable! I have written up a POC of this as-written (albeit for single round VDAFs only), to evaluate the costs that @branlwyd alluded to. My initial impression is that the drawbacks are small, if any, and may be mitigated (e.g. by maximizing aggregation job size). If that proves the case with some more testing, we may be able to drop sync support entirely.",
          "createdAt": "2024-07-26T23:51:59Z",
          "updatedAt": "2024-07-26T23:51:59Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "I think our best bet is to keep sync optional for now.",
          "createdAt": "2024-07-29T15:10:41Z",
          "updatedAt": "2024-07-29T15:10:41Z"
        },
        {
          "author": "inahga",
          "authorAssociation": "NONE",
          "body": "In perusing prior art for HTTP polling in IETF specs, I consulted ACME https://www.rfc-editor.org/rfc/rfc8555#section-7.5.1, which more or less taught me \"less is more\". In my latest pass I've edited for brevity, as I think overspecifying the behavior partially leads to the perception of this proposal being too complex. (Not that it's an invalid perception, but overspecifying doesn't help the matter :smile:).\r\n\r\nNotably I hoist up mention of media types, shortened mentions of URIs, and dropped the `MAY wait for job to be ready...` statements. In the latter case, I believe it goes without saying, since the behavior is 100% legal with or without mention of it.",
          "createdAt": "2024-08-06T21:47:05Z",
          "updatedAt": "2024-08-06T21:47:05Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs5_kAb4",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Looks good over all! One high level question: How does the Helper multiple PUTs for the same aggregation job? (This may be covered, but I wanted to confirm.)",
          "createdAt": "2024-06-25T23:54:26Z",
          "updatedAt": "2024-06-26T00:07:56Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "Should this include \"MAY return a Retry-After\" to feature-match collection?",
              "createdAt": "2024-06-25T23:54:26Z",
              "updatedAt": "2024-06-26T00:07:37Z"
            },
            {
              "originalPosition": 28,
              "body": "I don't think this is necessary.\r\n```suggestion\r\nHelper attempts to initialize VDAF preparation (see\r\n```",
              "createdAt": "2024-06-25T23:54:51Z",
              "updatedAt": "2024-06-26T00:07:37Z"
            },
            {
              "originalPosition": 49,
              "body": "This transition is a little awkward I think. I like the first paragraph (\"it immediately responds with ...\"). Perhaps tack on something like: \"Later on, the Leader will make send an HTTP GET to <blah blah blah> to get the response. The Helper constructs the response as follows.\"",
              "createdAt": "2024-06-26T00:00:14Z",
              "updatedAt": "2024-06-26T00:07:37Z"
            },
            {
              "originalPosition": 44,
              "body": "Should this be 202?",
              "createdAt": "2024-06-26T00:01:06Z",
              "updatedAt": "2024-06-26T00:07:37Z"
            },
            {
              "originalPosition": 61,
              "body": "is \"aggregation job URI\" well-defined? In any case, I think it would help the reader to either spell it out here or add a reference to the definition.",
              "createdAt": "2024-06-26T00:02:34Z",
              "updatedAt": "2024-06-26T00:07:37Z"
            },
            {
              "originalPosition": 153,
              "body": "I may have missed it, but this should apply to the initial agg request as well.",
              "createdAt": "2024-06-26T00:07:16Z",
              "updatedAt": "2024-06-26T00:07:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_seIT",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T16:25:43Z",
          "updatedAt": "2024-06-26T16:44:45Z",
          "comments": [
            {
              "originalPosition": 44,
              "body": "No, 201 Created is what we use in response to `PUT /tasks/{task-id}/collection_jobs/{collection-job-id}`, so it's appropriate here. 202 Accepted is what we use in response to `GET /tasks/{task-id}/collection_jobs/{collection-job-id}` if the job is not done yet.",
              "createdAt": "2024-06-26T16:25:43Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 153,
              "body": "```suggestion\r\nheader field to suggest a polling interval to the Leader.\r\n```",
              "createdAt": "2024-06-26T16:27:24Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 15,
              "body": "```suggestion\r\n\"application/dap-aggregation-job-init-req\". After receiving HTTP status 201\r\nCreated, the Leader proceeds to poll GET requests to the aggregation job URI\r\n```",
              "createdAt": "2024-06-26T16:31:07Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 15,
              "body": "Further, the use of the verb \"poll\" is awkward: what's being polled is the aggregation job, not the GET request. The GET request is the representation of polling an aggregation job. Something like \"the Leader polls the aggregation job by sending GET requests to the aggregation job URI\" is better.",
              "createdAt": "2024-06-26T16:32:38Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 17,
              "body": "This new text should be moved a few lines below, to where it says \"The Helper's response will be...\".\r\n\r\nFirst, it's awkward to discuss response handling and then have a sentence discussing Leader->Helper authentication, which is relevant to the request.\r\n\r\nSecond, the paragraph beginning \"The Helper's response will be an `AggregationJobResp` message...\" should be updated, because it's not clear whether that's talking about the PUT or a subsequent GET. \r\n\r\nSecond, ",
              "createdAt": "2024-06-26T16:33:30Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 73,
              "body": "@cjpatton I believe this paragraph answers your question \"How does the Helper multiple PUTs for the same aggregation job?\"\r\n\r\nPUT multiple times is legal for idempotence, but mutation is illegal.",
              "createdAt": "2024-06-26T16:40:19Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 110,
              "body": "Can you sketch out the scenario this is meant to catch? Per the previous paragraph, it should be OK for the leader to send the same `AggregationJobContinueReq` twice for idempotency reasons.",
              "createdAt": "2024-06-26T16:42:34Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            },
            {
              "originalPosition": 157,
              "body": "Interesting implication of this is that it means the helper must keep track of the order in which prep steps occurred in the request. Janus already does this, but perhaps there are implementations that relied upon having the leader request in memory.",
              "createdAt": "2024-06-26T16:44:21Z",
              "updatedAt": "2024-06-26T16:44:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_t49F",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T19:04:26Z",
          "updatedAt": "2024-06-26T19:04:26Z",
          "comments": [
            {
              "originalPosition": 61,
              "body": "It's not concretely defined--I copied the vernacular from the collection flow. I think I will expand it out instead of using the shorthand (and do the same for the other references).",
              "createdAt": "2024-06-26T19:04:26Z",
              "updatedAt": "2024-06-26T19:04:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_t7y5",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T19:11:27Z",
          "updatedAt": "2024-06-26T19:11:27Z",
          "comments": [
            {
              "originalPosition": 110,
              "body": "Ah, you're right. I was thinking this would handle the case where the leader sends two different POST requests, but as you stated the idempotency check would catch this.\r\n\r\nI will strike this step, and revert the reordering of steps (as it's not necessary).",
              "createdAt": "2024-06-26T19:11:27Z",
              "updatedAt": "2024-06-26T19:11:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_t_Fl",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T19:19:33Z",
          "updatedAt": "2024-06-26T19:19:34Z",
          "comments": [
            {
              "originalPosition": 157,
              "body": "A point in favor of https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/217, but I doubt that discussion needs to be rehashed.",
              "createdAt": "2024-06-26T19:19:34Z",
              "updatedAt": "2024-06-26T19:19:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_uKte",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T19:48:59Z",
          "updatedAt": "2024-06-26T19:48:59Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "I rewrote this to match what I did for continuation\r\n```\r\nIf the Helper finds the `AggregationJobInitReq` to be valid, it immediately\r\nresponds with HTTP status 201 Created. The Helper is now ready to process each\r\nreport share into an outbound prepare step, asynchronously from any further\r\nrequests from the leader.\r\n\r\nThe following structures are computed:\r\n```\r\n\r\nLMK if that's also awkward, then I can fix both :D",
              "createdAt": "2024-06-26T19:48:59Z",
              "updatedAt": "2024-06-26T19:48:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs5_uK2-",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-26T19:49:21Z",
          "updatedAt": "2024-06-26T19:49:22Z",
          "comments": [
            {
              "originalPosition": 61,
              "body": "Done. I've replaced mention of `aggregation job URI` and `collection job URI` with the explicit paths.",
              "createdAt": "2024-06-26T19:49:21Z",
              "updatedAt": "2024-06-26T19:49:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AFW23",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-06-28T20:12:06Z",
          "updatedAt": "2024-06-28T20:12:06Z",
          "comments": [
            {
              "originalPosition": 49,
              "body": "Much better!",
              "createdAt": "2024-06-28T20:12:06Z",
              "updatedAt": "2024-06-28T20:12:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AFYl2",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "@inahga a couple of questions for you and others, but overall this looks correct!",
          "createdAt": "2024-06-28T20:15:14Z",
          "updatedAt": "2024-06-28T20:25:47Z",
          "comments": [
            {
              "originalPosition": 28,
              "body": "A bit more clear\r\n```suggestion\r\nThe Helper's response MAY include a Retry-After header field to suggest a polling\r\n```",
              "createdAt": "2024-06-28T20:15:15Z",
              "updatedAt": "2024-06-28T20:25:47Z"
            },
            {
              "originalPosition": 119,
              "body": "Not part of this PR, but I think this should be be a proper MUST.\r\n```suggestion\r\nThe response's `prepare_resps` MUST include exactly the same report IDs in the\r\n```",
              "createdAt": "2024-06-28T20:18:25Z",
              "updatedAt": "2024-06-28T20:25:47Z"
            },
            {
              "originalPosition": 114,
              "body": "The same URI is used for each prep step. Do we foresee this being an issue? Perhaps this is actually desirable from an idempotency standpoint?\r\n\r\ncc/ @tgeoghegan, @branlwyd ",
              "createdAt": "2024-06-28T20:20:10Z",
              "updatedAt": "2024-06-28T20:25:47Z"
            },
            {
              "originalPosition": 148,
              "body": "What is the Helper trying to accomplish here? Does this response actually hep? If I understand correctly, the Helper's response would be the same had the Leader indicated the correct step.\r\n\r\n",
              "createdAt": "2024-06-28T20:22:49Z",
              "updatedAt": "2024-06-28T20:25:47Z"
            },
            {
              "originalPosition": 157,
              "body": "I don't think this requirement is any harder to meet as a result of taking this PR. Or am I missing something?\r\n\r\n",
              "createdAt": "2024-06-28T20:24:44Z",
              "updatedAt": "2024-06-28T20:25:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6ASe3b",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-01T22:22:58Z",
          "updatedAt": "2024-07-01T22:22:58Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "It was already the case that the same URI was used regardless of which step we were on. Asynchronous handling of aggregation jobs shouldn't change that.",
              "createdAt": "2024-07-01T22:22:58Z",
              "updatedAt": "2024-07-01T22:22:58Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6ASXDL",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "A few thoughts on Suman's proposal:\r\n\r\n* I interpret this proposal as removing the concept of \"aggregation jobs\" entirely, and allowing reports to be grouped arbitrarily (and even grouped differently across different aggregation steps?).\r\n* This is similar to the suggestion to drop batched requests entirely, sending each report in a separate request (likely requiring connection reuse/pipelining for efficiency). However, by batching we avoid that suggestion's issue with needing to repeatedly transmit batch ID/aggregation parameters on a per-report basis.\r\n* However, even with Suman's suggestion, we would still need to retransmit the aggregation parameter on a per-step basis. This would be most impactful for VDAFs with a large aggregation parameter.\r\n* The proposed URL scheme does not quite work out for the time-interval query type, where (a) the batch associated with each report is implicit (based on the timestamp) and (b) reports in a single aggregation job can currently be spread across multiple batches. I think (a) is more important than (b), but likely we can fix the URL scheme so this is not a fatal concern to the idea.\r\n* This would also increase the complexity of handling aggregation jobs, since certain per-batch errors would become per-report errors.\r\n* Implementations can also currently store the state for an aggregation job & its reports \"together\" (whatever that might mean for the underlying datastore); with this change, a Leader could potentially send a different grouping with each step, which might lead to less-optimal data access patterns. (More concretely, an implementation currently could store a single \"object\" for each aggregation job; I think with Suman's suggestion, implementations would be forced to store a single \"object\" for each report share, since the Helper has no guarantee that the grouping of reports per request will be stable.)\r\n\r\nI'm most concerned about retransmission of aggregation parameters,  as well as the increase in complexity.",
          "createdAt": "2024-07-01T21:47:18Z",
          "updatedAt": "2024-07-01T22:27:38Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I don't see a problem with it (practically-speaking; I didn't think too deeply about \"proper\" HTTP semantics).\r\n\r\nIn effect, the same resource (i.e. an aggregation job) is being repeatedly mutated as it is stepped from start to completion. I can't think of a reason to use more granular URIs (e.g. a URI per step?) -- if anything, I think this would add confusion as older steps are \"invalid\" and most implementations will not want to keep around the state that would allow them to be retrieved again.",
              "createdAt": "2024-07-01T21:47:19Z",
              "updatedAt": "2024-07-01T22:27:38Z"
            },
            {
              "originalPosition": 148,
              "body": "If I'm reading this correctly, this is the asynchronous version of our current skew-recovery logic. Since aggregation jobs are now asynchronous, if the Helper receives a repeated request it must treat it as if it needs to be handled asynchronously (since there is no way to communicate synchronous completion), though I suspect the implementation would effectively be to treat the job as already completed & return the results on the first attempt to poll.\r\n\r\nHowever, I think we need some text here that the expected response is to re-send the previous results -- right now, this reads as if we continue on to the next paragraph, which I think isn't intended.",
              "createdAt": "2024-07-01T21:55:03Z",
              "updatedAt": "2024-07-01T22:27:38Z"
            },
            {
              "originalPosition": 157,
              "body": "Hmm. In the Leader Continuation section, we have:\r\n\r\n> The PrepareContinues MUST be in the same order as the previous aggregate request.\r\n\r\nBut in the Helper Continuation section, we have:\r\n\r\n> Additionally, if any prep step appears out of order relative to the previous request, then the Helper MAY abort with error invalidMessage.\r\n\r\nThat is, we proscribe that the Leader MUST respect the original order of reports, but only that the Helper MAY respect the original order (and therefore MAY not). I'm not sure if there is a good reason for this mismatch -- it seems to me that this is Postel's Law (\"be conservative about what you send, liberal in what you accept\") thinking which is recognized as [causing reliability & security issues](https://en.wikipedia.org/wiki/Robustness_principle#Criticism). IMO, the Helper should require that reports be in the same order if the Leader does, i.e. the quoted MAY should be a MUST instead.\r\n\r\n(That said, I do think the ordering requirement is perhaps ill-justified, but it's not important enough to rehash #217.)",
              "createdAt": "2024-07-01T22:05:56Z",
              "updatedAt": "2024-07-01T22:27:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AdPJ1",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T01:03:59Z",
          "updatedAt": "2024-07-03T01:22:54Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "It looks like this requires constant polling from the leader end to check status of each job. Should it implement long polling and respond when there is response to return?",
              "createdAt": "2024-07-03T01:03:59Z",
              "updatedAt": "2024-07-03T01:22:54Z"
            },
            {
              "originalPosition": 80,
              "body": "If we are not doing long polling for any reason, the response should probably 200 with appropriate status response instead of 202, since we are not accepting a new request for processing.",
              "createdAt": "2024-07-03T01:11:04Z",
              "updatedAt": "2024-07-03T01:22:54Z"
            },
            {
              "originalPosition": 24,
              "body": "What the recommended way of polling here? Do we poll each job in batch sequentially until all jobs give expected response? This would be blocking operation on leader side and generates noisy request to the helper. It would be better to provide ability to query for completed jobs. This enables leader to process-and-fetch the completed jobs in a loop.",
              "createdAt": "2024-07-03T01:22:40Z",
              "updatedAt": "2024-07-03T01:22:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AjTec",
          "commit": {
            "abbreviatedOid": "23c7912"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T15:12:28Z",
          "updatedAt": "2024-07-03T15:12:28Z",
          "comments": [
            {
              "originalPosition": 157,
              "body": "It strikes me as an error/oversight, but perusing the blame looks inconclusive either way. I will change it to a `MUST`.",
              "createdAt": "2024-07-03T15:12:28Z",
              "updatedAt": "2024-07-03T15:12:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AjZvS",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T15:23:33Z",
          "updatedAt": "2024-07-03T15:23:33Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "What do you mean by \"long polling\"?",
              "createdAt": "2024-07-03T15:23:33Z",
              "updatedAt": "2024-07-03T15:23:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AjdTx",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T15:29:58Z",
          "updatedAt": "2024-07-03T15:29:59Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "Hmm, I see long polling as the state of the spec before this change, more or less. That is, leaders hold the HTTP connection open while the helper does processing. The helper returns a response when it's ready.\r\n\r\n> If we are not doing long polling for any reason, the response should probably 200 with appropriate status response instead of 202, since we are not accepting a new request for processing.\r\n\r\n202 signals to the leader that polling needs to continue. We could give 200 with an empty body, but I think 202 is most appropriate, [given my reading of MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/202). (FWIW I copied the semantics from collection, so if we argue against using 202 we would need to fix collection too).",
              "createdAt": "2024-07-03T15:29:59Z",
              "updatedAt": "2024-07-03T15:29:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ajwpz",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T16:05:46Z",
          "updatedAt": "2024-07-03T16:05:46Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "Earlier model is, you wait when you init a job. Now, we submit the job and then wait on a different thread. But leader has to wait for several jobs to complete in this model, right? So, it is not clear to me how leader keep polling for all jobs better than long polling.\r\n\r\n@cjpatton - http long polling - in this context, it translates to helper blocks the GET call until the job processing is done (with an optional timeout).",
              "createdAt": "2024-07-03T16:05:46Z",
              "updatedAt": "2024-07-03T16:05:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AkafR",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "mendess",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T17:38:42Z",
          "updatedAt": "2024-07-03T17:38:43Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "This is very hard to implement for the helper. It requires possibly storing every body indefinitely. I think we can take advantage of the fact that aggregation is now async to make this better.\r\n\r\nI propose that subsequent `PUT` requests to this enpoint carrying the same aggregation-job-id should be responded with the same response that previous requests were responded with regardless of , this means we have to, at most, store an (endpoint, response code) tuple. And we still maintain the idenpotency guarantees that we want, in case the leader retries the request",
              "createdAt": "2024-07-03T17:38:43Z",
              "updatedAt": "2024-07-03T17:38:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AlpeI",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:26:57Z",
          "updatedAt": "2024-07-03T20:26:57Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "The goal is to enable the helper to move expensive computation (checking uniqueness indices, heavyweight VDAF math, etc.) out of the request hot-path. \r\n\r\nIt is plausible to make no changes to the spec, which IIUC is equivalent to using a long-polling strategy. Implementations could still decouple expensive operations by leaving the connection open and internally polling a pool of compute workers.\r\n\r\nBut I (if I'm operating as a helper) am still left with the disadvantages of having to deal with long-running connections. It's tricky to load balance them, e.g. what do we do with existing connections when we need to scale up? Whereas for stateless GET requests with no body, load balancing them is trivial.\r\n\r\nIf we really want to avoid polling, we may want to look into the more robust 2-way protocols, i.e. websockets. (ah you did allude to this in your proposal).",
              "createdAt": "2024-07-03T20:26:57Z",
              "updatedAt": "2024-07-03T21:14:06Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AlrrW",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:31:03Z",
          "updatedAt": "2024-07-03T20:31:03Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "To summarize my proposed changes - \r\n1. aggregate init request can return either 200 with proper response body (sync) or 202 without body (async). This makes it backward compatible and supports sync model.\r\n2. When leader is ready to receive the init job response, it makes a GET <jobid> http request. Helper can\r\n  * respond with 202 immediately if the job init is not done on helper (frequent polling model)\r\n  * respond with 200 immediately with init job response (if job is completed)\r\n  * Block the caller and respond with 200 with init job response when job is finished (long polling).",
              "createdAt": "2024-07-03T20:31:03Z",
              "updatedAt": "2024-07-03T20:31:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Alu9u",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:37:36Z",
          "updatedAt": "2024-07-03T20:37:36Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "I disagree here: in practice, the Helper only needs to store a hash of the previous request to perform request-equivalence checking. So the storage overhead isn't to store all request bodies indefinitely, it's to store the hash of the last request body until the next request is successfully processed.\r\n\r\nI do think that checking that the request matches is very useful for \"request/response coherency\" reasons: if the Helper doesn't check the request body, we could end up in a situation where the Leader (due to a bug) repeats a request for some aggregation step with a different request, and the Helper would respond with the response body for the original request. This could be a very challenging situation to debug.",
              "createdAt": "2024-07-03T20:37:36Z",
              "updatedAt": "2024-07-03T20:37:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Alxq3",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:42:56Z",
          "updatedAt": "2024-07-03T20:42:56Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "Aggregation jobs are independent, so it shouldn't be necessary to process jobs sequentially. Rather the leader can/should eagerly send aggregate init/continue to the helper.\r\n\r\n> This would be blocking operation on leader side\r\n\r\nIndeed, but it shouldn't be much worse than blocking leader-side waiting for the aggregate init/continue request to finish.\r\n\r\nI'd actually expect this grants leader implementations more flexibility in how they schedule aggregation job processing. If a leader-side worker thread is blocked on an open HTTP request, it can't set it aside to do other work. In the proposed model, the leader-side worker can poll once, see that the aggregation job isn't ready, move on to other work (e.g. polling other workers), and poll again later.\r\n\r\n> generates noisy request to the helper\r\n\r\nIndeed that's a tradeoff, but I'm hoping that empty GET requests are fairly cheap in terms of bandwidth. And aggregator pairings are free to tune polling intervals and aggregation job volume such that the amount of noise is low.\r\n\r\n> It would be better to provide ability to query for completed jobs. This enables leader to process-and-fetch the completed jobs in a loop.\r\n\r\nThat's an interesting idea. Can you elaborate on the structure of such an endpoint?",
              "createdAt": "2024-07-03T20:42:56Z",
              "updatedAt": "2024-07-03T20:42:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AlyaQ",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:44:29Z",
          "updatedAt": "2024-07-03T20:44:29Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "I agree that the old semantics were effectively equivalent to long-polling.\r\n\r\nI also think that the semantics in this PR are compatible with a long-polling implementation: specifically, on receipt of a poll request the Helper would not respond at all until the job is complete (resulting in a 200 response) or some timeout occurs (resulting in a 202 response).\r\n\r\nBut now that we're thinking about this, I do wonder if this could be a source of friction between different implementations: the deployment considerations for immediate-polling are different than those for long-polling, so an implementation optimized for immediate-polling may not play well with an implementation optimized for long-polling.",
              "createdAt": "2024-07-03T20:44:29Z",
              "updatedAt": "2024-07-03T20:44:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Aly4F",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:45:31Z",
          "updatedAt": "2024-07-03T20:45:31Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "I hope the above clarifies some of the confusion. We are still doing it this process in two steps, but the later step can be optimized as mentioned above without compromising on proposed async model.",
              "createdAt": "2024-07-03T20:45:31Z",
              "updatedAt": "2024-07-03T20:45:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Al0yo",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T20:49:26Z",
          "updatedAt": "2024-07-03T20:49:26Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "> It requires possibly storing every body indefinitely.\r\n\r\nCan you clarify the drawbacks to this? In Janus, [we accomplish this by storing a SHA-256 of the request body](https://github.com/divviup/janus/blob/main/db/00000000000001_initial_schema.up.sql#L225) which is fairly cheap in terms of storage, and I don't think has caused us problems (yet).\r\n\r\nIf we have to store `(endpoint, response code)` anyway, then we may as well go all the way and give more \"truthful\" idempotency (i.e. retain this clause), assuming that storing a request hash is not burdensome.",
              "createdAt": "2024-07-03T20:49:26Z",
              "updatedAt": "2024-07-03T20:49:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AmOBn",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T21:29:03Z",
          "updatedAt": "2024-07-03T21:29:03Z",
          "comments": [
            {
              "originalPosition": 148,
              "body": "> if the Helper receives a repeated request it must treat it as if it needs to be handled asynchronously (since there is no way to communicate synchronous completion), though I suspect the implementation would effectively be to treat the job as already completed & return the results on the first attempt to poll.\r\n\r\nThat's more or less correct. My idea is that the repeated request is effectively a no-op, i.e. the helper-side aggregation job state does not need to change.\r\n\r\nI've added verbiage\r\n> If the Leader is one step behind (e.g., the Leader has resent the previous HTTP request), then the Helper MAY attempt to recover by sending HTTP 202 status Accepted and discontinuing any further work.\r\n\r\nto cut the transition to the next paragraph. Let me know if more elaboration is necessary.\r\n",
              "createdAt": "2024-07-03T21:29:03Z",
              "updatedAt": "2024-07-03T21:29:03Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AmYSo",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T21:57:34Z",
          "updatedAt": "2024-07-03T21:57:34Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "What textual change are you suggesting? My interpretation of the text in this PR is that an implementation is allowed to implement either frequent-polling or long-polling. There are some operational considerations which are not specified (e.g. if long-polling, how long does the Helper wait before giving up & returning 202?) but I think they can be left implementation-dependent.\r\n\r\nN.B. as it stands, the initial request must always return a 201 Created -- i.e. the old \"sync\" model has been replaced by the new \"async\" model. The prior discussion around this is in #557.",
              "createdAt": "2024-07-03T21:57:34Z",
              "updatedAt": "2024-07-03T21:57:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AmrK0",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T22:48:24Z",
          "updatedAt": "2024-07-03T22:48:24Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "> My interpretation of the text in this PR is that an implementation is allowed to implement either frequent-polling or long-polling.\r\n\r\nTrue, with caveats. The GET request that follow PUT/POST requests could long poll. But note that this PR changeset would have inserted a new request that didn't exist before, i.e. long polling on the PUT/POST requests is not possible, you'd still need to follow it up with a GET request.\r\n\r\nBut as @suman-ganta states, we can make it so that:\r\n> aggregate init request can return either 200 with proper response body (sync) or 202 without body (async). This makes it backward compatible and supports sync model.\r\n\r\nIn effect we're raising the question again we thought about in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/557#issuecomment-2166938611. I do see value in letting the polling behavior (long or short) be decided amongst implementations, but it does seem to come at the cost of spec complexity.",
              "createdAt": "2024-07-03T22:48:24Z",
              "updatedAt": "2024-07-03T22:49:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AmwFS",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T23:19:09Z",
          "updatedAt": "2024-07-03T23:19:09Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "I think we should be careful not to conflate the discussion of:\r\n\r\n* Sync vs async aggregation job handling (and whether to support both of these, or only one), first opened in #557 and continued in this PR.\r\n* Frequent-polling vs long-polling, first discussed in this PR.\r\n\r\nFWIW/IMO, I am concerned about the need for an extra request per aggregation job implied by the async model, but not enough to accept the complexity of specifying & implementing both sync & async behaviors -- I think we should specify only one of sync & async. I am still thinking through whether it is wise to allow both frequent-polling & long-polling behaviors.",
              "createdAt": "2024-07-03T23:19:09Z",
              "updatedAt": "2024-07-03T23:19:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AmyuS",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T23:37:19Z",
          "updatedAt": "2024-07-03T23:37:19Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "I think the PUT should always return 202. I like @suman-ganta's suggestion for the GET. I understand it to be:\r\n\r\n> If the response is ready, then return 200. Otherwise, if the response is not ready, the Helper MAY return 202 immediately, or it MAY wait until the response is ready and respond with 200.",
              "createdAt": "2024-07-03T23:37:19Z",
              "updatedAt": "2024-07-03T23:37:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AmzeB",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-03T23:41:32Z",
          "updatedAt": "2024-07-03T23:41:32Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "A textual change may make this clearer.",
              "createdAt": "2024-07-03T23:41:32Z",
              "updatedAt": "2024-07-03T23:41:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ap-Gt",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "mendess",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-04T09:38:48Z",
          "updatedAt": "2024-07-04T09:38:48Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "You're right, this can be implemented with a hash. My follow up question is, what happens if the leader does repeat the request and keeps the same body? What should the helper do then? If this is specified I'm missing where",
              "createdAt": "2024-07-04T09:38:48Z",
              "updatedAt": "2024-07-04T09:38:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6AzhAV",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-05T16:12:01Z",
          "updatedAt": "2024-07-05T16:12:01Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "I'm not seeing concrete mention of that scenario either. We've been treating repeated requests as idempotent, i.e. we return 201 Created.\r\n\r\nWe probably need language in this paragraph along the lines of \"If the `AggregationJobInitReq` is identical to the original request, the Leader MAY respond with 201 Created\". Thoughts from others?",
              "createdAt": "2024-07-05T16:12:01Z",
              "updatedAt": "2024-07-05T16:12:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Azkr0",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-05T16:25:36Z",
          "updatedAt": "2024-07-05T16:25:36Z",
          "comments": [
            {
              "originalPosition": 80,
              "body": "> I think the PUT should always return 202.\r\n\r\nClarify? PUT should always return 201 Created?\r\n\r\n> I like @suman-ganta's suggestion for the GET.\r\n\r\nI'm fine with this change as well.\r\n\r\n> A textual change may make this clearer.\r\n\r\nAgreed, I think the behavior is permissible with the text as-is, but it's not obvious.",
              "createdAt": "2024-07-05T16:25:36Z",
              "updatedAt": "2024-07-05T16:25:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6A0ywu",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-05T23:33:13Z",
          "updatedAt": "2024-07-05T23:33:13Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "My view is that the discussion of PUT in [RFC 9110](https://www.rfc-editor.org/rfc/rfc9110#section-9.3.4) makes it clear that such request are idempotent, but there's no downside to making that explicit here.",
              "createdAt": "2024-07-05T23:33:13Z",
              "updatedAt": "2024-07-05T23:33:13Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BXbhb",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "We should add some discussion about asynchronocity to the overview at the top of {{aggregate-flow}}: Mention that aggregation jobs are handled asynchronicity, and why.",
          "createdAt": "2024-07-10T21:35:50Z",
          "updatedAt": "2024-07-10T21:41:55Z",
          "comments": [
            {
              "originalPosition": 81,
              "body": "@suman-ganta suggested to me offline that the Helper should be able to wait to respond to the GET until the response is ready. I don't this text disallows this behavior, but could we add a \"MAY\" to make it more explicit.\r\n\r\nAlso, perhaps we should recommend sending the Retry-After header more strongly.\r\n```suggestion\r\nIf the aggregation job is not finished yet, the Helper responds with HTTP\r\nstatus 202 Accepted. The Helper's response SHOULD include a Retry-After header\r\nfield to suggest a polling interval to the Leader.\r\n```",
              "createdAt": "2024-07-10T21:35:50Z",
              "updatedAt": "2024-07-10T21:41:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BXgx6",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-10T21:43:18Z",
          "updatedAt": "2024-07-10T21:45:41Z",
          "comments": [
            {
              "originalPosition": 124,
              "body": "Since you also deleted this above\r\n```suggestion\r\n```",
              "createdAt": "2024-07-10T21:43:18Z",
              "updatedAt": "2024-07-10T21:45:41Z"
            },
            {
              "originalPosition": 157,
              "body": "I agree it's an oversight. It should be MUST.",
              "createdAt": "2024-07-10T21:45:08Z",
              "updatedAt": "2024-07-10T21:45:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Beith",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-11T15:21:12Z",
          "updatedAt": "2024-07-11T15:21:12Z",
          "comments": [
            {
              "originalPosition": 87,
              "body": "@inahga let's mention this scenario explicitly.",
              "createdAt": "2024-07-11T15:21:12Z",
              "updatedAt": "2024-07-11T15:21:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BgfFC",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-11T18:52:44Z",
          "updatedAt": "2024-07-11T21:42:23Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "> It would be better to provide ability to query for completed jobs\r\n\r\nWould it add too much complexity to support both? i.e., the leader can either query for a specific job's response (using the current proposed structure) or for any complete job response. This gives future leader implementations flexibility on how they want to query for completed jobs.",
              "createdAt": "2024-07-11T18:52:44Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            },
            {
              "originalPosition": 60,
              "body": "```suggestion\r\nresponds with HTTP status 201 Created. The Helper is now ready to process the\r\naggregation job asynchronously from any further requests from the leader.\r\n\r\nTo process an aggregation job, the Helper computes an outbound prepare step\r\nfor each report share. This includes the following structures:\r\n```",
              "createdAt": "2024-07-11T18:56:45Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            },
            {
              "originalPosition": 145,
              "body": "Why do we enforce the same ordering between requests for a given job? This seems unnecessarily strict when the Helper can just match based on the report_id.",
              "createdAt": "2024-07-11T21:29:56Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            },
            {
              "originalPosition": 167,
              "body": "Same nit as above - this wording frames the preparation of the reports as asynchronous from each other, rather than in reality where the job requests are what as processed async",
              "createdAt": "2024-07-11T21:35:29Z",
              "updatedAt": "2024-07-11T21:43:56Z"
            },
            {
              "originalPosition": 158,
              "body": "```suggestion\r\nMAY attempt to recover by sending HTTP status 202 Accepted and discontinuing\r\n```",
              "createdAt": "2024-07-11T21:37:33Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            },
            {
              "originalPosition": 159,
              "body": "Can you clarify what you mean by discontinuing further work? Future continue steps should be allowed to continue processing asynchronously.",
              "createdAt": "2024-07-11T21:38:31Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            },
            {
              "originalPosition": 221,
              "body": "Can we make this plural to indicate that Helpers can store anywhere from the most recent step to the all the steps in a job's history?",
              "createdAt": "2024-07-11T21:40:12Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            },
            {
              "originalPosition": 236,
              "body": "```suggestion\r\nrequest `GET /tasks/{task_id}/collection_jobs/{collection-job-id}` to check on\r\n```",
              "createdAt": "2024-07-11T21:41:13Z",
              "updatedAt": "2024-07-11T21:42:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BiMcp",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-11T23:08:46Z",
          "updatedAt": "2024-07-11T23:24:51Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "I think one challenge for \"give me any complete job\" is deciding when to have the Helper _stop_ returning a given job to this request type.\r\n\r\nIf the Helper returns a given job only once, an inopportune network issue, Leader crash, or other transient issue could lead to the job being \"lost\". (specifically, the condition for a job being \"lost\" would be the Helper considering the request successful, but the Leader not processing the response successfully)\r\n\r\nIf the Helper hands out the job until e.g. the Leader sends the next `AggregationJobContinueReq`, a Leader which sends multiple requests for completed jobs concurrently could see the same response multiple times. (Also, if we have reached the last aggregation step, there is no next `AggregationJobContinueReq` to be sent.)",
              "createdAt": "2024-07-11T23:08:47Z",
              "updatedAt": "2024-07-11T23:24:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Bq1Oc",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-12T18:13:38Z",
          "updatedAt": "2024-07-12T18:13:38Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Since this is async interaction, leader can send two POST requests to this URI, one with valid and another with invalid body. When leader query for status with GET, it would be ambiguous to interpret the response. Two options to resolve this -\r\n* Maintain a requestID so a GET can be pointed to a specific requestID\r\n* Somehow early reject 2nd request instead of always returning 202",
              "createdAt": "2024-07-12T18:13:38Z",
              "updatedAt": "2024-07-12T18:14:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B1wlR",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T18:06:26Z",
          "updatedAt": "2024-07-15T18:06:26Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Invalid in what sense? If the message is malformed or something, then the Helper would abort the request. I think it's the Helper's job to ensure that one and only one request is accepted.",
              "createdAt": "2024-07-15T18:06:26Z",
              "updatedAt": "2024-07-15T18:06:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B1yAJ",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T18:09:53Z",
          "updatedAt": "2024-07-15T18:09:54Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "When two requests are accepted with 202 on the same URL, there is no way to know the response fetched later belongs to request1 or request2. Hope that clarifies.",
              "createdAt": "2024-07-15T18:09:54Z",
              "updatedAt": "2024-07-15T18:09:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B19wX",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T18:38:26Z",
          "updatedAt": "2024-07-15T18:38:26Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "When would this happen? Are you thinking of something like a race condition, where the Leader erroneously sends two POSTs at the same time for the same aggregation job?",
              "createdAt": "2024-07-15T18:38:26Z",
              "updatedAt": "2024-07-15T18:38:26Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2Jp6",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T19:07:03Z",
          "updatedAt": "2024-07-15T19:07:04Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "It could happen for various reasons -\r\n1. Leader did not receive response due to network error and retries.\r\n2. Leader incorrectly tracks the step count and sends continue request with invalid step count.\r\n3. Two leader instances sends the same request\r\n4. Other malicious reasons\r\n\r\nI think without request to response correlation, it adds stateful processing responsibility on leader and still does not give a way to track this correlation in a consistent way.",
              "createdAt": "2024-07-15T19:07:04Z",
              "updatedAt": "2024-07-15T19:07:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2bsv",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T19:54:41Z",
          "updatedAt": "2024-07-15T19:54:41Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Given that the relevant VDAF functions are deterministic, when both aggregators are honestly following the protocol, there is exactly one correct next request for a given aggregation job[1].\r\n\r\nTherefore, I suspect the semantics we want are that the _first_ request received for a given aggregation request determines the correct request; if the Leader sends multiple, non-identical requests for the same aggregation job, something has gone wrong with the Leader & failure is OK. That is, we want to detect repeated non-identical requests, but the proper way to respond to them is with failure rather than tracking both requests.\r\n\r\n[1] This is true semantically but may not be true up to encoding; though DAP messages are deterministically-encoded if the underlying VDAF messages are, and to my knowledge all current VDAFs use messages which are deterministically-encoded.",
              "createdAt": "2024-07-15T19:54:41Z",
              "updatedAt": "2024-07-15T19:54:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2fmg",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T20:04:48Z",
          "updatedAt": "2024-07-15T20:04:48Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "VDAF preparation is deterministic. Given the current _strict_ ordering of reports in the job, there should be exactly one valid next message.",
              "createdAt": "2024-07-15T20:04:48Z",
              "updatedAt": "2024-07-15T20:04:48Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2gFT",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T20:06:01Z",
          "updatedAt": "2024-07-15T20:06:01Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Well, one thing that can change is that the Helper's decryption key gets rotated out, causing the report to be rejected.",
              "createdAt": "2024-07-15T20:06:01Z",
              "updatedAt": "2024-07-15T20:06:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B3PmT",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T21:54:37Z",
          "updatedAt": "2024-07-15T21:54:37Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I guess I don't get what response Helper gives in these cases. Response to last request or response to first request?\r\n\r\nIf it is giving response to first request to all - then an incorrect submission (request2) gets a valid response.\r\nIf it is giving response to last request to all - then a valid submission receives error response without knowing why.\r\n\r\nBesides this, My understanding is, leader does not have to be good citizen and can learn things and hence Helper implements\r\n* Prevention of replay attacks\r\n* Prevention of same share belonging to several batches etc.\r\nIs that right understanding and still holds good here?",
              "createdAt": "2024-07-15T21:54:37Z",
              "updatedAt": "2024-07-15T21:54:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B3WSo",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T22:17:13Z",
          "updatedAt": "2024-07-15T22:17:13Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Yes, both the Leader and Helper are responsible for preventing replays, including across batches. This PR shouldn't change that. (If it does, we have a problem.)",
              "createdAt": "2024-07-15T22:17:13Z",
              "updatedAt": "2024-07-15T22:17:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B4NyX",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T01:11:42Z",
          "updatedAt": "2024-07-16T01:11:42Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Okay. I still need to understand this part -\r\n\r\n> \r\n> I guess I don't get what response Helper gives in these cases. Response to last request or response to first request?\r\n> \r\n> If it is giving response to first request to all - then an incorrect submission (request2) gets a valid response.\r\n> If it is giving response to last request to all - then a valid submission receives error response without knowing why.",
              "createdAt": "2024-07-16T01:11:42Z",
              "updatedAt": "2024-07-16T01:11:42Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B4Ubf",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T01:35:59Z",
          "updatedAt": "2024-07-16T01:35:59Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": ">     1. Leader did not receive response due to network error and retries.\r\n\r\nIf the body of each request is the same, then so should be the Helper's respond. Otherwise, the Leader is in error, and it's up to the Helper to decide which order they're processed in.\r\n\r\n>     2. Leader incorrectly tracks the step count and sends continue request with invalid step count.\r\n\r\nThe Leader is in error. The Helper will abort, but it [might be possible to retry](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.5.2.3).\r\n\r\n>     3. Two leader instances sends the same request\r\n\r\nSame as for 1 I think.\r\n\r\n>     4. Other malicious reasons\r\n\r\nEven if the Leader is malicious, then it's still up to the Helper to enforce invariants like anti-replay. However there are probably any number of ways the Leader can break correctness.\r\n",
              "createdAt": "2024-07-16T01:35:59Z",
              "updatedAt": "2024-07-16T01:35:59Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B4adz",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T02:02:16Z",
          "updatedAt": "2024-07-16T02:02:16Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "If two mutating requests results in two different responses, then it is not deterministic to let Helper decide what to do with them. I think it is deterministic if the helper respond with the unique URL to track the POST request status. I think this behavior should not depend on what Leader's behavior is.\r\n\r\nNote that, this would not be a problem if the response is synchronous since it comes with request-response correlation built-in.",
              "createdAt": "2024-07-16T02:02:16Z",
              "updatedAt": "2024-07-16T02:02:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B9_Ky",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T14:54:23Z",
          "updatedAt": "2024-07-16T14:54:24Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Let's maybe take a step back for a second: what property do you want DAP to have that this PR doesn't have?\r\n\r\nIt sounds like you want some way for the Leader to recover if it makes a mistake. Is this correct?\r\n\r\nIt sounds like you're suggesting the following mechanism: the Leader chooses a unique identifier for each request (probably at random?), which the Helper is supposed to echo back in its response. Is this what you're suggesting?\r\n\r\nIf so, what errors does this help the Leader recover from? What errors does it not help it recover from?",
              "createdAt": "2024-07-16T14:54:23Z",
              "updatedAt": "2024-07-16T14:54:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B_nJv",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T17:58:15Z",
          "updatedAt": "2024-07-16T17:58:15Z",
          "comments": [
            {
              "originalPosition": 145,
              "body": "I think the discussion in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/217 is relevant.",
              "createdAt": "2024-07-16T17:58:15Z",
              "updatedAt": "2024-07-16T17:58:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B_oVa",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T18:00:59Z",
          "updatedAt": "2024-07-16T18:00:59Z",
          "comments": [
            {
              "originalPosition": 159,
              "body": "I mean discontinuing work in the context of that single request, i.e. in the code it would be effectively a \"return\" statement. I will reword for clarity.",
              "createdAt": "2024-07-16T18:00:59Z",
              "updatedAt": "2024-07-16T18:01:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CJEqY",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T16:21:43Z",
          "updatedAt": "2024-07-17T16:21:43Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "> When two requests are accepted with 202 on the same URL, there is no way to know the response fetched later belongs to request1 or request2.\r\n\r\nConcretely, in Janus, each insertion of aggregation job is transactional since we use a relational database. The operations are serialized--a second request will reject with a database conflict error, since we use a uniqueness index on `(task, aggregation_job_id)`. That is, the only scenario where we would return 202 for a repeated request is if the second request is exactly identical to the first.\r\n\r\nHowever, I don't think we have any requirement for transactionality, either in this PR or generally in DAP. An aggregator that lacks transactionality _could_ accept multiple PUTs, and indeed it would be implementation-defined as to which request the subsequent GET request is responding to.\r\n\r\nSo I think @suman-ganta is correct. Perhaps we should add clarification that the helper should only accept one PUT request?",
              "createdAt": "2024-07-17T16:21:43Z",
              "updatedAt": "2024-07-17T16:58:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CJVX9",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T16:56:28Z",
          "updatedAt": "2024-07-17T16:56:28Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "Something to note is that async init/continue raises interesting possibilities as to how many reports are contained within an `AggregationJobInitReq`.\r\n\r\nRight now, the leader/helper have to limit the number of reports based on how long HTTP timeouts are configured helper-side.\r\n\r\nWith async, the leader can send arbitrarily large aggregation jobs without concerns about the HTTP gateway timeout. If taking advantage of that, there are fewer aggregation jobs that require bookkeeping and polling.",
              "createdAt": "2024-07-17T16:56:28Z",
              "updatedAt": "2024-07-17T16:56:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CKByb",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T18:08:18Z",
          "updatedAt": "2024-07-17T18:08:22Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "It might be useful to clarify that only one request should be accepted at each step, whether PUT for init and POST for continue.\r\n\r\nAlso @suman-ganta and @kristineguo point out offline that, if a POST comes in for step 2 while we're processing step 1, then we should abort the POST request.",
              "createdAt": "2024-07-17T18:08:18Z",
              "updatedAt": "2024-07-17T18:08:22Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CLyhX",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T21:37:31Z",
          "updatedAt": "2024-07-17T21:37:32Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Eh. Reading through it again as I try to write it:\r\n```\r\nChanging an aggregation job's parameters is illegal, so further requests to\r\n`PUT /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` for the same\r\n`aggregation-job-id` but with a different `AggregationJobInitReq` in the body\r\nMUST fail with an HTTP client error status code.\r\n```\r\nI feel we are adequately signaling that only one PUT request should be accepted. i.e. a helper that is accepting multiple different PUT requests is not following the spec. Similarly, I think the continue section with step recovery strongly implies the need for transactionality/consistency.\r\n\r\nSo I think my point earlier:\r\n> However, I don't think we have any requirement for transactionality, either in this PR or generally in DAP.\r\n\r\nis not valid. Sorry, I should have given a close read before commenting.\r\n\r\nSo @suman-ganta, I think the questions in https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1679566081 still apply.\r\n\r\n---\r\n\r\n> Also @suman-ganta and @kristineguo point out offline that, if a POST comes in for step 2 while we're processing step 1, then we should abort the POST request.\r\n\r\nI think stepMismatch covers this? If the helper receives a request for step 2 while step 1 is still in flight, the aggregation job will still be on step 1, thus being errored out per this paragraph.\r\n```\r\nNext, the Helper checks if the continuation step indicated by the request is\r\ncorrect. (For the first `AggregationJobContinueReq` the value should be `1`;\r\nfor the second the value should be `2`; and so on.) If the Leader is one step\r\nbehind (e.g., the Leader has resent the previous HTTP request), then the Helper\r\nMAY attempt to recover by sending HTTP 202 status Accepted and discontinuing\r\nany further work. In this case it SHOULD verify that the contents of the\r\n`AggregationJobContinueReq` are identical to the previous message (see\r\n{{aggregation-step-skew-recovery}}). Otherwise, if the step is incorrect, the\r\nHelper MUST abort with error `stepMismatch`.\r\n```",
              "createdAt": "2024-07-17T21:37:32Z",
              "updatedAt": "2024-07-17T21:37:32Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CLzIk",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T21:39:17Z",
          "updatedAt": "2024-07-17T21:39:17Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "Resolving this subthread: I feel strongly that per-job polling is sufficient, and that implementations can control the volume of GET polls by tweaking the aggregation job size and polling interval.\r\n\r\nFeel free to unresolve if there are dissents.",
              "createdAt": "2024-07-17T21:39:17Z",
              "updatedAt": "2024-07-17T21:39:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CL9v6",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:09:48Z",
          "updatedAt": "2024-07-17T22:09:49Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "Clarify? The Helper can store whatever it wants, but the minimum recommendation is to store whatever it needs to satisfy skew recovery, as far as the protocol is concerned.",
              "createdAt": "2024-07-17T22:09:49Z",
              "updatedAt": "2024-07-17T22:09:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CL-rz",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:13:09Z",
          "updatedAt": "2024-07-17T22:13:10Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "The language implies that only the most recent step's state SHOULD be stored. I think we can generalize this language to imply that _all_ steps' state SHOULD/MAY be stored to promote aggregation step skew recovery.",
              "createdAt": "2024-07-17T22:13:09Z",
              "updatedAt": "2024-07-17T22:13:29Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CL-uR",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:13:18Z",
          "updatedAt": "2024-07-17T22:13:19Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "Or in other words, the Helper could retain the entire step history and satisfy the SHOULD, but I'm not sure that possibility needs to be explicitly called out in protocol text.",
              "createdAt": "2024-07-17T22:13:19Z",
              "updatedAt": "2024-07-17T22:13:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMAfL",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:19:20Z",
          "updatedAt": "2024-07-17T22:19:20Z",
          "comments": [
            {
              "originalPosition": 81,
              "body": "For consistency, I've also called this out in collection as well.",
              "createdAt": "2024-07-17T22:19:20Z",
              "updatedAt": "2024-07-17T22:19:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMBK5",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:22:13Z",
          "updatedAt": "2024-07-17T22:22:13Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I don't know how often this scenario would actually come up in reality, but imagine a scenario where the Leader sends out two simultaneous requests:\r\n- `POST /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` for step 1 (already computed)\r\n  - Helper may return `202 Accepted` here if the request matches the previous message.\r\n- `POST /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}` for step 2 (new)\r\n\r\nDoes the Helper return `202 Accepted` for that second POST request? If the Helper does, what should the Helper return when the Leader does `GET /tasks/{task-id}/aggregation_jobs/{aggregation-job-id}`? The response for step 1 or 2?",
              "createdAt": "2024-07-17T22:22:13Z",
              "updatedAt": "2024-07-17T22:23:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMCBE",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:25:09Z",
          "updatedAt": "2024-07-17T22:25:09Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "To me, the confusion can arise because we're using the same request path for multiple different requests (one for each step). We could potentially alleviate this by making the request path more explicit by adding the step into the request path, so that the Helper always explicitly knows which step the Leader is requesting a response for.",
              "createdAt": "2024-07-17T22:25:09Z",
              "updatedAt": "2024-07-17T22:25:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMEPi",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:29:38Z",
          "updatedAt": "2024-07-17T22:29:38Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "In general, the aggregation step skew recovery section seems to focus on just recovering when there's a difference of 1 between the Helper and Leader. I think it would be beneficial to change the wording to imply that aggregation step skew recovery can allow the Leader to re-send any number of previously computed steps. Hence, specifying the Helper can store all previous steps' state follows naturally from this.",
              "createdAt": "2024-07-17T22:29:38Z",
              "updatedAt": "2024-07-17T22:29:38Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMF5l",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:36:01Z",
          "updatedAt": "2024-07-17T22:36:01Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "> The language implies that only the most recent step's state SHOULD be stored.\r\n\r\nI don't think I agree with that reading, but we could say \"Aggregator implementations SHOULD, at minimum, checkpoint...\".\r\n\r\n(n.b. this diff got roped into this PR because I noticed a typo in `AggregationJobResp`. We may consider taking this editorial into another PR, since I don't think it's strictly relevant to async jobs).",
              "createdAt": "2024-07-17T22:36:01Z",
              "updatedAt": "2024-07-17T22:36:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMHjQ",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T22:43:27Z",
          "updatedAt": "2024-07-17T22:43:27Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "> I think it would be beneficial to change the wording to imply that aggregation step skew recovery can allow the Leader to re-send any number of previously computed steps. Hence, specifying the Helper can store all previous steps' state follows naturally from this.\r\n\r\nAh, this is the part I was missing, thanks for clarifying. I do think we should rope this into another PR though, since I think there could be more wording changes necessary to make this clear. For one we only call out recovering from `n` to `n+1`.",
              "createdAt": "2024-07-17T22:43:27Z",
              "updatedAt": "2024-07-17T22:43:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMNtc",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T23:05:53Z",
          "updatedAt": "2024-07-17T23:05:54Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Good scenario, I think it's plausible. Perhaps not under normal operations, but it is a series of operations that I think would be accepted. Let me think about it out loud:\r\n\r\nI'm going to assume that aggregation jobs processing is perfectly transactional, and that these simultaneous requests don't leave the job in some indeterminate state. I believe these are valid assumptions, but stop me if they're not.\r\n\r\n---\r\nConsider the first scenario, where the order of requests is serialized to request 1 (i.e. your bullet point 1), then request 2.\r\n\r\nRequest 1: POST returns 202 Accepted, any subsequent GET requests _would_ return 200 with the proper body, since it was already computed.\r\nRequest 2: POST returns 202 Accepted, any subsequent GET requests would return 202 until the aggregation job is complete.\r\n\r\nI don't think anything is surprising here.\r\n\r\n---\r\nSecond scenario, the order of requests is flipped.\r\n\r\nRequest 2: POST returns 202 accepted, any subsequent GET requests would return 202 until the aggregation job is complete.\r\nRequest 1: Step recovery kicks in, because we're trying to POST for step 1. Uh oh. I think you're right that this situation is ambiguous. For step recovery to actually be possible, we'd need to abort continuation to step 2. Or the leader would need to wait for step 2 to be done, then try step recovery again.\r\n\r\n---\r\n\r\n> To me, the confusion can arise because we're using the same request path for multiple different requests (one for each step). We could potentially alleviate this by making the request path more explicit by adding the step into the request path, so that the Helper always explicitly knows which step the Leader is requesting a response for.\r\n\r\nI had originally considered having different paths for different steps, but couldn't find a scenario where this was strictly necessary, but I think you've identified one.\r\n\r\nLet me dwell on it for a bit, anyone else have thoughts as well?",
              "createdAt": "2024-07-17T23:05:54Z",
              "updatedAt": "2024-07-17T23:05:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMa-9",
          "commit": {
            "abbreviatedOid": "738ec18"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T23:48:36Z",
          "updatedAt": "2024-07-17T23:48:36Z",
          "comments": [
            {
              "originalPosition": 221,
              "body": "Opened a new PR on this edit: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/569\r\nWe can resolve this thread here \ud83d\ude01 ",
              "createdAt": "2024-07-17T23:48:36Z",
              "updatedAt": "2024-07-17T23:48:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CMdUq",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-17T23:54:55Z",
          "updatedAt": "2024-07-17T23:54:55Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "@inahga I don't think that we should be prescribing serializable transactions here - this is very limiting and operationally challenging for partitioned helpers.\r\n\r\nWith our helper, we've found that this aspect of the replay checks are the hardest part of the system to scale up - they eventually become a breaking point for the system unless you shard, and that has availability implications under draft 11 due to the synchronous nature of the protocol.\r\n\r\nIdeally we'd make things idempotent, or defer when transactional things have to occur until after jobs have been accepted for processing.",
              "createdAt": "2024-07-17T23:54:55Z",
              "updatedAt": "2024-07-17T23:54:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CUPMP",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T16:19:46Z",
          "updatedAt": "2024-07-18T16:19:46Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Fair points. It sounds like then simultaneous POSTs with different steps would be exceptionally painful for you, because it'll become indeterminate which request is being responded to for the subsequent GET. Does that sound right?",
              "createdAt": "2024-07-18T16:19:46Z",
              "updatedAt": "2024-07-18T16:19:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CUTQc",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T16:26:56Z",
          "updatedAt": "2024-07-18T16:27:03Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "Once a job is ready for collection, do _all_ polls have to return `200 OK` consistently?\r\n\r\nI ask because the usage of eventually consistent mechanisms could lead to polls even after one which returned 200 OK sometimes indicating that the job isn't ready.",
              "createdAt": "2024-07-18T16:26:56Z",
              "updatedAt": "2024-07-18T16:27:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CUayg",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T16:42:18Z",
          "updatedAt": "2024-07-18T16:42:18Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "An alternative to step specific path is to return 202 with a trackable URL. This URL contains unique ID in it generated by the helper that leader can GET. It responds with status of that specific submission.\r\n\r\nAnother simpler way is to synchronously check if there is a pending `continue` submission to be processed for the job, if so, abort synchronously with an error.",
              "createdAt": "2024-07-18T16:42:18Z",
              "updatedAt": "2024-07-18T16:42:18Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CUbBK",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T16:42:49Z",
          "updatedAt": "2024-07-18T16:42:49Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "I don't think a poll has to return 200 OK consistently for the protocol to work. Extra polls are harmless--the leader will simply try again. Or perhaps, whether it's harmless depends on the consistency guarantees between leader and helper, which I don't think DAP is prescriptive of right now.\r\n\r\nI also don't think that behavior runs awry of the text. Perhaps we could disambiguate by saying `the Helper SHOULD respond with...` but that's a bit awkward since it implies a compliant aggregator could never return 200 OK... (To be clear, I think the behavior you describe should be allowable).",
              "createdAt": "2024-07-18T16:42:49Z",
              "updatedAt": "2024-07-18T16:42:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CU9wO",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T18:02:40Z",
          "updatedAt": "2024-07-18T18:02:40Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Those resolutions all seem plausible to me.\r\n\r\nOK to summarize the thread direction: sharing the same URI for each step introduces ambiguity as to which request takes priority when requests for different steps come in simultaneously.\r\n\r\nThe solutions to this are:\r\n1. Per step request routing, e.g. `{task-id}/aggregation_jobs/{aggregation-job-id}/{step-number}`.\r\n1. Provide trackable URLs per PUT/POST, such that subsequent GETs can use it to get the proper response.\r\n1. Add prerequisite that aggregators MUST abort if another init/continue is still in progress.\r\n1. Do nothing--the scenario we painted above is admittedly somewhat pathological.\r\n\r\nI favor solution one, because I think it'll simplify step recovery substantially, because the leader can straightforwardly GET step it has lost track of.\r\n\r\nDoes anyone else who hasn't chimed in in a while have thoughts, @branlwyd @cjpatton?",
              "createdAt": "2024-07-18T18:02:40Z",
              "updatedAt": "2024-07-18T18:02:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CV8lG",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T20:23:30Z",
          "updatedAt": "2024-07-18T20:23:35Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "You've got it!",
              "createdAt": "2024-07-18T20:23:30Z",
              "updatedAt": "2024-07-18T20:23:35Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CV-ti",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "Noah-Kennedy",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T20:25:39Z",
          "updatedAt": "2024-07-18T20:25:39Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "I didn't think that behavior was proscribed by the text either, but wanted to verify.\r\n\r\nIt's probably worth calling this out as a possibility in the text as a heads up to leader node implementors, though I'm not sure why a leader would try and poll again after seeing a completion.",
              "createdAt": "2024-07-18T20:25:39Z",
              "updatedAt": "2024-07-18T20:25:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CW7EM",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T23:11:23Z",
          "updatedAt": "2024-07-18T23:30:46Z",
          "comments": [
            {
              "originalPosition": 76,
              "body": "```suggestion\r\nyet, the Helper MAY wait to respond until the job is complete. Once the aggregation job is\r\n```",
              "createdAt": "2024-07-18T23:11:23Z",
              "updatedAt": "2024-07-18T23:30:46Z"
            },
            {
              "originalPosition": 77,
              "body": "+1 to calling this out. No normative language required, just something like \"At least one GET will get the AggregationJobResp\" in return.",
              "createdAt": "2024-07-18T23:15:59Z",
              "updatedAt": "2024-07-18T23:30:46Z"
            },
            {
              "originalPosition": 114,
              "body": "Thanks @inahga for the super succinct summary!\r\n\r\nWith the major caveat that I've not implemented the Leader: I'm inclined to take 4. If I understand correctly, this case occurs because something has gone seriously wrong with the Leader. Generally speaking, [we have stricter operational requirements for the Leader than Helper](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-5.1.2-1). Thus I think we could reasonably \"give up\" and accept the data loss in this case.\r\n\r\nThat said, those that have implemented the Leader get a bigger vote share. I would nudge us towards 1 as it seems like the simplest way to get what we need. We might tweak it a bit to maintain the \"resource API\" semantics: `{task-id}/aggregation_jobs/{aggregation-job-id}?step={step-number}`",
              "createdAt": "2024-07-18T23:30:11Z",
              "updatedAt": "2024-07-18T23:30:46Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CXA7J",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T23:50:00Z",
          "updatedAt": "2024-07-18T23:50:00Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "It also seems like this will be helpful for https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/569.",
              "createdAt": "2024-07-18T23:50:00Z",
              "updatedAt": "2024-07-18T23:50:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CXemK",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-19T02:41:50Z",
          "updatedAt": "2024-07-19T02:41:50Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I like #3 or #1.\r\n\r\n#3 feels the cleanest since, in normal operation, only a single aggregation step will be computed at once; it's OK to fail if multiple steps are attempted at once. The step would be considered \"complete\" (allowing another step to be computed) once the Helper considers its response to be retrieved at least once.\r\n\r\nI'm not sure if #3 would step on the toes of some eventual-consistency scheme, however. If so, #1 seems the next cleanest.",
              "createdAt": "2024-07-19T02:41:50Z",
              "updatedAt": "2024-07-19T02:41:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CaqJf",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "mendess",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-19T11:46:39Z",
          "updatedAt": "2024-07-19T11:46:39Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I also agree with @cjpatton that for this scenario to happen the leader must be in some weird state, but I have also not implemented a leader. My thinking is that the leader would never even send a request for step 2 before having successfully polled the response for step 1 to completion, meaning the race condition could never occur. And as a helper, when I receive a request for step 2 after being done with step 1 I would just start aggregating step 2 and discard step 1 data. Meaning it would not be ambiguous for the helper which step the `GET` would be related to.\r\n\r\nNow, despite this confusion on my part, I would still vote for a solution which includes the step number in the request (number 1), since it probably makes debugging/observability easier because it makes the intent of the leader obvious",
              "createdAt": "2024-07-19T11:46:39Z",
              "updatedAt": "2024-07-19T11:46:39Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ccpvy",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-19T15:44:23Z",
          "updatedAt": "2024-07-19T15:44:23Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "One curiosity about solution 1 is that it makes the step number in the `AggregationJobContinueReq` somewhat redundant. We may be tempted to take a wire breaking change to prune that field, as I don't think we would want to keep vestigial fields around.\r\n\r\n---\r\n\r\n> I'm not sure if 3 would step on the toes of some eventual-consistency scheme, however.\r\n\r\nI had assumed so, hence my vote for 1. Each replica of an aggregator needs to have a perfectly consistent view of aggregation job status to be able to determine whether an aggregation job is in progress.\r\n\r\nIn some architectures, I believe this is really, really tricky. Naively I may implement async jobs as POST/PUTs pushing onto a message queue. Some dedicated message queueing software do not support introspecting the queue, e.g. [RabbitMQ](https://lists.rabbitmq.com/pipermail/rabbitmq-discuss/2011-August/014270.html). Some do, e.g. Sidekiq, but I believe that's because it's backed by a RDBMS.\r\n\r\nSo by imposing the requirement of 3, we have limited the implementation selection to software that can introspect the queue. Which, without having looked into it much, may only be well supported by relational datastores. _Even then_, depending on the sharding scheme being used, would a single request require introspecting the queues of all the shards?\r\n\r\n---\r\n\r\nI agree with all folks that the scenario only arises in the presence of a badly malfunctioning leader. But I think the fact that both requests can return 2xx responses as written in the spec is problematic, since only one request is obviously correct (i.e. the move to step 2), so I'm not favoring Do Nothing atm.",
              "createdAt": "2024-07-19T15:44:23Z",
              "updatedAt": "2024-07-19T15:44:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Cevt_",
          "commit": {
            "abbreviatedOid": "f0b803c"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-19T19:54:49Z",
          "updatedAt": "2024-07-19T19:54:49Z",
          "comments": [
            {
              "originalPosition": 77,
              "body": "I've been staring at this for a while and am having difficulty succinctly writing it out, without it sounding confusing.\r\n\r\nThe closest I've gotten so far is to add a sentence `For each aggregation job init/continue, the Helper will respond at least once with HTTP status 200 OK.`, but I'm not sure that actually covers the scenario we've painted.\r\n\r\nAlso the more I stare at it the less I'm onboard with taking it. I don't think a Leader will needs to take any different action or encode any different logic to cover this case. The only situation I can think of where this case comes up is where the 200 response is lost in transit, the leader queries again and receives 202. In which case, the solution is to proceed normally until 200 is actually received.",
              "createdAt": "2024-07-19T19:54:49Z",
              "updatedAt": "2024-07-19T19:54:49Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ce9Yq",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-19T20:32:22Z",
          "updatedAt": "2024-07-19T20:32:22Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "If you don't want to make a wire change, we could do:\r\n`PUT {task-id}/aggregation_jobs/{aggregation-job-id}`\r\n`GET {task-id}/aggregation_jobs/{aggregation-job-id}?step={step-number}`\r\n\r\nThe biggest issue is the `GET` has no way to explicitly identify the response it wants to retrieve. Leaving it up to the Helper to implicitly assume which step the Leader wants a response for leaves opportunity for (admittedly contrived) scenarios such as the one we've been discussing. By simply adding an explicit resource identifier of some sort, we avoid any potential for this confusion.\r\n\r\nHence why I'm partial to no.1, with step either as a query parameter or path component.",
              "createdAt": "2024-07-19T20:32:22Z",
              "updatedAt": "2024-07-19T20:32:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Cjqj_",
          "commit": {
            "abbreviatedOid": "6aa0869"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-21T19:27:20Z",
          "updatedAt": "2024-07-21T19:27:20Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "@inahga it sounds like 1 is the option with the most consensus. Let's take it for this PR. I'd be cool with removing the `step` field from the request body if you'd like to.",
              "createdAt": "2024-07-21T19:27:20Z",
              "updatedAt": "2024-07-21T19:28:01Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CpZPE",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-22T15:24:27Z",
          "updatedAt": "2024-07-22T15:24:28Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "> If you don't want to make a wire change, we could do:\r\n\r\nI meant the definition of `AggregationJobContinueReq`. It's defined as.\r\n```\r\nstruct {\r\n  uint16 step;\r\n  PrepareContinue prepare_continues<1..2^32-1>;\r\n} AggregationJobContinueReq;\r\n```\r\n\r\nWe may be tempted to remove the `step` field, since if `step` is a field of the URI, it's no longer necessary. That would be a simplification, but a wire breaking change.",
              "createdAt": "2024-07-22T15:24:28Z",
              "updatedAt": "2024-07-22T17:06:27Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CqPD2",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-22T17:08:23Z",
          "updatedAt": "2024-07-22T17:08:24Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "> If you don't want to make a wire change, we could do:\r\n\r\nSorry, I have Monday brain, I see what you're getting at now. We could let `AggregationJobContinueReq` retain the `step` field, retain calling `POST` as-is, but only use the `step` query parameter in the GET request. That would not be a wire breaking change.",
              "createdAt": "2024-07-22T17:08:23Z",
              "updatedAt": "2024-07-22T17:08:24Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CrAiO",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-22T18:43:01Z",
          "updatedAt": "2024-07-22T18:43:02Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I've sketched out how this looks in the latest commit, editorial changes still probably required. I went with striking the `step` field, mostly for consistency (don't need the same data in two different locations), and debug-ability (easier to see what the leader is thinking about through HTTP status logs).\r\n\r\nI've also moved from POST to PUT for continuation, since we use separate URIs for each step. This lets us use PUT semantics, i.e. idempotency.\r\n\r\nThat idempotency leads me to strike the entire job recovery section. Now I believe recovery is implicitly possible. If the Leader loses the acknowledgement of the PUT request, it can simply retransmit. Likewise with losing the response to the GET requests. Hopefully that's not overeager, as that seems like a pretty big win for simplicity of the text.\r\n\r\nI've replaced it with a small paragraph that allows the helper to discard information about old steps, so that it doesn't have to retain the entire step history of a (theoretical) VDAF with many steps, but allows for the flexibility of recovering the whole step history (a-la https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/569). I anticipate implementing this as `if step < job.step - 1 { return Err }`.",
              "createdAt": "2024-07-22T18:43:01Z",
              "updatedAt": "2024-07-22T18:43:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CsQSE",
          "commit": {
            "abbreviatedOid": "c0736b6"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "CHANGES_REQUESTED",
          "body": "",
          "createdAt": "2024-07-22T22:19:22Z",
          "updatedAt": "2024-07-22T22:23:12Z",
          "comments": [
            {
              "originalPosition": 235,
              "body": "```suggestion\r\nIf the response is ready, the Helper constructs an `AggregationJobResp`\r\n```",
              "createdAt": "2024-07-22T22:19:22Z",
              "updatedAt": "2024-07-22T22:23:12Z"
            },
            {
              "originalPosition": 114,
              "body": "@inahga this is a big and somewhat contentious change, and for that reason, we need to make sure to implement the smallest change necessary. Please revert all changes except for adding the step parameter to the URL.\r\n\r\nYou are probably right about the simplifications we can make, but it will help everyone if we handle this in separate PRs. Feel free to just stick those in a draft PR for now.",
              "createdAt": "2024-07-22T22:23:01Z",
              "updatedAt": "2024-07-22T22:23:12Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C1G9n",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-23T20:01:07Z",
          "updatedAt": "2024-07-23T20:01:07Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "Makes sense, I have done so.",
              "createdAt": "2024-07-23T20:01:07Z",
              "updatedAt": "2024-07-23T20:01:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C1WYd",
          "commit": {
            "abbreviatedOid": "41c40a3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-23T20:32:42Z",
          "updatedAt": "2024-07-23T20:32:42Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs6C1lmh",
          "commit": {
            "abbreviatedOid": "41c40a3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-23T21:02:09Z",
          "updatedAt": "2024-07-23T21:02:10Z",
          "comments": [
            {
              "originalPosition": 39,
              "body": "Let's add the step parameter here as well ?\r\n```suggestion\r\n`GET {helper}/tasks/{task-id}/aggregation_jobs/{aggregation-job-id}?step=0` until\r\n```",
              "createdAt": "2024-07-23T21:02:10Z",
              "updatedAt": "2024-07-23T21:02:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C9l-U",
          "commit": {
            "abbreviatedOid": "353b3e3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I haven't caught up on all the debates in this PR, but am comfortable deferring to the consensus of the group that has been designing this. The change accomplishes what it sets out to do.",
          "createdAt": "2024-07-24T16:05:28Z",
          "updatedAt": "2024-07-24T16:13:44Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "nit: I don't love this phrasing. `PUT` is the verb in the request sent to the path, not the recipient of the request.",
              "createdAt": "2024-07-24T16:05:28Z",
              "updatedAt": "2024-07-24T16:13:44Z"
            },
            {
              "originalPosition": 42,
              "body": "This last sentence suggests that the eventual 200 OK `AggregationJobResp` response should come with a `Retry-After` header. I think we should reorder the sentences.",
              "createdAt": "2024-07-24T16:07:02Z",
              "updatedAt": "2024-07-24T16:13:44Z"
            },
            {
              "originalPosition": 140,
              "body": "As above, this suggests the 200 OK should contain `Retry-After`. This sentence should be attached to the text describing the 202 Accepted response.",
              "createdAt": "2024-07-24T16:11:04Z",
              "updatedAt": "2024-07-24T16:13:44Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C98NK",
          "commit": {
            "abbreviatedOid": "353b3e3"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-24T16:45:34Z",
          "updatedAt": "2024-07-24T16:45:34Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "I mostly took this to be consistent with the existing phrasing of https://www.ietf.org/archive/id/draft-ietf-ppm-dap-09.html#section-4.6.1-14, but now perusing the draft it looks like that was in fact an outlier usage of `PUT /blah`. I will rephrase as `sends a PUT request to \"/blah\"`.",
              "createdAt": "2024-07-24T16:45:34Z",
              "updatedAt": "2024-07-24T16:45:34Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C-dFA",
          "commit": {
            "abbreviatedOid": "3c9752d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-24T17:54:01Z",
          "updatedAt": "2024-07-24T17:54:01Z",
          "comments": [
            {
              "originalPosition": 114,
              "body": "I'm closing this thread! @kristineguo or @suman-ganta please chime in if there's more to address.",
              "createdAt": "2024-07-24T17:54:01Z",
              "updatedAt": "2024-07-24T17:54:02Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C-dk3",
          "commit": {
            "abbreviatedOid": "39cfd4e"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-24T17:55:13Z",
          "updatedAt": "2024-07-24T18:00:23Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "\"the number of HTTP requests\"",
              "createdAt": "2024-07-24T17:55:13Z",
              "updatedAt": "2024-07-24T18:00:23Z"
            },
            {
              "originalPosition": 32,
              "body": "Instead of using empy/non-empty to distinguish the response, the Leader should use the media type. That is, if the response contains the media type header for AggregationJobResp (I don't remember what this is), then interpret the body of the response as an AggregationJobResp.",
              "createdAt": "2024-07-24T17:58:26Z",
              "updatedAt": "2024-07-24T18:00:23Z"
            },
            {
              "originalPosition": 55,
              "body": "Mention media type here as well.",
              "createdAt": "2024-07-24T17:58:57Z",
              "updatedAt": "2024-07-24T18:00:23Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C-2Tw",
          "commit": {
            "abbreviatedOid": "353b3e3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-24T18:44:42Z",
          "updatedAt": "2024-07-24T18:44:43Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "IMO consistency is king here, but also it's not worth sweating this too hard, because I suspect that things like request paths might get revisited as DAP makes its way to RFC (specifically I suspect we'll get forced to do an ACME style directory).",
              "createdAt": "2024-07-24T18:44:42Z",
              "updatedAt": "2024-07-24T18:44:43Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C_UwJ",
          "commit": {
            "abbreviatedOid": "39cfd4e"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-24T19:49:19Z",
          "updatedAt": "2024-07-24T19:49:19Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "I think it may be better if I strike `with no body`, as elsewhere in collection it's inferred that if we only say \"responds with STATUS\" there is no media type and body, as compared to explicitly saying \"responds with STATUS with a body of SomeResp and media type foo/bar\"",
              "createdAt": "2024-07-24T19:49:19Z",
              "updatedAt": "2024-07-24T19:49:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6C_cQl",
          "commit": {
            "abbreviatedOid": "bdbaeae"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-24T20:05:46Z",
          "updatedAt": "2024-07-24T20:05:46Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs6DAT1t",
          "commit": {
            "abbreviatedOid": "bdbaeae"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "I'm OK with the intent, a few nitpicky comments.",
          "createdAt": "2024-07-24T22:32:21Z",
          "updatedAt": "2024-07-24T23:18:45Z",
          "comments": [
            {
              "originalPosition": 105,
              "body": "I think that we should specify that the body is empty in this case, and perhaps that the content-type is unset. This would apply to everywhere we generate a response indicating \"not ready yet, poll for completion.\"",
              "createdAt": "2024-07-24T22:32:21Z",
              "updatedAt": "2024-07-24T23:18:45Z"
            },
            {
              "originalPosition": 122,
              "body": "A couple of points (the latter may be moot depending on how the former is resolved):\r\n1. Skew-recovery/idempotency is optional[1]; I'm not sure it's appropriate to say that the Helper MUST be able to respond with 201 Created. And if so, we should specify what it responds with, i.e. the same response as the original request.\r\n2. This effectively requires \"synchronous\" behavior on retries. Even though the Helper will already know the right answer (assuming it implements skew-recovery/idempotency), I think we should allow for implementations which always want to use the async approach, even when the result is already-computed.\r\n\r\n[1] I think aggregation initialization doesn't talk about idempotency or retries at all; the closest explicit text is in the [Recovering from Aggregation Step Skew](https://www.ietf.org/archive/id/draft-ietf-ppm-dap-11.html#name-recovering-from-aggregation) section, and that is both optional and written such that it only applies to aggregation continuation. Not for this PR, but perhaps that section should be generalized, since the initialization request is effectively \"step 0\".",
              "createdAt": "2024-07-24T22:45:30Z",
              "updatedAt": "2024-07-24T23:18:45Z"
            },
            {
              "originalPosition": 54,
              "body": "nitpicky: can we suggest that the original request can also contain a `Retry-After` header? obviously this is optional, but as-written we suggest the use of `Retry-After` when the Leader polls, but not a way for the Helper to hint at the initial delay before polling to the Leader.",
              "createdAt": "2024-07-24T22:56:14Z",
              "updatedAt": "2024-07-24T23:18:45Z"
            },
            {
              "originalPosition": 202,
              "body": "Similar to the other comment on skew-recovery/idempotency, this wording makes it sound like skew-recovery for continuation should always be done asynchronously (since synchronous recovery would respond with HTTP 200 OK). I don't think we should restrict the Helper in this way.",
              "createdAt": "2024-07-24T23:18:26Z",
              "updatedAt": "2024-07-24T23:18:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DAmer",
          "commit": {
            "abbreviatedOid": "bdbaeae"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-24T23:42:29Z",
          "updatedAt": "2024-07-24T23:42:30Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "I think idempotency is a required property of a [PUT request](https://www.rfc-editor.org/rfc/rfc9110#section-9.3.4-2), rather than related to skew recovery.\r\n\r\nWe could make it a SHOULD. The helper's behavior then should respond with an error code though, I don't think it's valid for the helper to accept more than one valid `AggregationJobInitReq`.\r\n\r\nYou're correct that we need to rewrite this paragraph to accommodate for both sync/async--I overlooked it.",
              "createdAt": "2024-07-24T23:42:29Z",
              "updatedAt": "2024-07-24T23:43:19Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DIY0P",
          "commit": {
            "abbreviatedOid": "bdbaeae"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-25T17:57:40Z",
          "updatedAt": "2024-07-25T17:57:40Z",
          "comments": [
            {
              "originalPosition": 105,
              "body": "I'm cool with it. I will save changing collection's wording for another PR to keep this one from continuing to sprawl. (In fact, I should probably back out all changes to collection and save them for a later editorial PR).",
              "createdAt": "2024-07-25T17:57:40Z",
              "updatedAt": "2024-07-25T17:57:41Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DQori",
          "commit": {
            "abbreviatedOid": "bdbaeae"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-26T15:45:31Z",
          "updatedAt": "2024-07-26T15:45:31Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "I've turned it into a SHOULD. I suppose for many-step VDAFs, the helper could want to discard its response to `AggregationJobInitReq` as later steps take place, i.e. `n-2, n-1, n`, `n-2` may be zero and can be discarded/overwritten while still supporting skew recovery. In which case the helper would respond with something like 410 Gone.",
              "createdAt": "2024-07-26T15:45:31Z",
              "updatedAt": "2024-07-26T15:45:31Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DQpyw",
          "commit": {
            "abbreviatedOid": "bdbaeae"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-26T15:47:16Z",
          "updatedAt": "2024-07-26T15:47:16Z",
          "comments": [
            {
              "originalPosition": 122,
              "body": "> Not for this PR, but perhaps that section should be generalized, since the initialization request is effectively \"step 0\".\r\n\r\nI'm hoping in the next PR that section can be stricken entirely, now that we have step encoded in the URI. Although I haven't thought about it in the face of supporting both sync/async.",
              "createdAt": "2024-07-26T15:47:16Z",
              "updatedAt": "2024-07-26T15:47:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6D_NXK",
          "commit": {
            "abbreviatedOid": "5ad2c4d"
          },
          "author": "martinthomson",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-08-02T02:15:23Z",
          "updatedAt": "2024-08-02T02:22:57Z",
          "comments": [
            {
              "originalPosition": 55,
              "body": "Where did `?step=0` come from?",
              "createdAt": "2024-08-02T02:15:23Z",
              "updatedAt": "2024-08-02T02:22:57Z"
            },
            {
              "originalPosition": 50,
              "body": "Shouldn't this response include a `Location` field indicating what was created?",
              "createdAt": "2024-08-02T02:15:42Z",
              "updatedAt": "2024-08-02T02:22:57Z"
            },
            {
              "originalPosition": 57,
              "body": "202 is not the right status code to return from polling.  If the goal is to have a resource that indicates the status of the job, it should always return 200 with a body that indicates what the status is.",
              "createdAt": "2024-08-02T02:16:40Z",
              "updatedAt": "2024-08-02T02:22:57Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6EFkr9",
          "commit": {
            "abbreviatedOid": "5ad2c4d"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-08-02T17:12:19Z",
          "updatedAt": "2024-08-02T17:22:36Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "I assume you mean https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Location?\r\n\r\nGreat question. We actually specify the path for the subsequent GET in this PR, so it might not be strictly necessary?\r\n\r\nOne potential oddness here: If the helper responds synchronously, i.e., the body of the response contains the `AggregationJobResp`, so there would be no subsequent GET, and therefore no Location to return. Perhaps the leader should respond with 200 OK in this case?",
              "createdAt": "2024-08-02T17:12:19Z",
              "updatedAt": "2024-08-02T17:22:36Z"
            },
            {
              "originalPosition": 55,
              "body": "\"step\" refers to the step of the aggregation job:\r\n\r\n* step 0 is the initial `AggregationJobInitReq`\r\n* step 1 is the first `AggregationJobContinueReq`\r\n* step 2 is the second `AggregationJobContinueReq`\r\n* and so on.\r\n\r\nNote that, for Prio3, aggregation is complete after step 0.\r\n\r\nWe decided to add the `step` parameter to the GET request so that the helper knows how to respond if the leader somehow ends up in a weird state.",
              "createdAt": "2024-08-02T17:15:26Z",
              "updatedAt": "2024-08-02T17:22:36Z"
            },
            {
              "originalPosition": 57,
              "body": "Got it, thanks for catching this.\r\n\r\nWhen the step is ready, the Helper returns 200 OK with an `AggregationJobResp`, so we would need to disambiguate this \"ready\" response from a \"not ready\" response. Could we use the content types for this? Something like:\r\n\r\n* \"ready\": content = [application/dap-aggregation-job-resp](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-8.1.4); body = [`AggregationJobResp`](https://ietf-wg-ppm.github.io/draft-ietf-ppm-dap/draft-ietf-ppm-dap.html#section-4.7.1.2-19)\r\n* \"not ready\": content = application/dap-aggregation-job-pending (or something); body = empty\r\n\r\nOr perhaps it would be more correct to change `AggregationJobResp` to convey whether the step is ready?",
              "createdAt": "2024-08-02T17:22:28Z",
              "updatedAt": "2024-08-02T17:22:36Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6Ec1iK",
          "commit": {
            "abbreviatedOid": "5ad2c4d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-08-06T20:09:00Z",
          "updatedAt": "2024-08-06T20:09:00Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "I think no action is required here. Per https://www.rfc-editor.org/rfc/rfc9110#section-15.3.2-1\r\n\r\n> The primary resource created by the request is identified by either a [Location](https://www.rfc-editor.org/rfc/rfc9110#field.location) header field in the response or, if no [Location](https://www.rfc-editor.org/rfc/rfc9110#field.location) header field is received, by the target URI.\r\n\r\nsince the PUT and GET URIs are the same, we shouldn't need a `Location` header. (please correct me if I'm misreading)",
              "createdAt": "2024-08-06T20:09:00Z",
              "updatedAt": "2024-08-06T20:09:00Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6EdQb1",
          "commit": {
            "abbreviatedOid": "5ad2c4d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-08-06T21:19:33Z",
          "updatedAt": "2024-08-06T21:19:33Z",
          "comments": [
            {
              "originalPosition": 50,
              "body": "Ah, my mistake, the PUT/POST and GET URIs are not actually the same since we don't require the step field in the PUT/POST request--in which case yes we should have a Location header.",
              "createdAt": "2024-08-06T21:19:33Z",
              "updatedAt": "2024-08-06T21:19:33Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6EdXSk",
          "commit": {
            "abbreviatedOid": "5ad2c4d"
          },
          "author": "inahga",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-08-06T21:42:31Z",
          "updatedAt": "2024-08-06T21:42:31Z",
          "comments": [
            {
              "originalPosition": 57,
              "body": "> 202 is not the right status code to return from polling. If the goal is to have a resource that indicates the status of the job, it should always return 200 with a body that indicates what the status is.\r\n\r\nThat tracks with prior art https://www.rfc-editor.org/rfc/rfc8555#section-7.5.1. (How embarrassing! I should have consulted ACME far earlier :wink:).\r\n\r\nI have added a `status` field to the `AggregationJobResp`. I think this brings nice text simplifications, because we express the mutual exclusion between a processing job and the presence of prep_resps through the struct definition (i.e. no need to talk about media types).",
              "createdAt": "2024-08-06T21:42:31Z",
              "updatedAt": "2024-08-06T21:51:06Z"
            }
          ]
        }
      ]
    },
    {
      "number": 565,
      "id": "PR_kwDOFEJYQs50XPJI",
      "title": "Specify aggregation with batched preparation",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/565",
      "state": "CLOSED",
      "author": "cjpatton",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Closes #561.\r\n\r\nAdds an alternative aggregation flow for VDAFs that support batched preparation.\r\n\r\nIn this mode of operation, prep shares for many reports are aggregated into a short message called a \"tag\". The tags computed by the Leader and Helper are used to decide validity of the reports: if the tags are valid, then every report is deemed valid; otherwise, at least one report is known to be invalid.\r\n\r\nThe Helper handles invalidity by aborting the request. In this case the Leader is allowed to retry, either with a subset of the reports or with the existing, pep-report preparation code path.\r\n\r\nDRAFT STATUS: This PR is not quite feature-complete. In fact, before merging it, we may need to make some changes to the VDAF draft. It is intended primarily to drive conversation about whether there is appetite to support the feature.",
      "createdAt": "2024-07-03T19:30:43Z",
      "updatedAt": "2024-07-29T18:00:38Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "35f1274853c1c06a5132612b111debefe4fd17ac",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "cjpatton/561-pitch",
      "headRefOid": "e192b0be3c106313c7b98df8e8983e10715bf198",
      "closedAt": "2024-07-29T18:00:38Z",
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "The issue was closed, so closing this PR as well.",
          "createdAt": "2024-07-29T18:00:38Z",
          "updatedAt": "2024-07-29T18:00:38Z"
        }
      ],
      "reviews": []
    },
    {
      "number": 566,
      "id": "PR_kwDOFEJYQs50kpE0",
      "title": "Document deviations from RFC 8446 PL",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/566",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Explains in detail how the syntax from RFC 8446 section 3.7 is repurposed for concise descriptions of structure variants. Additionally, the section on message encoding is moved to \"Protocol Definition\" to make the document flow better.\r\n\r\nCloses #472",
      "createdAt": "2024-07-06T00:05:12Z",
      "updatedAt": "2024-07-27T17:08:52Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "83f1ea77fb9ff1f304779e775bb15efcf51a9269",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/tls-presentation-exceptions",
      "headRefOid": "2eb189affa0f7bdd6a1dde10ae68ac4d40aaf4ee",
      "closedAt": "2024-07-27T17:08:51Z",
      "mergedAt": "2024-07-27T17:08:51Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "93fca5745c5b27d5a57a3b1195429d7935569961"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Reviewer note: @suman-ganta[observed](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/issues/472#issuecomment-1781933269) that TLS-syntax does not explicitly allow `case` blocks with multiple fields. We currently don't have any cases of this.\r\n\r\n",
          "createdAt": "2024-07-08T20:45:35Z",
          "updatedAt": "2024-07-08T20:45:48Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "`struct variant` is kind of indicating it is deviation from `struct`, but most of the `struct` semantics apply. `variant` alone is not defined anywhere, so it would leave every one for wild interpretations.",
          "createdAt": "2024-07-26T22:28:31Z",
          "updatedAt": "2024-07-26T22:28:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@tgeoghegan please squash and merge at will!",
          "createdAt": "2024-07-26T23:15:27Z",
          "updatedAt": "2024-07-26T23:15:27Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs6BAcIc",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-08T20:46:13Z",
          "updatedAt": "2024-07-08T20:46:13Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs6BBDwO",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-08T22:24:31Z",
          "updatedAt": "2024-07-08T22:38:37Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "This semantic is defined in [variant](https://datatracker.ietf.org/doc/html/rfc8446#section-3.8) section, right? Are we making `select` readable here? If so, do we need to do this for every structure? Or there is a special case where illustrating the above way makes it clear?\r\n\r\nSeeing same type definition multiple times with different fields could confuse readers. Instead of extending the semantics of `struct`, should this type be called `const` or `type`?",
              "createdAt": "2024-07-08T22:24:31Z",
              "updatedAt": "2024-07-08T22:38:57Z"
            },
            {
              "originalPosition": 52,
              "body": "How do we express multiple selects and their associated qualified members in this notation? (Such case does not exist in DAP so far I think)\r\n\r\n",
              "createdAt": "2024-07-08T22:34:09Z",
              "updatedAt": "2024-07-08T22:38:37Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BJ0vg",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-09T16:19:30Z",
          "updatedAt": "2024-07-09T16:24:14Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "Since this doesn't occur in DAP, I don't think we need to define it. Also, I would push back pretty hard on any struct definition that contained multiple selects. That would almost certainly allow construction of messages that aren't semantically valid, which would force extra validation checks on implementations. I think we would always prefer a struct contain a single enumeration where all its variants are valid over multiple enumerated types.",
              "createdAt": "2024-07-09T16:19:30Z",
              "updatedAt": "2024-07-09T16:24:14Z"
            },
            {
              "originalPosition": 54,
              "body": "The pertinent examples are various cases of constructing `PrepareResp` as shown [here](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-11#section-4.5.1.2-9) and [here](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-11#section-4.5.1.2-15) or [here](https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-11#section-4.5.2.2-13).\r\n\r\nI like the idea of annotating such partial constants somehow to make it clear this special syntax is in use. `const` doesn't feel right because only part of the structure is constant. Maybe `struct specialized` to indicate we are describing a particular specialization of the structure:\r\n\r\n```\r\nstruct specialized {\r\n  ReportID report_id;\r\n  PrepareRespState prepare_resp_state = continue;\r\n  opaque payload<0..2^32-1> = outbound;\r\n} PrepareResp;\r\n```\r\n\r\nOr perhaps `struct variant` is better, since we are dealing with enums here, and \"specialized\" implies generics.",
              "createdAt": "2024-07-09T16:24:04Z",
              "updatedAt": "2024-07-09T16:24:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BKkMb",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-09T18:06:10Z",
          "updatedAt": "2024-07-09T18:06:10Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "`struct variant` sounds good to me.",
              "createdAt": "2024-07-09T18:06:10Z",
              "updatedAt": "2024-07-09T18:06:10Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BKo1D",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-09T18:15:16Z",
          "updatedAt": "2024-07-09T18:15:16Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "From, the semantics of variants, it is possible to have several selects in a structure. Since variants allow dynamic members of structure, it sounds like legitimate definition to me. Am I missing anything here? Since we are coming up with a notation that could probably be followed in future DAP specs too, I'm trying to make sure we define something that lasts.\r\n\r\nI'm fine if you would like to defer this to later updates.\r\n\r\nOverall, In this notation, I didn't see the distinction between representing `alwaysExsting` fields vs fields added as a result of a specific value of enum. Since the order of fields matter, and these dynamic fields can appear in the middle of two static fields, some visual cue (bold/italicized etc) could help to locate them.",
              "createdAt": "2024-07-09T18:15:16Z",
              "updatedAt": "2024-07-09T18:15:16Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2SaD",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T19:29:56Z",
          "updatedAt": "2024-07-15T19:29:56Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "I'm confused by the need for special syntax. What is a \"partial constant\"?",
              "createdAt": "2024-07-15T19:29:56Z",
              "updatedAt": "2024-07-15T19:29:56Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2SvF",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T19:30:51Z",
          "updatedAt": "2024-07-15T19:30:51Z",
          "comments": [
            {
              "originalPosition": 52,
              "body": "Can you give an example of when multiple `select`s in the same `struct` is ambiguous? ",
              "createdAt": "2024-07-15T19:30:51Z",
              "updatedAt": "2024-07-15T19:30:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2qB1",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T20:30:28Z",
          "updatedAt": "2024-07-15T20:30:28Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "[RFC 8446 3.7](https://datatracker.ietf.org/doc/html/rfc8446#section-3.7) defines constant fields of structs:\r\n\r\n```\r\n struct {\r\n          T1 f1 = 8;  /* T.f1 must always be 8 */\r\n          T2 f2;\r\n      } T;\r\n```\r\n\r\nThis notation collides with our usage, where `PrepareResp.prepare_resp_state` can have many values, but we want to describe the variant where `prepare_resp_state = continue`.",
              "createdAt": "2024-07-15T20:30:28Z",
              "updatedAt": "2024-07-15T20:30:28Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B2xrC",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T20:45:03Z",
          "updatedAt": "2024-07-15T20:45:03Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "In the example given here, can't `f1` have many values?",
              "createdAt": "2024-07-15T20:45:03Z",
              "updatedAt": "2024-07-15T20:45:04Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B3bHE",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T22:31:46Z",
          "updatedAt": "2024-07-15T22:31:47Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "No, that is the point of the RFC 8446 constant notation. The comment says \"T.f1 must always be 8\". If you click through to 8446 s3.7 it says 'Fields and variables may be assigned a fixed value using \"=\"'.",
              "createdAt": "2024-07-15T22:31:46Z",
              "updatedAt": "2024-07-15T22:31:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B3ezM",
          "commit": {
            "abbreviatedOid": "334e2a7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T22:40:15Z",
          "updatedAt": "2024-07-15T22:40:15Z",
          "comments": [
            {
              "originalPosition": 54,
              "body": "Got it, thanks for clarifying. I think this could be clearer in the PR.",
              "createdAt": "2024-07-15T22:40:15Z",
              "updatedAt": "2024-07-15T22:40:15Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B4ImS",
          "commit": {
            "abbreviatedOid": "28fac09"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T00:50:39Z",
          "updatedAt": "2024-07-16T00:50:40Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "@tgeoghegan does this mean the syntax from `{{Section 3.7 of !RFC8446}}` is forbidden in this doc? \r\nIf a structure has the following def:\r\n```\r\nstruct {\r\n   uint32 always_present;\r\n   ExampleEnum type;\r\n}\r\n```\r\n\r\nThen does the below have ambiguity?\r\n```\r\nstruct variant {\r\n   uint32 always_present = 0;\r\n   ExampleEnum type = number;\r\n}\r\n```\r\nDoes it mean `always_present` can only be a constant 0 or it just happens to be 0 in this variant?",
              "createdAt": "2024-07-16T00:50:40Z",
              "updatedAt": "2024-07-16T00:50:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B-7KV",
          "commit": {
            "abbreviatedOid": "28fac09"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T16:26:39Z",
          "updatedAt": "2024-07-16T16:26:40Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "> @tgeoghegan does this mean the syntax from `{{Section 3.7 of !RFC8446}}` is forbidden in this doc?\r\n\r\nI don't think so. By introducing `struct variant`, we avoid collision with the RFC 8446 s3.7 notation, meaning it can be used without further complications. However, I don't think DAP ever should: TLS 1.3 needs that notation because it wants some its messages to be backward compatible with TLS 1.2. We have no such problem.\r\n\r\n> If a structure has the following def:\r\n> \r\n> ```\r\n> struct {\r\n>    uint32 always_present;\r\n>    ExampleEnum type;\r\n> }\r\n> ```\r\n> \r\n> Then does the below have ambiguity?\r\n> \r\n> ```\r\n> struct variant {\r\n>    uint32 always_present = 0;\r\n>    ExampleEnum type = number;\r\n> }\r\n> ```\r\n> \r\n> Does it mean `always_present` can only be a constant 0 or it just happens to be 0 in this variant?\r\n\r\nI don't think there's ambiguity because using `struct variant` makes it clear we're not using the notation from RFC 8446 section 3.7.",
              "createdAt": "2024-07-16T16:26:39Z",
              "updatedAt": "2024-07-16T16:26:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CAWSq",
          "commit": {
            "abbreviatedOid": "28fac09"
          },
          "author": "wangshan",
          "authorAssociation": "CONTRIBUTOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-16T19:19:21Z",
          "updatedAt": "2024-07-16T19:19:21Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "> \r\n> I don't think there's ambiguity because using `struct variant` makes it clear we're not using the notation from RFC 8446 section 3.7.\r\n\r\nI see, so it essentially overrides RFC 8446 s3.7. Do you think it's worth calling out this in the example? @tgeoghegan ",
              "createdAt": "2024-07-16T19:19:21Z",
              "updatedAt": "2024-07-16T19:19:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CsTZG",
          "commit": {
            "abbreviatedOid": "28fac09"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-22T22:32:20Z",
          "updatedAt": "2024-07-22T22:32:20Z",
          "comments": [
            {
              "originalPosition": 22,
              "body": "It does not override that section of RFC 8446. The notation in 8446 section 3.7 is `struct { ... } whatever`. The notation we introduce here is `struct variant { ... } whatever` which does not conflict.",
              "createdAt": "2024-07-22T22:32:20Z",
              "updatedAt": "2024-07-22T22:32:20Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DACaq",
          "commit": {
            "abbreviatedOid": "28fac09"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-24T21:31:07Z",
          "updatedAt": "2024-07-24T21:31:07Z",
          "comments": []
        }
      ]
    },
    {
      "number": 567,
      "id": "PR_kwDOFEJYQs50kqOj",
      "title": "Remove per-task HPKE configurations",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/567",
      "state": "MERGED",
      "author": "tgeoghegan",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "wire breaking"
      ],
      "body": "As discussed in #505, putting the task ID into the HPKE configuration request leaks which tasks a client is participating in, which can otherwise be concealed by OHTTP. This commit removes the notion of per-task HPKE keys from DAP, requiring global keys.\r\n\r\nCloses #505",
      "createdAt": "2024-07-06T00:17:07Z",
      "updatedAt": "2024-07-26T22:21:00Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "35f1274853c1c06a5132612b111debefe4fd17ac",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "timg/global-hpke-only",
      "headRefOid": "db5eed72626186587190f98c06ca81a4286d947c",
      "closedAt": "2024-07-26T22:21:00Z",
      "mergedAt": "2024-07-26T22:20:59Z",
      "mergedBy": "tgeoghegan",
      "mergeCommit": {
        "oid": "7b5eeca998d7871089249a7efc8224d9a3651df0"
      },
      "comments": [
        {
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "body": "We discussed this at PPM @ IETF 120 and nobody spoke up in defense of per-task HPKE configurations, so let's merge this!",
          "createdAt": "2024-07-26T22:20:48Z",
          "updatedAt": "2024-07-26T22:20:48Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs6A01Az",
          "commit": {
            "abbreviatedOid": "db5eed7"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-06T00:18:08Z",
          "updatedAt": "2024-07-06T00:18:08Z",
          "comments": [
            {
              "originalPosition": 30,
              "body": "This TODO is moot:\r\n\r\n1. If we only have global HPKE keys, we don't leak which tasks exist\r\n2. This would at best be a security consideration; DAP does not need to give implementations permission to use well-established HTTP semantics.",
              "createdAt": "2024-07-06T00:18:08Z",
              "updatedAt": "2024-07-06T00:18:09Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BAH5N",
          "commit": {
            "abbreviatedOid": "db5eed7"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-08T20:07:59Z",
          "updatedAt": "2024-07-08T20:07:59Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs6BXlIf",
          "commit": {
            "abbreviatedOid": "db5eed7"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "No objection to merging, but since this is making DAP a little less general, we need to make the case (at IETF 120) for this resolution and against alternatives. An alternative resolution is to discuss the side-effect for OHTTP in security considerations.",
          "createdAt": "2024-07-10T21:52:49Z",
          "updatedAt": "2024-07-10T21:53:03Z",
          "comments": []
        }
      ]
    },
    {
      "number": 568,
      "id": "PR_kwDOFEJYQs50wQ9x",
      "title": "DAP Version compatibility",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/568",
      "state": "MERGED",
      "author": "suman-ganta",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Closes #541.\r\n\r\nMade versioning compatibilities explicit and an optional way to express draft versions.",
      "createdAt": "2024-07-08T23:17:45Z",
      "updatedAt": "2024-08-15T21:28:58Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "9f05d0d21b14eecd3c539d643569f1c30c0ff472",
      "headRepository": "suman-ganta/draft-ietf-ppm-dap",
      "headRefName": "dap-versioning",
      "headRefOid": "5567c2073c782877337ca52e8d6762cb5e556e25",
      "closedAt": "2024-08-15T21:28:58Z",
      "mergedAt": "2024-08-15T21:28:58Z",
      "mergedBy": "cjpatton",
      "mergeCommit": {
        "oid": "c2f4e851a5989f4772d0c9e7c6d147611af7fd7b"
      },
      "comments": [
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> I like the idea of doing this as an optional parameter in the existing `Content-Type`/`Accept` headers. However, for that reason, I'm not sure we need this spelled out in the DAP protocol. DAP is built on HTTP, and RFC 9110 already makes it clear that media types can include \"semicolon-delimited parameters in the form of name/value pairs\" ([Section 8.3.1](https://www.rfc-editor.org/rfc/rfc9110.html#section-8.3.1)). We don't need to give DAP implementations permission to use well-defined HTTP semantics, just the same as we don't need to give them permission to use HTTP 3xx redirects.\r\n> \r\n> Since we don't specify what should be in this `version` parameter, nor do we specify how implementations should evaluate it, I'm not sure this text adds value to this protocol.\r\n\r\nI think it's a good idea to at least mention this is a possibilty, rather than leaving it to implementers to dig this up themselves. Language like \"MAY add a version parameter as specified in {{Section whatever of !RFC9110}}\" would be helpful.",
          "createdAt": "2024-07-09T19:06:47Z",
          "updatedAt": "2024-07-09T19:06:47Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@suman-ganta CI is failing due to trailing whitespace:\r\n```\r\ndraft-ietf-ppm-dap.md contains trailing whitespace\r\n```",
          "createdAt": "2024-07-15T22:22:43Z",
          "updatedAt": "2024-07-15T22:22:43Z"
        },
        {
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "body": "Addressed trailing whitespace issue.",
          "createdAt": "2024-07-20T22:25:29Z",
          "updatedAt": "2024-07-20T22:25:29Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "@suman-ganta this is on hold while we get some feedback from HTTP experts. I'm on it!",
          "createdAt": "2024-07-26T23:44:14Z",
          "updatedAt": "2024-07-26T23:44:14Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "What's being versioned here - the format itself, or the protocol overall?\r\n\r\nRegardless, I wouldn't recommend using a media type parameter to version either. Parameters are lost by some types of software, and if there are breaking changes in the version, that can cause compatibility issues.\r\n\r\nBest practice is to use a new media type to indicate breaking changes. ",
          "createdAt": "2024-07-30T00:30:31Z",
          "updatedAt": "2024-07-30T00:30:31Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "> @mnot What's being versioned here - the format itself, or the protocol overall?\r\n>\r\n> Regardless, I wouldn't recommend using a media type parameter to version either. Parameters are lost by some types of software, and if there are breaking changes in the version, that can cause compatibility issues.\r\n>\r\n> Best practice is to use a new media type to indicate breaking changes.\r\n\r\nThe protocol specifies the format of each message, so both.\r\n\r\nThe PR intends only to use the parameter as a hint for debugging purposes, not as a means of indicating the version or draft. We have other ways of handling breaking changes across drafts.\r\n\r\nWe plan to update the media types for future versions of the specification (i.e., future RFCs).\r\n\r\n\r\n",
          "createdAt": "2024-07-30T01:47:37Z",
          "updatedAt": "2024-07-30T01:47:37Z"
        },
        {
          "author": "mnot",
          "authorAssociation": "NONE",
          "body": "If it's very clearly just a hint and doesn't change behaviour or trigger normative requirements, that's OK. From what I see, it's more typical to do this inside the format itself, although you may have Reasons\u2122\ufe0f.",
          "createdAt": "2024-07-30T02:01:13Z",
          "updatedAt": "2024-07-30T02:01:13Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs6BBRxs",
          "commit": {
            "abbreviatedOid": "62cdae3"
          },
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "N.B. the cryptographic domain separation string used for encryption of report & aggregate shares contains the current draft number, so implementations of different drafts of DAP will not interoperate. But there's no reason we can't include this as well, it would likely be much easier to debug a version-skew issue using this mechanism.",
          "createdAt": "2024-07-08T23:26:22Z",
          "updatedAt": "2024-07-08T23:32:08Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I think this paragraph would not fit into the eventual RFC for DAP, since there will be no draft version to refer to. IMO, add a TODO to remove this before RFC.",
              "createdAt": "2024-07-08T23:26:22Z",
              "updatedAt": "2024-07-08T23:32:08Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BBUBb",
          "commit": {
            "abbreviatedOid": "62cdae3"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-08T23:36:49Z",
          "updatedAt": "2024-07-08T23:42:40Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "```suggestion\r\n## Message versioning\r\n\r\n```",
              "createdAt": "2024-07-08T23:36:49Z",
              "updatedAt": "2024-07-08T23:42:40Z"
            },
            {
              "originalPosition": 5,
              "body": "Wrap paragraphs at 80 columns.",
              "createdAt": "2024-07-08T23:37:06Z",
              "updatedAt": "2024-07-08T23:42:40Z"
            },
            {
              "originalPosition": 7,
              "body": "Agreed. Concretely, add something like this: \r\n```suggestion\r\n(NOTE TO RFC EDITOR: Remove this paragraph.) HTTP requests with DAP media types MAY express an optional parameter 'version'. Value of this parameter indicates current draft version of the protocol the component is using. This MAY be used as a hint by the receiver of the request to do compatibility checks between client and server.\r\n```\r\n\r\n",
              "createdAt": "2024-07-08T23:42:25Z",
              "updatedAt": "2024-07-08T23:42:40Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BJ5fi",
          "commit": {
            "abbreviatedOid": "62cdae3"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "I like the idea of doing this as an optional parameter in the existing `Content-Type`/`Accept` headers. However, for that reason, I'm not sure we need this spelled out in the DAP protocol. DAP is built on HTTP, and RFC 9110 already makes it clear that media types can include \"semicolon-delimited parameters  in the form of name/value pairs\" ([Section 8.3.1](https://www.rfc-editor.org/rfc/rfc9110.html#section-8.3.1)). We don't need to give DAP implementations permission to use well-defined HTTP semantics, just the same as we don't need to give them permission to use HTTP 3xx redirects.\r\n\r\nSince we don't specify what should be in this `version` parameter, nor do we specify how implementations should evaluate it, I'm not sure this text adds value to this protocol.",
          "createdAt": "2024-07-09T16:29:24Z",
          "updatedAt": "2024-07-09T16:33:55Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "I'd like to see an example of this optional `version` parameter. Would this be something like `Media-Type: application/dap-aggregation-job-init-req;version=11`? A reference to [RFC 9110 section 8.3](https://www.rfc-editor.org/rfc/rfc9110.html#section-8.3.1) (which describes this kind of thing in media types) also would help.",
              "createdAt": "2024-07-09T16:29:24Z",
              "updatedAt": "2024-07-09T16:33:55Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BsJMk",
          "commit": {
            "abbreviatedOid": "62cdae3"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-12T22:29:50Z",
          "updatedAt": "2024-07-12T22:29:50Z",
          "comments": [
            {
              "originalPosition": 7,
              "body": "incorporated both suggestions.",
              "createdAt": "2024-07-12T22:29:50Z",
              "updatedAt": "2024-07-12T22:29:50Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6BsJO9",
          "commit": {
            "abbreviatedOid": "62cdae3"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-12T22:30:05Z",
          "updatedAt": "2024-07-12T22:30:05Z",
          "comments": [
            {
              "originalPosition": 5,
              "body": "done",
              "createdAt": "2024-07-12T22:30:05Z",
              "updatedAt": "2024-07-12T22:30:05Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B1o49",
          "commit": {
            "abbreviatedOid": "a1467cd"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T17:47:47Z",
          "updatedAt": "2024-07-15T17:47:47Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "```suggestion\r\nFor example, a report submission to leader from a client that supports\r\ndraft-ietf-ppm-dap-09 could have the header `Media-Type:\r\napplication/dap-report;version=09`.\r\n```\r\n\r\nPrefer notation `draft-ietf-ppm-dap-09` because that's a bit more formal in IETF parlance. Also use \"could\" instead of \"would\", because this text isn't normative: it's completely up to implementations what to put in the `version` field and how to interpret it.",
              "createdAt": "2024-07-15T17:47:47Z",
              "updatedAt": "2024-07-15T17:47:47Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B1uI8",
          "commit": {
            "abbreviatedOid": "a1467cd"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "One more editorial comment from me, then I'm good to go!",
          "createdAt": "2024-07-15T18:00:37Z",
          "updatedAt": "2024-07-15T18:00:51Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Let's move this to a subsection of `## Protocol Message Media Types` immediately below this.",
              "createdAt": "2024-07-15T18:00:37Z",
              "updatedAt": "2024-07-15T18:00:51Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B11x5",
          "commit": {
            "abbreviatedOid": "a1467cd"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T18:18:54Z",
          "updatedAt": "2024-07-15T18:18:54Z",
          "comments": [
            {
              "originalPosition": 4,
              "body": "Moved.",
              "createdAt": "2024-07-15T18:18:54Z",
              "updatedAt": "2024-07-15T18:18:54Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B116h",
          "commit": {
            "abbreviatedOid": "a1467cd"
          },
          "author": "suman-ganta",
          "authorAssociation": "NONE",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T18:19:14Z",
          "updatedAt": "2024-07-15T18:19:14Z",
          "comments": [
            {
              "originalPosition": 17,
              "body": "Sounds good. Done.",
              "createdAt": "2024-07-15T18:19:14Z",
              "updatedAt": "2024-07-15T18:19:14Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6B19Pt",
          "commit": {
            "abbreviatedOid": "0e9313c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-15T18:37:08Z",
          "updatedAt": "2024-07-15T18:37:08Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOFEJYQs6B2c2v",
          "commit": {
            "abbreviatedOid": "0e9313c"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-15T19:57:41Z",
          "updatedAt": "2024-07-15T19:57:45Z",
          "comments": [
            {
              "originalPosition": 13,
              "body": "```suggestion\r\nValue of this parameter indicate current draft version of the protocol the \r\n```",
              "createdAt": "2024-07-15T19:57:41Z",
              "updatedAt": "2024-07-15T19:57:45Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6CsS0H",
          "commit": {
            "abbreviatedOid": "808d873"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-22T22:29:44Z",
          "updatedAt": "2024-07-22T22:29:49Z",
          "comments": [
            {
              "originalPosition": 14,
              "body": "This is non-normative IMO\r\n```suggestion\r\ncomponent is using. This might be used as a hint by the receiver of the request\r\n```",
              "createdAt": "2024-07-22T22:29:44Z",
              "updatedAt": "2024-07-22T22:29:49Z"
            }
          ]
        }
      ]
    },
    {
      "number": 569,
      "id": "PR_kwDOFEJYQs51tJAT",
      "title": "Allow for aggregation step skew recovery of arbitrary number of steps",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/569",
      "state": "OPEN",
      "author": "kristineguo",
      "authorAssociation": "NONE",
      "assignees": [],
      "labels": [],
      "body": "Context: https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1674725591\r\n\r\nThe current draft focuses on only the recovery from `n` to `n+1`. We should generalize this section to allow aggregation step skew recovery of any arbitrary number of previous steps.",
      "createdAt": "2024-07-17T23:47:44Z",
      "updatedAt": "2024-07-29T16:25:59Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "35f1274853c1c06a5132612b111debefe4fd17ac",
      "headRepository": "kristineguo/draft-ietf-ppm-dap",
      "headRefName": "step-skew-recovery",
      "headRefOid": "5e1e842d03644b4d931104820fe85b1ccf65f4dd",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't object to generalizing the protocol in this way (as long as implementations are still free to only implement `n`/`n+1` skew recovery); but I believe the semantics were written this way originally because the only skew possible is `n`/`n+1` skew, that is, implementing single-step recovery is good enough for all scenarios.\r\n\r\nIs that incorrect? Is there a plausible way for the Leader/Helper to skew more than one step apart via transient network failures or other distributed-systems concerns?",
          "createdAt": "2024-07-18T16:56:15Z",
          "updatedAt": "2024-07-18T16:56:45Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "It depends on leader implementation. If the leader is checkpointing its state to durable storage after every aggregation step, then you're correct that it only ever has to implement `n`/`n+1` skew recovery.\r\n\r\nHowever, I don't think it's fair to assume that every leader implementation will be doing this. Some leader implementations may choose to omit this checkpointing (e.g., to reduce I/O or complexity) and perform a job's aggregation all in memory.\r\n\r\nSo if the job fails or if the leader goes down, it may have to retry the entire aggregation job from step 0 (init). The protocol should allow for this as long as the requests re-sent are identical, effectively allowing an aggregator to replay its state transitions to catch back up.",
          "createdAt": "2024-07-18T17:56:35Z",
          "updatedAt": "2024-07-18T17:56:35Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "That's fair. I think this leads to several different skew-recovery models, each with fairly different implementation requirements:\r\n\r\n* In the \"current\" model, where only `n`/`n+1` skew recovery is supported, the Helper needs to store the hash of the previous request, and the entirety of the previous response (or enough information to recover this response based on the previous request, i.e. the previous VDAF preparation states, I believe).\r\n* In the model where the Leader always restarts aggregation from the beginning, the Helper needs to store the hash the initial request, but does not need to store any responses (since they can be recomputed on retry).\r\n* In the model where the Leader restarts from anywhere in the aggregation process, the Helper needs to store the hash of every request, and every response (or enough information to recompute it based on the relevant request).\r\n\r\nThese differences might be an interoperability pain point, e.g. if the Leader expects to be able to restart aggregations from the beginning but the Helper implements only `n`/`n+1`-skew recovery.\r\n\r\nPerhaps we can pick just one of these skew-recovery methods? I would be happiest with either the `n`/`n+1`-skew recovery that is specified today, or a recovery method that always restarts from the beginning. However, my implementation experience is with 1-step VDAFs -- where these models are equivalent -- so I would appreciate the perspective of folks who have deployed multi-step VDAFs.",
          "createdAt": "2024-07-18T23:17:10Z",
          "updatedAt": "2024-07-18T23:18:23Z"
        },
        {
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "body": "Because we're not requiring the Helper to recover, I don't think we need to mandate any particular recovery strategy. This is an area where I think flexibility is good. One thing that would be useful is to know what step the Leader expects to recover. @inahga's [option 1](https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564#discussion_r1683276020) should make this easier.\r\n\r\nI'm in favor of taking this change.",
          "createdAt": "2024-07-18T23:54:04Z",
          "updatedAt": "2024-07-18T23:54:21Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Because we're not requiring the Helper to recover, I don't think we need to mandate any particular recovery strategy.\r\n\r\nOK, I disagree here -- the downside of a mismatch in the Leader/Helper's recovery strategies may mean that recovery is impossible, effectively leading to the reports in the \"bad\" aggregation job being lost. (This would be the case e.g. if the Helper implements `n`/`n+1` recovery, while the Leader expects to be able to restart, and the VDAF is >1 step.) I see this as a step backwards in interoperability.\r\n\r\nIf a single strategy isn't acceptable, we _could_ protocol-negotiate how recovery works somehow -- but that is likely too much complexity to add to the protocol, and would not remove the interoperability issue (though it would be apparent at time of task configuration, rather than at time of attempt to recover).\r\n\r\nOr perhaps we could support both \"restart from beginning\" and \"restart from previous step\", but not \"restart from arbitrary step\" -- the storage requirements for supporting \"restart from beginning\" are a single request hash, while the storage requirements for \"restart from arbitrary step\" are either the entire sequence of responses or the entire sequence of VDAF preparation states. (This could be an implementation detail, of course -- but if this is the way we go, I'd like to avoid normative language that suggests the Helper store every step.)",
          "createdAt": "2024-07-19T00:04:00Z",
          "updatedAt": "2024-07-19T00:09:40Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "> In the model where the Leader restarts from anywhere in the aggregation process, the Helper needs to store the hash of every request, and every response (or enough information to recompute it based on the relevant request).\r\n\r\nIs this not already the case? If a Helper supports idempotent init and cont request handling, it should store either state (to recompute the response) or the cached response itself so that it can re-transmit the previously computed AggregationJobResp at any point.\r\n\r\nOr are you implying that once we advance to say step `n+2`, any requests to step `n` should be aborted?\r\n\r\n> These differences might be an interoperability pain point\r\n\r\nI honestly think the exact recovery mechanism can be an implementation detail worked out between aggregators. There's no need to be partial to any one of these strategies in the spec when all have equally valid use cases.\r\n\r\nKeeping it purposefully vague (quoting Chris: `If the Leader re-sends an AggregationJobContinueReq from a previous step, the Helper SHOULD respond with the corresponding AggregationJobResp`) and using SHOULD language on how much previous state a Helper should persist should allow for variations.",
          "createdAt": "2024-07-19T00:05:10Z",
          "updatedAt": "2024-07-19T00:07:42Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> > In the model where the Leader restarts from anywhere in the aggregation process, the Helper needs to store the hash of every request, and every response (or enough information to recompute it based on the relevant request).\r\n> \r\n> Is this not already the case? If a Helper supports idempotent init and cont request handling, it should store either state (to recompute the response) or the cached response itself so that it can re-transmit the previously computed AggregationJobResp at any point.\r\n\r\nYes -- the existing specification language prior to this PR (and, FWIW, Janus' implementation) require the Helper to store state for only a single \"old\" step, specifically the one directly prior to the \"current\" step. This works if the Leader is expected to durably store the result of each step -- the Leader can only be one step \"behind\" due to transient failures, in this model.\r\n\r\n> Or are you implying that once we advance to say step `n+2`, any requests to step `n` should be aborted?\r\n\r\nYes, that is the current state of things (emphasis mine):\r\n\r\n> To make that kind of recovery possible, Aggregator implementations SHOULD checkpoint the **most recent step's** prep state and messages to durable storage such that the Leader can re-construct continuation requests and the Helper can re-construct continuation responses as needed.",
          "createdAt": "2024-07-19T00:33:55Z",
          "updatedAt": "2024-07-19T00:35:11Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "If we want to limit step skew recovery to just `n`/`n+1`, we should make it more explicit in this section that the Leader is expected to store its state at each step.\r\n\r\nBut it seems that the amount of stored state is overall the same between Aggregators - either we make the Leader store state of each step (currently implied model), or the Helper stores the state of each step (this PR's suggestion). \r\n\r\nI had always assumed the helper's APIs were idempotent - the same request could be repeated independently of other requests that have been sent and still receive the same response as it had. You seem to be disagreeing with this. If a Helper has moved on to step `n+2`, even if the same request is sent by the leader for step `n`, it will be aborted.",
          "createdAt": "2024-07-19T20:38:01Z",
          "updatedAt": "2024-07-19T20:38:35Z"
        },
        {
          "author": "kristineguo",
          "authorAssociation": "NONE",
          "body": "Discussion was taken offline but to summarize here for posterity's sake:\r\n\r\n**Conclusion:** **We will defer this issue.** The current status quo and what's proposed here are functionally the same for 1-round and 2-round VDAfs. Once more multi-round VDAFs are implemented and the async conversations (https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/564) are converged, we can revisit this issue.\r\n\r\n**Details:**\r\nThere are two options:\r\n1. Stick with leader being the one to store state for every step and only supporting `n` / `n+1` recovery.\r\n2. Move to the helper storing state (i.e., sufficient to reconstruct the same original response) and supporting arbitrary step recovery.\r\n\r\nThe current draft assumes that the leader takes the responsibility for tracking state and that the leader will only ever need to recover from the most recent step (i.e., a transient failure caused the most recent response to be dropped). If this is maintained, only option 1 (the status quo) is required.\r\n\r\nHowever, if we want to support aggregation job recovery from either the first step, we will need the helper to maintain state for every step. This is so that we can support idempotency: aggregators should ensure that each step that\u2019s retried is the same as before. So supporting this recovery strategy requires per-step state storage on the helper side. \r\n\r\n**Tangent: Meaning of \"Aggregated\" Reports**\r\nThis discussion on \"recovery from the beginning\" brought up a relevant tangent on what it means for a report to have been \"aggregated.\" @cjpatton mentioned that Daphne delays the anti-replay validation until right before the output shares are aggregated, at which point the job is \"committed\". Before this point, the reports could be sent in a brand new aggregation job if the leader wished to start over. This will be explored more in a separate issue by @branlwyd.\r\n\r\n**Personal Notes**\r\nI still personally prefer option 2, since it\u2019s ultimately the same amount of state stored in sum between the aggregators and allows for more flexibility in recovery. ",
          "createdAt": "2024-07-22T23:00:55Z",
          "updatedAt": "2024-07-22T23:01:54Z"
        },
        {
          "author": "simon-friedberger",
          "authorAssociation": "CONTRIBUTOR",
          "body": "@kristineguo Why does the leader have to store all steps anyway? If the leader receives a response for step x, then x can be discarded, right? Basically, we would need to store 3 steps. What the leader thinks the current step is. What the helper thinks the current step is and the previous step on the helper in case the leader hasn't received the response and retries.",
          "createdAt": "2024-07-29T06:48:02Z",
          "updatedAt": "2024-07-29T06:48:02Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "> @kristineguo Why does the leader have to store all steps anyway? If the leader receives a response for step x, then x can be discarded, right? Basically, we would need to store 3 steps. What the leader thinks the current step is. What the helper thinks the current step is and the previous step on the helper in case the leader hasn't received the response and retries.\r\n\r\nI think that's correct. I think the summary of the situation discussed so far is as follows:\r\n\r\nNo matter what skew-recovery scheme is implemented, both the Leader & the Helper must store their view of the current aggregation state to enable \"normal\" (happy-path) operation of the protocol.\r\n\r\nThe information that must be additionally stored to enable a skew-recovery scheme differs by scheme:\r\n* In `n`/`n+1` recovery, the Helper stores the hash of the previous request & the previous response (or enough other information to reconstruct the previous response from the previous request).\r\n* In restart-from-beginning skew recovery, the Helper must store the hash of the original request.\r\n* In arbitrary-step skew recovery, the Helper must store the hash of every previous request & every previous response (or enough other information to reconstruct that response from the associated request).\r\n\r\n(I don't believe the Leader needs to store additional state for any of the skew-recovery schemes discussed so far.)",
          "createdAt": "2024-07-29T16:25:57Z",
          "updatedAt": "2024-07-29T16:25:57Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs6CXBCt",
          "commit": {
            "abbreviatedOid": "5e1e842"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "",
          "createdAt": "2024-07-18T23:50:48Z",
          "updatedAt": "2024-07-18T23:51:01Z",
          "comments": [
            {
              "originalPosition": 24,
              "body": "Let's make this a bit more normative.\r\n\r\n> If the Leader re-sends an `AggregationJobContinueReq` from a previous step, the Helper SHOULD respond with the corresponding `AggregateShareResp`.",
              "createdAt": "2024-07-18T23:50:48Z",
              "updatedAt": "2024-07-18T23:51:01Z"
            }
          ]
        }
      ]
    },
    {
      "number": 570,
      "id": "PR_kwDOFEJYQs52xnHQ",
      "title": "Clarify what it means for a report to be \"previously aggregated.\"",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/570",
      "state": "MERGED",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "editorial"
      ],
      "body": "This is mostly a clarification; some implementations were treating reports as aggregated if they had ever been included in an aggregation job, whether or not that job was successful or the individual report had successfully completed aggregation.",
      "createdAt": "2024-07-29T17:23:59Z",
      "updatedAt": "2024-08-01T20:38:16Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "93fca5745c5b27d5a57a3b1195429d7935569961",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "bran/clarify-aggregated",
      "headRefOid": "661fe048eb720bef0813551d5dc1162ad3099e64",
      "closedAt": "2024-08-01T19:19:18Z",
      "mergedAt": "2024-08-01T19:19:18Z",
      "mergedBy": "branlwyd",
      "mergeCommit": {
        "oid": "9f05d0d21b14eecd3c539d643569f1c30c0ff472"
      },
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "This fell out of discussion in #569 -- other than being conceptually correct, this would help with some skew-recovery schemes.",
          "createdAt": "2024-07-29T17:24:31Z",
          "updatedAt": "2024-07-29T17:24:31Z"
        },
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "+@kristineguo for visibility.",
          "createdAt": "2024-07-29T17:58:42Z",
          "updatedAt": "2024-07-29T17:58:42Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOFEJYQs6DeWga",
          "commit": {
            "abbreviatedOid": "413cc94"
          },
          "author": "cjpatton",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-29T19:05:21Z",
          "updatedAt": "2024-07-29T19:06:07Z",
          "comments": [
            {
              "originalPosition": 16,
              "body": "```suggestion\r\n   MUST be marked as invalid with the error `report_replayed`. A report is\r\n   considered aggregated if its contribution would be include in a relevant\r\n   collection job.\r\n```",
              "createdAt": "2024-07-29T19:05:21Z",
              "updatedAt": "2024-07-29T19:06:07Z"
            },
            {
              "originalPosition": 4,
              "body": "I probably wouldn't list this in the CL.",
              "createdAt": "2024-07-29T19:06:04Z",
              "updatedAt": "2024-07-29T19:06:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOFEJYQs6DnhbM",
          "commit": {
            "abbreviatedOid": "413cc94"
          },
          "author": "tgeoghegan",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-07-30T16:35:08Z",
          "updatedAt": "2024-07-30T16:35:08Z",
          "comments": []
        }
      ]
    },
    {
      "number": 573,
      "id": "PR_kwDOFEJYQs55wiJ8",
      "title": "Add `max_aggregation_job_size` task parameter.",
      "url": "https://github.com/ietf-wg-ppm/draft-ietf-ppm-dap/pull/573",
      "state": "OPEN",
      "author": "branlwyd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This parameter configures the maximum aggregation job size, allowing a standard way for the Helper to opt out of processing aggregation jobs which are too large.",
      "createdAt": "2024-08-28T21:26:24Z",
      "updatedAt": "2024-08-28T21:29:29Z",
      "baseRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "baseRefName": "main",
      "baseRefOid": "c2f4e851a5989f4772d0c9e7c6d147611af7fd7b",
      "headRepository": "ietf-wg-ppm/draft-ietf-ppm-dap",
      "headRefName": "bran/max-agg-job-size",
      "headRefOid": "c2c481872781617515edb1228fb03d3315e2ed2a",
      "closedAt": null,
      "mergedAt": null,
      "mergedBy": null,
      "mergeCommit": null,
      "comments": [
        {
          "author": "branlwyd",
          "authorAssociation": "COLLABORATOR",
          "body": "Some notes:\r\n* The Leader behavior is a `MUST`, while the Helper behavior is a `SHOULD`. Since this parameter is intended to allow the Helper to protect itself from the Leader producing too-large aggregation jobs, I think this is acceptable. (Notably, this parameter does not need to be checked by both aggregators to achieve privacy/robustness properties, so it is at least _permissible_ for the Helper to ignore this check.)\r\n* We could get away without specifying this at all -- the Helper could simply abort on a too-large aggregation job even without this protocol text. However, I think it is useful to specify this so that we have a standardized error for debugging purposes, and so that task-provisioning schemes like taskprov can eventually standardize a way to negotiate this parameter.",
          "createdAt": "2024-08-28T21:29:29Z",
          "updatedAt": "2024-08-28T21:29:29Z"
        }
      ],
      "reviews": []
    }
  ]
}