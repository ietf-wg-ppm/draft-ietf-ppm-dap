---
title: "Private Data Aggregation Protocol"
docname: draft-pda-protocol-latest
category: std
ipr: trust200902
area: ART
workgroup: HTTPBIS

stand_alone: yes
pi: [toc, sortrefs, symrefs, docmapping]

author:
  -
    ins: S. People
    name: Some People
    org: Somewhere
    email: over@therainbow.net

informative:

  CB17:
    title: "Prio: Private, Robust, and Scalable Computation of Aggregate Statistics"
    date: 2017-03-14
    target: "https://crypto.stanford.edu/prio/paper.pdf"
    author:
      - ins: H. Corrigan-Gibbs
      - ins: D. Boneh

  BBCp21:
    title: "Lightweight Techniques for Private Heavy Hitters"
    date: 2021-01-05
    target: "https://eprint.iacr.org/2021/017"
    author:
      -ins: D. Boneh
      -ins: E. Boyle
      -ins: H. Corrigan-Gibbs
      -ins: N. Gilboa
      -ins: Y. Ishai

--- abstract

TODO: writeme

--- middle

# Introduction

This document describes a framework for specifying protocols for
privacy-preserving data-aggregation. Each protocol is executed by a large set of
clients and a small set of servers. The servers' goal is to compute some
aggregate statistic over the clients' inputs without learning the inputs
themselves. This is made possible by distributing the computation among the
servers in such a way that, as long as at least one of them executes the
protocol honestly, no input is ever seen in the clear by any server.

## DISCLAIMER

This document is a work in progress. We have not yet settled on the design of
the protocol framework or the set of features we intend to support.

## Terminology

This section defines some terminology we will use in the remainder of this
document.

1. Aggregation function: The function computed over the users' inputs.
1. Aggregator: An endpoint that runs the input-validation protocol and
   accumulates input shares.
1. Batch size: The number of valid input shares accumulated by each aggregator
   before computing the final output.
1. Client: The endpoint from which a user sends data to be aggregated, e.g., a
   web browser.
1. Collector: The endpoint that receives the output of the aggreagtion function.
   It also specifies the parameters of the protocol.
1. False input: An input that is valid, but incorrect. For example, if the data
   being gathered is whether or not users have clicked on a particular button, a
   client could report clicks when none occurred.
1. Input: The measurement (or measurements) emitted by a client, before any
   encryption or secret sharing scheme is applied.
1. Input share: one of the shares output by feeding an input into a secret
   sharing scheme. Each share is to be transmitted to one of the participating
   aggregators.
1. Input validation protocol: The protocol executed by the client and
   aggregators in order to validate the client's input without leaking its value
   to the aggregators.
1. Invalid input: An input for which the input validation protocol fails. For
   example, if the input is meant to be a  bit vectors, then `[2, 1, 0]` is
   invalid.
1. Measurement: A single value (e.g., a count) being reported by a client.
   Multiple measurements may be grouped into a single protocol input.
1. Leader: A distinguished aggregator that coordinates input validation and data
   collection.
1. Output: A reduction over the inputs, for instance a statistical aggregation,
   which is of interest to a collector. This is the output of the aggregation
   function.
1. Output share: The share of an output emitted by an aggregator. Output shares
   can be reassembled by the leader into the final output.
1. Prio v1: Mozilla's [Origin Telemetry project](https://blog.mozilla.org/security/2019/06/06/next-steps-in-privacy-preserving-telemetry-with-prio/).
1. Prio v2: Contact tracing project by Apple, Google, and ISRG.
1. Proof: A value generated by the client used by the aggregators to verify the
   client's input.
1. Proof share: A share of a proof, used by an aggregator during the
   input-validation protocol.
1. Report: Uploaded to the leader from the client. A report contains the
   secret-shared and encrypted input and proof.

# Overview {#overview}

[OPEN ISSUE: Rework this section in light of issue #44.]

The protocol is executed by a large set of clients and a small set of servers.
We call the servers the *aggregators*. Each client's input to the protocol is a
set of measurements (e.g., counts of some user behavior). Given the input set
of measurements `x_1, ..., x_n` held by `n` users, the goal of a
*private aggregation (PA) protocol* is to compute `y = F(x_1, ..., x_n)` for
some aggregation function `F` while revealing nothing else about the
measurements.

## Private aggregation via secret sharing

The main cryptographic tool used for achieving this privacy goal is *additive
secret sharing*. Rather than send its input in the clear, each client splits
its measurements into a sequence of *shares* and sends a share to each of the
aggregators. Additive secret sharing has two important properties:
- It's impossible to deduce the measurement without knowing *all* of the shares.
- It allows the aggregators to compute the final output by first adding up their
  measurements shares locally, then combining the results to obtain the final
  output.

Consider an illustrative example. Suppose there are three clients and two
aggregators. Each client `i` holds a single measurement in the form of a
positive integer `x[i]`, and our goal is to compute the sum of the measurements
of all clients. In this case, the protocol input is a single measurement
consisting of a single positive integer; no additional encoding is done. Given
this input, the first client splits its measurement `x[1]` with additive
secret-sharing into a pair of integers `X[1,1]` and `X[1,2]` for which `x[1]` is
equal to `X[1,1] + X[1,2]` modulo a prime number `p`. (For convenience, we will
omit the mod `p` operator in the rest of this section.) It then uploads `X[1,1]`
to one server `X[1,2]` to the other. The second client splits its measurement
`x[2]` into `X[1,2]` and `X[2,2]`, uploads them to the servers, and so on.

Now the first aggregator is in possession of shares `X[1,1]`, `X[2,1]`, and
`X[3,1]` and the second aggregator is in possession of shares `X[2,1]`,
`X[2,2]`, and `X[2,3]`. Each aggregator computes the sum of its shares; let
`A[1]` denote the first aggregator's share of the sum and let `A[2]` denote the
second aggregator's share of the sum. In the last step, aggregators combine
their sum shares to obtain the final output `y = A[1] + A[2]`. This is correct
because modular addition is commutative. I.e.,

~~~
    y = A[1] + A[2]
      = (x[1,1] + x[2,1] + x[3,1]) + (x[1,2] + x[2,2] + x[3,2])
      = (x[1,1] + x[1,2]) + (x[2,1] + x[2,2]) + (x[3,1] + x[3,2])
      = x[1] + x[2] + x[3]
      = F(x[1], x[2], x[3])
~~~

**Prio.**
This approach can be used to privately compute any function `F` that can be
expressed as a function of the sum of the users' inputs. In Prio {{CB17}}, each
user splits its input into shares and sends each share to one of the
aggregators. The aggregators sum up their input shares. Once all the shares have
been aggregated, they combine their shares of the aggregate to get the final
output.

Not all aggregate functions can be expressed this way efficiently, however. Prio
supports only a limited set of aggregation functions, some of which we highlight
below:

- Simple statistics, like sum, mean, min, max, variance, and standard deviation;
- Histograms with fixed bin sizes (also allows estimation of quantiles, e.g.,
  the median);
- More advanced statistics, like linear regression;
- Bitwise-OR and -AND on bit strings; and
- Computation of data structures, like Bloom filters, counting Bloom filters,
  and count-min sketches, that approximately represent (multi-)sets of strings.

This variety of aggregate types is sufficient to support a wide variety of
data aggregation tasks.

**Hits.**
A common PA task that can't be solved efficiently with Prio is the
`t`-*heavy-hitters* problem {{BBCp21}}. In this setting, each user is in
possession of a single `n`-bit string, and the goal is to compute the compute
the set of strings that occur at least `t` times. One reason that Prio doesn't
apply to this problem is that the proof generated by the client would be huge.

[TODO: Provide an overview of the protocol of {{BBCp21}} and provide some
intuition about how additive secret sharing is used.]

## Validating inputs in zero knowledge

An essential task of any data collection pipeline is ensuring that the input
data is "valid". Going back to the example above, it's often useful to assert
that each measurement is in a certain range, e.g., `[0, 2^k)` for some `k`.
This straight-forward task is complicated in our setting by the fact that the
inputs are secret shared. In particular, a malicious client can corrupt the
computation by submitting random integers instead of a proper secret sharing of
a valid input.

To solve this problem, in each PA protocol, the client generates a
zero-knowledge proof of its input's validity that the aggregators use
to verify that their shares correspond to as valid input. The verification
procedure is designed to ensure that the aggregators learn nothing about the
input beyond its validity.

After encoding its measurements as an input to the PA protocol, the client
generates a *proof* of the input's validity. It then splits the proof into
shares and sends a share of both the proof and input to each aggregator. The
aggregators use their shares of the proof to decide if their input shares
correspond to a valid input.

## Collecting reports

As noted above, each client has a collection of measurements that it
wants to send. Each measurement is characterized by a set of
parameters that are centrally configured and provided to each client:

- A unique identifier (e.g., "dns-queries-mean")
- A description of how to collect the measurement (e.g., "count
  the number of DNS queries")
- The statistic to be computed over the measurement values (e.g., mean)
- The rules for what constitutes a valid value (e.g., must be between 0
  and 10000)

Once the client has collected the measurements to send, it needs to
turn them into a set of reports. Naively, each measurement would be
sent in its own report, but it is also possible to have multiple
measurements in a single report; clients need to be configured with
the mapping from measurements to reports. The set of measurements
that go into a report is referred to as the "input" to the report.
Because each report is independent, for the remainder of this document
we focus on a single report and its inputs.

[NOTE(cjpatton): This paragraph is slightly misleading. If you want to do a
range check for the measurement (this will usually be necessary, IMO) then
you'll need a few extra field elements to encode the input.]
The client uses the statistic to be computed in order to know how to
encode the measurement. For instance, if the statistic is mean, then
the measurement can be encoded directly. However, if the statistic is
standard deviation, then the client must send both `x` and `x^2`. Section
[TODO: cite to internal description of how to encode]
describes how to encode measurements for each statistic.
The client uses the validity rules to construct the zero knowledge
proof showing that the encoded measurement is valid.

## Data flow

[TODO: Rework this subsection so that all terms needed in the rest of the
document are defined.]

[TODO: Explain that the downside of using secret sharing is that the protocol
requires at least two servers to be online during the entire data aggregation
process. To ameliorate this problem, we run the protocol in parallel with
multiple pairs of aggregators.]

Each PA task in this document is divided into three sub-protocols as follows.

~~~
                    +------------+
                    |            |
                    |   Helper   <---------------+
                    |            |               |
                    +-----^------+               |
                          |                      |
                       2. |                   3. |
                          |                      |
+--------+  1.      +-----v------+         +-----v-----+
|        +---------->            |      3. |           |
| Client +---------->   Leader   +---------> Collector |
|        +---------->            |         |           |
+--------+          +-----^------+         +-----^-----+
                          |                      |
                       2. |                   3. |
                          |                      |
                    +-----V------+               |
                    |            |               |
                    |   Helper   <---------------+
                    |            |
                    +------------+
~~~

1. **Upload:** Each client assembles the measurements into an input for the given
   PA protocol. It generates a proof of its input's validity and splits the
   input and proof into two shares, one for the leader and another for a helper.
   Rather than send each share to each aggregator directly, the client encrypts
   each share under the helper's public key and sends the ciphertext to the
   leader. The client repeats this procedure for each helper specified by the
   leader.
1. **Verify:** The leader initializes the input-validation protocol by sending
   the encrypted shares to the aggregators. If the input is deemed valid, then
   each aggregator stores its input share for processing later on.
1. **Collect:** Finally, the collector interacts with the aggregators in order
   to obtain the final output of the protocol.

# PA protocols {#pa}

This section specifies a protocol for executing generic PA tasks. Concrete
PA protocols are described in {{prio}} and {{hits}}.

Each round of the protocol corresponds to an HTTP request and response. The
content type of each request is "application/octet-stream". We assume that some
transport layer security protocol (e.g., TLS or QUIC) is used between each pair
of parties and that the server is always authenticated.

[TODO: Decide how to authenticate the leader in leader-to-helper and
aggregator-to-collector connections. One option is to use client certificates
for TLS; another is to have the leader sign its messages directly, as in Prio
v2.]

[TODO: @chris-wood suggested we specify APIs for producing and consuming each of
the messages in the protocol. Specific PA protocols would implement this API.]

[OPEN ISSUE: This needs to be reworked in order to account for protocols, like
Hits, that involve multiple rounds of verify/collect. See issue #44.]

**Error handling.**
In this section, we will use the verbs "abort" and "alert with `[some error
message]`" to describe how protocol participants react to various error
conditions. The behavior is specified in {{pa-error}}. For common errors, we may
elide the verbs altogether and refer to {{pa-error-common-aborts}}.

## Configuration {#pa-config}

### Tasks
Each PA protocol is associated with a *PA task* that specifies the measurements
that are to be collected and the protocol that will be used to collect them:

~~~
struct {
  uint16 version;
  opaque id[16];
} PATask;

~~~

The first field, `version` specifies the version of this document. The second
field, `id` is an opaque identifier used by the clients, aggregators, and
collector to uniquely identify the PA task at hand. We will call it the *task
id* in the remainder.

[TODO: Decide how the `PATask` is configured. Eventually this will be
distributed, in an authenticated manner, from the collector the other parties.
For now, we just assume this value is negotiated out-of-band.]

### Parameters

Associated to each task is the set of PA protocol parameters. These are encoded
by the `PAParam` structure, which also includes the task:

~~~
struct {
  PATask task;
  uint64 batch_size;
  PAProto proto;
  select (PAClientParam.proto) {
    case prio: PrioParam;
    case hits: HitsParam;
  }
} PAParam;

enum { prio(0), hits(1) } PAProto;
~~~

The `batch_size` field encodes the *batch size*, the number of input shares
accumulated by each aggregator before emitting its output share.  The `proto`
field identifies the specific PA protocol in use. The rest of the structure
contains any protocol-specific parameters that are required.

### Helper key configuration

Our protocol uses HPKE for public-key encryption {{!I-D.irtf-cfrg-hpke}}.  Each
helper specifies the HPKE public key that clients use to encrypt the helper's
share. The public key and associated parameters are structured as follows:

~~~
struct {
  uint8 id;
  HpkeKemId kem_id;
  HpkeKdfId kdf_id;
  HpkeAeadKdfId aead_id;
  HpkePublicKey public_key;
} HpkeConfig;

opaque HpkePublicKey<1..2^16-1>;
uint16 HpkeAeadId; // Defined in I-D.irtf-cfrg-hpke
uint16 HpkeKemId;  // Defined in I-D.irtf-cfrg-hpke
uint16 HpkeKdfId;  // Defined in I-D.irtf-cfrg-hpke
~~~
[TODO: Decide whether to re-use the config from OHTTP/ECH. This would add
support for multiple cipher suites.]

We call this the helper's *key configation*. The key configuration is used to
set up a base-mode HPKE context to use to derive symmetric keys for protecting
the shares sent to the helper. The *config id*, `HpkeConfig.id`, is forwarded
by the client to the helper, who uses this value to decide if it knows how to
decrypt a share it receives.

## Pre-conditions

We assume the following conditions hold before the client begins uploading its
data:

1. The client, aggregators, and collector are configured with a specific PA task.
1. The client knows the URL of the leader endpoint, e.g., `example.com/metrics`.
   We write this URL as `[leader]` below. (We write `[helper]` for a helper's
   URL.)
1. The client and leader can establish a leader-authenticated secure channel.
1. The leader and each helper can establish a leader-authenticated secure
   channel.
1. Each helper has chosen an HPKE key pair.
1. The aggregators agree on a set of PA tasks, as well as the PA protocol and
   parameters used for each task.

[TODO: It would be clearer to include a "pre-conditions" section prior to each
"phase" of the protocol.]

## Upload {#pa-upload}

[TODO: Add an illustration of this sub-protocol.]

Uploading a report involves two requests to the leader. In the  *upload start
request*, the client discovers the protocol-specific parameters it needs to
generate the report, as well as the endpoint URL of each helper. In the *upload
finish request*, it uploads its report to the leader.

[NOTE: @acmiyaguchi pointed out that the use of an anonymizing proxy for
uploading shares might be easier to implement if the "upload" phase involved a
single HTTP request. However, OHTTP
(https//www.ietf.org/archive/id/draft-thomson-http-oblivious-01.html) allows
clients to make multiple requests through a proxy, so these kinds of use cases
should work.]

[NOTE: @cjpatton: Breaking the upload phase into two requests is useful for
supporting Prio-like proof systems in which the leader sends the client a
"challenge" it uses to generate the proof. (See the SIMD construction of
{{BBCp19}}, Section 5.2. Here, the "challenge" is a randomly generated field
element.)]

### Upload Start

The client sends a POST request to `[leader]/upload_start` with the following
message:

~~~
struct {
  PATask task;
} PAUploadStartReq;
~~~

The `task` field corresponds to the PA task for which a report will be
generated.

The leader responds to well-formed requests to `[leader]/upload_start` with
status 200 and the following message:

~~~
struct {
  PAParam param;
  Url helper_urls<1..2^16-1>;
  select (PAUploadStartResp.param.proto) {
    case prio: PrioUploadStartResp;
    case hits: HitsUploadStartResp;
  }
} PAUploadStartResp;

opaque Url<1..2^16-1>;
~~~

The message includes the URL of each helper and any protocol-specific parameters
the client needs to generate its report. The leader's response to malformed
requests is specified in {{pa-error-common-aborts}}.

### Upload Finish

For each URL `[helper]` in `PAUploadStartResp.helper_urls`, the client sends a
GET request to `[helper]/key_config`. The helper responds with status 200 and an
`HpkeConfig` message. Next, the client collects the set of helpers it will
upload shares to. It ignores a helper if:

* the client and helper failed to establish a secure, helper-authenticated
  channel;
* the GET request to the helper URL failed or didn't return a valid key config;
  or
* the key config specifies a KEM, KDF, or AEAD algorithm the client doesn't
  recognize.

If the set of supported helpers is empty, then the client aborts and alerts the
leader with "no supported helpers". Otherwise, for each supported helper the
client issues a POST request to `[leader]/upload_finish` with a payload
constructed as described below.

[OPEN ISSUE: @chris-wood: Can't the leader determine if helpers are "online"?
This seems to reveal information that's specific to clients. Imagine, for
example, that clients are prohibited from talking to helpers but not the leader.
Is it OK that leaders learn that about a client? I'm not sure, so I'd be
inclined to remove this unless we have a concrete use case.]

The client begins by setting up an HPKE {{!I-D.irtf-cfrg-hpke}} context for
the helper by running

~~~
enc, context = SetupBaseS(pk, [TODO])
~~~

where `pk` is the KEM public key encoded by `HpkeConfig.public_key`. The outputs
are the helper's encapsulated context `enc` and the context `context`.

[TODO: Decide what the info string should be. At a minimum it should include
`PAParam` so that the PA parameters are implicitly authenticated by the helper.
when it decapsulates `enc`. We might consider using the hash of the transcript
between the client and leader so far, i.e., `PAUploadStartReq`, ...,
`PAUploadStartResp`, as this would be the most conservative thing. We just need
to ensure that the helper can reproduce this transcript. This should be possible
given the information it has.]

Next, the client encodes its measurements as an input for the PA protocol. It
then generates a validity proof for its input and uses `context` to split the
input and proof into a *leader share* and a *helper share*, where the latter is
protected by the HPKE context. Note that the details of each of these processing
steps --- encode, prove, split, and encrypt --- are specific to the PA protocol.

[OPEN ISSUE: Is it safe to generate the proof once, then secret-share between
each (leader, helper) pair? Probably not in general, but maybe for Prio?]

[OPEN ISSUE: allow server to send joint randomness in UploadStartResp, and then
enforce uniqueness via double-spend state or something else]

The payload of the POST request to `[leader]/upload_finish` is structured as
follows:

~~~
struct {
  PATask task;                 // Equal to PAUploadStartReq.task
  uint8 helper_hpke_config_id; // Equal to HpkeConfig.id
  Url helper_url;
  PAHelperShare helper_share;
  PALeaderShare leader_share;
} PAUploadFinishReq;
~~~

We sometimes refer to this message as the *report*. The message contains the
`task` fields of the previous request. In addition, it includes the helper's
HPKE config id, endpoint URL, and the helper and leader shares.  The helper
share has the following structure:

~~~
struct {
  opaque enc<1..2^16-1>;
  PAProto proto;
  select (PAHelperShare.proto) {
    case prio: PrioHelperShare;
    case hits: HitsHelperShare;
  }
} PAHelperShare;
~~~

Field `enc` encodes the helper's encapsulated HPKE context. The remainder of the
structure contains the share itself, the structure of which is specific to the
PA protocol. The structure of the leader share is similarly protocol specific:

~~~
struct {
  PAProto proto;
  select (PALeaderShare.proto) {
    case prio: PrioLeaderShare;
    case hits: HitsLeaderShare;
  };
} PALeaderShare;
~~~

Note that the leader share is sent not encrypted.

The leader responds to well-formed requests to `[leader]/upload_finish` with
status 200 and an empty body. Malformed requests are handled as described in
{{pa-error-common-aborts}}.

[TODO: Since we're running the protocol with multiple cohorts of aggregators, the
collector needs to decide how to pick which cohort has the "correct" output.
This might be the cohort with the largest batch of inputs. Figure this out once
the collection is specified.]

## Verify {#pa-verify}

[TODO: Add an illustration of this sub-protocol.]

After the client uploads a report to the leader, the leader and helper verify in
zero knowledge that the proof is well-formed. The exact procedure for doing so
is protocol specific, but all protocols have the same basic structure. In
particular, the protocol is comprised of a sequence of HTTPS requests from the
leader to the helper. At the end of this phase, the leader and helper will have
decided whether a set of client inputs are valid. For each valid input, they
proceed as described in {{pa-collect}}.

The leader begins by collecting a sequence of reports that are all associated
with the same PA task, helper URL, and helper HPKE config id. Let `[helper]`
denote the the URL. The leader sends a POST request to `[helper]/verify` with
the following message:

~~~
struct {
  PATask task;
  uint8 hpke_config_id;
  PAVerifyReq seq<1..2^24-1>;
} PAVerifyReqSeq;
~~~

The structure contains the PA task, the HPKE config id, and a sequence of
*sub-requests*, each corresponding to a unique client report. Sub-requests are
structured as follows:

~~~
struct {
  opaque enc<1..2^16-1>;
  PAProto proto;
  select (PAVerifyReq.proto) {
    case prio:
      PrioHelperShare;
      PrioVerifyReq;
    case hits:
      HitsHelperShare;
      HitsVerifyReq;
  }
} PAVerifyReq;
~~~

The `enc` field is the helper's encapsulated HPKE context sent in the report.
The remainder of the me structure is dedicated to the protocol-specific helper
share and request parameters used for the current round.

The helper handles well-formed requests as follows. (As usual, malformed
requests are handled as described in {{pa-error-common-aborts}}.) It first looks
for the PA parameters `PAParam` for which `PAVerifyReq.task.id ==
PAParam.task.id`. Next, it looks up the HPKE config and corresponding secret key
associated with `PAVerifyReq.key_config_id`. If not found, then it aborts and
alerts the leader with "unrecognized key config". [NOTE: In this situation, the
leader has no choice but to abort. This falls into the class of error scenarios
that are addressable by running with multiple helpers.]

The response is structured as a sequence of *sub-responses*, where the i-th
sub-response corresponds to the sub-request for each i. The structure of each
sub-response is specific to the PA protocol:

~~~
struct {
   PAVerifyResp seq<1..2^24-1>;
} PAVerifyRespSeq;

struct {
  PAProto;
  select (PAVerifyResp.proto) {
    case prio: PrioVerifyResp;
    case hits: HitsVerifyResp;
  }
} PAVerifyResp;
~~~

For each sub-request `PAVerifyReq`, the helper computes the corresponding
sub-response as follows. It first checks that checks that `PAVerifyReq.proto ==
PAParam.proto`. If not, it aborts and alerts the leader with "incorrect protocol
for sub-request". Otherwise, It computes the HPKE context as

~~~
context = SetupBaseR(PAVerifyReq.enc, sk, [TODO])
~~~

where `sk` is the secret key corresponding to the HPKE config. Next, it computes
the body of the `PAVerifyResp` according to the PA protocol.

[OPEN ISSUE: encrypt and store Helper state at the Leader]

## Collect {#pa-collect}

[TODO]

## Error handling {#pa-error}

An *alert* is a message sent either in an HTTP request or response that signals
to the receiver that the peer has aborted the protocol. The payload is

~~~
struct {
  PATask task;
  opaque payload<1..255>;
} PAAlert;
~~~

where `task` is the associated PA task (this value is always known) and
`payload` is the message. When sent by an aggregator in response to an HTTP
request, the response status is 400. When sent in a request to an aggregator,
the URL is always `[aggregator]/error`, where `[aggregator]` is the URL of the
aggregator endpoint.

## Common abort conditions {#pa-error-common-aborts}

The following specify the "boiler-plate" behavior for various error conditions.

- The message type for the payload of each request and response is unique for a
  given URL. If ever a client, aggregator, or collector receives a request or
  response to a request with a malformed payload, then the receiver aborts and
  alerts the peer with "unrecognized message".

- Each POST request to an aggregator contains a `PATask`. If the aggregator does not
  recognize the task, i.e., it can't find a `PAParam` for which `PATask.id ==
  PAParam.task.id`, then it aborts and alerts the peer with "unrecognized task".

# Prio {#prio}

[TODO: Define `PrioParam`]

[TODO: Define `PrioUploadStartResp`]

[TODO: Define `PrioHelperShare`]

[TODO: Define `PrioLeaderShare`]

[TODO: Define `PrioVerifyReq`]

[TODO: Define `PrioVerifyResp`]

## Parameters

### Finite field arithmetic

The algorithms that comprise the input-validation protocol --- Prove, Query, and
Decide --- are constructed by generating and evaluating polynomials over a
finite field. As such, the main ingredient of Prio is an implementation of
arithmetic in a finite field suitable for the given application.

We will use a prime field. The choice of prime is influenced by the following
criteria:

1. **Field size.** How big the field needs to be depends on the type of data
   being aggregated and how many users there are. The field size also impacts
   the security level: the longer the validity circuit, the larger the field
   needs to be in order to effectively detect malicious clients. Typically the
   soundness error (i.e., the probability of an invalid input being deemed valid
   by the aggregators) will be 2n/(p-n), where n is the size of the input and p
   is the prime modulus.
1. **Fast polynomial operations.** In order to make Prio practical, it's
   important that implementations employ FFT to speed up polynomial operations.
   In particular, the prime modulus p should be chosen so that (p-1) = 2^b * s
   for large b and odd s. Then g^s is a principle, 2^b-th root of unity (i.e.,
   g^(s\*2^b) = 1), where g is the generator of the multiplicative subgroup.
   This fact allows us to quickly evaluate and interpolate polynomials at 2^a-th
   roots of unity for any 1 <= a <= b. Note that b imposes n upper bound on the
   size of proofs, so it should be large enough to accommodate all foreseeable
   use cases. Something like b >= 20 is probably good enough.
1. **As close to a power of two as possible.** We use rejection sampling to map
   a PRNG seed to a pseudorandom sequence of field elements (see {{prio-prng}).
   In order to minimize the probability of a simple being rejected, the modulus
   should be as close to a power of 2 as possible.
1. **Code optimization.** [[TODO: What properties of the field make
   it possible to write faster implementations?]]

The table below lists parameters that meet these criteria at various levels of
security. (Note that \#1 is the field used in "Prio v2".) The "size" column
indicates the number of bits required to represent elements of the field.

| # | size | p                                      | g  | b   | s                |
|---|------|----------------------------------------|----|-----|------------------|
| 1 | 32   | 4293918721                             | 19 | 20  | 3^2 * 5 * 7 * 13 |
| 2 | 64   | 15564440312192434177                   | 5  | 59  | 3^3              |
| 3 | 80   | 779190469673491460259841               | 14 | 72  | 3 * 5 * 11       |
| 4 | 123  | 9304595970494411110326649421962412033  | 3  | 120 | 7                |
| 5 | 126  | 74769074762901517850839147140769382401 | 7  | 118 | 3^2 * 5^2        |

[TODO: Choose new parameters for 2, 3, and 5 so that p is as close to 2^size as
possible without going over. (4 is already close enough; 1 is already deployed
and can't be changed.]

**Finding suitable primes.**
One way to find suitable primes is to first choose choose b, then "probe" to
find a prime of the desired size. The following SageMath script prints the
parameters of a number of (probable) primes larger than 2^b for a given b:

~~~
b = 116
for s in range(0,1000,1):
    B = 2^b
    p = (B*s).next_prime()
    if p-(B*s) == 1:
        bits = round(math.log2(p), 2)
        print(bits, p, GF(p).multiplicative_generator(), b, factor(s))
~~~

### Pseudorandom number generation {#prio-prng}

A suitable PRNG will have the following syntax. Fix a finite field K:

1. x := PRNG(k, n) denotes generation of a vector of n elements of K.

This can be instantiated using a standard stream cipher, e.g., AES-CTR, as
follows. Interpret the seed k as the key and IV for generating the AES-CTR key
stream. Proceed by rejection sampling, as follows. Let m be the number of bits
needed to encode an element of K. Generate the next m bits of key stream and
interpret the bytes as an integer x, clearing the most significant m - l bits,
where l is the bit-length of the modulus p. If x < p, then output x. Otherwise,
generate the next m bits of key stream and try again. Repeat this process
indefinitely until a suitable output is found.

# Hits {#hits}

[TODO: Define `HitsParam`]

[TODO: Define `HitsUploadStartResp`]

[TODO: Define `HitsHelperShare`]

[TODO: Define `HitsLeaderShare`]

[TODO: Define `HitsVerifyReq`]

[TODO: Define `HitsVerifyResp`]

# System design

[[OPEN ISSUE: This section seems like a catch-all for things not in other
sections. Perhaps there is a natural home for aggregator discovery, share
uploading, open issues, and system parameters?]]

## Aggregator discovery

[[OPEN ISSUE: writeme]]

## Share uploading

[[OPEN ISSUE: writeme]]

## Open questions and system parameters {#questions-and-params}

[[OPEN ISSUE: discuss batch size parameter and thresholds]]
[[OPEN ISSUE: discuss f^ leakage differences from [GB17]]]


# Operational Considerations

Prio has inherent constraints derived from the tradeoff between privacy
guarantees and computational complexity. These tradeoffs influence how
applications may choose to utilize services implementing the specification.

## Data resolution limitations

Privacy comes at the cost of computational complexity. While affine-aggregatable
encodings (AFEs) can compute many useful statistics, they require more bandwidth
and CPU cycles to account for finite-field arithmetic during input-validation.
The increased work from verifying inputs decreases the throughput of the system
or the inputs processed per unit time. Throughput is related to the verification
circuit's complexity and the available compute-time to each aggregator.

Applications that utilize proofs with a large number of multiplication gates or
a high frequency of inputs may need to limit inputs into the system to meet
bandwidth or compute constraints. Some methods of overcoming these limitations
include choosing a better representation for the data or introducing sampling
into the data collection methodology.

[[TODO: Discuss explicit key performance indicators, here or elsewhere.]]

## Aggregation utility and soft batch deadlines

A soft real-time system should produce a response within a deadline to
be useful. This constraint may be relevant when the value of an aggregate
decreases over time. A missed deadline can reduce an aggregate's utility
but not necessarily cause failure in the system.

An example of a soft real-time constraint is the expectation that input data can
be verified and aggregated in a period equal to data collection, given some
computational budget. Meeting these deadlines will require efficient
implementations of the input-validation protocol. Applications might batch
requests or utilize more efficient serialization to improve throughput.

Some applications may be constrained by the time that it takes to reach a
privacy threshold defined by a minimum number of input shares. One possible
solution is to increase the reporting period so more samples can be collected,
balanced against the urgency of responding to a soft deadline.

## Data integrity constraints

Data integrity concerns the accuracy and correctness of the outputs in the
system. The integrity of the output can be influenced by an incomplete round of
aggregation caused by network partitions, or by bad actors attempting to cause
inaccuracies in the aggregates. An example data integrity constraint is that
every share must be processed exactly once by all aggregators. Data integrity
constraints may be at odds with the threat model if meeting the constraints
requires replaying data.

Aggregator operators should expect to encounter invalid inputs during regular
operation due to misconfigured or malicious clients. Low volumes of errors are
tolerable; the input-verification protocol and AFEs are robust in the face of
malformed data. Aggregators may need to detect and mitigate statistically
significant floods of invalid or identical inputs that affect accuracy, e.g.,
denial of service (DoS) events.

Certain classes of errors do not exist in the input-validation protocol
considered in this document. For example, packet loss errors when clients make
requests directly to aggregators are not relevant when the leader proxies
requests and controls the schedule for signaling aggregation rounds.

# Security Considerations {#sec-considerations}

## Security overview {#security-requirements}

Prio assumes a powerful adversary with the ability to compromise an unbounded
number of clients. In doing so, the adversary can provide malicious (yet
truthful) inputs to the aggregation function. Prio also assumes that all but one
server operates honestly, where a dishonest server does not execute the protocol
faithfully as specified. The system also assumes that servers communicate over
secure and mutually authenticated channels. In practice, this can be done by TLS
or some other form of application-layer authentication.

In the presence of this adversary, Prio provides two important properties for
computing an aggergation function F:

1. Privacy. The aggregators and collector learn only the output of F computed
   over all client inputs, and nothing else.
1. Robustness. As long as the aggregators execute the input-validation protocol
   correctly, a malicious client can skew the output of F only by reporting
   false (untruthful) input. The output cannot be influenced in any other way.

There are several additional constraints that a Prio deployment must satisfy in
order to achieve these goals:

1. Minimum batch size. The aggregation batch size has an obvious impact on
   privacy. (A batch size of one hides nothing of the input.)
   {{questions-and-params}} discusses appropriate batch sizes and how they
   pertains to privacy in more detail.
2. Aggregation function choice. Some aggregation functions leak slightly more
   than the function output itself. {{questions-and-params}} discusses the
   leakage profiles of various aggregation functions in more detail.

### Threat model

In this section, we enumerate the actors participating in the Prio system and
enumerate their assets (secrets that are either inherently valuable or which
confer some capability that enables further attack on the system), the
capabilities that a malicious or compromised actor has, and potential
mitigations for attacks enabled by those capabilities.

This model assumes that all participants have previously agreed upon and
exchanged all shared parameters over some unspecified secure channel.

#### Client/user

##### Assets

1. Unshared inputs. Clients are the only actor that can ever see the original
   inputs.
1. Unencrypted input shares.

##### Capabilities

1. Individual users can reveal their own input and compromise their own privacy.
     * Since this does not affect the privacy of others in the system, it is
       outside the threat model.
1. Clients (that is, software which might be used by many users of the system)
can defeat privacy by leaking input outside of the Prio system.
     * In the current threat model, other participants have no insight into what
       clients do besides uploading input shares. Accordingly, such attacks are
       outside of the threat model.
1. Clients may affect the quality of aggregations by reporting false input.
     * Prio can only prove that submitted input is valid, not that it is true.
       False input can be mitigated orthogonally to the Prio protocol (e.g., by
       requiring that aggregations include a minimum number of contributions)
       and so these attacks are considered to be outside of the threat model.
1. Clients can send invalid encodings of input.

##### Mitigations

1. The input validation protocol executed by the aggregators prevents either
individual clients or coalitions of clients from compromising the robustness
property.

#### Aggregator

##### Assets

1. Unencrypted input shares.
1. Input share decryption keys.
1. Client identifying information.
1. Output shares.
1. Aggregator identity.

##### Capabilities

1. Aggregators may defeat the robustness of the system by emitting bogus output
   shares.
1. If clients reveal identifying information to aggregators (such as a trusted
   identity during client authentication), aggregators can learn which clients
   are contributing input.
     1. Aggregators may reveal that a particular client contributed input.
     1. Aggregators may attack robustness by selectively omitting inputs from
        certain clients.
          * For example, omitting submissions from a particular geographic
            region to falsely suggest that a particular localization is not
            being used.
1. Individual aggregators may compromise availability of the system by refusing
to emit output shares.
1. Input validity proof forging. Any aggregator can collude with a malicious
client to craft a proof share that will fool honest aggregators into accepting
invalid input.

##### Mitigations

1. The linear secret sharing scheme employed by the client ensures that privacy
   is preserved as long as at least one aggregator does not reveal its input
   shares.
1. If computed over a sufficient number of input shares, output shares reveal
   nothing about either the inputs or the participating clients.

#### Leader

The leader is also an aggregator, and so all the assets, capabilities and
mitigations available to aggregators also apply to the leader.

##### Capabilities

1. Input validity proof verification. The leader can forge proofs and collude
   with a malicious client to trick aggregators into aggregating invalid inputs.
     * This capability is no stronger than any aggregator's ability to forge
       validity proof shares in collusion with a malicious client.
1. Relaying messages between aggregators. The leader can compromise availability
   by dropping messages.
     * This capability is no stronger than any aggregator's ability to refuse to
       emit output shares.
1. Shrinking the anonymity set. The leader instructs aggregators to construct
   output parts and so could request aggregations over few inputs.

##### Mitigations

1. Aggregators enforce agreed upon minimum aggregation thresholds to prevent
   deanonymizing.

#### Collector

##### Capabilities

1. Advertising shared configuration parameters (e.g., minimum thresholds for
   aggregations, joint randomness, arithmetic circuits).
1. Collectors may trivially defeat availability by discarding output shares
   submitted by aggregators.

##### Mitigations

1. Aggregators should refuse shared parameters that are trivially insecure
   (i.e., aggregation threshold of 1 contribution).

#### Aggregator collusion

If all aggregators collude (e.g. by promiscuously sharing unencrypted input
shares), then none of the properties of the system hold. Accordingly, such
scenarios are outside of the threat model.

#### Attacker on the network

We assume the existence of attackers on the network links between participants.

##### Capabilities

1. Observation of network traffic. Attackers may observe messages exchanged
   between participants at the IP layer.
     1. The time of transmission of input shares by clients could reveal
        information about user activity.
          * For example, if a user opts into a new feature, and the client
            immediately reports this to aggregators, then just by observing
            network traffic, the attacker can infer what the user did.
     1. Observation of message size could allow the attacker to learn how much
        input is being submitted by a client.
          * For example, if the attacker observes an encrypted message of some
            size, they can infer the size of the plaintext, plus or minus the
            cipher block size. From this they may be able to infer which
            aggregations the user has opted into or out of.
1. Tampering with network traffic. Attackers may drop messages or inject new
   messages into communications between participants.

##### Mitigations

1. All messages exchanged between participants in the system should be
   encrypted.
1. All messages exchanged between aggregators, the collector and the leader
   should be mutually authenticated so that network attackers cannot impersonate
   participants.
1. Clients should be required to submit inputs at regular intervals so that the
   timing of individual messages does not reveal anything.
1. Clients should submit dummy inputs even for aggregations the user has not
   opted into.

[[OPEN ISSUE: The threat model for Prio --- as it's described in the original
paper and [BBG+19] --- considers **either** a malicious client (attacking
soundness) **or** a malicious subset of aggregators (attacking privacy). In
particular, soundness isn't guaranteed if any one of the aggregators is
malicious; in theory it may be possible for a malicious client and aggregator to
collude and break soundness. Is this a contingency we need to address? There are
techniques in [BBG+19] that account for this; we need to figure out if they're
practical.]]

### Future work and possible extensions

In this section we discuss attacks that are not considered in the above threat
model, and suggest mitigations that could be incorporated into implementations
of this protocol or future revisions of this specfication.

#### Client authentication

Attackers can impersonate Prio clients and submit large amounts of false input
in order to spoil aggregations. Deployments could require clients to
authenticate before they may contribute inputs. For example, by requiring
submissions to be signed with a key trusted by aggregators. However some
deployments may opt to accept the risk of false inputs to avoid having to figure
out how to distribute trusted identities to clients.

#### Client attestation

In the current threat model, servers participating in the protocol have no
insight into the activities of clients except that they have uploaded input into
a Prio aggregation, meaning that clients could covertly leak a user's data into
some other channel which compromises privacy. If we introduce the notion of a
trusted computing base which can attest to the properties or activities of a
client, then users and aggregators can be assured that their private data only
goes into Prio. For instance, clients could use the trusted computing base to
attest to software measurements over reproducible builds, or a trusted operating
system could attest to the client's network activity, allowing external
observers to be confident that no data is being exfiltrated.

#### Trusted anonymizing and authenticating proxy

While the input shares transmitted by clients to aggregators reveal nothing
about the original input, the aggregator can still learn auxiliary information
received messages (for instance, source IP or HTTP user agent), which can
identify participating clients or permit some attacks on robustness. This is
worse if client authentication used, since incoming messages would be bound to a
cryptographic identity. Deployments could include a trusted anonymizing proxy,
which would be responsible for receiving input shares from clients, stripping
any identifying information from them (including client authentication) and
forwarding them to aggregators. There should still be a confidential and
authenticated channel from the client to the aggregator to ensure that no actor
besides the aggregator may decrypt the input shares.

#### Multiple protocol runs

Prio is _robust_ against malicious clients, and _private_ against malicious
servers, but cannot provide robustness against malicious servers. Any aggregator
can simply emit bogus output shares and undetectably spoil aggregates. If enough
aggregators were available, this could be mitigated by running the protocol
multiple times with distinct subsets of aggregators chosen so that no aggregator
appears in all subsets and checking all the outputs against each other. If all
the protocol runs do not agree, then participants know that at least one
aggregator is defective, and it may be possible to identify the defector (i.e.,
if a majority of runs agree, and a single aggregator appears in every run that
disagrees). See
[#22](https://github.com/abetterinternet/prio-documents/issues/22) for
discussion.

### Security considerations

#### Infrastructure diversity

Prio deployments should ensure that aggregators do not have common dependencies
that would enable a single vendor to reassemble inputs. For example, if all
participating aggregators stored unencrypted input shares on the same cloud
object storage service, then that cloud vendor would be able to reassemble all
the input shares and defeat privacy.

## System requirements {#operational-requirements}

### Data types

# IANA Considerations

TODO

--- back
