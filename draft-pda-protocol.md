---
title: "Private Data Aggregation Protocol"
docname: draft-pda-protocol-latest
category: std
ipr: trust200902
area: ART
workgroup: HTTPBIS

stand_alone: yes
pi: [toc, sortrefs, symrefs, docmapping]

author:
  -
    ins: S. People
    name: Some People
    org: Somewhere
    email: over@therainbow.net

informative:

  CB17:
    title: "Prio: Private, Robust, and Scalable Computation of Aggregate Statistics"
    date: 2017-03-14
    target: "https://crypto.stanford.edu/prio/paper.pdf"
    author:
      - ins: H. Corrigan-Gibbs
      - ins: D. Boneh

  BBCp19:
    title: "Zero-Knowledge Proofs on Secret-Shared Data via Fully Linear PCPs"
    date: 2021-01-05
    target: "https://eprint.iacr.org/2019/188"
    author:
      -ins: D. Boneh
      -ins: E. Boyle
      -ins: H. Corrigan-Gibbs
      -ins: N. Gilboa
      -ins: Y. Ishai

  BBCp21:
    title: "Lightweight Techniques for Private Heavy Hitters"
    date: 2021-01-05
    target: "https://eprint.iacr.org/2021/017"
    author:
      -ins: D. Boneh
      -ins: E. Boyle
      -ins: H. Corrigan-Gibbs
      -ins: N. Gilboa
      -ins: Y. Ishai

--- abstract

TODO: writeme

--- middle

# Introduction

This document describes a framework for specifying protocols for
privacy-preserving data-aggregation. Each protocol is executed by a large set of
clients and a small set of servers. The servers' goal is to compute some
aggregate statistic over the clients' inputs without learning the inputs
themselves. This is made possible by distributing the computation among the
servers in such a way that, as long as at least one of them executes the
protocol honestly, no input is ever seen in the clear by any server.

## DISCLAIMER

This document is a work in progress. We have not yet settled on the design of
the protocol framework or the set of features we intend to support.

## Terminology

This section defines some terminology we will use in the remainder of this
document.

1. Aggregation function: The function computed over the users' inputs.
1. Aggregator: An endpoint that runs the input-validation protocol and
   accumulates input shares.
1. Batch: A set of reports that are aggregated into an output.
1. Batch size: The minimum size of a batch.
1. Batch window: The minimum time difference between the oldest and newest
   report in a batch (in seconds).
1. Client: The endpoint from which a user sends data to be aggregated, e.g., a
   web browser.
1. Collector: The endpoint that receives the output of the aggreagtion function.
   It also specifies the parameters of the protocol.
1. False input: An input that is valid, but incorrect. For example, if the data
   being gathered is whether or not users have clicked on a particular button, a
   client could report clicks when none occurred.
1. Input: The measurement (or measurements) emitted by a client, before any
   encryption or secret sharing scheme is applied.
1. Input share: one of the shares output by feeding an input into a secret
   sharing scheme. Each share is to be transmitted to one of the participating
   aggregators.
1. Input validation protocol: The protocol executed by the client and
   aggregators in order to validate the client's input without leaking its value
   to the aggregators.
1. Invalid input: An input for which the input validation protocol fails. For
   example, if the input is meant to be a  bit vectors, then `[2, 1, 0]` is
   invalid.
1. Measurement: A single value (e.g., a count) being reported by a client.
   Multiple measurements may be grouped into a single protocol input.
1. Leader: A distinguished aggregator that coordinates input validation and data
   collection.
1. Output: A reduction over the inputs, for instance a statistical aggregation,
   which is of interest to a collector. This is the output of the aggregation
   function.
1. Output share: The share of an output emitted by an aggregator. Output shares
   can be reassembled by the leader into the final output.
1. Prio v1: Mozilla's [Origin Telemetry project](https://blog.mozilla.org/security/2019/06/06/next-steps-in-privacy-preserving-telemetry-with-prio/).
1. Prio v2: Contact tracing project by Apple, Google, and ISRG.
1. Proof: A value generated by the client used by the aggregators to verify the
   client's input.
1. Proof share: A share of a proof, used by an aggregator during the
   input-validation protocol.
1. Report: Uploaded to the leader from the client. A report contains the
   secret-shared and encrypted input and proof.
1. Server: An aggregator or collector.

# Overview {#overview}

[OPEN ISSUE: Rework this section in light of issue #44.]

The protocol is executed by a large set of clients and a small set of servers.
We call the servers the *aggregators*. Each client's input to the protocol is a
set of measurements (e.g., counts of some user behavior). Given the input set
of measurements `x_1, ..., x_n` held by `n` users, the goal of a
*private aggregation (PA) protocol* is to compute `y = F(x_1, ..., x_n)` for
some aggregation function `F` while revealing nothing else about the
measurements.

## Private aggregation via secret sharing

The main cryptographic tool used for achieving this privacy goal is *additive
secret sharing*. Rather than send its input in the clear, each client splits
its measurements into a sequence of *shares* and sends a share to each of the
aggregators. Additive secret sharing has two important properties:
- It's impossible to deduce the measurement without knowing *all* of the shares.
- It allows the aggregators to compute the final output by first adding up their
  measurements shares locally, then combining the results to obtain the final
  output.

Consider an illustrative example. Suppose there are three clients and two
aggregators. Each client `i` holds a single measurement in the form of a
positive integer `x[i]`, and our goal is to compute the sum of the measurements
of all clients. In this case, the protocol input is a single measurement
consisting of a single positive integer; no additional encoding is done. Given
this input, the first client splits its measurement `x[1]` with additive
secret-sharing into a pair of integers `X[1,1]` and `X[1,2]` for which `x[1]` is
equal to `X[1,1] + X[1,2]` modulo a prime number `p`. (For convenience, we will
omit the mod `p` operator in the rest of this section.) It then uploads `X[1,1]`
to one server `X[1,2]` to the other. The second client splits its measurement
`x[2]` into `X[1,2]` and `X[2,2]`, uploads them to the servers, and so on.

Now the first aggregator is in possession of shares `X[1,1]`, `X[2,1]`, and
`X[3,1]` and the second aggregator is in possession of shares `X[2,1]`,
`X[2,2]`, and `X[2,3]`. Each aggregator computes the sum of its shares; let
`A[1]` denote the first aggregator's share of the sum and let `A[2]` denote the
second aggregator's share of the sum. In the last step, aggregators combine
their sum shares to obtain the final output `y = A[1] + A[2]`. This is correct
because modular addition is commutative. I.e.,

~~~
    y = A[1] + A[2]
      = (x[1,1] + x[2,1] + x[3,1]) + (x[1,2] + x[2,2] + x[3,2])
      = (x[1,1] + x[1,2]) + (x[2,1] + x[2,2]) + (x[3,1] + x[3,2])
      = x[1] + x[2] + x[3]
      = F(x[1], x[2], x[3])
~~~

**Prio.**
This approach can be used to privately compute any function `F` that can be
expressed as a function of the sum of the users' inputs. In Prio {{CB17}}, each
user splits its input into shares and sends each share to one of the
aggregators. The aggregators sum up their input shares. Once all the shares have
been aggregated, they combine their shares of the aggregate to get the final
output.

Not all aggregate functions can be expressed this way efficiently, however. Prio
supports only a limited set of aggregation functions, some of which we highlight
below:

- Simple statistics, like sum, mean, min, max, variance, and standard deviation;
- Histograms with fixed bin sizes (also allows estimation of quantiles, e.g.,
  the median);
- More advanced statistics, like linear regression;
- Bitwise-OR and -AND on bit strings; and
- Computation of data structures, like Bloom filters, counting Bloom filters,
  and count-min sketches, that approximately represent (multi-)sets of strings.

This variety of aggregate types is sufficient to support a wide variety of
data aggregation tasks.

**Hits.**
A common PA task that can't be solved efficiently with Prio is the
`t`-*heavy-hitters* problem {{BBCp21}}. In this setting, each user is in
possession of a single `n`-bit string, and the goal is to compute the compute
the set of strings that occur at least `t` times. One reason that Prio doesn't
apply to this problem is that the proof generated by the client would be huge.

[TODO: Provide an overview of the protocol of {{BBCp21}} and provide some
intuition about how additive secret sharing is used.]

## Validating inputs in zero knowledge

An essential task of any data collection pipeline is ensuring that the input
data is "valid". Going back to the example above, it's often useful to assert
that each measurement is in a certain range, e.g., `[0, 2^k)` for some `k`.
This straight-forward task is complicated in our setting by the fact that the
inputs are secret shared. In particular, a malicious client can corrupt the
computation by submitting random integers instead of a proper secret sharing of
a valid input.

To solve this problem, in each PA protocol, the client generates a
zero-knowledge proof of its input's validity that the aggregators use
to verify that their shares correspond to as valid input. The verification
procedure is designed to ensure that the aggregators learn nothing about the
input beyond its validity.

After encoding its measurements as an input to the PA protocol, the client
generates a *proof* of the input's validity. It then splits the proof into
shares and sends a share of both the proof and input to each aggregator. The
aggregators use their shares of the proof to decide if their input shares
correspond to a valid input.

## Collecting reports

As noted above, each client has a collection of measurements that it
wants to send. Each measurement is characterized by a set of
parameters that are centrally configured and provided to each client:

- A unique identifier (e.g., "dns-queries-mean")
- A description of how to collect the measurement (e.g., "count
  the number of DNS queries")
- The statistic to be computed over the measurement values (e.g., mean)
- The rules for what constitutes a valid value (e.g., must be between 0
  and 10000)

Once the client has collected the measurements to send, it needs to
turn them into a set of reports. Naively, each measurement would be
sent in its own report, but it is also possible to have multiple
measurements in a single report; clients need to be configured with
the mapping from measurements to reports. The set of measurements
that go into a report is referred to as the "input" to the report.
Because each report is independent, for the remainder of this document
we focus on a single report and its inputs.

[NOTE(cjpatton): This paragraph is slightly misleading. If you want to do a
range check for the measurement (this will usually be necessary, IMO) then
you'll need a few extra field elements to encode the input.]
The client uses the statistic to be computed in order to know how to
encode the measurement. For instance, if the statistic is mean, then
the measurement can be encoded directly. However, if the statistic is
standard deviation, then the client must send both `x` and `x^2`. Section
[TODO: cite to internal description of how to encode]
describes how to encode measurements for each statistic.
The client uses the validity rules to construct the zero knowledge
proof showing that the encoded measurement is valid.

## Data flow

Each PA task consists of two sub-protocols, *upload* and *collect*, which are
executed concurrently. Each sub-protocol consists of a sequence of HTTP requests
made from one entity to another.

~~~~
+-------------+ 1.      +-------------+
|             +--------->             |
|   Client    |         |   Leader    |
|             |    +----+             |
+------+------+    | 2. +------^------+
       | 1.        |           |
       |           |           |
       |           |           | 2.
+------v------+    |    +------+------+
|             <----+    |             |
|   Helper    |         |  Collector  |
|             |         |             |
+-------------+         +-------------+
~~~~
{: #pa-topology title="Who makes requests to whom while executing a PA task."}

1. **Upload:** Each client assembles its measurements into an input for the
   given PA protocol. It generates a proof of its input's validity and splits
   the input and proof into two shares, one for the leader and another for a
   helper. The client then encrypts the leader's share and helper's share under,
   respectively, the leader's public key and the helper's public key. (The keys
   are obtained by making requests to the leader and helper.) Finally, the
   client uploads the encrypted shares to the leader.
2. **Collect:** The collector makes one or more requests to the leader in order
   to obtain the final output of the protocol. Before the output can be
   computed, the aggregators (i.e, the leader and helper) need to have verified
   and aggregated a sufficient number of inputs. Depending on the PA protocol,
   it may be possible for the aggregators to do so immediately when reports are
   uploaded. (See {{prio}}.) However, in general it is necessary for them to
   wait until (a) enough reports have been uploaded and (b) the collector has
   made a request. (See {{hits}}.)

# PA protocols {#pa}

This section specifies a protocol for executing generic PA tasks. Concrete
PA protocols are described in {{prio}} and {{hits}}.

Each round of the protocol corresponds to an HTTP request and response. The
content type of each request is "application/octet-stream". We assume that some
transport layer security protocol (e.g., TLS or QUIC) is used between each pair
of parties and that the server is authenticated.

[TODO: Decide how to provide mutual authentication in leader-to-helper and
collector-to-leader connections. One option is to use client certificates for
TLS; another is to have the leader sign its messages directly, as in Prio v2.
For collector-to-leader connections, we may just have this be up to deployment.
(For instance, the collector might authenticate themselves by logging into a
website that has some trust relationship with the leader.)]

[TODO: @chris-wood suggested we specify APIs for producing and consuming each of
the messages in the protocol. Specific PA protocols would implement this API.]

**Error handling.**
In this section, we will use the verbs "abort" and "alert with `[some error
message]`" to describe how protocol participants react to various error
conditions. The behavior is specified in {{pa-error}}. For common errors, we may
elide the verbs altogether and refer to {{pa-error-common-aborts}}.

[TODO: Fix the bounds for length-prefixed parameters in protocol messages.
(E.g., `<23..479>` instead of `<1..2^16-1>`.)]

## Configuration {#pa-config}

### Tasks

Each PA protocol is associated with a *PA task* that specifies the measurements
that are to be collected. Associated to each task is a set of *PA Parameters*,
encoded by the following `PAParam` structure, which specify the protocol used to
verify and aggregate the clients' measurements:

~~~
struct {
  Url leader_url;
  Url helper_url;
  HpkeConfig collector_config; // [TODO: Remove this?]
  uint64 batch_size;
  uint64 batch_window;
  PAProto proto;
  uint16 length; // Length of the remainder.
  select (PAClientParam.proto) {
    case prio: PrioParam;
    case hits: HitsParam;
  }
} PAParam;

enum { prio(0), hits(1) } PAProto;

opaque Url<1..2^16-1>;
~~~

* `leader_url`: The leader's endpoint URL.
* `helper_url`: The helper's endpoint URL.
* `collector_config`: The HPKE configuration of the collector (described in
  {{hpke-config}}). [OPEN ISSUE: Maybe the collector's HPKE config should be
  carried by the collect request?]
* `batch_size`: The batch size, i.e., the minimum number of reports that are
  aggregated into an output.
* `batch_window`: The batch window, i.e., the minimum time difference between
  the oldest and newest report in a batch.
* `proto`: The PA protocol, e.g., Prio or Hits. The rest of the structure
  contains the protocol specific parameters.

Each task has a unique *task id* derived from the PA parameters:

~~~
opaque PATaskID[32];
~~~

The task id is derived using the following procedure. [TODO: Specify derivation
of the task ID.]

### HPKE key configuration {#hpke-config}

Our protocol uses HPKE for public-key encryption {{!I-D.irtf-cfrg-hpke}}. Each
aggregator specifies the HPKE public key that clients use to encrypt its input
share, and the collector specifies the HPKE public key that helpers use to
encrypt output shares during collection. The public key and associated
parameters are structured as follows:

~~~
struct {
  uint8 id;
  HpkeKemId kem_id;
  HpkeKdfId kdf_id;
  HpkeAeadKdfId aead_id;
  HpkePublicKey public_key;
} HpkeConfig;

opaque HpkePublicKey<1..2^16-1>;
uint16 HpkeAeadId; // Defined in I-D.irtf-cfrg-hpke
uint16 HpkeKemId;  // Defined in I-D.irtf-cfrg-hpke
uint16 HpkeKdfId;  // Defined in I-D.irtf-cfrg-hpke
~~~
[TODO: Decide whether to use the same config structure as OHTTP/ECH. This would
add support for multiple cipher suites.]

We call this a *key configation*. The key configuration is used to set up a
base-mode HPKE context to use to derive symmetric keys for protecting: (1) input
shares sent from the client to an aggregator; or (2) output shares sent from the
helper to the collector. The *config id*, `HpkeConfig.id`, is forwarded by the
sender to the receiver to help the receiver decide if it knows the decryption
key.

## Pre-conditions

We assume the following conditions hold before execution of any PA task begins:

1. The aggregators agree on a set of PA tasks, as well as the PA parameters
   associated to each task.
1. Each aggregator has a clock that is roughly in sync with true time, i.e.,
   within the batch window specified by the PA parameters. (This is necessary to
   prevent the same report from appearing in multiple batches.)
1. Each client has selected a PA task for which it will upload a report. It is
   also configured with the task's parameters.
1. Each client and the leader can establish a leader-authenticated secure
   channel.
1. The leader and each helper can establish a helper-authenticated secure
   channel.
1. The collector and leader can establish a leader-authenticated secure channel.
1. The collector has chosen an HPKE configuration and corresponding secret key.
1. Each aggregator has chosen an HPKE configuration and corresponding secret key.

[TODO: It would be clearer to include a "pre-conditions" section prior to each
"phase" of the protocol.]

## Upload {#pa-upload}

~~~~
Client          Leader         Helper
  |  key config  |              |
  <-------------->              |
  |              |  key config  |
  <----------------------------->
  |  upload      |              |
  <-------------->              |
  v              v              v
~~~~
{: #pa-upload-flow title="Flow of the upload process"}

[NOTE: @acmiyaguchi pointed out that the use of an anonymizing proxy for
uploading shares might be easier to implement if the "upload" phase involved a
single HTTP request. However, OHTTP
(https//www.ietf.org/archive/id/draft-thomson-http-oblivious-01.html) allows
clients to make multiple requests through a proxy, so these kinds of use cases
should work.]

### Key Config Request

Before the client can upload its report to the leader, it must first discover
the key configs of each of the aggregators. To do so, the client sends a GET
request to `[aggregator]/key_config`, where `[aggregator]` is the aggregator's
endpoint URL. The aggregator responds to well-formed requests with status 200
and an `HpkeConfig`.

The client issues a key config request to `PAParam.leader_url` and
`PAParam.helper_Url`. It aborts if any of the following happen for either
request:

* the client and aggregator failed to establish a secure,
  aggregator-authenticated channel;
* the GET request failed or didn't return a valid key config; or
* the key config specifies a KEM, KDF, or AEAD algorithm the client doesn't
  recognize.

[OPEN ISSUE: @chris-wood: Can't the leader determine if helpers are "online"?
This seems to reveal information that's specific to clients. Imagine, for
example, that clients are prohibited from talking to helpers but not the leader.
Is it OK that leaders learn that about a client? I'm not sure, so I'd be
inclined to remove this unless we have a concrete use case.]

### Upload Request

Next, the client issues a POST request to `[leader]/upload`, where `[leader]` is
the leader's endpoint URL. The payload is structured as follows:

~~~
struct {
  PATaskID task_id;
  PAReport reports<1..2^24-1>;
} PAUploadReq;
~~~

This message consists of a task id and any number of *reports* for the task.
Reports are structured as:

~~~
struct {
  uint64 time; // UNIX time (in seconds).
  PAEncryptedInputShare encrypted_input_shares<1..2^16-1>;
} PAReport;
~~~

where `time` is the time (in seconds since the beginning of UNIX time) at which
the report was generated and `encrypted_input_shares` is the sequence of
encrypted input shares, each intended for one of the aggregators. These have the
following structure:

~~~
struct {
  uint8 config_id;
  opaque enc<1..2^16-1>;
  opaque payload<1..2^16-1>;
} PAEncryptedInputShare;
~~~

* `config_id` is equal to `HpkeConfig.id`, where `HpkeConfig` is the
  aggregator's key config.
* `enc` is the encapsulated HPKE context, used by the aggregator to decrypt its
  input share.
* `payload` is the encrypted input share.

To generate a report, the client begins by encoding its measurements as an input
for the PA protocol and splitting it into input shares. (Note that the structure
of each input share depends on the PA protocol in use, its parameters, and the
role of aggregator, i.e., whether the aggregator is a leader or helper.) To
encrypt an input share, the client first generates an HPKE
{{!I-D.irtf-cfrg-hpke}} context for the aggregator by running

~~~
enc, context = SetupBaseS(pk, [TODO])
~~~

where `pk` is the aggregator's public key. `enc` is the encapsulate HPKE
context and `context` is the HPKE context used by the client for encryption.
The payload is encrypted as

~~~
payload = context.Seal(input_share, [TODO])
~~~

where `input_share` is the aggregator's input share.

[TODO: Fully specify encryption of the shares. We need to make sure we
authenticate things like the PA parameters and the report timestamp. We need to
decide what the info string for SetupBaseS() will be, as well as the aad for
context.Seal(). The aad might be the entire "transcript" between the client and
aggregator.]

The leader responds to well-formed requests to `[leader]/upload` with status 200
and an empty body. Malformed requests are handled as described in
{{pa-error-common-aborts}}.

#### Batched Uploading

Usually the client's upload request will consist of a single report, but there
are applications for which it's useful to be able to upload many reports in the
same request. For example, reports might be collected from a set of clients and
uploaded to the leader by some entity in the middle.

## Collect {#pa-collect}

~~~~
Collector     Leader           Helper
  |  collect 1  |                |
  +------------->                |
  |             |  aggregate 1   |
  |             <---------------->
  |             |  ...           |
  |             |  aggregate L   |
  |             <---------------->
  |             |  output share  |
  |             <---------------->
  <-------------+                |
  |  ...        |                |
  |  collect N  |                |
  +------------->                |
  |             |  aggregate 1   |
  |             <---------------->
  |             |     ...        |
  |             |  aggregate L   |
  |             <---------------->
  |             |  output share  |
  |             <---------------->
  <-------------+                |
  v             v                v
~~~~
{: #pa-collect-flow title="Flow of the collect process with N collect requests
and L aggregate requests per collect request."}

[TODO: Decide if and how the collector's request is authenticated.]

The collector interacts with the leader to produce the final aggregate output.
This process consists of a sequence of *collect requests* issued to the leader.
Before a request can succeed, the aggregators must have verified and aggregated
enough reports and the leader must have obtained the helper's encrypted output
share (see {{pa-aggregate}}). In general, the procedure by which the aggregators
verify and aggregate reports depends on parameters carried by the collect
request. This procedure is described below in {{pa-aggregate}}.

### Collect Request

[TODO: Decide whether to specify things in terms of functions. @ekr pointed out
that it would be clearer to just talk about protocol messages.]

The collect protocol is an iterative procedure driven by the collector and
parameterized by a PAParam. At a high level, it proceeds as follows. First, the
collector initializes local per-protocol state using the corresponding
PAParam. This state object has the following member functions:

- CreateRequest(): Creates a PACollectReq object, defined below, that is sent to
  the leader to complete one iteration of the protocol. Along with any
  protocol-specific parameters, the message specifies a time interval
  `[batch_start, batch_end)` that determines the batch of reports to be
  aggregated. It must be that `batch_end - batch_start >= PAParam.batch_window`
  and `batch_start` and `batch_end` are multiples of `PAParam.batch_window`.

~~~
struct {
  PATaskID task_id;
  uint64 batch_start; // The beginning of the batch in UNIX time.
  Uint64 batch_end;   // The end of the batch in UNIX time (exclusive).
  PAProto proto;
  select (PACollectReq.proto) {
    case prio: PrioCollectReq;
    case hits: HitsCollectReq;
  }
} PACollectReq;
~~~

- Update(resp: PACollectResp): Consumes an opaque PACollectResp object, defined
  below, that is received from the leader in response to a PACollectReq.

~~~
struct {
  PATaskID task_id;
  PAProto proto;
  PAOutputShare leader_share;
  opaque encrypted_helper_share;
} PACollectResp;
~~~

- Finished(): Returns a boolean indicating indicating whether or not the collection
  procedure is complete. This function returns true when the collect iteration
  is complete.
- Output(): Returns the aggregate output corresponding to the protocol.

The collect procedure for a given PAParam structure `param` is then driven with
the following algorithm:

~~~
state = CreateState(param)
while not state.Finished():
   req = state.CreateRequest()
   resp = fetch([leader]/collect, req)
   state.Update(resp)
return state.Output()
~~~

[OPEN ISSUE: Describe how intra-protocol errors yield collect errors (see
issue#57). For example, how does a leader respond to a collect request if the
helper drops out?]

Each collect request involves one of the helpers specified by the PA parameters.
If more than one helper is specified, the collector may issue the requests in
any order.

### Verifying and Aggregating Reports {#pa-aggregate}

After the client uploads a report to the leader, the leader and helper verify in
zero knowledge that the input is valid. The exact procedure for doing so is
protocol specific, but all protocols have the same basic structure. In
particular, the protocol is comprised of a sequence of *aggregate requests* from
the leader to the helper. At the end of this procedure, the leader and helper
will have have aggregated a set of valid client inputs (though not necessarily a
complete batch).

#### Aggregate Request

The process begins with a PACollectReq. The leader collects a sequence of
reports that are all associated with the same PA task. Let `[helper]` denote
`PAParam.helper_url`, where `PAParam` is the PA parameters structure associated
`PAAggregateReq.task.id`. The leader sends a POST request to
`[helper]/aggregate` with the following message:

~~~
struct {
  PATaskID task_id;
  uint8 helper_hpke_config_id;
  opaque helper_state<0..2^16>;
  PAAggregateSubReq seq<1..2^24-1>;
} PAAggregateReq;
~~~

The structure contains the PA task, the helper's HPKE config id, an opaque
*helper state* string, and a sequence of *sub-requests*, each corresponding to a
unique client report. Sub-requests are structured as follows:

~~~
struct {
  uint64 time; // UNIX timestamp (seconds) of the report.
  PAEncryptedInputShare helper_share;
  select (PAParam.proto) { // PAParam for the PA task
    case prio: PrioAggregateSubReq;
    case hits: HitsAggregateSubReq;
  }
} PAAggregateSubReq;
~~~

The `helper_share` field is the helper's encrypted input share as it appeared in
the report uploaded by the client. [OPEN ISSUE: We usually only need to send
this in the first aggregate request. Shall we exclude it in subsequent requests
somehow?] The `time` field should match the timestamp of the corresponding
report. The remainder of the structure is dedicated to the protocol-specific
helper share and request parameters used for the current round.

The helper handles well-formed requests as follows. (As usual, malformed
requests are handled as described in {{pa-error-common-aborts}}.) It first looks
for the PA parameters `PAParam` for which `PAAggregateReq.task_id` is equal to
the task id derived from `PAParam`.  Next, it looks up the HPKE config and
corresponding secret key associated with `PAAggregateReq.helper_hpke_config_id`.
If not found, then it aborts and alerts the leader with "unrecognized key
config". [NOTE: In this situation, the leader has no choice but to abort. This
falls into the class of error scenarios that are addressable by running with
multiple helpers.]

[TODO: Don't require all sub-requests to pertain to the same HPKE config.]

The response consists of the helper's updated state and a sequence of
*sub-responses*, where the i-th sub-response corresponds to the sub-request for
each i. The structure of each sub-response is specific to the PA protocol:

~~~
struct {
  opaque helper_state<0..2^16>;
  PAAggregateSubResp seq<1..2^24-1>;
} PAAggregateResp;

struct {
  select (PAParam.proto) { // PAParam for the PA task
    case prio: PrioAggregateSubResp;
    case hits: HitsAggregateSubResp;
  }
} PAAggregateSubResp;
~~~

The helper handles each sub-request `PAAggregateSubReq` as follows.  It first
checks that checks that `PAAggregateSubReq.proto == PAParam.proto`. If not, it
aborts and alerts the leader with "incorrect protocol for sub-request".
Otherwise, It computes the HPKE context as

~~~
context = SetupBaseR(PAAggregateSubReq.enc, sk, [TODO])
~~~

where `sk` is its HPKE key and computes the body of the `PAAggregateSubResp`. It
then updates its state according to the PA protocol. After processing all of the
sub-requests, the helper encrypts its updated state and constructs its response
to the aggregate request.

##### Helper State

The helper state is an optional parameter of an aggregate request that the can
helper use to carry state across requests. At least part of the state will
usually need to be encrypted in order to protect user privacy. However, the
details of precisely how the state is encrypted and the information that it
carries is up to the helper implementation.

#### Output Share Request

Once the aggregators have verified at least as many reports as required for the
PA task, the leader issues an *output share request* to the helper. The helper
responds to this request by extracting its output share from its state and
encrypting it under the collector's HPKE public key.

The leader sends a POST request to `[helper]/output_share` with the following
message:

~~~
struct {
  PATaskID task_id;
  uint64 batch_start; // Same as PACollectReq.batch_start.
  Uint64 batch_end;   // Same as PACollectReq.batch_end.
  opaque helper_state<0..2^16>;
} PAOutputShareReq;
~~~

To respond to valid output share requests, the helper first checks that
`batch_start` and `batch_end` are multiples of `task.batch_window` and that
`batch_end - batch_start >= task.batch_window`. Next, it extracts from its state
the set of input shares that fall in the window `[batch_start, batch_end)`. If
the size of the batch is less than `task.batch_size`, then it aborts and alerts
the leader with "insufficient data". Otherwise, it computes its output share,
which has the following structure:

~~~
struct {
  PAProto proto;
  select (PAOutputShare.proto) {
    case prio: PrioOutputShare;
    case hits: HitsOutputShare;
  }
} PAOutputShare;
~~~

Next, it encrypts its output share under the collector's HPKE public key:

~~~
enc, context = SetupBaseS(pk, [TODO])
encrypted_output_share = context.Seal(output_share, [TODO])
~~~

where `pk` is the HPKE public key encoded by the collector's HPKE key
configuration and `output_share` is its serialized output share.

It responds with the following message:

~~~
struct {
  uint8 collector_hpke_config_id;
  opaque enc<1..2^16-1>;
  opaque encrypted_output_share<1..2^16>;
} PAOutputShareResp;
~~~

The leader uses the helper's output share response to respond to the collector's
collect request (see {{pa-collect}}).

## Error handling {#pa-error}

An *alert* is a message sent either in an HTTP request or response that signals
to the receiver that the peer has aborted the protocol. The payload is

~~~
struct {
  PATaskID task_id;
  opaque payload<1..255>;
} PAAlert;
~~~

where `task` is the associated PA task (this value is always known) and
`payload` is the message. When sent by an aggregator in response to an HTTP
request, the response status is 400. When sent in a request to an aggregator,
the URL is always `[aggregator]/error`, where `[aggregator]` is the URL of the
aggregator endpoint.

## Common abort conditions {#pa-error-common-aborts}

The following specify the "boiler-plate" behavior for various error conditions.

- The message type for the payload of each request and response is unique for a
  given URL. If ever a client, aggregator, or collector receives a request or
  response to a request with a malformed payload, then the receiver aborts and
  alerts the peer with "unrecognized message".

- Each POST request to an aggregator contains a `PATaskID`. If the aggregator
  does not recognize the task, i.e., it can't find a `PAParam` for which the
  derived task id matches the `PATaskID`, then it aborts and alerts the peer
  with "unrecognized task".

# Prio {#prio}

[TODO: Define Prio-specific protocol messages.]

## Parameters

### Finite field arithmetic

The algorithms that comprise the input-validation protocol --- Prove, Query, and
Decide --- are constructed by generating and evaluating polynomials over a
finite field. As such, the main ingredient of Prio is an implementation of
arithmetic in a finite field suitable for the given application.

We will use a prime field. The choice of prime is influenced by the following
criteria:

1. **Field size.** How big the field needs to be depends on the type of data
   being aggregated and how many users there are. The field size also impacts
   the security level: the longer the validity circuit, the larger the field
   needs to be in order to effectively detect malicious clients. Typically the
   soundness error (i.e., the probability of an invalid input being deemed valid
   by the aggregators) will be 2n/(p-n), where n is the size of the input and p
   is the prime modulus.
1. **Fast polynomial operations.** In order to make Prio practical, it's
   important that implementations employ FFT to speed up polynomial operations.
   In particular, the prime modulus p should be chosen so that (p-1) = 2^b * s
   for large b and odd s. Then g^s is a principle, 2^b-th root of unity (i.e.,
   g^(s\*2^b) = 1), where g is the generator of the multiplicative subgroup.
   This fact allows us to quickly evaluate and interpolate polynomials at 2^a-th
   roots of unity for any 1 <= a <= b. Note that b imposes an upper bound on the
   size of proofs, so it should be large enough to accommodate all foreseeable
   use cases. Something like b >= 20 is probably good enough.
1. **As close to a power of two as possible.** We use rejection sampling to map
   a PRNG seed to a pseudorandom sequence of field elements (see {{prio-prng}).
   In order to minimize the probability of a simple being rejected, the modulus
   should be as close to a power of 2 as possible.
1. **Code optimization.** [[TODO: What properties of the field make
   it possible to write faster implementations?]]

The table below lists parameters that meet these criteria at various levels of
security. (Note that \#1 is the field used in "Prio v2".) The "size" column
indicates the number of bits required to represent elements of the field.

| # | size | p                                      | g  | b   | s                |
|---|------|----------------------------------------|----|-----|------------------|
| 1 | 32   | 4293918721                             | 19 | 20  | 3^2 * 5 * 7 * 13 |
| 2 | 64   | 15564440312192434177                   | 5  | 59  | 3^3              |
| 3 | 80   | 779190469673491460259841               | 14 | 72  | 3 * 5 * 11       |
| 4 | 123  | 9304595970494411110326649421962412033  | 3  | 120 | 7                |
| 5 | 126  | 74769074762901517850839147140769382401 | 7  | 118 | 3^2 * 5^2        |

[TODO: Choose new parameters for 2, 3, and 5 so that p is as close to 2^size as
possible without going over. (4 is already close enough; 1 is already deployed
and can't be changed.]

**Finding suitable primes.**
One way to find suitable primes is to first choose choose b, then "probe" to
find a prime of the desired size. The following SageMath script prints the
parameters of a number of (probable) primes larger than 2^b for a given b:

~~~
b = 116
for s in range(0,1000,1):
    B = 2^b
    p = (B*s).next_prime()
    if p-(B*s) == 1:
        bits = round(math.log2(p), 2)
        print(bits, p, GF(p).multiplicative_generator(), b, factor(s))
~~~

### Pseudorandom number generation {#prio-prng}

A suitable PRNG will have the following syntax. Fix a finite field K:

1. x := PRNG(k, n) denotes generation of a vector of n elements of K.

This can be instantiated using a standard stream cipher, e.g., AES-CTR, as
follows. Interpret the seed k as the key and IV for generating the AES-CTR key
stream. Proceed by rejection sampling, as follows. Let m be the number of bits
needed to encode an element of K. Generate the next m bits of key stream and
interpret the bytes as an integer x, clearing the most significant m - l bits,
where l is the bit-length of the modulus p. If x < p, then output x. Otherwise,
generate the next m bits of key stream and try again. Repeat this process
indefinitely until a suitable output is found.

# Hits {#hits}

[TODO: Define Hits-specific protocol messages.]

# System design

[[OPEN ISSUE: This section seems like a catch-all for things not in other
sections. Perhaps there is a natural home for aggregator discovery, share
uploading, open issues, and system parameters?]]

## Aggregator discovery

[[OPEN ISSUE: writeme]]

## Share uploading

[[OPEN ISSUE: writeme]]

## Open questions and system parameters {#questions-and-params}

[[OPEN ISSUE: discuss batch size parameter and thresholds]]
[[OPEN ISSUE: discuss f^ leakage differences from [GB17]]]


# Operational Considerations

Prio has inherent constraints derived from the tradeoff between privacy
guarantees and computational complexity. These tradeoffs influence how
applications may choose to utilize services implementing the specification.

## Data resolution limitations

Privacy comes at the cost of computational complexity. While affine-aggregatable
encodings (AFEs) can compute many useful statistics, they require more bandwidth
and CPU cycles to account for finite-field arithmetic during input-validation.
The increased work from verifying inputs decreases the throughput of the system
or the inputs processed per unit time. Throughput is related to the verification
circuit's complexity and the available compute-time to each aggregator.

Applications that utilize proofs with a large number of multiplication gates or
a high frequency of inputs may need to limit inputs into the system to meet
bandwidth or compute constraints. Some methods of overcoming these limitations
include choosing a better representation for the data or introducing sampling
into the data collection methodology.

[[TODO: Discuss explicit key performance indicators, here or elsewhere.]]

## Aggregation utility and soft batch deadlines

A soft real-time system should produce a response within a deadline to
be useful. This constraint may be relevant when the value of an aggregate
decreases over time. A missed deadline can reduce an aggregate's utility
but not necessarily cause failure in the system.

An example of a soft real-time constraint is the expectation that input data can
be verified and aggregated in a period equal to data collection, given some
computational budget. Meeting these deadlines will require efficient
implementations of the input-validation protocol. Applications might batch
requests or utilize more efficient serialization to improve throughput.

Some applications may be constrained by the time that it takes to reach a
privacy threshold defined by a minimum number of input shares. One possible
solution is to increase the reporting period so more samples can be collected,
balanced against the urgency of responding to a soft deadline.

## Data integrity constraints

Data integrity concerns the accuracy and correctness of the outputs in the
system. The integrity of the output can be influenced by an incomplete round of
aggregation caused by network partitions, or by bad actors attempting to cause
inaccuracies in the aggregates. An example data integrity constraint is that
every share must be processed exactly once by all aggregators. Data integrity
constraints may be at odds with the threat model if meeting the constraints
requires replaying data.

Aggregator operators should expect to encounter invalid inputs during regular
operation due to misconfigured or malicious clients. Low volumes of errors are
tolerable; the input-verification protocol and AFEs are robust in the face of
malformed data. Aggregators may need to detect and mitigate statistically
significant floods of invalid or identical inputs that affect accuracy, e.g.,
denial of service (DoS) events.

Certain classes of errors do not exist in the input-validation protocol
considered in this document. For example, packet loss errors when clients make
requests directly to aggregators are not relevant when the leader proxies
requests and controls the schedule for signaling aggregation rounds.

# Security Considerations {#sec-considerations}

## Security overview {#security-requirements}

Prio assumes a powerful adversary with the ability to compromise an unbounded
number of clients. In doing so, the adversary can provide malicious (yet
truthful) inputs to the aggregation function. Prio also assumes that all but one
server operates honestly, where a dishonest server does not execute the protocol
faithfully as specified. The system also assumes that servers communicate over
secure and mutually authenticated channels. In practice, this can be done by TLS
or some other form of application-layer authentication.

In the presence of this adversary, Prio provides two important properties for
computing an aggergation function F:

1. Privacy. The aggregators and collector learn only the output of F computed
   over all client inputs, and nothing else.
1. Robustness. As long as the aggregators execute the input-validation protocol
   correctly, a malicious client can skew the output of F only by reporting
   false (untruthful) input. The output cannot be influenced in any other way.

There are several additional constraints that a Prio deployment must satisfy in
order to achieve these goals:

1. Minimum batch size. The aggregation batch size has an obvious impact on
   privacy. (A batch size of one hides nothing of the input.)
   {{questions-and-params}} discusses appropriate batch sizes and how they
   pertains to privacy in more detail.
2. Aggregation function choice. Some aggregation functions leak slightly more
   than the function output itself. {{questions-and-params}} discusses the
   leakage profiles of various aggregation functions in more detail.

### Threat model

In this section, we enumerate the actors participating in the Prio system and
enumerate their assets (secrets that are either inherently valuable or which
confer some capability that enables further attack on the system), the
capabilities that a malicious or compromised actor has, and potential
mitigations for attacks enabled by those capabilities.

This model assumes that all participants have previously agreed upon and
exchanged all shared parameters over some unspecified secure channel.

#### Client/user

##### Assets

1. Unshared inputs. Clients are the only actor that can ever see the original
   inputs.
1. Unencrypted input shares.

##### Capabilities

1. Individual users can reveal their own input and compromise their own privacy.
     * Since this does not affect the privacy of others in the system, it is
       outside the threat model.
1. Clients (that is, software which might be used by many users of the system)
can defeat privacy by leaking input outside of the Prio system.
     * In the current threat model, other participants have no insight into what
       clients do besides uploading input shares. Accordingly, such attacks are
       outside of the threat model.
1. Clients may affect the quality of aggregations by reporting false input.
     * Prio can only prove that submitted input is valid, not that it is true.
       False input can be mitigated orthogonally to the Prio protocol (e.g., by
       requiring that aggregations include a minimum number of contributions)
       and so these attacks are considered to be outside of the threat model.
1. Clients can send invalid encodings of input.

##### Mitigations

1. The input validation protocol executed by the aggregators prevents either
individual clients or coalitions of clients from compromising the robustness
property.

#### Aggregator

##### Assets

1. Unencrypted input shares.
1. Input share decryption keys.
1. Client identifying information.
1. Output shares.
1. Aggregator identity.

##### Capabilities

1. Aggregators may defeat the robustness of the system by emitting bogus output
   shares.
1. If clients reveal identifying information to aggregators (such as a trusted
   identity during client authentication), aggregators can learn which clients
   are contributing input.
     1. Aggregators may reveal that a particular client contributed input.
     1. Aggregators may attack robustness by selectively omitting inputs from
        certain clients.
          * For example, omitting submissions from a particular geographic
            region to falsely suggest that a particular localization is not
            being used.
1. Individual aggregators may compromise availability of the system by refusing
to emit output shares.
1. Input validity proof forging. Any aggregator can collude with a malicious
client to craft a proof share that will fool honest aggregators into accepting
invalid input.

##### Mitigations

1. The linear secret sharing scheme employed by the client ensures that privacy
   is preserved as long as at least one aggregator does not reveal its input
   shares.
1. If computed over a sufficient number of input shares, output shares reveal
   nothing about either the inputs or the participating clients.

#### Leader

The leader is also an aggregator, and so all the assets, capabilities and
mitigations available to aggregators also apply to the leader.

##### Capabilities

1. Input validity proof verification. The leader can forge proofs and collude
   with a malicious client to trick aggregators into aggregating invalid inputs.
     * This capability is no stronger than any aggregator's ability to forge
       validity proof shares in collusion with a malicious client.
1. Relaying messages between aggregators. The leader can compromise availability
   by dropping messages.
     * This capability is no stronger than any aggregator's ability to refuse to
       emit output shares.
1. Shrinking the anonymity set. The leader instructs aggregators to construct
   output parts and so could request aggregations over few inputs.

##### Mitigations

1. Aggregators enforce agreed upon minimum aggregation thresholds to prevent
   deanonymizing.

#### Collector

##### Capabilities

1. Advertising shared configuration parameters (e.g., minimum thresholds for
   aggregations, joint randomness, arithmetic circuits).
1. Collectors may trivially defeat availability by discarding output shares
   submitted by aggregators.

##### Mitigations

1. Aggregators should refuse shared parameters that are trivially insecure
   (i.e., aggregation threshold of 1 contribution).

#### Aggregator collusion

If all aggregators collude (e.g. by promiscuously sharing unencrypted input
shares), then none of the properties of the system hold. Accordingly, such
scenarios are outside of the threat model.

#### Attacker on the network

We assume the existence of attackers on the network links between participants.

##### Capabilities

1. Observation of network traffic. Attackers may observe messages exchanged
   between participants at the IP layer.
     1. The time of transmission of input shares by clients could reveal
        information about user activity.
          * For example, if a user opts into a new feature, and the client
            immediately reports this to aggregators, then just by observing
            network traffic, the attacker can infer what the user did.
     1. Observation of message size could allow the attacker to learn how much
        input is being submitted by a client.
          * For example, if the attacker observes an encrypted message of some
            size, they can infer the size of the plaintext, plus or minus the
            cipher block size. From this they may be able to infer which
            aggregations the user has opted into or out of.
1. Tampering with network traffic. Attackers may drop messages or inject new
   messages into communications between participants.

##### Mitigations

1. All messages exchanged between participants in the system should be
   encrypted.
1. All messages exchanged between aggregators, the collector and the leader
   should be mutually authenticated so that network attackers cannot impersonate
   participants.
1. Clients should be required to submit inputs at regular intervals so that the
   timing of individual messages does not reveal anything.
1. Clients should submit dummy inputs even for aggregations the user has not
   opted into.

[[OPEN ISSUE: The threat model for Prio --- as it's described in the original
paper and [BBG+19] --- considers **either** a malicious client (attacking
soundness) **or** a malicious subset of aggregators (attacking privacy). In
particular, soundness isn't guaranteed if any one of the aggregators is
malicious; in theory it may be possible for a malicious client and aggregator to
collude and break soundness. Is this a contingency we need to address? There are
techniques in [BBG+19] that account for this; we need to figure out if they're
practical.]]

### Future work and possible extensions

In this section we discuss attacks that are not considered in the above threat
model, and suggest mitigations that could be incorporated into implementations
of this protocol or future revisions of this specfication.

#### Client authentication

Attackers can impersonate Prio clients and submit large amounts of false input
in order to spoil aggregations. Deployments could require clients to
authenticate before they may contribute inputs. For example, by requiring
submissions to be signed with a key trusted by aggregators. However some
deployments may opt to accept the risk of false inputs to avoid having to figure
out how to distribute trusted identities to clients.

#### Client attestation

In the current threat model, servers participating in the protocol have no
insight into the activities of clients except that they have uploaded input into
a Prio aggregation, meaning that clients could covertly leak a user's data into
some other channel which compromises privacy. If we introduce the notion of a
trusted computing base which can attest to the properties or activities of a
client, then users and aggregators can be assured that their private data only
goes into Prio. For instance, clients could use the trusted computing base to
attest to software measurements over reproducible builds, or a trusted operating
system could attest to the client's network activity, allowing external
observers to be confident that no data is being exfiltrated.

#### Trusted anonymizing and authenticating proxy

While the input shares transmitted by clients to aggregators reveal nothing
about the original input, the aggregator can still learn auxiliary information
received messages (for instance, source IP or HTTP user agent), which can
identify participating clients or permit some attacks on robustness. This is
worse if client authentication used, since incoming messages would be bound to a
cryptographic identity. Deployments could include a trusted anonymizing proxy,
which would be responsible for receiving input shares from clients, stripping
any identifying information from them (including client authentication) and
forwarding them to aggregators. There should still be a confidential and
authenticated channel from the client to the aggregator to ensure that no actor
besides the aggregator may decrypt the input shares.

#### Multiple protocol runs

Prio is _robust_ against malicious clients, and _private_ against malicious
servers, but cannot provide robustness against malicious servers. Any aggregator
can simply emit bogus output shares and undetectably spoil aggregates. If enough
aggregators were available, this could be mitigated by running the protocol
multiple times with distinct subsets of aggregators chosen so that no aggregator
appears in all subsets and checking all the outputs against each other. If all
the protocol runs do not agree, then participants know that at least one
aggregator is defective, and it may be possible to identify the defector (i.e.,
if a majority of runs agree, and a single aggregator appears in every run that
disagrees). See
[#22](https://github.com/abetterinternet/prio-documents/issues/22) for
discussion.

### Security considerations

#### Infrastructure diversity

Prio deployments should ensure that aggregators do not have common dependencies
that would enable a single vendor to reassemble inputs. For example, if all
participating aggregators stored unencrypted input shares on the same cloud
object storage service, then that cloud vendor would be able to reassemble all
the input shares and defeat privacy.

## System requirements {#operational-requirements}

### Data types

# IANA Considerations

TODO

--- back
