---
title: "Private Data Aggregation Protocol"
docname: draft-pda-protocol-latest
category: std
ipr: trust200902
area: ART
workgroup: HTTPBIS

stand_alone: yes
pi: [toc, sortrefs, symrefs, docmapping]

author:
  -
    ins: S. People
    name: Some People
    org: Somewhere
    email: over@therainbow.net

informative:

  CB17:
    title: "Prio: Private, Robust, and Scalable Computation of Aggregate Statistics"
    date: 2017-03-14
    target: "https://crypto.stanford.edu/prio/paper.pdf"
    author:
      - ins: H. Corrigan-Gibbs
      - ins: D. Boneh

  BBCp19:
    title: "Zero-Knowledge Proofs on Secret-Shared Data via Fully Linear PCPs"
    date: 2021-01-05
    target: "https://eprint.iacr.org/2019/188"
    author:
      -ins: D. Boneh
      -ins: E. Boyle
      -ins: H. Corrigan-Gibbs
      -ins: N. Gilboa
      -ins: Y. Ishai

  BBCp21:
    title: "Lightweight Techniques for Private Heavy Hitters"
    date: 2021-01-05
    target: "https://eprint.iacr.org/2021/017"
    author:
      -ins: D. Boneh
      -ins: E. Boyle
      -ins: H. Corrigan-Gibbs
      -ins: N. Gilboa
      -ins: Y. Ishai

  JD02:
    title: "The Sybil Attack"
    date: 2022-10-10
    target: "https://link.springer.com/chapter/10.1007/3-540-45748-8_24"
    author:
      -ins: J. Douceur

  SV16:
    title: "The Complexity of Differential Privacy"
    date: 2016-08-09
    target: "https://privacytools.seas.harvard.edu/files/privacytools/files/complexityprivacy_1.pdf"
    author:
      -ins: S. Vadhan

normative:

  FIPS180-4:
    title: NIST FIPS 180-4, Secure Hash Standard
    author:
      name: NIST
      ins: National Institute of Standards and Technology, U.S. Department of Commerce
    date: 2012-03
    target: http://csrc.nist.gov/publications/fips/fips180-4/fips-180-4.pdf

--- abstract

TODO: writeme

--- middle

# Introduction

This document describes a framework for specifying protocols for
privacy-preserving data-aggregation. Each protocol is executed by a large set of
clients and a small set of servers. The servers' goal is to compute some
aggregate statistic over the clients' inputs without learning the inputs
themselves. This is made possible by distributing the computation among the
servers in such a way that, as long as at least one of them executes the
protocol honestly, no input is ever seen in the clear by any server.

## DISCLAIMER

This document is a work in progress. We have not yet settled on the design of
the protocol framework or the set of features we intend to support.

## Terminology

This section defines some terminology we will use in the remainder of this
document.

1. Aggregation function: The function computed over the users' inputs.
1. Aggregator: An endpoint that runs the input-validation protocol and
   accumulates input shares.
1. Batch: A set of reports that are aggregated into an output.
1. Batch size: The minimum size of a batch.
1. Batch window: The minimum time difference between the oldest and newest
   report in a batch (in seconds).
1. Client: The endpoint from which a user sends data to be aggregated, e.g., a
   web browser.
1. Collector: The endpoint that receives the output of the aggreagtion function.
   It also specifies the parameters of the protocol.
1. False input: An input that is valid, but incorrect. For example, if the data
   being gathered is whether or not users have clicked on a particular button, a
   client could report clicks when none occurred.
1. Input: The measurement (or measurements) emitted by a client, before any
   encryption or secret sharing scheme is applied.
1. Input share: one of the shares output by feeding an input into a secret
   sharing scheme. Each share is to be transmitted to one of the participating
   aggregators.
1. Input validation protocol: The protocol executed by the client and
   aggregators in order to validate the client's input without leaking its value
   to the aggregators.
1. Invalid input: An input for which the input validation protocol fails. For
   example, if the input is meant to be a  bit vectors, then `[2, 1, 0]` is
   invalid.
1. Measurement: A single value (e.g., a count) being reported by a client.
   Multiple measurements may be grouped into a single protocol input.
1. Leader: A distinguished aggregator that coordinates input validation and data
   collection.
1. Output: A reduction over the inputs, for instance a statistical aggregation,
   which is of interest to a collector. This is the output of the aggregation
   function.
1. Output share: The share of an output emitted by an aggregator. Output shares
   can be reassembled by the leader into the final output.
1. Prio v1: Mozilla's [Origin Telemetry project](https://blog.mozilla.org/security/2019/06/06/next-steps-in-privacy-preserving-telemetry-with-prio/).
1. Prio v2: Contact tracing project by Apple, Google, and ISRG.
1. Proof: A value generated by the client used by the aggregators to verify the
   client's input.
1. Proof share: A share of a proof, used by an aggregator during the
   input-validation protocol.
1. Report: Uploaded to the leader from the client. A report contains the
   secret-shared and encrypted input and proof.
1. Server: An aggregator or collector.

# Overview {#overview}

[OPEN ISSUE: Rework this section in light of issue #44.]

The protocol is executed by a large set of clients and a small set of servers.
We call the servers the *aggregators*. Each client's input to the protocol is a
set of measurements (e.g., counts of some user behavior). Given the input set
of measurements `x_1, ..., x_n` held by `n` users, the goal of a
*private data aggregation (PDA) protocol* is to compute `y = F(x_1, ..., x_n)` for
some aggregation function `F` while revealing nothing else about the
measurements.

## Private aggregation via secret sharing

The main cryptographic tool used for achieving this privacy goal is *additive
secret sharing*. Rather than send its input in the clear, each client splits
its measurements into a sequence of *shares* and sends a share to each of the
aggregators. Additive secret sharing has two important properties:
- It's impossible to deduce the measurement without knowing *all* of the shares.
- It allows the aggregators to compute the final output by first adding up their
  measurements shares locally, then combining the results to obtain the final
  output.

Consider an illustrative example. Suppose there are three clients and two
aggregators. Each client `i` holds a single measurement in the form of a
positive integer `x[i]`, and our goal is to compute the sum of the measurements
of all clients. In this case, the protocol input is a single measurement
consisting of a single positive integer; no additional encoding is done. Given
this input, the first client splits its measurement `x[1]` with additive
secret-sharing into a pair of integers `X[1,1]` and `X[1,2]` for which `x[1]` is
equal to `X[1,1] + X[1,2]` modulo a prime number `p`. (For convenience, we will
omit the mod `p` operator in the rest of this section.) It then uploads `X[1,1]`
to one server `X[1,2]` to the other. The second client splits its measurement
`x[2]` into `X[1,2]` and `X[2,2]`, uploads them to the servers, and so on.

Now the first aggregator is in possession of shares `X[1,1]`, `X[2,1]`, and
`X[3,1]` and the second aggregator is in possession of shares `X[2,1]`,
`X[2,2]`, and `X[2,3]`. Each aggregator computes the sum of its shares; let
`A[1]` denote the first aggregator's share of the sum and let `A[2]` denote the
second aggregator's share of the sum. In the last step, aggregators combine
their sum shares to obtain the final output `y = A[1] + A[2]`. This is correct
because modular addition is commutative. I.e.,

~~~
    y = A[1] + A[2]
      = (x[1,1] + x[2,1] + x[3,1]) + (x[1,2] + x[2,2] + x[3,2])
      = (x[1,1] + x[1,2]) + (x[2,1] + x[2,2]) + (x[3,1] + x[3,2])
      = x[1] + x[2] + x[3]
      = F(x[1], x[2], x[3])
~~~

### Prio {#prio-variant}

This approach can be used to privately compute any function `F` that can be
expressed as a function of the sum of the users' inputs. In Prio {{CB17}}, each
user splits its input into shares and sends each share to one of the
aggregators. The aggregators sum up their input shares. Once all the shares have
been aggregated, they combine their shares of the aggregate to get the final
output.

Not all aggregate functions can be expressed this way efficiently, however. Prio
supports only a limited set of aggregation functions, some of which we highlight
below:

- Simple statistics, like sum, mean, min, max, variance, and standard deviation;
- Histograms with fixed bin sizes (also allows estimation of quantiles, e.g.,
  the median);
- More advanced statistics, like linear regression;
- Bitwise-OR and -AND on bit strings; and
- Computation of data structures, like Bloom filters, counting Bloom filters,
  and count-min sketches, that approximately represent (multi-)sets of strings.

This variety of aggregate types is sufficient to support a wide variety of
data aggregation tasks.

### Hits {#hits-variant}

A common PDA task that can't be solved efficiently with Prio is the
`t`-*heavy-hitters* problem {{BBCp21}}. In this setting, each user is in
possession of a single `n`-bit string, and the goal is to compute the compute
the set of strings that occur at least `t` times. One reason that Prio doesn't
apply to this problem is that the proof generated by the client would be huge.

[TODO: Provide an overview of the protocol of {{BBCp21}} and provide some
intuition about how additive secret sharing is used.]

## Validating inputs in zero knowledge

An essential task of any data collection pipeline is ensuring that the input
data is "valid". Going back to the example above, it's often useful to assert
that each measurement is in a certain range, e.g., `[0, 2^k)` for some `k`.
This straight-forward task is complicated in our setting by the fact that the
inputs are secret shared. In particular, a malicious client can corrupt the
computation by submitting random integers instead of a proper secret sharing of
a valid input.

To solve this problem, in each PDA protocol, the client generates a
zero-knowledge proof of its input's validity that the aggregators use
to verify that their shares correspond to as valid input. The verification
procedure is designed to ensure that the aggregators learn nothing about the
input beyond its validity.

After encoding its measurements as an input to the PDA protocol, the client
generates a *proof* of the input's validity. It then splits the proof into
shares and sends a share of both the proof and input to each aggregator. The
aggregators use their shares of the proof to decide if their input shares
correspond to a valid input.

## Collecting reports

As noted above, each client has a collection of measurements that it
wants to send. Each measurement is characterized by a set of
parameters that are centrally configured and provided to each client:

- A unique identifier (e.g., "dns-queries-mean")
- A description of how to collect the measurement (e.g., "count
  the number of DNS queries")
- The statistic to be computed over the measurement values (e.g., mean)
- The rules for what constitutes a valid value (e.g., must be between 0
  and 10000)

Once the client has collected the measurements to send, it needs to
turn them into a set of reports. Naively, each measurement would be
sent in its own report, but it is also possible to have multiple
measurements in a single report; clients need to be configured with
the mapping from measurements to reports. The set of measurements
that go into a report is referred to as the "input" to the report.
Because each report is independent, for the remainder of this document
we focus on a single report and its inputs.

[NOTE(cjpatton): This paragraph is slightly misleading. If you want to do a
range check for the measurement (this will usually be necessary, IMO) then
you'll need a few extra field elements to encode the input.]
The client uses the statistic to be computed in order to know how to
encode the measurement. For instance, if the statistic is mean, then
the measurement can be encoded directly. However, if the statistic is
standard deviation, then the client must send both `x` and `x^2`. Section
[TODO: cite to internal description of how to encode]
describes how to encode measurements for each statistic.
The client uses the validity rules to construct the zero knowledge
proof showing that the encoded measurement is valid.

## Data flow

Each PDA task consists of two sub-protocols, *upload* and *collect*, which are
executed concurrently. Each sub-protocol consists of a sequence of HTTP requests
made from one entity to another.

~~~~
+-------------+ 1.      +-------------+
|             +--------->             |
|   Client    |         |   Leader    |
|             |    +----+             |
+------+------+    | 2. +------^------+
       | 1.        |           |
       |           |           |
       |           |           | 2.
+------v------+    |    +------+------+
|             <----+    |             |
|   Helper    |         |  Collector  |
|             |         |             |
+-------------+         +-------------+
~~~~
{: #pa-topology title="Who makes requests to whom while executing a PDA task."}

1. **Upload:** Each client assembles its measurements into an input for the
   given PDA protocol. It generates a proof of its input's validity and splits
   the input and proof into two shares, one for the leader and another for a
   helper. The client then encrypts the leader's share and helper's share under,
   respectively, the leader's public key and the helper's public key. (The keys
   are obtained by making requests to the leader and helper.) Finally, the
   client uploads the encrypted shares to the leader.
2. **Collect:** The collector makes one or more requests to the leader in order
   to obtain the final output of the protocol. Before the output can be
   computed, the aggregators (i.e, the leader and helper) need to have verified
   and aggregated a sufficient number of inputs. Depending on the PDA protocol,
   it may be possible for the aggregators to do so immediately when reports are
   uploaded. (See {{prio}}.) However, in general it is necessary for them to
   wait until (a) enough reports have been uploaded and (b) the collector has
   made a request. (See {{hits}}.)

The operational capabilities of each entity are described further in
{{entity-capabilities}}.

# PDA protocols {#pa}

This section specifies a protocol for executing generic PDA tasks. Concrete
PDA protocols are described in {{prio}} and {{hits}}.

Each round of the protocol corresponds to an HTTP request and response. The
content type of each request is "application/octet-stream". We assume that some
transport layer security protocol (e.g., TLS or QUIC) is used between each pair
of parties and that the server is authenticated.

[TODO: Decide how to provide mutual authentication in leader-to-helper and
collector-to-leader connections. One option is to use client certificates for
TLS; another is to have the leader sign its messages directly, as in Prio v2.
For collector-to-leader connections, we may just have this be up to deployment.
(For instance, the collector might authenticate themselves by logging into a
website that has some trust relationship with the leader.)]

**Error handling.**
In this section, we will use the verbs "abort" and "alert with `[some error
message]`" to describe how protocol participants react to various error
conditions. The behavior is specified in {{pa-error}}. For common errors, we may
elide the verbs altogether and refer to {{pa-error-common-aborts}}.

[TODO: Fix the bounds for length-prefixed parameters in protocol messages.
(E.g., `<23..479>` instead of `<1..2^16-1>`.)]

## Configuration {#pa-config}

### Tasks

Each PDA protocol is associated with a *PDA task* that specifies the measurements
that are to be collected. Associated to each task is a set of *PDA Parameters*,
encoded by the following `PDAParam` structure, which specify the protocol used to
verify and aggregate the clients' measurements:

~~~
struct {
  opaque nonce[16];
  Url leader_url;
  Url helper_url;
  HpkeConfig collector_config; // [TODO: Remove this?]
  uint64 batch_size;
  Duration batch_window;
  PDAProto proto;
  uint16 length; // Length of the remainder.
  select (PDAClientParam.proto) {
    case prio: PrioParam;
    case hits: HitsParam;
  }
} PDAParam;

enum { prio(0), hits(1) } PDAProto;

opaque Url<1..2^16-1>;

Duration uint64; /* Number of seconds elapsed between two instants */

Time uint64; /* seconds elapsed since start of UNIX epoch */
~~~

* `nonce`: A unique sequence of bytes used  to ensure that two otherwise
  identical `PDAParam` instances will have distinct `PDATaskID`s. It is
  RECOMMENDED that this be set to a random 16-byte string derived from a
  cryptographically secure pseurandom number generator.
* `leader_url`: The leader's endpoint URL.
* `helper_url`: The helper's endpoint URL.
* `collector_config`: The HPKE configuration of the collector (described in
  {{hpke-config}}). [OPEN ISSUE: Maybe the collector's HPKE config should be
  carried by the collect request?]
* `batch_size`: The batch size, i.e., the minimum number of reports that are
  aggregated into an output.
* `batch_window`: The batch window, i.e., the minimum time difference between
  the oldest and newest report in a batch.
* `proto`: The PDA protocol, e.g., Prio or Hits. The rest of the structure
  contains the protocol specific parameters.

Each task has a unique *task ID* derived from the PDA parameters:

~~~
opaque PDATaskID[32];
~~~

The task ID of a `PDAParam` is derived using the following procedure:

~~~
task_id = SHA-256(param)
~~~

Where `SHA-256` is as specified in [FIPS180-4].

### HPKE key configuration {#hpke-config}

Our protocol uses HPKE for public-key encryption {{!I-D.irtf-cfrg-hpke}}. Each
aggregator specifies the HPKE public key that clients use to encrypt its input
share, and the collector specifies the HPKE public key that helpers use to
encrypt output shares during collection. The public key and associated
parameters are structured as follows:

~~~
struct {
  uint8 id;
  HpkeKemId kem_id;
  HpkeKdfId kdf_id;
  HpkeAeadKdfId aead_id;
  HpkePublicKey public_key;
} HpkeConfig;

opaque HpkePublicKey<1..2^16-1>;
uint16 HpkeAeadId; // Defined in I-D.irtf-cfrg-hpke
uint16 HpkeKemId;  // Defined in I-D.irtf-cfrg-hpke
uint16 HpkeKdfId;  // Defined in I-D.irtf-cfrg-hpke
~~~
[TODO: Decide whether to use the same config structure as OHTTP/ECH. This would
add support for multiple cipher suites.]

We call this a *key configation*. The key configuration is used to set up a
base-mode HPKE context to use to derive symmetric keys for protecting: (1) input
shares sent from the client to an aggregator; or (2) output shares sent from the
helper to the collector. The *config id*, `HpkeConfig.id`, is forwarded by the
sender to the receiver to help the receiver decide if it knows the decryption
key.

## Pre-conditions

We assume the following conditions hold before execution of any PDA task begins:

1. The aggregators agree on a set of PDA tasks, as well as the PDA parameters
   associated to each task.
1. Each aggregator has a clock that is roughly in sync with true time, i.e.,
   within the batch window specified by the PDA parameters. (This is necessary to
   prevent the same report from appearing in multiple batches.)
1. Each client has selected a PDA task for which it will upload a report. It is
   also configured with the task's parameters.
1. Each client and the leader can establish a leader-authenticated secure
   channel.
1. The leader and each helper can establish a helper-authenticated secure
   channel.
1. The collector and leader can establish a leader-authenticated secure channel.
1. The collector has chosen an HPKE configuration and corresponding secret key.
1. Each aggregator has chosen an HPKE configuration and corresponding secret key.

[TODO: It would be clearer to include a "pre-conditions" section prior to each
"phase" of the protocol.]

## Upload {#pa-upload}

~~~~
Client          Leader         Helper
  |  key config  |              |
  <-------------->              |
  |              |  key config  |
  <----------------------------->
  |  upload      |              |
  <-------------->              |
  v              v              v
~~~~
{: #pa-upload-flow title="Flow of the upload process"}

[NOTE: @acmiyaguchi pointed out that the use of an anonymizing proxy for
uploading shares might be easier to implement if the "upload" phase involved a
single HTTP request. However, OHTTP
(https//www.ietf.org/archive/id/draft-thomson-http-oblivious-01.html) allows
clients to make multiple requests through a proxy, so these kinds of use cases
should work.]

### Key Config Request

Before the client can upload its report to the leader, it must first discover
the key configs of each of the aggregators. To do so, the client sends a GET
request to `[aggregator]/key_config`, where `[aggregator]` is the aggregator's
endpoint URL. The aggregator responds to well-formed requests with status 200
and an `HpkeConfig`.

The client issues a key config request to `PDAParam.leader_url` and
`PDAParam.helper_Url`. It aborts if any of the following happen for either
request:

* the client and aggregator failed to establish a secure,
  aggregator-authenticated channel;
* the GET request failed or didn't return a valid key config; or
* the key config specifies a KEM, KDF, or AEAD algorithm the client doesn't
  recognize.

[OPEN ISSUE: @chris-wood: Can't the leader determine if helpers are "online"?
This seems to reveal information that's specific to clients. Imagine, for
example, that clients are prohibited from talking to helpers but not the leader.
Is it OK that leaders learn that about a client? I'm not sure, so I'd be
inclined to remove this unless we have a concrete use case.]

### Upload Request

Next, the client issues a POST request to `[leader]/upload`, where `[leader]` is
the leader's endpoint URL. The payload is structured as follows:

~~~
struct {
  PDATaskID task_id;
  Time time;
  uint64 jitter;
  Extension extensions<4..2^16-1>;
  PDAEncryptedInputShare encrypted_input_shares<1..2^16-1>;
} PDAReport;
~~~

This message is called the client's *report*. It contains the following fields:

* `task_id` is the task ID of the task for which the report is intended.
* `time` is the time at which the report was generated. This field is used by
  the aggregators to ensure the report appears in at most one batch. (See
  {{anti-replay}}.)
* `jitter` is a random number chosen by the client generating the report. This
  and the timestamp field are used by the aggregators to ensure that each report
  appears at most once in a batch. (See {{anti-replay}}.)
* `extensions` is a list of extensions to be included in the Upload flow; see
  {{upload-extensions}}.
* `encrypted_input_shares` contains the encrypted input shares of each of the
  aggregators.

Encrypted input shares are structured as follows:

~~~
struct {
  uint8 config_id;
  opaque enc<1..2^16-1>;
  opaque payload<1..2^16-1>;
} PDAEncryptedInputShare;
~~~

* `config_id` is equal to `HpkeConfig.id`, where `HpkeConfig` is the
  aggregator's key config.
* `enc` is the encapsulated HPKE context, used by the aggregator to decrypt its
  input share.
* `payload` is the encrypted input share.

To generate the report, the client begins by encoding its measurements as an
input for the PDA protocol and splitting it into input shares. (Note that the
structure of each input share depends on the PDA protocol in use, its parameters,
and the role of aggregator, i.e., whether the aggregator is a leader or helper.)
To encrypt an input share, the client first generates an HPKE
{{!I-D.irtf-cfrg-hpke}} context for the aggregator by running

~~~
enc, context = SetupBaseS(pk, "pda input share" || server_role)
~~~

where `pk` is the aggregator's public key and `server_role` is a byte whose
value is `0x01` if the aggregator is the leader and `0x00` if the aggregator is
the helper. `enc` is the encapsulate HPKE context and `context` is the HPKE
context used by the client for encryption. The payload is encrypted as

~~~
payload = context.Seal(input_share, task_id || time || jitter || extensions)
~~~

where `input_share` is the aggregator's input share and `task_id`, `time`, and
`jitter` are the fields of `PDAReport`.

The leader responds to well-formed requests to `[leader]/upload` with status 200
and an empty body. Malformed requests are handled as described in
{{pa-error-common-aborts}}. Clients SHOULD NOT upload the same measurement value
in more than one report if the leader responds with status 200 and an empty body.

### Upload Extensions {#upload-extensions}

Each PAUploadReq carries a list of extensions that clients may use to convey
additional, authenticated information in the report. Each extension is a tag-length
encoded value of the following form:

~~~
  struct {
      ExtensionType extension_type;
      opaque extension_data<0..2^16-1>;
  } Extension;

  enum {
      TBD(0),
      (65535)
  } ExtensionType;
~~~

"extension_type" indicates the type of extension, and "extension_data" contains
information specific to the extension.

## Collect {#pa-collect}

~~~~
Collector     Leader           Helper
  |  collect 1  |                |
  +------------->                |
  |             |  aggregate 1   |
  |             <---------------->
  |             |  ...           |
  |             |  aggregate L   |
  |             <---------------->
  |             |  output share  |
  |             <---------------->
  <-------------+                |
  |  ...        |                |
  |  collect N  |                |
  +------------->                |
  |             |  aggregate 1   |
  |             <---------------->
  |             |  ...           |
  |             |  aggregate L   |
  |             <---------------->
  |             |  output share  |
  |             <---------------->
  <-------------+                |
  v             v                v
~~~~
{: #pa-collect-flow title="Flow of the collect process with N collect requests
and L aggregate requests per collect request."}

[TODO: Decide if and how the collector's request is authenticated.]

The collector interacts with the leader to produce the final aggregate output.
This process consists of a sequence of *collect requests* issued to the leader.
Before a request can succeed, the aggregators must have verified and aggregated
enough reports and the leader must have obtained the helper's encrypted output
share (see {{pa-aggregate}}). In general, the procedure by which the aggregators
verify and aggregate reports depends on parameters carried by the collect
request.

### Collect Request

A collect request is associated with a PDA task. Along with the task ID, the
request includes a time interval that determines the batch of reports to be
aggregated. To make a collect request, the collector issues a POST request to
`[leader]/collect`, where `[leader]` is the leader's endpoint URL. The body of
the request is structured as follows:

~~~
struct {
  PDATaskID task_id;
  Time batch_start;  // The beginning of the batch.
  Time batch_end;    // The end of the batch (exclusive).
  PDAProto proto;    // [TODO: Remove and use PDAParam.proto]
  select (PDACollectReq.proto) {
    case prio: PrioCollectReq;
    case hits: HitsCollectReq;
  }
} PDACollectReq;
~~~

The batch window of the request is the interval `[batch_start, batch_end)`. A
collect request is said to be valid if all of the following conditions hold (let
`PDAParam` denote the parameters for the PDA task):

1. The batch window of the request aligns with the size of the batch window for
   the task, i.e., `batch_start` and `batch_end` are multiples of
   `PDAParam.batch_window`.
1. The batch window of the request is at least the minimum batch window for the
   task, i.e., `batch_end - batch_start >= PDAParam.batch_window`.
1. The batch window of the request does not overlap with the batch window of any
   previous request. [TODO: Enforcing this condition breaks hits, which
   explicitly requires multiple collect requests on the same batch. We'll need
   to fix this.]

The leader responds to valid collect requests by first interacting with the
helper as described in {{pa-aggregate}}. Once it has obtained the helper's
encrypted output share for the batch, it responds to the collector's request
with the following message:

~~~
struct {
  PDATaskID task_id;
  PDAProto proto;
  PDAOutputShare leader_share;
  opaque encrypted_helper_share;
} PDACollectResp;
~~~

[OPEN ISSUE: Describe how intra-protocol errors yield collect errors (see
issue#57). For example, how does a leader respond to a collect request if the
helper drops out?]

### Verifying and Aggregating Reports {#pa-aggregate}

After the client uploads a report to the leader, the leader and helper verify in
zero knowledge that the input is valid. The exact procedure for doing so is
protocol specific, but all protocols have the same basic structure. In
particular, the protocol is comprised of a sequence of *aggregate requests* from
the leader to the helper. At the end of this procedure, the leader and helper
will have aggregated a set of valid client inputs (though not necessarily a
complete batch).

#### Aggregate Request

The process begins with a PDACollectReq. The leader collects a sequence of
reports that are all associated with the same PDA task. Let `[helper]` denote
`PDAParam.helper_url`, where `PDAParam` is the PDA parameters structure associated
`PDAAggregateReq.task.id`. The leader sends a POST request to
`[helper]/aggregate` with the following message:

~~~
struct {
  PDATaskID task_id;
  opaque helper_state<0..2^16>;
  PDAAggregateSubReq seq<1..2^24-1>;
} PDAAggregateReq;
~~~

The structure contains the PDA task, the helper's HPKE config id, an opaque
*helper state* string, and a sequence of *sub-requests*, each corresponding to a
unique client report. Sub-requests are structured as follows:

~~~
struct {
  Time time;                       // Equal to PDAReport.time.
  uint64 jitter;                   // Equal to PDAReport.jitter.
  Extension extensions<4..2^16-1>; // Equal to PDAReport.extensions.
  PDAEncryptedInputShare helper_share;
  select (PDAParam.proto) { // PDAParam for the PDA task
    case prio: PrioAggregateSubReq;
    case hits: HitsAggregateSubReq;
  }
} PDAAggregateSubReq;
~~~

The `time`, `jitter`, and `extensions` fields have the same value as those in the
report uploaded by the client. Similarly, the `helper_share` field is the helper's
encrypted input share as it appeared in the report. [OPEN ISSUE: We usually only
need to send this in the first aggregate request. Shall we exclude it in
subsequent requests somehow?] The remainder of the structure is dedicated to the
protocol-specific request parameters.

In order to provide replay protection, the leader is required to send aggregate
sub-requests in ascending order, where the ordering on sub-requests is
determined by the algorithm defined in {{anti-replay}}. Specifically, the leader
constructs its request such that:
* each sub-request follows the previous sub-request; and
* the first sub-request follows the last sub-request in the previous aggregate
  request.

The helper handles well-formed requests as follows. (As usual, malformed
requests are handled as described in {{pa-error-common-aborts}}.) It first looks
for the PDA parameters `PDAParam` for which `PDAAggregateReq.task_id` is equal
to the task ID derived from `PDAParam`. It then filters out out-of-order
sub-requests by ignoring any sub-request that does not follow the previous one
(See {{anti-replay}}.)

The response consists of the helper's updated state and a sequence of
*sub-responses*, where the i-th sub-response corresponds to the sub-request for
each i. The structure of each sub-response is specific to the PDA protocol:

~~~
struct {
  opaque helper_state<0..2^16>;
  PDAAggregateSubResp seq<1..2^24-1>;
} PDAAggregateResp;

struct {
  Time time;     // Equal to PDAAggregateSubReq.time.
  uint64 jitter; // Equal to PDAAggregateSubReq.jitter.
  select (PDAParam.proto) { // PDAParam for the PDA task
    case prio: PrioAggregateSubResp;
    case hits: HitsAggregateSubResp;
  }
} PDAAggregateSubResp;
~~~

The helper handles each sub-request `PDAAggregateSubReq` as follows. It first
looks up the HPKE config and corresponding secret key associated with
`helper_share.config_id`. If not found, then the sub-response consists of an
"unrecognized config" alert. [TODO: We'll want to be more precise about what
this means. See issue#57.] Next, it attempts to decrypt the payload with the
following procedure:

~~~
context = SetupBaseR(helper_share.enc, sk,
                     "pda input share" || server_role)
input_share = context.Open(helper_share,
                           task_id || time || jitter || extensions)
~~~

where `sk` is the HPKE secret key and `server_role` is the role of the server
(`0x01` for the leader and `0x00` for the helper). If decryption fails, then the
sub-response consists of a "decryption error" alert. [See issue#57.] Otherwise,
the helper handles the request for its plaintext input share `input_share` and
updates its state as specified by the PDA protocol.

After processing all of the sub-requests, the helper encrypts its updated state
and constructs its response to the aggregate request.

##### Leader State

The leader is required to issue aggregate requests in order, but reports are
likely to arrive out-of-order. The leader SHOULD buffer reports for a time
period proportional to the batch window before issuing the first aggregate
request. Failure to do so will result in out-of-order reports being dropped by
the helper.

##### Helper State

The helper state is an optional parameter of an aggregate request that the can
helper use to carry state across requests. At least part of the state will
usually need to be encrypted in order to protect user privacy. However, the
details of precisely how the state is encrypted and the information that it
carries is up to the helper implementation.

#### Output Share Request

Once the aggregators have verified at least as many reports as required for the
PDA task, the leader issues an *output share request* to the helper. The helper
responds to this request by extracting its output share from its state and
encrypting it under the collector's HPKE public key.

The leader sends a POST request to `[helper]/output_share` with the following
message:

~~~
struct {
  PDATaskID task_id;
  Time batch_start; // Same as PDACollectReq.batch_start.
  Time batch_end;   // Same as PDACollectReq.batch_end.
  opaque helper_state<0..2^16>;
} PDAOutputShareReq;
~~~

To respond to valid output share requests, the helper first checks that the
request corresponds to a valid collect request. Next, it extracts from its state
the set of input shares that fall in the window `[batch_start, batch_end)`. If
the size of the batch is less than `task.batch_size`, then it aborts and alerts
the leader with "insufficient data". Otherwise, it computes its output share,
which has the following structure:

~~~
struct {
  PDAProto proto;
  select (PDAOutputShare.proto) {
    case prio: PrioOutputShare;
    case hits: HitsOutputShare;
  }
} PDAOutputShare;
~~~

Next, it encrypts its output share under the collector's HPKE public key:

~~~
enc, context = SetupBaseS(pk, "pda output share")
encrypted_output_share = context.Seal(output_share,
                            task_id || batch_start || batch_end)
~~~

where `pk` is the HPKE public key encoded by the collector's HPKE key
configuration and `output_share` is its serialized output share.

It responds with the following message:

~~~
struct {
  uint8 collector_hpke_config_id;
  opaque enc<1..2^16-1>;
  opaque encrypted_output_share<1..2^16>;
} PDAOutputShareResp;
~~~

The leader uses the helper's output share response to respond to the collector's
collect request (see {{pa-collect}}).

## Error handling {#pa-error}

An *alert* is a message sent either in an HTTP request or response that signals
to the receiver that the peer has aborted the protocol. The payload is

~~~
struct {
  PDATaskID task_id;
  opaque payload<1..255>;
} PDAAlert;
~~~

where `task` is the associated PDA task (this value is always known) and
`payload` is the message. When sent by an aggregator in response to an HTTP
request, the response status is 400. When sent in a request to an aggregator,
the URL is always `[aggregator]/error`, where `[aggregator]` is the URL of the
aggregator endpoint.

## Common abort conditions {#pa-error-common-aborts}

The following specify the "boiler-plate" behavior for various error conditions.

- The message type for the payload of each request and response is unique for a
  given URL. If ever a client, aggregator, or collector receives a request or
  response to a request with a malformed payload, then the receiver aborts and
  alerts the peer with "unrecognized message".

- Each POST request to an aggregator contains a `PDATaskID`. If the aggregator
  does not recognize the task, i.e., it can't find a `PDAParam` for which the
  derived task ID matches the `PDATaskID`, then it aborts and alerts the peer
  with "unrecognized task".

# Prio {#prio}

[TODO: Define Prio-specific protocol messages.]

## Parameters

### Finite field arithmetic

The algorithms that comprise the input-validation protocol --- Prove, Query, and
Decide --- are constructed by generating and evaluating polynomials over a
finite field. As such, the main ingredient of Prio is an implementation of
arithmetic in a finite field suitable for the given application.

We will use a prime field. The choice of prime is influenced by the following
criteria:

1. **Field size.** How big the field needs to be depends on the type of data
   being aggregated and how many users there are. The field size also impacts
   the security level: the longer the validity circuit, the larger the field
   needs to be in order to effectively detect malicious clients. Typically the
   soundness error (i.e., the probability of an invalid input being deemed valid
   by the aggregators) will be 2n/(p-n), where n is the size of the input and p
   is the prime modulus.
1. **Fast polynomial operations.** In order to make Prio practical, it's
   important that implementations employ FFT to speed up polynomial operations.
   In particular, the prime modulus p should be chosen so that (p-1) = 2^b * s
   for large b and odd s. Then g^s is a principle, 2^b-th root of unity (i.e.,
   g^(s\*2^b) = 1), where g is the generator of the multiplicative subgroup.
   This fact allows us to quickly evaluate and interpolate polynomials at 2^a-th
   roots of unity for any 1 <= a <= b. Note that b imposes an upper bound on the
   size of proofs, so it should be large enough to accommodate all foreseeable
   use cases. Something like b >= 20 is probably good enough.
1. **As close to a power of two as possible.** We use rejection sampling to map
   a PRNG seed to a pseudorandom sequence of field elements (see {{prio-prng}).
   In order to minimize the probability of a simple being rejected, the modulus
   should be as close to a power of 2 as possible.
1. **Code optimization.** [[TODO: What properties of the field make
   it possible to write faster implementations?]]

The table below lists parameters that meet these criteria at various levels of
security. (Note that \#1 is the field used in "Prio v2".) The "size" column
indicates the number of bits required to represent elements of the field.

| # | size | p                                      | g  | b   | s                |
|---|------|----------------------------------------|----|-----|------------------|
| 1 | 32   | 4293918721                             | 19 | 20  | 3^2 * 5 * 7 * 13 |
| 2 | 64   | 15564440312192434177                   | 5  | 59  | 3^3              |
| 3 | 80   | 779190469673491460259841               | 14 | 72  | 3 * 5 * 11       |
| 4 | 123  | 9304595970494411110326649421962412033  | 3  | 120 | 7                |
| 5 | 126  | 74769074762901517850839147140769382401 | 7  | 118 | 3^2 * 5^2        |

[TODO: Choose new parameters for 2, 3, and 5 so that p is as close to 2^size as
possible without going over. (4 is already close enough; 1 is already deployed
and can't be changed.]

**Finding suitable primes.**
One way to find suitable primes is to first choose choose b, then "probe" to
find a prime of the desired size. The following SageMath script prints the
parameters of a number of (probable) primes larger than 2^b for a given b:

~~~
b = 116
for s in range(0,1000,1):
    B = 2^b
    p = (B*s).next_prime()
    if p-(B*s) == 1:
        bits = round(math.log2(p), 2)
        print(bits, p, GF(p).multiplicative_generator(), b, factor(s))
~~~

### Pseudorandom number generation {#prio-prng}

A suitable PRNG will have the following syntax. Fix a finite field K:

1. x := PRNG(k, n) denotes generation of a vector of n elements of K.

This can be instantiated using a standard stream cipher, e.g., AES-CTR, as
follows. Interpret the seed k as the key and IV for generating the AES-CTR key
stream. Proceed by rejection sampling, as follows. Let m be the number of bits
needed to encode an element of K. Generate the next m bits of key stream and
interpret the bytes as an integer x, clearing the most significant m - l bits,
where l is the bit-length of the modulus p. If x < p, then output x. Otherwise,
generate the next m bits of key stream and try again. Repeat this process
indefinitely until a suitable output is found.

# Hits {#hits}

[TODO: Define Hits-specific protocol messages.]

# Operational Considerations {#operational-capabilities}

PDA protocols have inherent constraints derived from the tradeoff between privacy
guarantees and computational complexity. These tradeoffs influence how
applications may choose to utilize services implementing the specification.

## Protocol participant capabilities {#entity-capabilities}

The design in this document has different assumptions and requirements for
different protocol participants, including clients, aggregators, and
collectors. This section describes these capabilities in more detail.

### Client capabilities

Clients have limited capabilities and requirements. Their only inputs to the protocol
are (1) the PDAParam structure configured out of band and (2) a measurement. Clients
are not expected to store any state across any upload
flows, nor are they required to implement any sort of report upload retry mechanism.
By design, the protocol in this document is robust against individual client upload
failures since the protocol output is an aggregate over all inputs.

### Aggregator capabilities

Helpers and leaders have different operational requirements. The design in this
document assumes an operationally competent leader, i.e., one that has no storage
or computation limitations or constraints, but only a modestly provisioned helper, i.e., one that
has computation, bandwidth, and storage constraints. By design, leaders must be
at least as capable as helpers, where helpers are generally required to:

- Support the collect protocol, which includes verifying and aggregating
  sets of reports in a given batch; and
- Publish and manage an HPKE configuration that can be used for the upload protocol.

In addition, for each PDAParam instance, helpers are required to:

- Implement some form of batch-to-report index, as well as inter- and intra-batch
  replay mitigation storage, which includes some way of tracking batch report size
  with optional support for state offloading. Some of this state may be used for
  replay attack mitigation. The replay mitigation strategy is described in {{anti-replay}}.

Beyond the minimal capabilities required of helpers, leaders are generally required to:

- Support the upload protocol and store client reports for a given PDAParam instance,
  where each report maps uniquely to a single batch, and index this storage by batch durations;
- Track batch report size during each collect flow and request encrypted output shares
  from helpers.

In addition, for each PDAParam instance, leaders are required to:

- Implement and store state for the form of inter- and intra-batch replay mitigation in {{anti-replay}}; and
- Store helper state associated with a given PDAParam batch.

### Collector capabilities

Collectors statefully interact with aggregators to produce an aggregate output. Their
input to the protocol is the PDAParam structure, configured out of band, which contains
the corresponding batch window and size. For each collect invocation, collectors are
required to keep state from the start of the protocol to the end as needed to produce
the final aggregate output.

Collectors must also maintain state for the lifetime of each PDAParam value, which includes
key material associated with the HPKE key configuration.

## Data resolution limitations

Privacy comes at the cost of computational complexity. While affine-aggregatable
encodings (AFEs) can compute many useful statistics, they require more bandwidth
and CPU cycles to account for finite-field arithmetic during input-validation.
The increased work from verifying inputs decreases the throughput of the system
or the inputs processed per unit time. Throughput is related to the verification
circuit's complexity and the available compute-time to each aggregator.

Applications that utilize proofs with a large number of multiplication gates or
a high frequency of inputs may need to limit inputs into the system to meet
bandwidth or compute constraints. Some methods of overcoming these limitations
include choosing a better representation for the data or introducing sampling
into the data collection methodology.

[[TODO: Discuss explicit key performance indicators, here or elsewhere.]]

## Aggregation utility and soft batch deadlines

A soft real-time system should produce a response within a deadline to
be useful. This constraint may be relevant when the value of an aggregate
decreases over time. A missed deadline can reduce an aggregate's utility
but not necessarily cause failure in the system.

An example of a soft real-time constraint is the expectation that input data can
be verified and aggregated in a period equal to data collection, given some
computational budget. Meeting these deadlines will require efficient
implementations of the input-validation protocol. Applications might batch
requests or utilize more efficient serialization to improve throughput.

Some applications may be constrained by the time that it takes to reach a
privacy threshold defined by a minimum number of input shares. One possible
solution is to increase the reporting period so more samples can be collected,
balanced against the urgency of responding to a soft deadline.

# Security Considerations {#sec-considerations}

Prio assumes a powerful adversary with the ability to compromise an unbounded
number of clients. In doing so, the adversary can provide malicious (yet
truthful) inputs to the aggregation function. Prio also assumes that all but one
server operates honestly, where a dishonest server does not execute the protocol
faithfully as specified. The system also assumes that servers communicate over
secure and mutually authenticated channels. In practice, this can be done by TLS
or some other form of application-layer authentication.

In the presence of this adversary, Prio provides two important properties for
computing an aggregation function F:

1. Privacy. The aggregators and collector learn only the output of F computed
   over all client inputs, and nothing else.
1. Robustness. As long as the aggregators execute the input-validation protocol
   correctly, a malicious client can skew the output of F only by reporting
   false (untruthful) input. The output cannot be influenced in any other way.

There are several additional constraints that a Prio deployment must satisfy in
order to achieve these goals:

1. Minimum batch size. The aggregation batch size has an obvious impact on
   privacy. (A batch size of one hides nothing of the input.)
2. Aggregation function choice. Some aggregation functions leak slightly more
   than the function output itself.

[TODO: discuss these in more detail.]

## Threat model

In this section, we enumerate the actors participating in the Prio system and
enumerate their assets (secrets that are either inherently valuable or which
confer some capability that enables further attack on the system), the
capabilities that a malicious or compromised actor has, and potential
mitigations for attacks enabled by those capabilities.

This model assumes that all participants have previously agreed upon and
exchanged all shared parameters over some unspecified secure channel.

### Client/user

#### Assets

1. Unshared inputs. Clients are the only actor that can ever see the original
   inputs.
1. Unencrypted input shares.

#### Capabilities

1. Individual users can reveal their own input and compromise their own privacy.
1. Clients (that is, software which might be used by many users of the system)
can defeat privacy by leaking input outside of the Prio system.
1. Clients may affect the quality of aggregations by reporting false input.
     * Prio can only prove that submitted input is valid, not that it is true.
       False input can be mitigated orthogonally to the Prio protocol (e.g., by
       requiring that aggregations include a minimum number of contributions)
       and so these attacks are considered to be outside of the threat model.
1. Clients can send invalid encodings of input.

#### Mitigations

1. The input validation protocol executed by the aggregators prevents either
individual clients or coalitions of clients from compromising the robustness
property.
1. If aggregator output satisifes differential privacy {{dp}}, then all records
not leaked by malicious clients are still protected.

### Aggregator

#### Assets

1. Unencrypted input shares.
1. Input share decryption keys.
1. Client identifying information.
1. Output shares.
1. Aggregator identity.

#### Capabilities

1. Aggregators may defeat the robustness of the system by emitting bogus output
   shares.
1. If clients reveal identifying information to aggregators (such as a trusted
   identity during client authentication), aggregators can learn which clients
   are contributing input.
     1. Aggregators may reveal that a particular client contributed input.
     1. Aggregators may attack robustness by selectively omitting inputs from
        certain clients.
          * For example, omitting submissions from a particular geographic
            region to falsely suggest that a particular localization is not
            being used.
1. Individual aggregators may compromise availability of the system by refusing
to emit output shares.
1. Input validity proof forging. Any aggregator can collude with a malicious
client to craft a proof share that will fool honest aggregators into accepting
invalid input.

#### Mitigations

1. The linear secret sharing scheme employed by the client ensures that privacy
   is preserved as long as at least one aggregator does not reveal its input
   shares.
1. If computed over a sufficient number of input shares, output shares reveal
   nothing about either the inputs or the participating clients.

### Leader

The leader is also an aggregator, and so all the assets, capabilities and
mitigations available to aggregators also apply to the leader.

#### Capabilities

1. Input validity proof verification. The leader can forge proofs and collude
   with a malicious client to trick aggregators into aggregating invalid inputs.
     * This capability is no stronger than any aggregator's ability to forge
       validity proof shares in collusion with a malicious client.
1. Relaying messages between aggregators. The leader can compromise availability
   by dropping messages.
     * This capability is no stronger than any aggregator's ability to refuse to
       emit output shares.
1. Shrinking the anonymity set. The leader instructs aggregators to construct
   output parts and so could request aggregations over few inputs.

#### Mitigations

1. Aggregators enforce agreed upon minimum aggregation thresholds to prevent
   deanonymizing.
1. If aggregator output satisifes differential privacy {{dp}}, then genuine
   records are protected regardless of the size of the anonymity set.

### Collector

#### Capabilities

1. Advertising shared configuration parameters (e.g., minimum thresholds for
   aggregations, joint randomness, arithmetic circuits).
1. Collectors may trivially defeat availability by discarding output shares
   submitted by aggregators.
1. Known input injection. Collectors may collude with clients to send known
   input to the aggregators, allowing collectors to shrink the effective
   anonymity set by subtracting the known inputs from the final output.
   Sybil attacks {{JD02}} could be used to amplify this capability.

#### Mitigations

1. Aggregators should refuse shared parameters that are trivially insecure
   (i.e., aggregation threshold of 1 contribution).
1. If aggregator output satisifes differential privacy {{dp}}, then genuine
   records are protected regardless of the size of the anonymity set.

### Aggregator collusion

If all aggregators collude (e.g. by promiscuously sharing unencrypted input
shares), then none of the properties of the system hold. Accordingly, such
scenarios are outside of the threat model.

### Attacker on the network

We assume the existence of attackers on the network links between participants.

#### Capabilities

1. Observation of network traffic. Attackers may observe messages exchanged
   between participants at the IP layer.
     1. The time of transmission of input shares by clients could reveal
        information about user activity.
          * For example, if a user opts into a new feature, and the client
            immediately reports this to aggregators, then just by observing
            network traffic, the attacker can infer what the user did.
     1. Observation of message size could allow the attacker to learn how much
        input is being submitted by a client.
          * For example, if the attacker observes an encrypted message of some
            size, they can infer the size of the plaintext, plus or minus the
            cipher block size. From this they may be able to infer which
            aggregations the user has opted into or out of.
1. Tampering with network traffic. Attackers may drop messages or inject new
   messages into communications between participants.

#### Mitigations

1. All messages exchanged between participants in the system should be
   encrypted.
1. All messages exchanged between aggregators, the collector and the leader
   should be mutually authenticated so that network attackers cannot impersonate
   participants.
1. Clients should be required to submit inputs at regular intervals so that the
   timing of individual messages does not reveal anything.
1. Clients should submit dummy inputs even for aggregations the user has not
   opted into.

[[OPEN ISSUE: The threat model for Prio --- as it's described in the original
paper and [BBG+19] --- considers **either** a malicious client (attacking
soundness) **or** a malicious subset of aggregators (attacking privacy). In
particular, soundness isn't guaranteed if any one of the aggregators is
malicious; in theory it may be possible for a malicious client and aggregator to
collude and break soundness. Is this a contingency we need to address? There are
techniques in [BBG+19] that account for this; we need to figure out if they're
practical.]]

## Client authentication or attestation

[TODO: Solve issue#89]

## Anonymizing proxies {#anon-proxy}

Client reports can contain auxiliary information such as source IP, HTTP user
agent or in deployments which use it, client authentication information, which
could be used by aggregators to identify participating clients or permit some
attacks on robustness. This auxiliary information could be removed by having
clients submit reports to an anonymizing proxy server which would then use
Oblivous HTTP {{!I-D.thomson-http-oblivious}} to forward inputs to the PDA
leader, without requiring any server participating in PDA to be aware of
whatever client authentication or attestation scheme is in use.

## Batch sizes and privacy

An important parameter of a PDA deployment is the minimum batch size. If an
aggregation includes too few inputs, then the outputs can reveal information
about individual participants. Aggregators use the batch size field of the
`PDAParams` structure to enforce minimum batch size during the collect protocol,
but server implementations may also opt out of participating in a PDA task if
the minimum batch size is too small. This document does not specify how to
choose minimum batch sizes.

## Differential privacy {#dp}

Optionally, PDA deployments can choose to ensure their output F achieves
differential privacy {{SV16}}. A simple approach would require the aggregators
to add two-sided noise (e.g. sampled from a two-sided geometric distribution)
to outputs. Since each aggregator is adding noise independently, privacy can be
guaranteed even if all but one of the aggregators is malicious. Differential
privacy is a strong privacy definition, and protects users in extreme
circumstances: Even if an adversary has prior knowledge of every input in a
batch except for one, that one record is still formally protected.

[OPEN ISSUE: While parameters configuring the differential privacy noise (like
specific distributions / variance) can be agreed upon out of band by the
aggregators and collector, there may be benefits to adding explicit protocol
support by encoding them into `PDAParams`.]

## Multiple protocol runs

Prio is _robust_ against malicious clients, and _private_ against malicious
servers, but cannot provide robustness against malicious servers. Any aggregator
can simply emit bogus output shares and undetectably spoil aggregates. If enough
aggregators were available, this could be mitigated by running the protocol
multiple times with distinct subsets of aggregators chosen so that no aggregator
appears in all subsets and checking all the outputs against each other. If all
the protocol runs do not agree, then participants know that at least one
aggregator is defective, and it may be possible to identify the defector (i.e.,
if a majority of runs agree, and a single aggregator appears in every run that
disagrees). See
[#22](https://github.com/abetterinternet/prio-documents/issues/22) for
discussion.

## Infrastructure diversity

Prio deployments should ensure that aggregators do not have common dependencies
that would enable a single vendor to reassemble inputs. For example, if all
participating aggregators stored unencrypted input shares on the same cloud
object storage service, then that cloud vendor would be able to reassemble all
the input shares and defeat privacy.

#### Anti-replay {#anti-replay}

Using a report in multiple batches, or multiple times within a single batch, is
considered a privacy violation, since it may leak more information about that
report than intended by the PDA protocol. For example, this may violate
differential privacy. To mitigate this issue, the core specification imposes an
ordering on reports so that aggregators can cheaply prevent replays attacks.

Aggregate requests are ordered as follows: We say that a report `R2` *follows*
report `R1` if either `R2.time > R1.time` or `R2.time == R1.time` and `R2.jitter
> R1.jitter`. If `R2.time < R1.time`, or `R2.time == R1.time` but `R2.jitter <=
R1.jitter`, then we say that `R2` *does not follow* `R1`.

To prevent replay attacks, each aggregator ensures that each report it consumes
follows the previous one it consumed. To prevent the adversary from tampering
with the ordering of reports, honest clients incorporate the ordering-sensitive
parameters `(time, jitter)` into the AAD for HPKE encryption. Note that this
strategy may result in dropping reports that happen to have the same timestamp
and jitter value.

Aggregators prevent the same report from being used in multiple batches by only
consuming valid collect requests, as described in {{pa-collect}}.

## System requirements {#operational-requirements}

### Data types

# IANA Considerations

## Upload Extension Registry

This document requests creation of a new registry for extensions to the Upload
protocol. This registry should contain the following columns:

[TODO: define how we want to structure this registry when the time comes]

--- back
